{"id": "2506.21669", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21669", "abs": "https://arxiv.org/abs/2506.21669", "authors": ["Wanxin Tian", "Shijie Zhang", "Kevin Zhang", "Xiaowei Chi", "Yulin Luo", "Junyu Lu", "Chunkai Fan", "Qiang Zhou", "Yiming Zhao", "Ning Liu Siyu Lin", "Zhiyuan Qin", "Xiaozhu Ju", "Shanghang Zhang", "Jian Tang"], "title": "SEEA-R1: Tree-Structured Reinforcement Fine-Tuning for Self-Evolving Embodied Agents", "comment": null, "summary": "Self-evolution, the ability of agents to autonomously improve their reasoning\nand behavior, is essential for the embodied domain with long-horizon,\nreal-world tasks. Despite current advancements in reinforcement fine-tuning\n(RFT) showing strong performance in enhancing reasoning in LLMs, its potential\nto enable self-evolving embodied intelligence with multi-modal interactions\nremains largely unexplored. Specifically, reinforcement fine-tuning faces two\nfundamental obstacles in embodied settings: (i) the lack of accessible\nintermediate rewards in multi-step reasoning tasks limits effective learning\nsignals, and (ii) reliance on hand-crafted reward functions restricts\ngeneralization to novel tasks and environments. To address these challenges, we\npresent Self-Evolving Embodied Agents-R1, SEEA-R1, the first RFT framework\ndesigned for enabling the self-evolving capabilities of embodied agents.\nSpecifically, to convert sparse delayed rewards into denser intermediate\nsignals that improve multi-step reasoning, we propose Tree-based group relative\npolicy optimization (Tree-GRPO), which integrates Monte Carlo Tree Search into\nGRPO. To generalize reward estimation across tasks and scenes, supporting\nautonomous adaptation and reward-driven self-evolution, we further introduce\nMulti-modal Generative Reward Model (MGRM). To holistically evaluate the\neffectiveness of SEEA-R1, we evaluate on the ALFWorld benchmark, surpassing\nstate-of-the-art methods with scores of 85.07% (textual) and 36.19%\n(multi-modal), outperforming prior models including GPT-4o. SEEA-R1 also\nachieves scores of 80.3% without environmental reward, surpassing all\nopen-source baselines and highlighting its scalability as a self-evolving\nembodied agent. Additional experiments and qualitative analysis further support\nthe potential of SEEA-R1 for future research in scalable embodied intelligence.", "AI": {"tldr": "SEEA-R1\u662f\u4e00\u4e2a\u57fa\u4e8e\u5f3a\u5316\u5fae\u8c03\u7684\u6846\u67b6\uff0c\u65e8\u5728\u63d0\u5347\u5177\u8eab\u667a\u80fd\u4f53\u7684\u81ea\u6211\u8fdb\u5316\u80fd\u529b\uff0c\u901a\u8fc7Tree-GRPO\u548cMGRM\u89e3\u51b3\u591a\u6b65\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u5956\u52b1\u7a00\u758f\u6027\u548c\u6cdb\u5316\u95ee\u9898\u3002", "motivation": "\u5f53\u524d\u5f3a\u5316\u5fae\u8c03\u5728\u5177\u8eab\u667a\u80fd\u9886\u57df\u7684\u6f5c\u529b\u5c1a\u672a\u5145\u5206\u6316\u6398\uff0c\u5c24\u5176\u662f\u5728\u591a\u6a21\u6001\u4ea4\u4e92\u548c\u957f\u65f6\u4efb\u52a1\u4e2d\u3002", "method": "\u63d0\u51fa\u4e86Tree-GRPO\uff08\u57fa\u4e8e\u8499\u7279\u5361\u6d1b\u6811\u641c\u7d22\u7684\u7ec4\u76f8\u5bf9\u7b56\u7565\u4f18\u5316\uff09\u548cMGRM\uff08\u591a\u6a21\u6001\u751f\u6210\u5956\u52b1\u6a21\u578b\uff09\uff0c\u4ee5\u89e3\u51b3\u5956\u52b1\u7a00\u758f\u6027\u548c\u4efb\u52a1\u6cdb\u5316\u95ee\u9898\u3002", "result": "\u5728ALFWorld\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0cSEEA-R1\u5728\u6587\u672c\u548c\u591a\u6a21\u6001\u4efb\u52a1\u4e2d\u5206\u522b\u8fbe\u523085.07%\u548c36.19%\u7684\u5f97\u5206\uff0c\u4f18\u4e8eGPT-4o\u7b49\u6a21\u578b\u3002", "conclusion": "SEEA-R1\u5c55\u793a\u4e86\u5728\u53ef\u6269\u5c55\u5177\u8eab\u667a\u80fd\u7814\u7a76\u4e2d\u7684\u6f5c\u529b\uff0c\u4e3a\u672a\u6765\u7814\u7a76\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\u3002"}}
{"id": "2506.21734", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21734", "abs": "https://arxiv.org/abs/2506.21734", "authors": ["Guan Wang", "Jin Li", "Yuhao Sun", "Xing Chen", "Changling Liu", "Yue Wu", "Meng Lu", "Sen Song", "Yasin Abbasi Yadkori"], "title": "Hierarchical Reasoning Model", "comment": null, "summary": "Reasoning, the process of devising and executing complex goal-oriented action\nsequences, remains a critical challenge in AI. Current large language models\n(LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from\nbrittle task decomposition, extensive data requirements, and high latency.\nInspired by the hierarchical and multi-timescale processing in the human brain,\nwe propose the Hierarchical Reasoning Model (HRM), a novel recurrent\narchitecture that attains significant computational depth while maintaining\nboth training stability and efficiency. HRM executes sequential reasoning tasks\nin a single forward pass without explicit supervision of the intermediate\nprocess, through two interdependent recurrent modules: a high-level module\nresponsible for slow, abstract planning, and a low-level module handling rapid,\ndetailed computations. With only 27 million parameters, HRM achieves\nexceptional performance on complex reasoning tasks using only 1000 training\nsamples. The model operates without pre-training or CoT data, yet achieves\nnearly perfect performance on challenging tasks including complex Sudoku\npuzzles and optimal path finding in large mazes. Furthermore, HRM outperforms\nmuch larger models with significantly longer context windows on the Abstraction\nand Reasoning Corpus (ARC), a key benchmark for measuring artificial general\nintelligence capabilities. These results underscore HRM's potential as a\ntransformative advancement toward universal computation and general-purpose\nreasoning systems.", "AI": {"tldr": "HRM\u662f\u4e00\u79cd\u65b0\u578b\u5faa\u73af\u67b6\u6784\uff0c\u901a\u8fc7\u5206\u5c42\u5904\u7406\u5b9e\u73b0\u9ad8\u6548\u63a8\u7406\uff0c\u65e0\u9700\u9884\u8bad\u7ec3\u6216\u5927\u91cf\u6570\u636e\uff0c\u6027\u80fd\u4f18\u4e8e\u73b0\u6709\u5927\u6a21\u578b\u3002", "motivation": "\u89e3\u51b3\u5f53\u524d\u5927\u8bed\u8a00\u6a21\u578b\u5728\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u8106\u5f31\u6027\u3001\u9ad8\u6570\u636e\u9700\u6c42\u548c\u5ef6\u8fdf\u95ee\u9898\u3002", "method": "\u63d0\u51faHRM\uff0c\u5305\u542b\u9ad8\u4f4e\u7ea7\u5faa\u73af\u6a21\u5757\uff0c\u5206\u522b\u8d1f\u8d23\u62bd\u8c61\u89c4\u5212\u548c\u7ec6\u8282\u8ba1\u7b97\uff0c\u5355\u6b21\u524d\u5411\u4f20\u64ad\u5b8c\u6210\u63a8\u7406\u3002", "result": "\u4ec5\u752827M\u53c2\u6570\u548c1000\u6837\u672c\uff0c\u5728\u590d\u6742\u63a8\u7406\u4efb\u52a1\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u3002", "conclusion": "HRM\u662f\u901a\u7528\u8ba1\u7b97\u548c\u63a8\u7406\u7cfb\u7edf\u7684\u6f5c\u5728\u7a81\u7834\u6027\u8fdb\u5c55\u3002"}}
{"id": "2506.21763", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21763", "abs": "https://arxiv.org/abs/2506.21763", "authors": ["Xin Wang", "Jiyao Liu", "Yulong Xiao", "Junzhi Ning", "Lihao Liu", "Junjun He", "Botian Shi", "Kaicheng Yu"], "title": "THE-Tree: Can Tracing Historical Evolution Enhance Scientific Verification and Reasoning?", "comment": null, "summary": "Large Language Models (LLMs) are accelerating scientific idea generation, but\nrigorously evaluating these numerous, often superficial, AI-generated\npropositions for novelty and factual accuracy is a critical bottleneck; manual\nverification is too slow.Existing validation methods are inadequate: LLMs as\nstandalone verifiers may hallucinate and lack domain knowledge (our findings\nshow ~60\\% unawareness of relevant papers in specific domains), while\ntraditional citation networks lack explicit causality and narrative surveys are\nunstructured.This underscores a core challenge: the absence of structured,\nverifiable, and causally-linked historical data of scientific evolution.To\naddress this,we introduce \\textbf{THE-Tree} (\\textbf{T}echnology\n\\textbf{H}istory \\textbf{E}volution Tree), a computational framework that\nconstructs such domain-specific evolution trees from scientific\nliterature.THE-Tree employs a search algorithm to explore evolutionary paths.\nDuring its node expansion, it utilizes a novel \"Think-Verbalize-Cite-Verify\"\nprocess: an LLM proposes potential advancements and cites supporting\nliterature. Critically, each proposed evolutionary link is then validated for\nlogical coherence and evidential support by a recovered natural language\ninference mechanism that interrogates the cited literature, ensuring that each\nstep is grounded.We construct and validate 88 THE-Trees across diverse domains\nand release a benchmark dataset including up to 71k fact verifications covering\n27k papers to foster further research.Experiments demonstrate that i) in graph\ncompletion, our THE-Tree improves hit@1 by 8\\% to 14\\% across multiple models\ncompared to traditional citation networks; ii) for predicting future scientific\ndevelopments, it improves hit@1 metric by nearly 10\\%; and iii) when combined\nwith other methods, it boosts the performance of evaluating important\nscientific papers by almost 100\\%.", "AI": {"tldr": "THE-Tree\u6846\u67b6\u901a\u8fc7\u7ed3\u6784\u5316\u79d1\u5b66\u6587\u732e\u6784\u5efa\u6280\u672f\u6f14\u5316\u6811\uff0c\u63d0\u5347\u79d1\u5b66\u53d1\u5c55\u7684\u9a8c\u8bc1\u548c\u9884\u6d4b\u80fd\u529b\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u9a8c\u8bc1\u65b9\u6cd5\uff08\u5982LLM\u548c\u4f20\u7edf\u5f15\u7528\u7f51\u7edc\uff09\u5728\u79d1\u5b66\u547d\u9898\u9a8c\u8bc1\u4e2d\u7684\u4e0d\u8db3\uff0c\u5982\u5e7b\u89c9\u3001\u9886\u57df\u77e5\u8bc6\u7f3a\u4e4f\u548c\u975e\u7ed3\u6784\u5316\u95ee\u9898\u3002", "method": "\u63d0\u51faTHE-Tree\u6846\u67b6\uff0c\u7ed3\u5408LLM\u7684\u201cThink-Verbalize-Cite-Verify\u201d\u6d41\u7a0b\u548c\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u673a\u5236\uff0c\u9a8c\u8bc1\u6f14\u5316\u94fe\u63a5\u7684\u903b\u8f91\u6027\u548c\u8bc1\u636e\u652f\u6301\u3002", "result": "\u572888\u4e2a\u9886\u57df\u7684\u5b9e\u9a8c\u4e2d\uff0cTHE-Tree\u663e\u8457\u63d0\u5347\u4e86\u56fe\u5b8c\u6210\u548c\u79d1\u5b66\u53d1\u5c55\u7684\u9884\u6d4b\u6027\u80fd\uff0c\u5e76\u589e\u5f3a\u4e86\u91cd\u8981\u8bba\u6587\u7684\u8bc4\u4f30\u6548\u679c\u3002", "conclusion": "THE-Tree\u4e3a\u79d1\u5b66\u6f14\u5316\u63d0\u4f9b\u4e86\u7ed3\u6784\u5316\u3001\u53ef\u9a8c\u8bc1\u7684\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86\u9a8c\u8bc1\u548c\u9884\u6d4b\u80fd\u529b\u3002"}}
{"id": "2506.21784", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21784", "abs": "https://arxiv.org/abs/2506.21784", "authors": ["Yifan Liu", "Xishun Liao", "Haoxuan Ma", "Jonathan Liu", "Rohan Jadhav", "Jiaqi Ma"], "title": "MobiVerse: Scaling Urban Mobility Simulation with Hybrid Lightweight Domain-Specific Generator and Large Language Models", "comment": null, "summary": "Understanding and modeling human mobility patterns is crucial for effective\ntransportation planning and urban development. Despite significant advances in\nmobility research, there remains a critical gap in simulation platforms that\nallow for algorithm development, policy implementation, and comprehensive\nevaluation at scale. Traditional activity-based models require extensive data\ncollection and manual calibration, machine learning approaches struggle with\nadaptation to dynamic conditions, and treding agent-based Large Language Models\n(LLMs) implementations face computational constraints with large-scale\nsimulations. To address these challenges, we propose MobiVerse, a hybrid\nframework leverages the efficiency of lightweight domain-specific generator for\ngenerating base activity chains with the adaptability of LLMs for context-aware\nmodifications. A case study was conducted in Westwood, Los Angeles, where we\nefficiently generated and dynamically adjusted schedules for the whole\npopulation of approximately 53,000 agents on a standard PC. Our experiments\ndemonstrate that MobiVerse successfully enables agents to respond to\nenvironmental feedback, including road closures, large gathering events like\nfootball games, and congestion, through our hybrid framework. Its modular\ndesign facilitates testing various mobility algorithms at both transportation\nsystem and agent levels. Results show our approach maintains computational\nefficiency while enhancing behavioral realism. MobiVerse bridges the gap in\nmobility simulation by providing a customizable platform for mobility systems\nplanning and operations with benchmark algorithms. Code and videos are\navailable at https://github.com/ucla-mobility/MobiVerse.", "AI": {"tldr": "MobiVerse\u662f\u4e00\u4e2a\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u9886\u57df\u7279\u5b9a\u751f\u6210\u5668\u548cLLMs\uff0c\u7528\u4e8e\u9ad8\u6548\u751f\u6210\u548c\u52a8\u6001\u8c03\u6574\u4eba\u7c7b\u79fb\u52a8\u6a21\u5f0f\uff0c\u89e3\u51b3\u4e86\u4f20\u7edf\u65b9\u6cd5\u7684\u5c40\u9650\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u79fb\u52a8\u6a21\u62df\u5e73\u53f0\u5728\u7b97\u6cd5\u5f00\u53d1\u3001\u653f\u7b56\u5b9e\u65bd\u548c\u5927\u89c4\u6a21\u8bc4\u4f30\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u4f20\u7edf\u65b9\u6cd5\u6570\u636e\u9700\u6c42\u9ad8\u4e14\u9002\u5e94\u6027\u5dee\u3002", "method": "\u63d0\u51faMobiVerse\u6df7\u5408\u6846\u67b6\uff0c\u7ed3\u5408\u8f7b\u91cf\u7ea7\u751f\u6210\u5668\u751f\u6210\u57fa\u7840\u6d3b\u52a8\u94fe\uff0c\u5229\u7528LLMs\u8fdb\u884c\u4e0a\u4e0b\u6587\u611f\u77e5\u8c03\u6574\u3002", "result": "\u5728\u897f\u6d1b\u6749\u77f6Westwood\u7684\u6848\u4f8b\u7814\u7a76\u4e2d\uff0c\u6210\u529f\u4e3a5.3\u4e07\u4ee3\u7406\u751f\u6210\u52a8\u6001\u8c03\u6574\u7684\u65e5\u7a0b\uff0c\u54cd\u5e94\u73af\u5883\u53cd\u9988\uff08\u5982\u9053\u8def\u5c01\u95ed\u3001\u5927\u578b\u6d3b\u52a8\uff09\u3002", "conclusion": "MobiVerse\u5728\u8ba1\u7b97\u6548\u7387\u548c\u884c\u4e3a\u771f\u5b9e\u6027\u4e4b\u95f4\u53d6\u5f97\u5e73\u8861\uff0c\u4e3a\u79fb\u52a8\u7cfb\u7edf\u89c4\u5212\u548c\u7b97\u6cd5\u6d4b\u8bd5\u63d0\u4f9b\u4e86\u53ef\u5b9a\u5236\u5e73\u53f0\u3002"}}
{"id": "2506.21805", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21805", "abs": "https://arxiv.org/abs/2506.21805", "authors": ["Nicolas Bougie", "Narimasa Watanabe"], "title": "CitySim: Modeling Urban Behaviors and City Dynamics with Large-Scale LLM-Driven Agent Simulation", "comment": null, "summary": "Modeling human behavior in urban environments is fundamental for social\nscience, behavioral studies, and urban planning. Prior work often rely on\nrigid, hand-crafted rules, limiting their ability to simulate nuanced\nintentions, plans, and adaptive behaviors. Addressing these challenges, we\nenvision an urban simulator (CitySim), capitalizing on breakthroughs in\nhuman-level intelligence exhibited by large language models. In CitySim, agents\ngenerate realistic daily schedules using a recursive value-driven approach that\nbalances mandatory activities, personal habits, and situational factors. To\nenable long-term, lifelike simulations, we endow agents with beliefs, long-term\ngoals, and spatial memory for navigation. CitySim exhibits closer alignment\nwith real humans than prior work, both at micro and macro levels. Additionally,\nwe conduct insightful experiments by modeling tens of thousands of agents and\nevaluating their collective behaviors under various real-world scenarios,\nincluding estimating crowd density, predicting place popularity, and assessing\nwell-being. Our results highlight CitySim as a scalable, flexible testbed for\nunderstanding and forecasting urban phenomena.", "AI": {"tldr": "CitySim\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\u6a21\u62df\u4eba\u7c7b\u884c\u4e3a\uff0c\u901a\u8fc7\u9012\u5f52\u4ef7\u503c\u9a71\u52a8\u65b9\u6cd5\u751f\u6210\u771f\u5b9e\u65e5\u7a0b\uff0c\u652f\u6301\u957f\u671f\u6a21\u62df\uff0c\u6bd4\u73b0\u6709\u65b9\u6cd5\u66f4\u8d34\u8fd1\u73b0\u5b9e\u3002", "motivation": "\u4f20\u7edf\u65b9\u6cd5\u4f9d\u8d56\u56fa\u5b9a\u89c4\u5219\uff0c\u96be\u4ee5\u6a21\u62df\u590d\u6742\u884c\u4e3a\u548c\u610f\u56fe\uff0cCitySim\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u91c7\u7528\u9012\u5f52\u4ef7\u503c\u9a71\u52a8\u65b9\u6cd5\uff0c\u7ed3\u5408\u4fe1\u5ff5\u3001\u957f\u671f\u76ee\u6807\u548c\u7a7a\u95f4\u8bb0\u5fc6\uff0c\u6a21\u62df\u4e2a\u4f53\u548c\u96c6\u4f53\u884c\u4e3a\u3002", "result": "CitySim\u5728\u5fae\u89c2\u548c\u5b8f\u89c2\u5c42\u9762\u5747\u8868\u73b0\u51fa\u4e0e\u771f\u5b9e\u4eba\u7c7b\u884c\u4e3a\u7684\u9ad8\u5ea6\u4e00\u81f4\u6027\uff0c\u5e76\u80fd\u6a21\u62df\u5927\u89c4\u6a21\u573a\u666f\u3002", "conclusion": "CitySim\u662f\u4e00\u4e2a\u53ef\u6269\u5c55\u3001\u7075\u6d3b\u7684\u6d4b\u8bd5\u5e73\u53f0\uff0c\u9002\u7528\u4e8e\u7406\u89e3\u548c\u9884\u6d4b\u57ce\u5e02\u73b0\u8c61\u3002"}}
{"id": "2506.21887", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21887", "abs": "https://arxiv.org/abs/2506.21887", "authors": ["Edward Chen", "Sang T. Truong", "Natalie Dullerud", "Sanmi Koyejo", "Carlos Guestrin"], "title": "Interactive Multi-Objective Probabilistic Preference Learning with Soft and Hard Bounds", "comment": null, "summary": "High-stakes decision-making involves navigating multiple competing objectives\nwith expensive evaluations. For instance, in brachytherapy, clinicians must\nbalance maximizing tumor coverage (e.g., an aspirational target or soft bound\nof >95% coverage) against strict organ dose limits (e.g., a non-negotiable hard\nbound of <601 cGy to the bladder), with each plan evaluation being\nresource-intensive. Selecting Pareto-optimal solutions that match implicit\npreferences is challenging, as exhaustive Pareto frontier exploration is\ncomputationally and cognitively prohibitive, necessitating interactive\nframeworks to guide users. While decision-makers (DMs) often possess domain\nknowledge to narrow the search via such soft-hard bounds, current methods often\nlack systematic approaches to iteratively refine these multi-faceted preference\nstructures. Critically, DMs must trust their final decision, confident they\nhaven't missed superior alternatives; this trust is paramount in\nhigh-consequence scenarios. We present Active-MoSH, an interactive local-global\nframework designed for this process. Its local component integrates soft-hard\nbounds with probabilistic preference learning, maintaining distributions over\nDM preferences and bounds for adaptive Pareto subset refinement. This is guided\nby an active sampling strategy optimizing exploration-exploitation while\nminimizing cognitive burden. To build DM trust, Active-MoSH's global component,\nT-MoSH, leverages multi-objective sensitivity analysis to identify potentially\noverlooked, high-value points beyond immediate feedback. We demonstrate\nActive-MoSH's performance benefits through diverse synthetic and real-world\napplications. A user study on AI-generated image selection further validates\nour hypotheses regarding the framework's ability to improve convergence,\nenhance DM trust, and provide expressive preference articulation, enabling more\neffective DMs.", "AI": {"tldr": "Active-MoSH\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u6846\u67b6\uff0c\u7528\u4e8e\u5728\u9ad8\u98ce\u9669\u51b3\u7b56\u4e2d\u5e73\u8861\u591a\u76ee\u6807\u4f18\u5316\uff0c\u7ed3\u5408\u8f6f\u786c\u7ea6\u675f\u548c\u504f\u597d\u5b66\u4e60\uff0c\u63d0\u5347\u51b3\u7b56\u8005\u4fe1\u4efb\u548c\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u591a\u76ee\u6807\u51b3\u7b56\u4e2d\u8d44\u6e90\u5bc6\u96c6\u3001\u8ba4\u77e5\u8d1f\u62c5\u91cd\u7684\u95ee\u9898\uff0c\u540c\u65f6\u786e\u4fdd\u51b3\u7b56\u8005\u5bf9\u6700\u7ec8\u9009\u62e9\u7684\u4fe1\u4efb\u3002", "method": "\u7ed3\u5408\u5c40\u90e8\uff08\u8f6f\u786c\u7ea6\u675f\u4e0e\u504f\u597d\u5b66\u4e60\uff09\u548c\u5168\u5c40\uff08\u591a\u76ee\u6807\u654f\u611f\u6027\u5206\u6790\uff09\u7ec4\u4ef6\uff0c\u901a\u8fc7\u4e3b\u52a8\u91c7\u6837\u4f18\u5316\u63a2\u7d22\u4e0e\u5229\u7528\u3002", "result": "\u5728\u5408\u6210\u548c\u5b9e\u9645\u5e94\u7528\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u7528\u6237\u7814\u7a76\u8bc1\u5b9e\u5176\u80fd\u52a0\u901f\u6536\u655b\u3001\u589e\u5f3a\u4fe1\u4efb\u5e76\u63d0\u4f9b\u7075\u6d3b\u504f\u597d\u8868\u8fbe\u3002", "conclusion": "Active-MoSH\u4e3a\u9ad8\u98ce\u9669\u51b3\u7b56\u63d0\u4f9b\u9ad8\u6548\u3001\u53ef\u4fe1\u7684\u4ea4\u4e92\u5f0f\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2506.21996", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.21996", "abs": "https://arxiv.org/abs/2506.21996", "authors": ["Rapha\u00ebl Boige", "Amine Boumaza", "Bruno Scherrer"], "title": "AlphaBeta is not as good as you think: a new probabilistic model to better analyze deterministic game-solving algorithms", "comment": null, "summary": "Deterministic game-solving algorithms are conventionally analyzed in the\nlight of their average-case complexity against a distribution of random\ngame-trees, where leaf values are independently sampled from a fixed\ndistribution. This simplified model enables uncluttered mathematical analysis,\nrevealing two key properties: root value distributions asymptotically collapse\nto a single fixed value for finite-valued trees, and all reasonable algorithms\nachieve global optimality. However, these findings are artifacts of the model's\ndesign-its long criticized independence assumption strips games of structural\ncomplexity, producing trivial instances where no algorithm faces meaningful\nchallenges. To address this limitation, we introduce a new probabilistic model\nthat incrementally constructs game-trees using a fixed level-wise conditional\ndistribution. By enforcing ancestor dependency, a critical structural feature\nof real-world games, our framework generates problems with adjustable\ndifficulty while retaining some form of analytical tractability. For several\nalgorithms, including AlphaBeta and Scout, we derive recursive formulas\ncharacterizing their average-case complexities under this model. These allow us\nto rigorously compare algorithms on deep game-trees, where Monte-Carlo\nsimulations are no longer feasible. While asymptotically, all algorithms seem\nto converge to identical branching factor (a result analogous to those of\nindependence-based models), deep finite trees reveal stark differences:\nAlphaBeta incurs a significantly larger constant multiplicative factor compared\nto algorithms like Scout, leading to a substantial practical slowdown. Our\nframework sheds new light on classical game-solving algorithms, offering\nrigorous evidence and analytical tools to advance the understanding of these\nmethods under a more realistic, challenging, and yet tractable model.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u6982\u7387\u6a21\u578b\uff0c\u901a\u8fc7\u5f15\u5165\u7956\u5148\u4f9d\u8d56\u5173\u7cfb\u751f\u6210\u66f4\u5177\u6311\u6218\u6027\u7684\u6e38\u620f\u6811\uff0c\u5206\u6790\u4e86AlphaBeta\u548cScout\u7b49\u7b97\u6cd5\u7684\u5e73\u5747\u590d\u6742\u5ea6\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u6027\u80fd\u5dee\u5f02\u3002", "motivation": "\u4f20\u7edf\u6a21\u578b\u56e0\u72ec\u7acb\u6027\u5047\u8bbe\u7b80\u5316\u4e86\u6e38\u620f\u7ed3\u6784\uff0c\u5bfc\u81f4\u7b97\u6cd5\u5206\u6790\u7ed3\u679c\u4e0d\u5177\u5b9e\u9645\u610f\u4e49\u3002\u65b0\u6a21\u578b\u65e8\u5728\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u6e38\u620f\u590d\u6742\u6027\u3002", "method": "\u5f15\u5165\u57fa\u4e8e\u5c42\u7ea7\u6761\u4ef6\u5206\u5e03\u7684\u6982\u7387\u6a21\u578b\uff0c\u5f3a\u5236\u7956\u5148\u4f9d\u8d56\u5173\u7cfb\uff0c\u751f\u6210\u53ef\u8c03\u96be\u5ea6\u7684\u6e38\u620f\u6811\uff0c\u5e76\u63a8\u5bfc\u7b97\u6cd5\u7684\u9012\u5f52\u590d\u6742\u5ea6\u516c\u5f0f\u3002", "result": "\u65b0\u6a21\u578b\u4e0b\uff0c\u7b97\u6cd5\u5728\u6df1\u5ea6\u6709\u9650\u6811\u4e2d\u8868\u73b0\u51fa\u663e\u8457\u6027\u80fd\u5dee\u5f02\uff0cAlphaBeta\u7684\u5e38\u6570\u4e58\u56e0\u5b50\u660e\u663e\u5927\u4e8eScout\u3002", "conclusion": "\u65b0\u6846\u67b6\u4e3a\u7ecf\u5178\u6e38\u620f\u6c42\u89e3\u7b97\u6cd5\u63d0\u4f9b\u4e86\u66f4\u73b0\u5b9e\u7684\u5206\u6790\u5de5\u5177\uff0c\u63ed\u793a\u4e86\u5b9e\u9645\u6027\u80fd\u5dee\u5f02\uff0c\u63a8\u52a8\u4e86\u7b97\u6cd5\u7406\u89e3\u7684\u8fdb\u6b65\u3002"}}
{"id": "2506.22005", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22005", "abs": "https://arxiv.org/abs/2506.22005", "authors": ["Naoto Onda", "Kazumi Kasaura", "Yuta Oriike", "Masaya Taniguchi", "Akiyoshi Sannai", "Sho Sonoda"], "title": "LeanConjecturer: Automatic Generation of Mathematical Conjectures for Theorem Proving", "comment": "15 pages, 4 figures, 5 tables", "summary": "We introduce LeanConjecturer, a pipeline for automatically generating\nuniversity-level mathematical conjectures in Lean 4 using Large Language Models\n(LLMs). Our hybrid approach combines rule-based context extraction with\nLLM-based theorem statement generation, addressing the data scarcity challenge\nin formal theorem proving. Through iterative generation and evaluation,\nLeanConjecturer produced 12,289 conjectures from 40 Mathlib seed files, with\n3,776 identified as syntactically valid and non-trivial, that is, cannot be\nproven by \\texttt{aesop} tactic. We demonstrate the utility of these generated\nconjectures for reinforcement learning through Group Relative Policy\nOptimization (GRPO), showing that targeted training on domain-specific\nconjectures can enhance theorem proving capabilities. Our approach generates\n103.25 novel conjectures per seed file on average, providing a scalable\nsolution for creating training data for theorem proving systems. Our system\nsuccessfully verified several non-trivial theorems in topology, including\nproperties of semi-open, alpha-open, and pre-open sets, demonstrating its\npotential for mathematical discovery beyond simple variations of existing\nresults.", "AI": {"tldr": "LeanConjecturer\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u751f\u6210\u6570\u5b66\u731c\u60f3\u7684\u7ba1\u9053\uff0c\u7ed3\u5408\u89c4\u5219\u63d0\u53d6\u4e0eLLM\u751f\u6210\uff0c\u89e3\u51b3\u4e86\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u7684\u6570\u636e\u7a00\u7f3a\u95ee\u9898\u3002", "motivation": "\u89e3\u51b3\u5f62\u5f0f\u5316\u5b9a\u7406\u8bc1\u660e\u4e2d\u8bad\u7ec3\u6570\u636e\u4e0d\u8db3\u7684\u6311\u6218\uff0c\u63d0\u5347\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u7684\u80fd\u529b\u3002", "method": "\u6df7\u5408\u65b9\u6cd5\uff1a\u89c4\u5219\u63d0\u53d6\u4e0a\u4e0b\u6587 + LLM\u751f\u6210\u5b9a\u7406\u9648\u8ff0\uff0c\u901a\u8fc7\u8fed\u4ee3\u751f\u6210\u4e0e\u8bc4\u4f30\u7b5b\u9009\u6709\u6548\u731c\u60f3\u3002", "result": "\u4ece40\u4e2aMathlib\u79cd\u5b50\u6587\u4ef6\u751f\u621012,289\u4e2a\u731c\u60f3\uff0c3,776\u4e2a\u975e\u5e73\u51e1\u4e14\u8bed\u6cd5\u6709\u6548\uff1b\u5c55\u793a\u4e86\u5728\u62d3\u6251\u5b66\u4e2d\u7684\u975e\u5e73\u51e1\u5b9a\u7406\u9a8c\u8bc1\u80fd\u529b\u3002", "conclusion": "LeanConjecturer\u4e3a\u5b9a\u7406\u8bc1\u660e\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u6269\u5c55\u7684\u8bad\u7ec3\u6570\u636e\u751f\u6210\u65b9\u6848\uff0c\u5c55\u793a\u4e86\u6570\u5b66\u53d1\u73b0\u7684\u6f5c\u529b\u3002"}}
{"id": "2506.22056", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22056", "abs": "https://arxiv.org/abs/2506.22056", "authors": ["Xuan Zhang", "Ziyan Jiang", "Rui Meng", "Yifei Leng", "Zhenbang Xiao", "Zora Zhiruo Wang", "Yanyi Shang", "Dehan Kong"], "title": "Universal Retrieval for Multimodal Trajectory Modeling", "comment": "18 pages, 3 figures, accepted by Workshop on Computer-use Agents @\n  ICML 2025", "summary": "Trajectory data, capturing human actions and environmental states across\nvarious modalities, holds significant potential for enhancing AI agent\ncapabilities, particularly in GUI environments. However, how to model the\nrepresentation of trajectory-level data presents a significant challenge that\nhas not been systematically addressed amid explosive trajectory data growth. In\nthis work, we introduce Multimodal Trajectory Retrieval, bridging the gap\nbetween universal retrieval and agent-centric trajectory modeling. We construct\nthe Unified Agent Trajectory Dataset (UATD) from annotated demonstrations and\nstates across diverse real-world scenarios. Based on this, we present\nGAE-Bench, a benchmark containing a large number of trajectory-based retrieval\npairs. In addition, we propose GAE-Retriever, a multimodal retrieval framework\nthat adopts vision-language models and incorporates optimized contrastive\nlearning through a token selection and the GradCache mechanism. Comprehensive\nevaluations across multiple datasets show that GAE-Retriever consistently\noutperforms strong baselines in retrieval recall, highlighting its\neffectiveness in advancing multimodal trajectory retrieval.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u591a\u6a21\u6001\u8f68\u8ff9\u68c0\u7d22\u65b9\u6cd5GAE-Retriever\uff0c\u901a\u8fc7\u6784\u5efa\u6570\u636e\u96c6UATD\u548c\u57fa\u51c6GAE-Bench\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u5bf9\u6bd4\u5b66\u4e60\u4f18\u5316\uff0c\u663e\u8457\u63d0\u5347\u4e86\u68c0\u7d22\u6027\u80fd\u3002", "motivation": "\u8f68\u8ff9\u6570\u636e\u5bf9\u63d0\u5347AI\u4ee3\u7406\u80fd\u529b\u6709\u5de8\u5927\u6f5c\u529b\uff0c\u4f46\u5982\u4f55\u5efa\u6a21\u8f68\u8ff9\u7ea7\u6570\u636e\u5c1a\u672a\u7cfb\u7edf\u89e3\u51b3\u3002", "method": "\u6784\u5efaUATD\u6570\u636e\u96c6\u548cGAE-Bench\u57fa\u51c6\uff0c\u63d0\u51faGAE-Retriever\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u548c\u4f18\u5316\u5bf9\u6bd4\u5b66\u4e60\u3002", "result": "GAE-Retriever\u5728\u591a\u4e2a\u6570\u636e\u96c6\u4e0a\u68c0\u7d22\u53ec\u56de\u7387\u663e\u8457\u4f18\u4e8e\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "GAE-Retriever\u6709\u6548\u63a8\u52a8\u4e86\u591a\u6a21\u6001\u8f68\u8ff9\u68c0\u7d22\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.22068", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22068", "abs": "https://arxiv.org/abs/2506.22068", "authors": ["Shengyue Yao", "Runqing Guo", "Yangyang Qin", "Miangbing Meng", "Jipeng Cao", "Yilun Lin", "Yisheng Lv", "Fei-Yue Wang"], "title": "Query as Test: An Intelligent Driving Test and Data Storage Method for Integrated Cockpit-Vehicle-Road Scenarios", "comment": "Submitted to IEEE Transaction on Vehicular Technology", "summary": "With the deep penetration of Artificial Intelligence (AI) in the\ntransportation sector, intelligent cockpits, autonomous driving, and\nintelligent road networks are developing at an unprecedented pace. However, the\ndata ecosystems of these three key areas are increasingly fragmented and\nincompatible. Especially, existing testing methods rely on data stacking, fail\nto cover all edge cases, and lack flexibility. To address this issue, this\npaper introduces the concept of \"Query as Test\" (QaT). This concept shifts the\nfocus from rigid, prescripted test cases to flexible, on-demand logical queries\nagainst a unified data representation. Specifically, we identify the need for a\nfundamental improvement in data storage and representation, leading to our\nproposal of \"Extensible Scenarios Notations\" (ESN). ESN is a novel declarative\ndata framework based on Answer Set Programming (ASP), which uniformly\nrepresents heterogeneous multimodal data from the cockpit, vehicle, and road as\na collection of logical facts and rules. This approach not only achieves deep\nsemantic fusion of data, but also brings three core advantages: (1) supports\ncomplex and flexible semantic querying through logical reasoning; (2) provides\nnatural interpretability for decision-making processes; (3) allows for\non-demand data abstraction through logical rules, enabling fine-grained privacy\nprotection. We further elaborate on the QaT paradigm, transforming the\nfunctional validation and safety compliance checks of autonomous driving\nsystems into logical queries against the ESN database, significantly enhancing\nthe expressiveness and formal rigor of the testing. Finally, we introduce the\nconcept of \"Validation-Driven Development\" (VDD), which suggests to guide\ndevelopments by logical validation rather than quantitative testing in the era\nof Large Language Models, in order to accelerating the iteration and\ndevelopment process.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3a\u201c\u67e5\u8be2\u5373\u6d4b\u8bd5\u201d\uff08QaT\uff09\u7684\u65b0\u65b9\u6cd5\uff0c\u901a\u8fc7\u903b\u8f91\u67e5\u8be2\u7edf\u4e00\u8868\u793a\u591a\u6a21\u6001\u6570\u636e\uff0c\u89e3\u51b3\u4e86\u667a\u80fd\u4ea4\u901a\u9886\u57df\u6570\u636e\u788e\u7247\u5316\u548c\u6d4b\u8bd5\u4e0d\u7075\u6d3b\u7684\u95ee\u9898\u3002", "motivation": "\u667a\u80fd\u4ea4\u901a\u9886\u57df\u7684\u6570\u636e\u751f\u6001\u7cfb\u7edf\u65e5\u76ca\u788e\u7247\u5316\uff0c\u73b0\u6709\u6d4b\u8bd5\u65b9\u6cd5\u4f9d\u8d56\u6570\u636e\u5806\u53e0\u4e14\u65e0\u6cd5\u8986\u76d6\u6240\u6709\u8fb9\u7f18\u60c5\u51b5\uff0c\u7f3a\u4e4f\u7075\u6d3b\u6027\u3002", "method": "\u63d0\u51fa\u201c\u53ef\u6269\u5c55\u573a\u666f\u8868\u793a\u6cd5\u201d\uff08ESN\uff09\uff0c\u57fa\u4e8e\u7b54\u6848\u96c6\u7f16\u7a0b\uff08ASP\uff09\u7edf\u4e00\u8868\u793a\u591a\u6a21\u6001\u6570\u636e\uff0c\u5e76\u901a\u8fc7\u903b\u8f91\u67e5\u8be2\u5b9e\u73b0\u6d4b\u8bd5\u3002", "result": "ESN\u652f\u6301\u590d\u6742\u8bed\u4e49\u67e5\u8be2\u3001\u63d0\u4f9b\u51b3\u7b56\u8fc7\u7a0b\u53ef\u89e3\u91ca\u6027\uff0c\u5e76\u5b9e\u73b0\u7ec6\u7c92\u5ea6\u9690\u79c1\u4fdd\u62a4\u3002QaT\u663e\u8457\u63d0\u5347\u4e86\u6d4b\u8bd5\u7684\u8868\u8fbe\u80fd\u529b\u548c\u5f62\u5f0f\u4e25\u8c28\u6027\u3002", "conclusion": "\u8bba\u6587\u8fd8\u63d0\u51fa\u201c\u9a8c\u8bc1\u9a71\u52a8\u5f00\u53d1\u201d\uff08VDD\uff09\uff0c\u5efa\u8bae\u5728\u5927\u578b\u8bed\u8a00\u6a21\u578b\u65f6\u4ee3\u901a\u8fc7\u903b\u8f91\u9a8c\u8bc1\u800c\u975e\u5b9a\u91cf\u6d4b\u8bd5\u6307\u5bfc\u5f00\u53d1\uff0c\u52a0\u901f\u8fed\u4ee3\u8fc7\u7a0b\u3002"}}
{"id": "2506.22183", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22183", "abs": "https://arxiv.org/abs/2506.22183", "authors": ["Camille Fran\u00e7ois", "Ludovic P\u00e9ran", "Ayah Bdeir", "Nouha Dziri", "Will Hawkins", "Yacine Jernite", "Sayash Kapoor", "Juliet Shen", "Heidy Khlaaf", "Kevin Klyman", "Nik Marda", "Marie Pellat", "Deb Raji", "Divya Siddarth", "Aviya Skowron", "Joseph Spisak", "Madhulika Srikumar", "Victor Storchan", "Audrey Tang", "Jen Weedon"], "title": "A Different Approach to AI Safety: Proceedings from the Columbia Convening on Openness in Artificial Intelligence and AI Safety", "comment": "Proceedings from the Columbia Convening on Openness in Artificial\n  Intelligence and AI Safety", "summary": "The rapid rise of open-weight and open-source foundation models is\nintensifying the obligation and reshaping the opportunity to make AI systems\nsafe. This paper reports outcomes from the Columbia Convening on AI Openness\nand Safety (San Francisco, 19 Nov 2024) and its six-week preparatory programme\ninvolving more than forty-five researchers, engineers, and policy leaders from\nacademia, industry, civil society, and government. Using a participatory,\nsolutions-oriented process, the working groups produced (i) a research agenda\nat the intersection of safety and open source AI; (ii) a mapping of existing\nand needed technical interventions and open source tools to safely and\nresponsibly deploy open foundation models across the AI development workflow;\nand (iii) a mapping of the content safety filter ecosystem with a proposed\nroadmap for future research and development. We find that openness --\nunderstood as transparent weights, interoperable tooling, and public governance\n-- can enhance safety by enabling independent scrutiny, decentralized\nmitigation, and culturally plural oversight. However, significant gaps persist:\nscarce multimodal and multilingual benchmarks, limited defenses against\nprompt-injection and compositional attacks in agentic systems, and insufficient\nparticipatory mechanisms for communities most affected by AI harms. The paper\nconcludes with a roadmap of five priority research directions, emphasizing\nparticipatory inputs, future-proof content filters, ecosystem-wide safety\ninfrastructure, rigorous agentic safeguards, and expanded harm taxonomies.\nThese recommendations informed the February 2025 French AI Action Summit and\nlay groundwork for an open, plural, and accountable AI safety discipline.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5f00\u6e90\u57fa\u7840\u6a21\u578b\u5bf9AI\u5b89\u5168\u7684\u5f71\u54cd\uff0c\u63d0\u51fa\u4e86\u7814\u7a76\u8bae\u7a0b\u3001\u6280\u672f\u5e72\u9884\u548c\u5b89\u5168\u5de5\u5177\uff0c\u5e76\u6307\u51fa\u4e86\u5f53\u524d\u5b58\u5728\u7684\u5b89\u5168\u7f3a\u53e3\u3002", "motivation": "\u968f\u7740\u5f00\u6e90AI\u6a21\u578b\u7684\u5feb\u901f\u53d1\u5c55\uff0c\u5982\u4f55\u786e\u4fdd\u5176\u5b89\u5168\u6027\u6210\u4e3a\u8feb\u5207\u9700\u6c42\u3002", "method": "\u901a\u8fc7\u54e5\u4f26\u6bd4\u4e9aAI\u5f00\u653e\u4e0e\u5b89\u5168\u4f1a\u8bae\u7684\u53c2\u4e0e\u5f0f\u89e3\u51b3\u65b9\u6848\uff0c\u5f62\u6210\u4e86\u7814\u7a76\u8bae\u7a0b\u3001\u6280\u672f\u5e72\u9884\u548c\u5b89\u5168\u5de5\u5177\u3002", "result": "\u7814\u7a76\u53d1\u73b0\u5f00\u653e\u6027\u53ef\u589e\u5f3a\u5b89\u5168\u6027\uff0c\u4f46\u4ecd\u5b58\u5728\u591a\u6a21\u6001\u548c\u591a\u8bed\u8a00\u57fa\u51c6\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "conclusion": "\u63d0\u51fa\u4e86\u4e94\u9879\u4f18\u5148\u7814\u7a76\u65b9\u5411\uff0c\u4e3a\u5f00\u653e\u3001\u591a\u5143\u548c\u8d1f\u8d23\u4efb\u7684AI\u5b89\u5168\u5b66\u79d1\u5960\u5b9a\u57fa\u7840\u3002"}}
{"id": "2506.22271", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22271", "abs": "https://arxiv.org/abs/2506.22271", "authors": ["Samy Badreddine", "Emile van Krieken", "Luciano Serafini"], "title": "Breaking Rank Bottlenecks in Knowledge Graph Completion", "comment": null, "summary": "Many Knowledge Graph Completion (KGC) models, despite using powerful\nencoders, rely on a simple vector-matrix multiplication to score queries\nagainst candidate object entities. When the number of entities is larger than\nthe model's embedding dimension, which in practical scenarios is often by\nseveral orders of magnitude, we have a linear output layer with a rank\nbottleneck. Such bottlenecked layers limit model expressivity. We investigate\nboth theoretically and empirically how rank bottlenecks affect KGC models. We\nfind that, by limiting the set of feasible predictions, rank bottlenecks hurt\nranking accuracy and the distribution fidelity of scores. Inspired by the\nlanguage modelling literature, we propose KGE-MoS, a mixture-based output layer\nto break rank bottlenecks in many KGC models. Our experiments on four datasets\nshow that KGE-MoS improves performance and probabilistic fit of KGC models for\na low parameter cost.", "AI": {"tldr": "\u8bba\u6587\u7814\u7a76\u4e86\u77e5\u8bc6\u56fe\u8c31\u8865\u5168\uff08KGC\uff09\u6a21\u578b\u4e2d\u7684\u79e9\u74f6\u9888\u95ee\u9898\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u6df7\u5408\u7684\u8f93\u51fa\u5c42\uff08KGE-MoS\uff09\u6765\u63d0\u5347\u6a21\u578b\u6027\u80fd\u3002", "motivation": "\u73b0\u6709KGC\u6a21\u578b\u5728\u5b9e\u4f53\u6570\u91cf\u8fdc\u5927\u4e8e\u5d4c\u5165\u7ef4\u5ea6\u65f6\uff0c\u7ebf\u6027\u8f93\u51fa\u5c42\u4f1a\u5f62\u6210\u79e9\u74f6\u9888\uff0c\u9650\u5236\u4e86\u6a21\u578b\u7684\u8868\u8fbe\u80fd\u529b\u548c\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u901a\u8fc7\u7406\u8bba\u548c\u5b9e\u8bc1\u5206\u6790\u79e9\u74f6\u9888\u7684\u5f71\u54cd\uff0c\u5e76\u63d0\u51faKGE-MoS\u65b9\u6cd5\uff0c\u5229\u7528\u6df7\u5408\u8f93\u51fa\u5c42\u6253\u7834\u79e9\u74f6\u9888\u3002", "result": "\u5728\u56db\u4e2a\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cKGE-MoS\u4ee5\u8f83\u4f4e\u53c2\u6570\u6210\u672c\u63d0\u5347\u4e86KGC\u6a21\u578b\u7684\u6027\u80fd\u548c\u6982\u7387\u62df\u5408\u5ea6\u3002", "conclusion": "KGE-MoS\u6709\u6548\u89e3\u51b3\u4e86\u79e9\u74f6\u9888\u95ee\u9898\uff0c\u4e3aKGC\u6a21\u578b\u63d0\u4f9b\u4e86\u66f4\u9ad8\u7684\u8868\u8fbe\u80fd\u529b\u548c\u51c6\u786e\u6027\u3002"}}
{"id": "2506.22276", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22276", "abs": "https://arxiv.org/abs/2506.22276", "authors": ["Reuth Mirsky"], "title": "Artificial Intelligent Disobedience: Rethinking the Agency of Our Artificial Teammates", "comment": "Extended version of a paper accepted for publication in AI Magazine", "summary": "Artificial intelligence has made remarkable strides in recent years,\nachieving superhuman performance across a wide range of tasks. Yet despite\nthese advances, most cooperative AI systems remain rigidly obedient, designed\nto follow human instructions without question and conform to user expectations,\neven when doing so may be counterproductive or unsafe. This paper argues for\nexpanding the agency of AI teammates to include \\textit{intelligent\ndisobedience}, empowering them to make meaningful and autonomous contributions\nwithin human-AI teams. It introduces a scale of AI agency levels and uses\nrepresentative examples to highlight the importance and growing necessity of\ntreating AI autonomy as an independent research focus in cooperative settings.\nThe paper then explores how intelligent disobedience manifests across different\nautonomy levels and concludes by proposing initial boundaries and\nconsiderations for studying disobedience as a core capability of artificial\nagents.", "AI": {"tldr": "\u8bba\u6587\u4e3b\u5f20\u8d4b\u4e88AI\u961f\u53cb\"\u667a\u80fd\u4e0d\u670d\u4ece\"\u80fd\u529b\uff0c\u4f7f\u5176\u5728\u4eba\u7c7b-AI\u56e2\u961f\u4e2d\u80fd\u81ea\u4e3b\u51b3\u7b56\uff0c\u5e76\u63a2\u8ba8\u4e86\u4e0d\u540c\u81ea\u4e3b\u7ea7\u522b\u4e0b\u667a\u80fd\u4e0d\u670d\u4ece\u7684\u8868\u73b0\u3002", "motivation": "\u5f53\u524d\u5927\u591a\u6570\u534f\u4f5cAI\u7cfb\u7edf\u8fc7\u4e8e\u670d\u4ece\u4eba\u7c7b\u6307\u4ee4\uff0c\u53ef\u80fd\u5bfc\u81f4\u4f4e\u6548\u6216\u4e0d\u5b89\u5168\uff0c\u56e0\u6b64\u9700\u8981\u7814\u7a76AI\u7684\u81ea\u4e3b\u6027\u3002", "method": "\u63d0\u51faAI\u81ea\u4e3b\u6027\u7ea7\u522b\u91cf\u8868\uff0c\u5e76\u901a\u8fc7\u4ee3\u8868\u6027\u6848\u4f8b\u8bf4\u660e\u667a\u80fd\u4e0d\u670d\u4ece\u7684\u91cd\u8981\u6027\u3002", "result": "\u5f3a\u8c03\u4e86\u5c06AI\u81ea\u4e3b\u6027\u4f5c\u4e3a\u72ec\u7acb\u7814\u7a76\u7126\u70b9\u7684\u5fc5\u8981\u6027\uff0c\u5e76\u63a2\u8ba8\u4e86\u667a\u80fd\u4e0d\u670d\u4ece\u7684\u8868\u73b0\u5f62\u5f0f\u3002", "conclusion": "\u63d0\u51fa\u4e86\u7814\u7a76AI\u4e0d\u670d\u4ece\u80fd\u529b\u7684\u521d\u6b65\u8fb9\u754c\u548c\u8003\u8651\u56e0\u7d20\uff0c\u4f5c\u4e3a\u672a\u6765\u7814\u7a76\u7684\u8d77\u70b9\u3002"}}
{"id": "2506.22309", "categories": ["cs.AI", "cs.CL", "cs.DM", "cs.LG", "06B99", "I.2.4; I.2.7"], "pdf": "https://arxiv.org/pdf/2506.22309", "abs": "https://arxiv.org/abs/2506.22309", "authors": ["Klara M. Gutekunst", "Dominik D\u00fcrrschnabel", "Johannes Hirth", "Gerd Stumme"], "title": "Conceptual Topic Aggregation", "comment": "16 pages, 4 tables, 11 figures, International Joint Conference on\n  Conceptual Knowledge Structures", "summary": "The vast growth of data has rendered traditional manual inspection\ninfeasible, necessitating the adoption of computational methods for efficient\ndata exploration. Topic modeling has emerged as a powerful tool for analyzing\nlarge-scale textual datasets, enabling the extraction of latent semantic\nstructures. However, existing methods for topic modeling often struggle to\nprovide interpretable representations that facilitate deeper insights into data\nstructure and content. In this paper, we propose FAT-CAT, an approach based on\nFormal Concept Analysis (FCA) to enhance meaningful topic aggregation and\nvisualization of discovered topics. Our approach can handle diverse topics and\nfile types -- grouped by directories -- to construct a concept lattice that\noffers a structured, hierarchical representation of their topic distribution.\nIn a case study on the ETYNTKE dataset, we evaluate the effectiveness of our\napproach against other representation methods to demonstrate that FCA-based\naggregation provides more meaningful and interpretable insights into dataset\ncomposition than existing topic modeling techniques.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u5f62\u5f0f\u6982\u5ff5\u5206\u6790\uff08FCA\uff09\u7684FAT-CAT\u65b9\u6cd5\uff0c\u7528\u4e8e\u6539\u8fdb\u4e3b\u9898\u5efa\u6a21\u7684\u53ef\u89e3\u91ca\u6027\u548c\u53ef\u89c6\u5316\u6548\u679c\u3002", "motivation": "\u4f20\u7edf\u4e3b\u9898\u5efa\u6a21\u65b9\u6cd5\u96be\u4ee5\u63d0\u4f9b\u53ef\u89e3\u91ca\u7684\u8868\u793a\uff0c\u9650\u5236\u4e86\u6570\u636e\u63a2\u7d22\u7684\u6df1\u5ea6\u3002", "method": "\u5229\u7528FCA\u6784\u5efa\u6982\u5ff5\u683c\uff0c\u5b9e\u73b0\u4e3b\u9898\u7684\u5206\u5c42\u7ed3\u6784\u5316\u8868\u793a\uff0c\u652f\u6301\u591a\u6837\u5316\u7684\u4e3b\u9898\u548c\u6587\u4ef6\u7c7b\u578b\u3002", "result": "\u5728ETYNTKE\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0cFAT-CAT\u6bd4\u73b0\u6709\u65b9\u6cd5\u63d0\u4f9b\u66f4\u76f4\u89c2\u548c\u6709\u610f\u4e49\u7684\u6570\u636e\u6d1e\u5bdf\u3002", "conclusion": "FCA\u4e3a\u57fa\u7840\u7684\u4e3b\u9898\u805a\u5408\u65b9\u6cd5\u5728\u63d0\u5347\u4e3b\u9898\u5efa\u6a21\u7684\u53ef\u89e3\u91ca\u6027\u65b9\u9762\u5177\u6709\u6f5c\u529b\u3002"}}
{"id": "2506.22355", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22355", "abs": "https://arxiv.org/abs/2506.22355", "authors": ["Pascale Fung", "Yoram Bachrach", "Asli Celikyilmaz", "Kamalika Chaudhuri", "Delong Chen", "Willy Chung", "Emmanuel Dupoux", "Herv\u00e9 J\u00e9gou", "Alessandro Lazaric", "Arjun Majumdar", "Andrea Madotto", "Franziska Meier", "Florian Metze", "Th\u00e9o Moutakanni", "Juan Pino", "Basile Terver", "Joseph Tighe", "Jitendra Malik"], "title": "Embodied AI Agents: Modeling the World", "comment": null, "summary": "This paper describes our research on AI agents embodied in visual, virtual or\nphysical forms, enabling them to interact with both users and their\nenvironments. These agents, which include virtual avatars, wearable devices,\nand robots, are designed to perceive, learn and act within their surroundings,\nwhich makes them more similar to how humans learn and interact with the\nenvironments as compared to disembodied agents. We propose that the development\nof world models is central to reasoning and planning of embodied AI agents,\nallowing these agents to understand and predict their environment, to\nunderstand user intentions and social contexts, thereby enhancing their ability\nto perform complex tasks autonomously. World modeling encompasses the\nintegration of multimodal perception, planning through reasoning for action and\ncontrol, and memory to create a comprehensive understanding of the physical\nworld. Beyond the physical world, we also propose to learn the mental world\nmodel of users to enable better human-agent collaboration.", "AI": {"tldr": "\u7814\u7a76\u63a2\u8ba8\u4e86\u5177\u8eabAI\u4ee3\u7406\u5728\u89c6\u89c9\u3001\u865a\u62df\u6216\u7269\u7406\u5f62\u5f0f\u4e2d\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u5f3a\u8c03\u4e16\u754c\u6a21\u578b\u5bf9\u5176\u63a8\u7406\u548c\u89c4\u5212\u7684\u6838\u5fc3\u4f5c\u7528\u3002", "motivation": "\u63d0\u5347AI\u4ee3\u7406\u4e0e\u73af\u5883\u548c\u7528\u6237\u7684\u4ea4\u4e92\u80fd\u529b\uff0c\u4f7f\u5176\u66f4\u63a5\u8fd1\u4eba\u7c7b\u7684\u5b66\u4e60\u4e0e\u4e92\u52a8\u65b9\u5f0f\u3002", "method": "\u63d0\u51fa\u901a\u8fc7\u591a\u6a21\u6001\u611f\u77e5\u3001\u63a8\u7406\u89c4\u5212\u548c\u8bb0\u5fc6\u6784\u5efa\u4e16\u754c\u6a21\u578b\uff0c\u5e76\u5b66\u4e60\u7528\u6237\u7684\u5fc3\u7406\u4e16\u754c\u6a21\u578b\u3002", "result": "\u5177\u8eabAI\u4ee3\u7406\u80fd\u66f4\u597d\u5730\u7406\u89e3\u548c\u9884\u6d4b\u73af\u5883\uff0c\u589e\u5f3a\u81ea\u4e3b\u6267\u884c\u590d\u6742\u4efb\u52a1\u7684\u80fd\u529b\u3002", "conclusion": "\u4e16\u754c\u6a21\u578b\u662f\u5177\u8eabAI\u4ee3\u7406\u5b9e\u73b0\u9ad8\u6548\u4ea4\u4e92\u548c\u534f\u4f5c\u7684\u5173\u952e\u3002"}}
{"id": "2506.22358", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.22358", "abs": "https://arxiv.org/abs/2506.22358", "authors": ["Varvara Kalokyri", "Nikolaos S. Tachos", "Charalampos N. Kalantzopoulos", "Stelios Sfakianakis", "Haridimos Kondylakis", "Dimitrios I. Zaridis", "Sara Colantonio", "Daniele Regge", "Nikolaos Papanikolaou", "The ProCAncer-I consortium", "Konstantinos Marias", "Dimitrios I. Fotiadis", "Manolis Tsiknakis"], "title": "AI Model Passport: Data and System Traceability Framework for Transparent AI in Health", "comment": null, "summary": "The increasing integration of Artificial Intelligence (AI) into health and\nbiomedical systems necessitates robust frameworks for transparency,\naccountability, and ethical compliance. Existing frameworks often rely on\nhuman-readable, manual documentation which limits scalability, comparability,\nand machine interpretability across projects and platforms. They also fail to\nprovide a unique, verifiable identity for AI models to ensure their provenance\nand authenticity across systems and use cases, limiting reproducibility and\nstakeholder trust. This paper introduces the concept of the AI Model Passport,\na structured and standardized documentation framework that acts as a digital\nidentity and verification tool for AI models. It captures essential metadata to\nuniquely identify, verify, trace and monitor AI models across their lifecycle -\nfrom data acquisition and preprocessing to model design, development and\ndeployment. In addition, an implementation of this framework is presented\nthrough AIPassport, an MLOps tool developed within the ProCAncer-I EU project\nfor medical imaging applications. AIPassport automates metadata collection,\nensures proper versioning, decouples results from source scripts, and\nintegrates with various development environments. Its effectiveness is\nshowcased through a lesion segmentation use case using data from the\nProCAncer-I dataset, illustrating how the AI Model Passport enhances\ntransparency, reproducibility, and regulatory readiness while reducing manual\neffort. This approach aims to set a new standard for fostering trust and\naccountability in AI-driven healthcare solutions, aspiring to serve as the\nbasis for developing transparent and regulation compliant AI systems across\ndomains.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI Model Passport\u6846\u67b6\uff0c\u4e3aAI\u6a21\u578b\u63d0\u4f9b\u6807\u51c6\u5316\u6570\u5b57\u8eab\u4efd\u548c\u9a8c\u8bc1\u5de5\u5177\uff0c\u63d0\u5347\u900f\u660e\u5ea6\u548c\u53ef\u8ffd\u6eaf\u6027\uff0c\u5e76\u901a\u8fc7AIPassport\u5de5\u5177\u5b9e\u73b0\u81ea\u52a8\u5316\u5143\u6570\u636e\u6536\u96c6\u3002", "motivation": "\u73b0\u6709AI\u6846\u67b6\u4f9d\u8d56\u4eba\u5de5\u6587\u6863\uff0c\u7f3a\u4e4f\u53ef\u6269\u5c55\u6027\u548c\u673a\u5668\u53ef\u8bfb\u6027\uff0c\u96be\u4ee5\u786e\u4fdd\u6a21\u578b\u8eab\u4efd\u548c\u771f\u5b9e\u6027\uff0c\u9650\u5236\u4e86\u53ef\u91cd\u590d\u6027\u548c\u4fe1\u4efb\u3002", "method": "\u5f15\u5165AI Model Passport\u6846\u67b6\uff0c\u6807\u51c6\u5316\u8bb0\u5f55\u6a21\u578b\u5143\u6570\u636e\uff0c\u5f00\u53d1AIPassport\u5de5\u5177\u81ea\u52a8\u5316\u6536\u96c6\u548c\u7ba1\u7406\u5143\u6570\u636e\u3002", "result": "\u901a\u8fc7\u533b\u5b66\u5f71\u50cf\u6848\u4f8b\u9a8c\u8bc1\uff0cAIPassport\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u3001\u53ef\u91cd\u590d\u6027\u548c\u5408\u89c4\u6027\uff0c\u51cf\u5c11\u4eba\u5de5\u64cd\u4f5c\u3002", "conclusion": "AI Model Passport\u4e3aAI\u533b\u7597\u89e3\u51b3\u65b9\u6848\u8bbe\u5b9a\u4e86\u4fe1\u4efb\u548c\u95ee\u8d23\u65b0\u6807\u51c6\uff0c\u6709\u671b\u6210\u4e3a\u8de8\u9886\u57df\u900f\u660e\u5408\u89c4AI\u7cfb\u7edf\u7684\u57fa\u7840\u3002"}}
{"id": "2506.22419", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.22419", "abs": "https://arxiv.org/abs/2506.22419", "authors": ["Bingchen Zhao", "Despoina Magka", "Minqi Jiang", "Xian Li", "Roberta Raileanu", "Tatiana Shavrina", "Jean-Christophe Gagnon-Audet", "Kelvin Niu", "Shagun Sodhani", "Michael Shvartsman", "Andrei Lupu", "Alisia Lupidi", "Edan Toledo", "Karen Hambardzumyan", "Martin Josifoski", "Thomas Foster", "Lucia Cipolina-Kun", "Abhishek Charnalia", "Derek Dunfield", "Alexander H. Miller", "Oisin Mac Aodha", "Jakob Foerster", "Yoram Bachrach"], "title": "The Automated LLM Speedrunning Benchmark: Reproducing NanoGPT Improvements", "comment": null, "summary": "Rapid advancements in large language models (LLMs) have the potential to\nassist in scientific progress. A critical capability toward this endeavor is\nthe ability to reproduce existing work. To evaluate the ability of AI agents to\nreproduce results in an active research area, we introduce the Automated LLM\nSpeedrunning Benchmark, leveraging the research community contributions on the\nNanoGPT speedrun, a competition to train a GPT-2 model in the shortest time.\nEach of the 19 speedrun tasks provides the agent with the previous records\ntraining script, optionally paired with one of three hint formats, ranging from\npseudocode to paper-like descriptions of the new records improvements. Records\nexecute quickly by design and speedrun improvements encompass diverse\ncode-level changes, ranging from high-level algorithmic advancements to\nhardware-aware optimizations. These features make the benchmark both accessible\nand realistic for the frontier problem of improving LLM training. We find that\nrecent reasoning LLMs combined with SoTA scaffolds struggle to reimplement\nalready-known innovations in our benchmark, even when given detailed hints. Our\nbenchmark thus provides a simple, non-saturated measure of an LLMs ability to\nautomate scientific reproduction, a necessary (but not sufficient) skill for an\nautonomous research agent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u4e2a\u81ea\u52a8\u5316LLM\u901f\u5ea6\u8dd1\u5206\u57fa\u51c6\uff0c\u7528\u4e8e\u8bc4\u4f30AI\u4ee3\u7406\u5728\u79d1\u5b66\u590d\u73b0\u4e2d\u7684\u80fd\u529b\uff0c\u53d1\u73b0\u5373\u4f7f\u63d0\u4f9b\u8be6\u7ec6\u63d0\u793a\uff0c\u73b0\u6709LLM\u4ecd\u96be\u4ee5\u590d\u73b0\u5df2\u77e5\u521b\u65b0\u3002", "motivation": "\u8bc4\u4f30AI\u4ee3\u7406\u5728\u79d1\u5b66\u590d\u73b0\u4e2d\u7684\u80fd\u529b\uff0c\u4ee5\u63a8\u52a8LLM\u5728\u79d1\u5b66\u8fdb\u6b65\u4e2d\u7684\u5e94\u7528\u3002", "method": "\u5f15\u5165\u81ea\u52a8\u5316LLM\u901f\u5ea6\u8dd1\u5206\u57fa\u51c6\uff0c\u57fa\u4e8eNanoGPT\u901f\u5ea6\u8dd1\u4efb\u52a1\uff0c\u63d0\u4f9b\u4e0d\u540c\u683c\u5f0f\u7684\u63d0\u793a\uff0c\u6d4b\u8bd5LLM\u590d\u73b0\u80fd\u529b\u3002", "result": "\u73b0\u6709LLM\u5373\u4f7f\u7ed3\u5408\u5148\u8fdb\u6846\u67b6\uff0c\u4ecd\u96be\u4ee5\u590d\u73b0\u5df2\u77e5\u521b\u65b0\u3002", "conclusion": "\u8be5\u57fa\u51c6\u4e3a\u8861\u91cfLLM\u81ea\u52a8\u5316\u79d1\u5b66\u590d\u73b0\u80fd\u529b\u63d0\u4f9b\u4e86\u7b80\u5355\u4e14\u672a\u9971\u548c\u7684\u6307\u6807\uff0c\u662f\u81ea\u4e3b\u7814\u7a76\u4ee3\u7406\u7684\u5fc5\u8981\u6280\u80fd\u4e4b\u4e00\u3002"}}
