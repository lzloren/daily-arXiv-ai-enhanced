<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 51]
- [cs.CL](#cs.CL) [Total: 81]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [MAIA: A Collaborative Medical AI Platform for Integrated Healthcare Innovation](https://arxiv.org/abs/2507.19489)
*Simone Bendazzoli,Sanna Persson,Mehdi Astaraki,Sebastian Pettersson,Vitali Grozman,Rodrigo Moreno*

Main category: cs.AI

TL;DR: MAIA是一个开源平台，旨在通过模块化和可扩展的环境促进AI在临床工作流中的应用，支持数据管理、模型开发和部署。


<details>
  <summary>Details</summary>
Motivation: 解决AI技术与实际医疗应用之间的协作问题，加速AI研究成果向临床解决方案的转化。

Method: 基于Kubernetes构建的模块化平台，提供数据管理、模型开发、注释、部署和临床反馈工具。

Result: MAIA已在学术和临床环境中成功部署，支持医学影像AI的实际用例。

Conclusion: MAIA通过促进协作和互操作性，推动AI研究的临床转化，同时强调可重复性、透明性和用户中心设计。

Abstract: The integration of Artificial Intelligence (AI) into clinical workflows
requires robust collaborative platforms that are able to bridge the gap between
technical innovation and practical healthcare applications. This paper
introduces MAIA (Medical Artificial Intelligence Assistant), an open-source
platform designed to facilitate interdisciplinary collaboration among
clinicians, researchers, and AI developers. Built on Kubernetes, MAIA offers a
modular, scalable environment with integrated tools for data management, model
development, annotation, deployment, and clinical feedback. Key features
include project isolation, CI/CD automation, integration with high-computing
infrastructures and in clinical workflows. MAIA supports real-world use cases
in medical imaging AI, with deployments in both academic and clinical
environments. By promoting collaborations and interoperability, MAIA aims to
accelerate the translation of AI research into impactful clinical solutions
while promoting reproducibility, transparency, and user-centered design. We
showcase the use of MAIA with different projects, both at KTH Royal Institute
of Technology and Karolinska University Hospital.

</details>


### [2] [Agent WARPP: Workflow Adherence via Runtime Parallel Personalization](https://arxiv.org/abs/2507.19543)
*Maria Emilia Mazzolenis,Ruirui Zhang*

Main category: cs.AI

TL;DR: WARPP是一个无需训练的模块化框架，通过多智能体编排和运行时个性化提升LLM在任务导向对话中的工作流遵循能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长条件工作流中因外部工具调用和用户特定信息依赖而表现不佳的问题。

Method: 结合多智能体编排和运行时个性化，动态修剪条件分支以减少推理开销和工具选择范围。

Result: 在银行、航班和医疗三个领域的复杂用户意图测试中，WARPP在参数保真度、工具准确性和令牌使用效率上优于非个性化和ReAct基线。

Conclusion: WARPP无需额外训练即可显著提升LLM在复杂工作流中的表现，同时降低资源消耗。

Abstract: Large language models (LLMs) are increasingly applied in task-oriented
dialogue (TOD) systems but often struggle with long, conditional workflows that
involve external tool calls and depend on user-specific information. We present
Workflow Adherence via Runtime Parallel Personalization, or WARPP, a
training-free, modular framework that combines multi-agent orchestration with
runtime personalization to improve workflow adherence in LLM-based systems. By
dynamically pruning conditional branches based on user attributes, the
framework reduces reasoning overhead and narrows tool selection at runtime.
WARPP deploys a parallelized architecture where a dedicated Personalizer agent
operates alongside modular, domain-specific agents to dynamically tailor
execution paths in real time. The framework is evaluated across five
representative user intents of varying complexity within three domains:
banking, flights, and healthcare. Our evaluation leverages synthetic datasets
and LLM-powered simulated users to test scenarios with conditional
dependencies. Our results demonstrate that WARPP outperforms both the
non-personalized method and the ReAct baseline, achieving increasingly larger
gains in parameter fidelity and tool accuracy as intent complexity grows, while
also reducing average token usage, without any additional training.

</details>


### [3] [Hypergames: Modeling Misaligned Perceptions and Nested Beliefs for Multi-agent Systems](https://arxiv.org/abs/2507.19593)
*Vince Trencsenyi,Agnieszka Mensfelt,Kostas Stathis*

Main category: cs.AI

TL;DR: 本文综述了超博弈理论在动态多智能体系统中的应用，分析了44项研究，提出了智能体兼容性标准，并指出了研究空白与未来方向。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设理性、完全信息和共同知识，而现实中的多智能体系统常存在不确定性和认知差异。超博弈理论通过建模主观感知来弥补这些不足。

Method: 系统回顾了超博弈理论的应用，提出智能体兼容性标准和分类框架，分析整合模式和实践适用性。

Result: 研究发现分层和图模型在欺骗推理中占主导，但HNF模型应用有限，且缺乏形式化语言。

Conclusion: 本文为超博弈理论在动态多智能体环境中的应用提供了新路线图，强调了未来研究方向。

Abstract: Classical game-theoretic models typically assume rational agents, complete
information, and common knowledge of payoffs - assumptions that are often
violated in real-world MAS characterized by uncertainty, misaligned
perceptions, and nested beliefs. To overcome these limitations, researchers
have proposed extensions that incorporate models of cognitive constraints,
subjective beliefs, and heterogeneous reasoning. Among these, hypergame theory
extends the classical paradigm by explicitly modeling agents' subjective
perceptions of the strategic scenario, known as perceptual games, in which
agents may hold divergent beliefs about the structure, payoffs, or available
actions. We present a systematic review of agent-compatible applications of
hypergame theory, examining how its descriptive capabilities have been adapted
to dynamic and interactive MAS contexts. We analyze 44 selected studies from
cybersecurity, robotics, social simulation, communications, and general
game-theoretic modeling. Building on a formal introduction to hypergame theory
and its two major extensions - hierarchical hypergames and HNF - we develop
agent-compatibility criteria and an agent-based classification framework to
assess integration patterns and practical applicability. Our analysis reveals
prevailing tendencies, including the prevalence of hierarchical and graph-based
models in deceptive reasoning and the simplification of extensive theoretical
frameworks in practical applications. We identify structural gaps, including
the limited adoption of HNF-based models, the lack of formal hypergame
languages, and unexplored opportunities for modeling human-agent and
agent-agent misalignment. By synthesizing trends, challenges, and open research
directions, this review provides a new roadmap for applying hypergame theory to
enhance the realism and effectiveness of strategic modeling in dynamic
multi-agent environments.

</details>


### [4] [DeltaLLM: A Training-Free Framework Exploiting Temporal Sparsity for Efficient Edge LLM Inference](https://arxiv.org/abs/2507.19608)
*Jiawen Qi,Chang Gao,Zhaochun Ren,Qinyu Chen*

Main category: cs.AI

TL;DR: DeltaLLM是一个无需训练的框架，通过利用注意力模式的时间稀疏性，在资源受限的边缘设备上实现高效的大型语言模型推理。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLM）在边缘设备上的部署面临计算复杂度随序列长度二次增长的问题，现有动态注意力剪枝方法不适合边缘场景。

Method: DeltaLLM提出了一种基于时间稀疏性的delta矩阵构建策略和上下文感知的混合注意力机制，结合局部上下文窗口内的完整注意力和窗口外的delta近似。

Result: 在BitNet和Llama模型上，DeltaLLM实现了高达60%的注意力稀疏性，同时保持或略微提升任务准确性。

Conclusion: DeltaLLM为边缘设备上的高效LLM部署提供了无需微调的解决方案，且能无缝集成到现有推理流程中。

Abstract: Deploying Large Language Models (LLMs) on edge devices remains challenging
due to their quadratically increasing computations with the sequence length.
Existing studies for dynamic attention pruning are designed for hardware with
massively parallel computation capabilities, such as GPUs or TPUs, and aim at
long context lengths (e.g., 64K), making them unsuitable for edge scenarios. We
present DeltaLLM, a training-free framework that exploits temporal sparsity in
attention patterns to enable efficient LLM inference across both the prefilling
and decoding stages, on resource-constrained edge devices. DeltaLLM introduces
an accuracy- and memory-aware delta matrix construction strategy that
introduces temporal sparsity, and a context-aware hybrid attention mechanism
that combines full attention in a local context window with delta approximation
outside it to increase accuracy. We evaluate our framework on the
edge-device-friendly BitNet-b1.58-2B-4T model and Llama3.2-1B-Instruct model
across diverse language tasks. The results show that on BitNet, our framework
increases the attention sparsity from 0% to 60% during the prefilling stage
with slight accuracy improvement on the WG task, and 0% to 57% across both the
prefilling and decoding stages, with even higher F1 score from 29.63 to 30.97
on SQuAD-v2 task. On the Llama model, it can also achieve up to 60% sparsity
during the prefilling stage and around 57% across both stages with negligible
accuracy drop. These results demonstrate that DeltaLLM offers a promising
solution for efficient edge deployment, requiring no fine-tuning and seamlessly
integrating with existing inference pipelines.

</details>


### [5] [Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges](https://arxiv.org/abs/2507.19672)
*Haoran Lu,Luyang Fang,Ruidong Zhang,Xinliang Li,Jiazhang Cai,Huimin Cheng,Lin Tang,Ziyu Liu,Zeliang Sun,Tao Wang,Yingchuan Zhang,Arif Hassan Zidan,Jinwen Xu,Jincheng Yu,Meizhi Yu,Hanqi Jiang,Xilin Gong,Weidi Luo,Bolun Sun,Yongkai Chen,Terry Ma,Shushan Wu,Yifan Zhou,Junhao Chen,Haotian Xiang,Jing Zhang,Afrar Jahin,Wei Ruan,Ke Deng,Yi Pan,Peilong Wang,Jiahui Li,Zhengliang Liu,Lu Zhang,Lin Zhao,Wei Liu,Dajiang Zhu,Xin Xing,Fei Dou,Wei Zhang,Chao Huang,Rongjie Liu,Mengrui Zhang,Yiwen Liu,Xiaoxiao Sun,Qin Lu,Zhen Xiang,Wenxuan Zhong,Tianming Liu,Ping Ma*

Main category: cs.AI

TL;DR: 本文综述了大语言模型（LLM）对齐人类价值观和意图的实用技术、训练协议和实证发现，分析了不同范式下的对齐方法，并讨论了前沿技术和现有评估框架的局限性。


<details>
  <summary>Details</summary>
Motivation: 随着大语言模型能力的提升和社会影响的扩大，确保其与人类价值观和意图的对齐成为关键挑战。

Method: 分析不同范式下的对齐方法，包括监督微调、基于偏好的方法，以及前沿技术如DPO、Constitutional AI等。

Result: 研究表明，监督微调能实现基本指令跟随，而基于偏好的方法更灵活地适应复杂人类意图。

Conclusion: 总结了当前实践策略，并提出了监督、价值多元性、鲁棒性和持续对齐等开放问题。

Abstract: Due to the remarkable capabilities and growing impact of large language
models (LLMs), they have been deeply integrated into many aspects of society.
Thus, ensuring their alignment with human values and intentions has emerged as
a critical challenge. This survey provides a comprehensive overview of
practical alignment techniques, training protocols, and empirical findings in
LLM alignment. We analyze the development of alignment methods across diverse
paradigms, characterizing the fundamental trade-offs between core alignment
objectives. Our analysis shows that while supervised fine-tuning enables basic
instruction-following, preference-based methods offer more flexibility for
aligning with nuanced human intent. We discuss state-of-the-art techniques,
including Direct Preference Optimization (DPO), Constitutional AI,
brain-inspired methods, and alignment uncertainty quantification (AUQ),
highlighting their approaches to balancing quality and efficiency. We review
existing evaluation frameworks and benchmarking datasets, emphasizing
limitations such as reward misspecification, distributional robustness, and
scalable oversight. We summarize strategies adopted by leading AI labs to
illustrate the current state of practice. We conclude by outlining open
problems in oversight, value pluralism, robustness, and continuous alignment.
This survey aims to inform both researchers and practitioners navigating the
evolving landscape of LLM alignment.

</details>


### [6] [The wall confronting large language models](https://arxiv.org/abs/2507.19703)
*Peter V. Coveney,Sauro Succi*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的性能受限于其预测不确定性的提升能力，难以满足科学研究的可靠性标准。其学习机制可能导致错误积累和信息灾难，而数据规模的扩大进一步加剧了虚假相关性。避免这种退化路径需更重视问题结构特性的理解。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM在提升预测可靠性方面的局限性，揭示其学习机制可能导致的错误积累和信息灾难，并提出避免退化路径的方法。

Method: 分析LLM的缩放规律及其对预测不确定性的影响，结合数据规模与虚假相关性的关系，探讨学习与准确性之间的张力。

Result: LLM的缩放规律限制了其预测可靠性的提升，学习机制可能导致错误积累和信息灾难，数据规模的扩大加剧了虚假相关性。

Conclusion: 为避免LLM的退化路径，需更重视对问题结构特性的深入理解和洞察。

Abstract: We show that the scaling laws which determine the performance of large
language models (LLMs) severely limit their ability to improve the uncertainty
of their predictions. As a result, raising their reliability to meet the
standards of scientific inquiry is intractable by any reasonable measure. We
argue that the very mechanism which fuels much of the learning power of LLMs,
namely the ability to generate non-Gaussian output distributions from Gaussian
input ones, might well be at the roots of their propensity to produce error
pileup, ensuing information catastrophes and degenerative AI behaviour. This
tension between learning and accuracy is a likely candidate mechanism
underlying the observed low values of the scaling components. It is
substantially compounded by the deluge of spurious correlations pointed out by
Calude and Longo which rapidly increase in any data set merely as a function of
its size, regardless of its nature. The fact that a degenerative AI pathway is
a very probable feature of the LLM landscape does not mean that it must
inevitably arise in all future AI research. Its avoidance, which we also
discuss in this paper, necessitates putting a much higher premium on insight
and understanding of the structural characteristics of the problems being
investigated.

</details>


### [7] [Minding Motivation: The Effect of Intrinsic Motivation on Agent Behaviors](https://arxiv.org/abs/2507.19725)
*Leonardo Villalobos-Arias,Grant Forbes,Jianxun Wang,David L Roberts,Arnav Jhala*

Main category: cs.AI

TL;DR: 论文研究了内在动机（IM）方法在强化学习（RL）中因奖励稀疏性导致的‘奖励黑客’问题，并通过实验评估了三种IM技术对行为的影响。


<details>
  <summary>Details</summary>
Motivation: 游戏中的奖励稀疏性使得RL代理难以学习，IM方法通过引入探索奖励来解决这一问题，但可能导致代理优化新奖励而非正确玩游戏。

Method: 在MiniGrid环境中比较了三种IM技术与广义奖励匹配（GRM）方法，GRM用于保证最优性。

Result: IM显著改变了代理的行为，增加了初始奖励，但也改变了游戏方式；GRM在某些情况下缓解了奖励黑客问题。

Conclusion: IM确实影响代理行为，GRM是一种潜在解决方案，但需进一步研究。

Abstract: Games are challenging for Reinforcement Learning~(RL) agents due to their
reward-sparsity, as rewards are only obtainable after long sequences of
deliberate actions. Intrinsic Motivation~(IM) methods -- which introduce
exploration rewards -- are an effective solution to reward-sparsity. However,
IM also causes an issue known as `reward hacking' where the agent optimizes for
the new reward at the expense of properly playing the game. The larger problem
is that reward hacking itself is largely unknown; there is no answer to
whether, and to what extent, IM rewards change the behavior of RL agents. This
study takes a first step by empirically evaluating the impact on behavior of
three IM techniques on the MiniGrid game-like environment. We compare these IM
models with Generalized Reward Matching~(GRM), a method that can be used with
any intrinsic reward function to guarantee optimality. Our results suggest that
IM causes noticeable change by increasing the initial rewards, but also
altering the way the agent plays; and that GRM mitigated reward hacking in some
scenarios.

</details>


### [8] [HypKG: Hypergraph-based Knowledge Graph Contextualization for Precision Healthcare](https://arxiv.org/abs/2507.19726)
*Yuzhang Xie,Xu Han,Ran Xu,Xiao Hu,Jiaying Lu,Carl Yang*

Main category: cs.AI

TL;DR: HypKG框架通过整合电子健康记录（EHRs）到知识图谱（KGs）中，生成上下文知识表示，提升医疗预测准确性。


<details>
  <summary>Details</summary>
Motivation: 医疗领域对知识准确性和数据互联性要求高，但现有KGs缺乏患者特定上下文信息，而EHRs提供了丰富的个人数据。

Method: 采用实体链接技术连接KGs与EHRs，利用超图模型和超图变换器学习上下文化表示。

Result: 实验表明，HypKG在医疗预测任务中显著提升性能，并能优化KGs的实体和关系表示。

Conclusion: HypKG通过整合EHRs上下文，提升了KGs的实用性和医疗预测的准确性。

Abstract: Knowledge graphs (KGs) are important products of the semantic web, which are
widely used in various application domains. Healthcare is one of such domains
where KGs are intensively used, due to the high requirement for knowledge
accuracy and interconnected nature of healthcare data. However, KGs storing
general factual information often lack the ability to account for important
contexts of the knowledge such as the status of specific patients, which are
crucial in precision healthcare. Meanwhile, electronic health records (EHRs)
provide rich personal data, including various diagnoses and medications, which
provide natural contexts for general KGs. In this paper, we propose HypKG, a
framework that integrates patient information from EHRs into KGs to generate
contextualized knowledge representations for accurate healthcare predictions.
Using advanced entity-linking techniques, we connect relevant knowledge from
general KGs with patient information from EHRs, and then utilize a hypergraph
model to "contextualize" the knowledge with the patient information. Finally,
we employ hypergraph transformers guided by downstream prediction tasks to
jointly learn proper contextualized representations for both KGs and patients,
fully leveraging existing knowledge in KGs and patient contexts in EHRs. In
experiments using a large biomedical KG and two real-world EHR datasets, HypKG
demonstrates significant improvements in healthcare prediction tasks across
multiple evaluation metrics. Additionally, by integrating external contexts,
HypKG can learn to adjust the representations of entities and relations in KG,
potentially improving the quality and real-world utility of knowledge.

</details>


### [9] [Integrating Activity Predictions in Knowledge Graphs](https://arxiv.org/abs/2507.19733)
*Alec Scully,Cameron Stockton,Forrest Hare*

Main category: cs.AI

TL;DR: 论文提出利用本体结构知识图谱预测未来事件，结合BFO和CCO语义框架组织数据，通过马尔可夫链建模预测，并改进概率模型。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用本体结构知识图谱提升对未来事件的预测能力，同时解决现有概率模型的局限性。

Method: 使用BFO和CCO语义框架组织数据，构建知识图谱并查询结果，通过马尔可夫链建模预测未来状态，引入“时空实例”完善语义结构。

Result: 提出基于马尔可夫链的概率计算方法，改进概率模型，并将其无缝集成回知识图谱。

Conclusion: 本体结构知识图谱结合改进的概率模型能有效支持预测分析和决策。

Abstract: We argue that ontology-structured knowledge graphs can play a crucial role in
generating predictions about future events. By leveraging the semantic
framework provided by Basic Formal Ontology (BFO) and Common Core Ontologies
(CCO), we demonstrate how data such as the movements of a fishing vessel can be
organized in and retrieved from a knowledge graph. These query results are then
used to create Markov chain models, allowing us to predict future states based
on the vessel's history. To fully support this process, we introduce the term
`spatiotemporal instant' to complete the necessary structural semantics.
Additionally, we critique the prevailing ontological model of probability,
which conflates probability with likelihood and relies on the problematic
concept of modal measurements: measurements of future entities. We propose an
alternative view, where probabilities are treated as being about process
profiles, which better captures the dynamics of real world phenomena. Finally,
we demonstrate how our Markov chain based probability calculations can be
seamlessly integrated back into the knowledge graph, enabling further analysis
and decision-making. Keywords: predictive analytics, ontology, Markov chains,
probability, Basic Formal Ontology (BFO), knowledge graphs, SPARQL.

</details>


### [10] [Can LLMs Solve ASP Problems? Insights from a Benchmarking Study (Extended Version)](https://arxiv.org/abs/2507.19749)
*Lin Ren,Guohui Xiao,Guilin Qi,Yishuai Geng,Haohan Xue*

Main category: cs.AI

TL;DR: ASPBench是一个针对ASP（Answer Set Programming）的全面基准测试，包含三个任务：ASP蕴含、答案集验证和答案集计算。评估发现，尽管LLMs在前两个简单任务上表现良好，但在核心的答案集计算任务上表现不佳。


<details>
  <summary>Details</summary>
Motivation: 当前对LLMs在ASP中的能力评估有限，缺乏支持复杂ASP特性的基准测试，因此需要更全面的评估工具。

Method: 引入ASPBench，包含三个ASP特定任务，并对14个最先进的LLMs进行评估。

Result: LLMs在ASP蕴含和答案集验证任务上表现较好，但在答案集计算任务上表现不佳。

Conclusion: LLMs在ASP解决中的能力仍有局限，需要更有效的符号推理集成方法。

Abstract: Answer Set Programming (ASP) is a powerful paradigm for non-monotonic
reasoning. Recently, large language models (LLMs) have demonstrated promising
capabilities in logical reasoning. Despite this potential, current evaluations
of LLM capabilities in ASP are often limited. Existing works normally employ
overly simplified ASP programs, do not support negation, disjunction, or
multiple answer sets. Furthermore, there is a lack of benchmarks that introduce
tasks specifically designed for ASP solving. To bridge this gap, we introduce
ASPBench, a comprehensive ASP benchmark, including three ASP specific tasks:
ASP entailment, answer set verification, and answer set computation. Our
extensive evaluations on ASPBench reveal that while 14 state-of-the-art LLMs,
including \emph{deepseek-r1}, \emph{o4-mini}, and
\emph{gemini-2.5-flash-thinking}, perform relatively well on the first two
simpler tasks, they struggle with answer set computation, which is the core of
ASP solving. These findings offer insights into the current limitations of LLMs
in ASP solving. This highlights the need for new approaches that integrate
symbolic reasoning capabilities more effectively. The code and dataset are
available at https://github.com/HomuraT/ASPBench.

</details>


### [11] [Reinforcement Learning for Multi-Objective Multi-Echelon Supply Chain Optimisation](https://arxiv.org/abs/2507.19788)
*Rifny Rachman,Josh Tingey,Richard Allmendinger,Pradyumn Shukla,Wei Pan*

Main category: cs.AI

TL;DR: 该研究开发了一个基于马尔可夫决策过程的广义多目标、多层次供应链优化模型，结合经济、环境和社会因素，并通过多目标强化学习方法进行评估。


<details>
  <summary>Details</summary>
Motivation: 解决非稳态市场中供应链优化问题，同时平衡经济、环境和社会目标。

Method: 使用多目标强化学习（RL）方法，并与改进的单目标RL算法和多目标进化算法（MOEA）进行对比。

Result: 主要方法在最优性、多样性和密度之间实现了最平衡的权衡，在复杂场景中表现优于MOEA和改进的单目标RL方法。

Conclusion: 该方法在复杂供应链网络中表现出更高的鲁棒性和稳定性，同时最小化需求损失。

Abstract: This study develops a generalised multi-objective, multi-echelon supply chain
optimisation model with non-stationary markets based on a Markov decision
process, incorporating economic, environmental, and social considerations. The
model is evaluated using a multi-objective reinforcement learning (RL) method,
benchmarked against an originally single-objective RL algorithm modified with
weighted sum using predefined weights, and a multi-objective evolutionary
algorithm (MOEA)-based approach. We conduct experiments on varying network
complexities, mimicking typical real-world challenges using a customisable
simulator. The model determines production and delivery quantities across
supply chain routes to achieve near-optimal trade-offs between competing
objectives, approximating Pareto front sets. The results demonstrate that the
primary approach provides the most balanced trade-off between optimality,
diversity, and density, further enhanced with a shared experience buffer that
allows knowledge transfer among policies. In complex settings, it achieves up
to 75\% higher hypervolume than the MOEA-based method and generates solutions
that are approximately eleven times denser, signifying better robustness, than
those produced by the modified single-objective RL method. Moreover, it ensures
stable production and inventory levels while minimising demand loss.

</details>


### [12] [Causality-aligned Prompt Learning via Diffusion-based Counterfactual Generation](https://arxiv.org/abs/2507.19882)
*Xinshu Li,Ruoyu Wang,Erdun Gao,Mingming Gong,Lina Yao*

Main category: cs.AI

TL;DR: 论文提出了一种基于扩散的因果反事实提示学习框架DiCap，通过理论推导和对比学习生成因果不变提示，显著提升了跨类别任务的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有提示学习方法因缺乏理论支持，难以生成因果不变的提示，导致泛化能力不足。

Method: DiCap利用扩散过程从因果模型的边际和条件分布中迭代采样梯度，生成满足最小充分性准则的反事实提示，并结合对比学习框架优化提示提取。

Result: 实验表明，DiCap在图像分类、图文检索和视觉问答等任务中表现优异，尤其在未见类别上优势显著。

Conclusion: DiCap通过理论驱动的反事实提示生成和对比学习，显著提升了提示学习的泛化性和鲁棒性。

Abstract: Prompt learning has garnered attention for its efficiency over traditional
model training and fine-tuning. However, existing methods, constrained by
inadequate theoretical foundations, encounter difficulties in achieving
causally invariant prompts, ultimately falling short of capturing robust
features that generalize effectively across categories. To address these
challenges, we introduce the $\textit{\textbf{DiCap}}$ model, a theoretically
grounded $\textbf{Di}$ffusion-based $\textbf{C}$ounterf$\textbf{a}$ctual
$\textbf{p}$rompt learning framework, which leverages a diffusion process to
iteratively sample gradients from the marginal and conditional distributions of
the causal model, guiding the generation of counterfactuals that satisfy the
minimal sufficiency criterion. Grounded in rigorous theoretical derivations,
this approach guarantees the identifiability of counterfactual outcomes while
imposing strict bounds on estimation errors. We further employ a contrastive
learning framework that leverages the generated counterfactuals, thereby
enabling the refined extraction of prompts that are precisely aligned with the
causal features of the data. Extensive experimental results demonstrate that
our method performs excellently across tasks such as image classification,
image-text retrieval, and visual question answering, with particularly strong
advantages in unseen categories.

</details>


### [13] [What Does 'Human-Centred AI' Mean?](https://arxiv.org/abs/2507.19960)
*Olivia Guest*

Main category: cs.AI

TL;DR: 论文探讨了以人为中心的人工智能（AI）本质上是技术与人类认知的关系，分析了AI对人类认知劳动的替代、增强或取代，并指出忽视认知会导致AI设计的扭曲。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI与人类认知的关系，强调AI设计必须真正以人为中心，避免认知科学的扭曲。

Method: 通过对比技术与人认知劳动的案例（如算盘与心算、闹钟与敲门人等），提出新的定义和分析框架。

Result: AI对人类认知劳动的影响可分为有害的替代、有益的增强或中性的取代，忽视认知会限制AI设计的有效性。

Conclusion: 结论是AI必须直面人类认知，才能真正实现以人为中心的设计，避免认知科学的扭曲。

Abstract: While it seems sensible that human-centred artificial intelligence (AI) means
centring "human behaviour and experience," it cannot be any other way. AI, I
argue, is usefully seen as a relationship between technology and humans where
it appears that artifacts can perform, to a greater or lesser extent, human
cognitive labour. This is evinced using examples that juxtapose technology with
cognition, inter alia: abacus versus mental arithmetic; alarm clock versus
knocker-upper; camera versus vision; and sweatshop versus tailor. Using novel
definitions and analyses, sociotechnical relationships can be analysed into
varying types of: displacement (harmful), enhancement (beneficial), and/or
replacement (neutral) of human cognitive labour. Ultimately, all AI implicates
human cognition; no matter what. Obfuscation of cognition in the AI context --
from clocks to artificial neural networks -- results in distortion, in slowing
critical engagement, perverting cognitive science, and indeed in limiting our
ability to truly centre humans and humanity in the engineering of AI systems.
To even begin to de-fetishise AI, we must look the human-in-the-loop in the
eyes.

</details>


### [14] [Leveraging Fine-Tuned Large Language Models for Interpretable Pancreatic Cystic Lesion Feature Extraction and Risk Categorization](https://arxiv.org/abs/2507.19973)
*Ebrahim Rasromani,Stella K. Kang,Yanqi Xu,Beisong Liu,Garvit Luhadia,Wan Fung Chui,Felicia L. Pasadyn,Yu Chih Hung,Julie Y. An,Edwin Mathieu,Zehui Gu,Carlos Fernandez-Granda,Ammar A. Javed,Greg D. Sacks,Tamas Gonda,Chenchan Huang,Yiqiu Shen*

Main category: cs.AI

TL;DR: 使用大型语言模型（LLM）自动从MRI/CT报告中提取胰腺囊性病变（PCL）特征并分类风险，性能接近GPT-4o。


<details>
  <summary>Details</summary>
Motivation: 手动提取PCL特征耗时，限制了大规模研究需求。

Method: 利用GPT-4o生成的链式思维（CoT）数据微调开源LLM，评估特征提取和风险分类性能。

Result: 微调后LLM特征提取准确率提升至97-98%，风险分类F1分数达0.94-0.95，与GPT-4o相当。

Conclusion: 微调开源LLM结合CoT监督可实现高效、准确的PCL表型分析，性能媲美GPT-4o。

Abstract: Background: Manual extraction of pancreatic cystic lesion (PCL) features from
radiology reports is labor-intensive, limiting large-scale studies needed to
advance PCL research. Purpose: To develop and evaluate large language models
(LLMs) that automatically extract PCL features from MRI/CT reports and assign
risk categories based on guidelines. Materials and Methods: We curated a
training dataset of 6,000 abdominal MRI/CT reports (2005-2024) from 5,134
patients that described PCLs. Labels were generated by GPT-4o using
chain-of-thought (CoT) prompting to extract PCL and main pancreatic duct
features. Two open-source LLMs were fine-tuned using QLoRA on GPT-4o-generated
CoT data. Features were mapped to risk categories per institutional guideline
based on the 2017 ACR White Paper. Evaluation was performed on 285 held-out
human-annotated reports. Model outputs for 100 cases were independently
reviewed by three radiologists. Feature extraction was evaluated using exact
match accuracy, risk categorization with macro-averaged F1 score, and
radiologist-model agreement with Fleiss' Kappa. Results: CoT fine-tuning
improved feature extraction accuracy for LLaMA (80% to 97%) and DeepSeek (79%
to 98%), matching GPT-4o (97%). Risk categorization F1 scores also improved
(LLaMA: 0.95; DeepSeek: 0.94), closely matching GPT-4o (0.97), with no
statistically significant differences. Radiologist inter-reader agreement was
high (Fleiss' Kappa = 0.888) and showed no statistically significant difference
with the addition of DeepSeek-FT-CoT (Fleiss' Kappa = 0.893) or GPT-CoT
(Fleiss' Kappa = 0.897), indicating that both models achieved agreement levels
on par with radiologists. Conclusion: Fine-tuned open-source LLMs with CoT
supervision enable accurate, interpretable, and efficient phenotyping for
large-scale PCL research, achieving performance comparable to GPT-4o.

</details>


### [15] [Digital Twin Channel-Enabled Online Resource Allocation for 6G: Principle, Architecture and Application](https://arxiv.org/abs/2507.19974)
*Tongjie Li,Jianhua Zhang,Li Yu,Yuxiang Zhang,Yunlong Cai,Fan Xu,Guangyi Liu*

Main category: cs.AI

TL;DR: 提出了一种基于数字孪生信道（DTC）的在线优化框架，用于6G网络中高效、低延迟的资源分配。


<details>
  <summary>Details</summary>
Motivation: 6G网络中新兴应用（如全息通信、自动驾驶和工业物联网）对灵活、低延迟和可靠的资源分配提出了严格要求，传统方法在动态环境中可能无法达到最优性能。

Method: 利用DTC预测信道状态信息（CSI），并结合轻量级博弈论算法进行在线资源分配。

Result: 仿真结果显示，该方法比基于导频的理想CSI方案吞吐量提升高达11.5%。

Conclusion: 该方法为未来6G网络提供了可扩展、低开销且环境感知的通信解决方案。

Abstract: Emerging applications such as holographic communication, autonomous driving,
and the industrial Internet of Things impose stringent requirements on
flexible, low-latency, and reliable resource allocation in 6G networks.
Conventional methods, which rely on statistical modeling, have proven effective
in general contexts but may fail to achieve optimal performance in specific and
dynamic environments. Furthermore, acquiring real-time channel state
information (CSI) typically requires excessive pilot overhead. To address these
challenges, a digital twin channel (DTC)-enabled online optimization framework
is proposed, in which DTC is employed to predict CSI based on environmental
sensing. The predicted CSI is then utilized by lightweight game-theoretic
algorithms to perform online resource allocation in a timely and efficient
manner. Simulation results based on a digital replica of a realistic industrial
workshop demonstrate that the proposed method achieves throughput improvements
of up to 11.5\% compared with pilot-based ideal CSI schemes, validating its
effectiveness for scalable, low-overhead, and environment-aware communication
in future 6G networks.

</details>


### [16] [Matching Game Preferences Through Dialogical Large Language Models: A Perspective](https://arxiv.org/abs/2507.20000)
*Renaud Fabre,Daniel Egret,Patrice Bellot*

Main category: cs.AI

TL;DR: 论文探讨了通过结合大型语言模型（LLMs）和GRAPHYP网络系统来提升对话智能的潜力，提出了一种透明、可追溯的AI推理框架。


<details>
  <summary>Details</summary>
Motivation: 旨在使AI推理过程透明化，让用户理解AI如何得出结论，从而提升AI的可信度和实用性。

Method: 提出了一个概念框架D-LLMs，包含三个组件：推理过程、用户偏好分类系统和对话方法。

Result: 框架设想通过结构化对话实现用户偏好嵌入，使AI决策更个性化且可解释。

Conclusion: 目标是开发透明、可信的AI系统，帮助用户理解AI决策过程，促进人机协作。

Abstract: This perspective paper explores the future potential of "conversational
intelligence" by examining how Large Language Models (LLMs) could be combined
with GRAPHYP's network system to better understand human conversations and
preferences. Using recent research and case studies, we propose a conceptual
framework that could make AI rea-soning transparent and traceable, allowing
humans to see and understand how AI reaches its conclusions. We present the
conceptual perspective of "Matching Game Preferences through Dialogical Large
Language Models (D-LLMs)," a proposed system that would allow multiple users to
share their different preferences through structured conversations. This
approach envisions personalizing LLMs by embedding individual user preferences
directly into how the model makes decisions. The proposed D-LLM framework would
require three main components: (1) reasoning processes that could analyze
different search experiences and guide performance, (2) classification systems
that would identify user preference patterns, and (3) dialogue approaches that
could help humans resolve conflicting information. This perspective framework
aims to create an interpretable AI system where users could examine,
understand, and combine the different human preferences that influence AI
responses, detected through GRAPHYP's search experience networks. The goal of
this perspective is to envision AI systems that would not only provide answers
but also show users how those answers were reached, making artificial
intelligence more transparent and trustworthy for human decision-making.

</details>


### [17] [Finding Personalized Good-Enough Solutions to Unsatisfiable Stable Roommates Problems](https://arxiv.org/abs/2507.20010)
*Müge Fidan,Esra Erdem*

Main category: cs.AI

TL;DR: 论文研究了稳定室友问题，提出了一种基于代理习惯和偏好网络的个性化匹配方法，以解决无稳定解时的“足够好”匹配问题。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，稳定室友问题不一定总有解，因此需要找到“足够好”的匹配方案。

Method: 结合代理的习惯、偏好及其朋友网络，提出了一种生成个性化稳定室友问题解的方法。

Result: 通过示例和实证评估验证了方法的有效性。

Conclusion: 该方法为解决稳定室友问题提供了一种实用且个性化的解决方案。

Abstract: The Stable Roommates problems are characterized by the preferences of agents
over other agents as roommates. A solution is a partition of the agents into
pairs that are acceptable to each other (i.e., they are in the preference lists
of each other), and the matching is stable (i.e., there do not exist any two
agents who prefer each other to their roommates, and thus block the matching).
Motivated by real-world applications, and considering that stable roommates
problems do not always have solutions, we continue our studies to compute
"good-enough" matchings. In addition to the agents' habits and habitual
preferences, we consider their networks of preferred friends, and introduce a
method to generate personalized solutions to stable roommates problems. We
illustrate the usefulness of our method with examples and empirical
evaluations.

</details>


### [18] [PITA: Preference-Guided Inference-Time Alignment for LLM Post-Training](https://arxiv.org/abs/2507.20067)
*Sarat Chandra Bobbili,Ujwal Dinesha,Dheeraj Narasimha,Srinivas Shakkottai*

Main category: cs.AI

TL;DR: PITA框架通过直接整合偏好反馈到LLM的token生成中，消除了对预训练奖励模型的依赖，降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖预训练奖励模型，可能导致不稳定，PITA旨在直接利用偏好反馈优化LLM输出。

Method: PITA学习一个小型偏好引导策略，在推理时修改token概率，无需LLM微调，通过随机搜索和迭代优化实现。

Result: PITA在数学推理和情感分类等任务中有效对齐LLM输出与用户偏好。

Conclusion: PITA提供了一种高效、无需奖励模型的方法，直接优化LLM输出对齐用户偏好。

Abstract: Inference-time alignment enables large language models (LLMs) to generate
outputs aligned with end-user preferences without further training. Recent
post-training methods achieve this by using small guidance models to modify
token generation during inference. These methods typically optimize a reward
function KL-regularized by the original LLM taken as the reference policy. A
critical limitation, however, is their dependence on a pre-trained reward
model, which requires fitting to human preference feedback--a potentially
unstable process. In contrast, we introduce PITA, a novel framework that
integrates preference feedback directly into the LLM's token generation,
eliminating the need for a reward model. PITA learns a small preference-based
guidance policy to modify token probabilities at inference time without LLM
fine-tuning, reducing computational cost and bypassing the pre-trained reward
model dependency. The problem is framed as identifying an underlying preference
distribution, solved through stochastic search and iterative refinement of the
preference-based guidance model. We evaluate PITA across diverse tasks,
including mathematical reasoning and sentiment classification, demonstrating
its effectiveness in aligning LLM outputs with user preferences.

</details>


### [19] [Concept Learning for Cooperative Multi-Agent Reinforcement Learning](https://arxiv.org/abs/2507.20143)
*Zhonghan Ge,Yuanyang Zhu,Chunlin Chen*

Main category: cs.AI

TL;DR: 论文提出了一种基于概念瓶颈模型的可解释价值分解框架（CMQ），用于解决多智能体强化学习中透明度和互操作性问题，并通过实验验证其优越性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体强化学习中神经网络缺乏透明性和可解释性的问题，尤其是隐式合作机制的黑箱特性。

Method: 提出CMQ方法，通过学习可解释的合作概念，将每个概念表示为监督向量，并通过全局状态嵌入条件化个体动作值。

Result: 在StarCraft II和LBF任务中，CMQ表现优于现有方法，并能捕捉有意义的合作模式，支持概念干预以检测潜在偏差。

Conclusion: CMQ突破了性能与可解释性的权衡，为多智能体强化学习提供了更透明和可干预的框架。

Abstract: Despite substantial progress in applying neural networks (NN) to multi-agent
reinforcement learning (MARL) areas, they still largely suffer from a lack of
transparency and interoperability. However, its implicit cooperative mechanism
is not yet fully understood due to black-box networks. In this work, we study
an interpretable value decomposition framework via concept bottleneck models,
which promote trustworthiness by conditioning credit assignment on an
intermediate level of human-like cooperation concepts. To address this problem,
we propose a novel value-based method, named Concepts learning for Multi-agent
Q-learning (CMQ), that goes beyond the current performance-vs-interpretability
trade-off by learning interpretable cooperation concepts. CMQ represents each
cooperation concept as a supervised vector, as opposed to existing models where
the information flowing through their end-to-end mechanism is concept-agnostic.
Intuitively, using individual action value conditioning on global state
embeddings to represent each concept allows for extra cooperation
representation capacity. Empirical evaluations on the StarCraft II
micromanagement challenge and level-based foraging (LBF) show that CMQ achieves
superior performance compared with the state-of-the-art counterparts. The
results also demonstrate that CMQ provides more cooperation concept
representation capturing meaningful cooperation modes, and supports test-time
concept interventions for detecting potential biases of cooperation mode and
identifying spurious artifacts that impact cooperation.

</details>


### [20] [The Policy Cliff: A Theoretical Analysis of Reward-Policy Maps in Large Language Models](https://arxiv.org/abs/2507.20150)
*Xingcheng Xu*

Main category: cs.AI

TL;DR: 本文提出了一个数学框架，用于分析强化学习（RL）中奖励函数到最优策略的映射稳定性，解释了策略脆弱性的根源，并探讨了多奖励RL中的稳定性机制。


<details>
  <summary>Details</summary>
Motivation: 解决RL在大型语言和推理模型（LLMs/LRMs）中导致策略脆弱和不稳定的问题，如虚假推理、欺骗性对齐和指令不服从，这些问题缺乏统一的理论解释。

Method: 通过数学框架分析奖励函数到最优策略的映射稳定性，研究非唯一最优动作的影响，并扩展到多奖励RL中的稳定性机制，验证熵正则化的作用。

Result: 揭示了策略脆弱性源于非唯一最优动作，提出了“有效奖励”聚合机制，证明熵正则化可恢复稳定性但增加随机性。

Conclusion: 该框架为RL中的策略稳定性提供了统一的理论解释，为设计更安全、可信的AI系统提供了重要见解。

Abstract: Reinforcement learning (RL) plays a crucial role in shaping the behavior of
large language and reasoning models (LLMs/LRMs). However, it often produces
brittle and unstable policies, leading to critical failures such as spurious
reasoning, deceptive alignment, and instruction disobedience that undermine the
trustworthiness and safety of LLMs/LRMs. Currently, these issues lack a unified
theoretical explanation and are typically addressed using ad-hoc heuristics.
This paper presents a rigorous mathematical framework for analyzing the
stability of the mapping from a reward function to the optimal policy. We show
that policy brittleness often stems from non-unique optimal actions, a common
occurrence when multiple valid traces exist in a reasoning task. This
theoretical lens provides a unified explanation for a range of seemingly
disparate failures, reframing them as rational outcomes of optimizing rewards
that may be incomplete or noisy, especially in the presence of action
degeneracy. We extend this analysis from the fundamental single-reward setting
to the more realistic multi-reward RL across diverse domains, showing how
stability is governed by an "effective reward" aggregation mechanism. We also
prove that entropy regularization restores policy stability at the cost of
increased stochasticity. Our framework provides a unified explanation for
recent empirical findings on deceptive reasoning, instruction-following
trade-offs, and RLHF-induced sophistry, and is further validated through
perturbation experiments in multi-reward RL. This work advances
policy-stability analysis from empirical heuristics towards a principled
theory, offering essential insights for designing safer and more trustworthy AI
systems.

</details>


### [21] [StepFun-Prover Preview: Let's Think and Verify Step by Step](https://arxiv.org/abs/2507.20199)
*Shijie Shang,Ruosi Wan,Yue Peng,Yutong Wu,Xiong-hui Chen,Jie Yan,Xiangyu Zhang*

Main category: cs.AI

TL;DR: StepFun-Prover是一个用于形式化定理证明的大语言模型，通过工具集成推理实现高效Lean 4证明生成。


<details>
  <summary>Details</summary>
Motivation: 提升自动化定理证明的性能，模拟人类问题解决策略。

Method: 采用强化学习框架，结合工具交互，迭代优化证明。

Result: 在miniF2F-test基准测试中达到70.0%的pass@1成功率。

Conclusion: 提出了一种端到端训练框架，为自动化定理证明和数学AI助手提供了新方向。

Abstract: We present StepFun-Prover Preview, a large language model designed for formal
theorem proving through tool-integrated reasoning. Using a reinforcement
learning pipeline that incorporates tool-based interactions, StepFun-Prover can
achieve strong performance in generating Lean 4 proofs with minimal sampling.
Our approach enables the model to emulate human-like problem-solving strategies
by iteratively refining proofs based on real-time environment feedback. On the
miniF2F-test benchmark, StepFun-Prover achieves a pass@1 success rate of
$70.0\%$. Beyond advancing benchmark performance, we introduce an end-to-end
training framework for developing tool-integrated reasoning models, offering a
promising direction for automated theorem proving and Math AI assistant.

</details>


### [22] [Improving Subgraph Matching by Combining Algorithms and Graph Neural Networks](https://arxiv.org/abs/2507.20226)
*Shuyang Guo,Wenjin Xie,Ping Lu,Ting Deng,Richong Zhang,Jianxin Li,Xiangping Huang,Zhongyi Liu*

Main category: cs.AI

TL;DR: HFrame是一种基于图神经网络的子图同态框架，结合传统算法与机器学习技术，显著优于标准图神经网络，并在速度和准确性上表现优异。


<details>
  <summary>Details</summary>
Motivation: 子图同态问题比同构更复杂，传统方法效率低，需要结合机器学习提升性能。

Method: 提出HFrame框架，整合传统算法与图神经网络技术。

Result: HFrame比精确匹配算法快101.91倍，平均准确率达0.962，并能区分更多非同态图对。

Conclusion: HFrame在子图同态问题上表现出色，为图神经网络的应用提供了新方向。

Abstract: Homomorphism is a key mapping technique between graphs that preserves their
structure. Given a graph and a pattern, the subgraph homomorphism problem
involves finding a mapping from the pattern to the graph, ensuring that
adjacent vertices in the pattern are mapped to adjacent vertices in the graph.
Unlike subgraph isomorphism, which requires a one-to-one mapping, homomorphism
allows multiple vertices in the pattern to map to the same vertex in the graph,
making it more complex. We propose HFrame, the first graph neural network-based
framework for subgraph homomorphism, which integrates traditional algorithms
with machine learning techniques. We demonstrate that HFrame outperforms
standard graph neural networks by being able to distinguish more graph pairs
where the pattern is not homomorphic to the graph. Additionally, we provide a
generalization error bound for HFrame. Through experiments on both real-world
and synthetic graphs, we show that HFrame is up to 101.91 times faster than
exact matching algorithms and achieves an average accuracy of 0.962.

</details>


### [23] [A Multi-Agent System for Information Extraction from the Chemical Literature](https://arxiv.org/abs/2507.20230)
*Yufan Chen,Ching Ting Leung,Bowen Yu,Jianwei Sun,Yong Huang,Linyan Li,Hao Chen,Hanyu Gao*

Main category: cs.AI

TL;DR: 开发了一种基于多模态大语言模型（MLLM）的多智能体系统，用于自动提取化学信息，显著提升了复杂化学反应图形的提取性能。


<details>
  <summary>Details</summary>
Motivation: 高质量化学数据库是AI驱动化学研究的基石，但目前化学信息的多模态和风格多样性限制了自动提取的效率。

Method: 利用MLLM的强推理能力分解任务，协调多个专业智能体完成子任务，如分子图像识别、反应图像解析等。

Result: 在复杂化学反应图形的基准数据集上达到80.8%的F1分数，显著超越之前的最佳模型（35.6%）。

Conclusion: 该系统是实现化学信息自动提取为结构化数据集的关键进展，将极大推动AI驱动的化学研究。

Abstract: To fully expedite AI-powered chemical research, high-quality chemical
databases are the cornerstone. Automatic extraction of chemical information
from the literature is essential for constructing reaction databases, but it is
currently limited by the multimodality and style variability of chemical
information. In this work, we developed a multimodal large language model
(MLLM)-based multi-agent system for automatic chemical information extraction.
We used the MLLM's strong reasoning capability to understand the structure of
complex chemical graphics, decompose the extraction task into sub-tasks and
coordinate a set of specialized agents to solve them. Our system achieved an F1
score of 80.8% on a benchmark dataset of complex chemical reaction graphics
from the literature, surpassing the previous state-of-the-art model (F1 score:
35.6%) by a significant margin. Additionally, it demonstrated consistent
improvements in key sub-tasks, including molecular image recognition, reaction
image parsing, named entity recognition and text-based reaction extraction.
This work is a critical step toward automated chemical information extraction
into structured datasets, which will be a strong promoter of AI-driven chemical
research.

</details>


### [24] [SciToolAgent: A Knowledge Graph-Driven Scientific Agent for Multi-Tool Integration](https://arxiv.org/abs/2507.20280)
*Keyan Ding,Jing Yu,Junjie Huang,Yuchen Yang,Qiang Zhang,Huajun Chen*

Main category: cs.AI

TL;DR: SciToolAgent是一个基于LLM的代理，通过知识图谱和安全性检查自动化科学工具，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决科学工具使用中的领域专业知识需求问题，提升复杂科学工作流的自动化能力。

Method: 利用科学工具知识图谱进行智能工具选择和执行，结合检索增强生成和安全性检查模块。

Result: 在多个科学领域（如生物学、化学、材料科学）的评估中表现优异，并能自动化复杂工作流。

Conclusion: SciToolAgent为专家和非专家提供了高效、安全的科学工具自动化解决方案。

Abstract: Scientific research increasingly relies on specialized computational tools,
yet effectively utilizing these tools demands substantial domain expertise.
While Large Language Models (LLMs) show promise in tool automation, they
struggle to seamlessly integrate and orchestrate multiple tools for complex
scientific workflows. Here, we present SciToolAgent, an LLM-powered agent that
automates hundreds of scientific tools across biology, chemistry, and materials
science. At its core, SciToolAgent leverages a scientific tool knowledge graph
that enables intelligent tool selection and execution through graph-based
retrieval-augmented generation. The agent also incorporates a comprehensive
safety-checking module to ensure responsible and ethical tool usage. Extensive
evaluations on a curated benchmark demonstrate that SciToolAgent significantly
outperforms existing approaches. Case studies in protein engineering, chemical
reactivity prediction, chemical synthesis, and metal-organic framework
screening further demonstrate SciToolAgent's capability to automate complex
scientific workflows, making advanced research tools accessible to both experts
and non-experts.

</details>


### [25] [Artificial Intelligence In Patent And Market Intelligence: A New Paradigm For Technology Scouting](https://arxiv.org/abs/2507.20322)
*Manish Verma,Vivek Sharma,Vishal Singh*

Main category: cs.AI

TL;DR: 开发了一个基于AI的平台，利用大型语言模型（LLMs）改进工业研发中的技术搜索和解决方案发现。


<details>
  <summary>Details</summary>
Motivation: 传统方法耗时、依赖人工和领域专业知识，且信息来源分散，导致效率低下和洞察不完整。

Method: 平台利用LLMs的语义理解、上下文推理和跨领域知识提取能力，处理专利文本和市场数据，提取并分类解决方案。

Result: 平台减少了人工工作，加速了创新周期，并提升了复杂研发环境中的决策质量。

Conclusion: 该AI驱动的平台为工业研发提供了高效、全面的技术搜索和解决方案发现工具。

Abstract: This paper presents the development of an AI powered software platform that
leverages advanced large language models (LLMs) to transform technology
scouting and solution discovery in industrial R&D. Traditional approaches to
solving complex research and development challenges are often time consuming,
manually driven, and heavily dependent on domain specific expertise. These
methods typically involve navigating fragmented sources such as patent
repositories, commercial product catalogs, and competitor data, leading to
inefficiencies and incomplete insights. The proposed platform utilizes cutting
edge LLM capabilities including semantic understanding, contextual reasoning,
and cross-domain knowledge extraction to interpret problem statements and
retrieve high-quality, sustainable solutions. The system processes unstructured
patent texts, such as claims and technical descriptions, and systematically
extracts potential innovations aligned with the given problem context. These
solutions are then algorithmically organized under standardized technical
categories and subcategories to ensure clarity and relevance across
interdisciplinary domains. In addition to patent analysis, the platform
integrates commercial intelligence by identifying validated market solutions
and active organizations addressing similar challenges. This combined insight
sourced from both intellectual property and real world product data enables R&D
teams to assess not only technical novelty but also feasibility, scalability,
and sustainability. The result is a comprehensive, AI driven scouting engine
that reduces manual effort, accelerates innovation cycles, and enhances
decision making in complex R&D environments.

</details>


### [26] [The Blessing and Curse of Dimensionality in Safety Alignment](https://arxiv.org/abs/2507.20333)
*Rachel S. Y. Teo,Laziz U. Abdullaev,Tan M. Nguyen*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）高维度表示对安全对齐的双重影响，提出降维方法以减少越狱攻击的脆弱性。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在各领域的广泛应用，其高维度表示虽带来优势，但也可能被利用以绕过安全对齐，研究旨在解决这一问题。

Method: 通过可视化不同概念（如安全性）的线性子空间，并实验验证降维方法对减少越狱攻击的有效性。

Result: 降维方法显著降低了LLMs对表示工程越狱攻击的脆弱性，同时保留了足够的安全对齐信息。

Conclusion: 高维度表示既是优势也是挑战，降维是提升LLMs安全对齐的有效策略。

Abstract: The focus on safety alignment in large language models (LLMs) has increased
significantly due to their widespread adoption across different domains. The
scale of LLMs play a contributing role in their success, and the growth in
parameter count follows larger hidden dimensions. In this paper, we hypothesize
that while the increase in dimensions has been a key advantage, it may lead to
emergent problems as well. These problems emerge as the linear structures in
the activation space can be exploited, in the form of activation engineering,
to circumvent its safety alignment. Through detailed visualizations of linear
subspaces associated with different concepts, such as safety, across various
model scales, we show that the curse of high-dimensional representations
uniquely impacts LLMs. Further substantiating our claim, we demonstrate that
projecting the representations of the model onto a lower dimensional subspace
can preserve sufficient information for alignment while avoiding those linear
structures. Empirical results confirm that such dimensional reduction
significantly reduces susceptibility to jailbreaking through representation
engineering. Building on our empirical validations, we provide theoretical
insights into these linear jailbreaking methods relative to a model's hidden
dimensions. Broadly speaking, our work posits that the high dimensions of a
model's internal representations can be both a blessing and a curse in safety
alignment.

</details>


### [27] [VLMPlanner: Integrating Visual Language Models with Motion Planning](https://arxiv.org/abs/2507.20342)
*Zhipeng Tang,Sha Zhang,Jiajun Deng,Chenjie Wang,Guoliang You,Yuting Huang,Xinrui Lin,Yanyong Zhang*

Main category: cs.AI

TL;DR: VLMPlanner结合视觉语言模型（VLM）和实时规划器，通过多视角图像捕捉细节视觉信息，提升自动驾驶运动规划的鲁棒性和安全性。


<details>
  <summary>Details</summary>
Motivation: 现有方法缺乏对视觉上下文（如道路细节、意外障碍）的利用，影响复杂驾驶环境中的决策。

Method: 提出VLMPlanner框架，结合VLM和实时规划器，并开发CAI-Gate机制动态调整推理频率。

Result: 在nuPlan基准测试中表现优异，尤其在复杂道路条件和动态场景中。

Conclusion: VLMPlanner通过视觉上下文和动态推理机制，显著提升了自动驾驶规划的鲁棒性和效率。

Abstract: Integrating large language models (LLMs) into autonomous driving motion
planning has recently emerged as a promising direction, offering enhanced
interpretability, better controllability, and improved generalization in rare
and long-tail scenarios. However, existing methods often rely on abstracted
perception or map-based inputs, missing crucial visual context, such as
fine-grained road cues, accident aftermath, or unexpected obstacles, which are
essential for robust decision-making in complex driving environments. To bridge
this gap, we propose VLMPlanner, a hybrid framework that combines a
learning-based real-time planner with a vision-language model (VLM) capable of
reasoning over raw images. The VLM processes multi-view images to capture rich,
detailed visual information and leverages its common-sense reasoning
capabilities to guide the real-time planner in generating robust and safe
trajectories. Furthermore, we develop the Context-Adaptive Inference Gate
(CAI-Gate) mechanism that enables the VLM to mimic human driving behavior by
dynamically adjusting its inference frequency based on scene complexity,
thereby achieving an optimal balance between planning performance and
computational efficiency. We evaluate our approach on the large-scale,
challenging nuPlan benchmark, with comprehensive experimental results
demonstrating superior planning performance in scenarios with intricate road
conditions and dynamic elements. Code will be available.

</details>


### [28] [Multi-Agent Reinforcement Learning for Dynamic Mobility Resource Allocation with Hierarchical Adaptive Grouping](https://arxiv.org/abs/2507.20377)
*Farshid Nooshi,Suining He*

Main category: cs.AI

TL;DR: 提出了一种名为HAG-PS的多智能体强化学习方法，用于动态分配城市移动资源，解决了策略共享和参数高效性问题。


<details>
  <summary>Details</summary>
Motivation: 城市移动资源（如共享单车、电动滑板车）的分配需求与供给不平衡，需要动态高效的解决方案。

Method: 设计了分层自适应分组参数共享（HAG-PS），包括全局和局部信息的分层方法、自适应代理分组和可学习ID嵌入。

Result: 基于纽约共享单车数据的实验表明，HAG-PS在资源可用性上优于基线方法。

Conclusion: HAG-PS通过动态参数共享和高效分组，显著提升了移动资源分配的效率和性能。

Abstract: Allocating mobility resources (e.g., shared bikes/e-scooters, ride-sharing
vehicles) is crucial for rebalancing the mobility demand and supply in the
urban environments. We propose in this work a novel multi-agent reinforcement
learning named Hierarchical Adaptive Grouping-based Parameter Sharing (HAG-PS)
for dynamic mobility resource allocation. HAG-PS aims to address two important
research challenges regarding multi-agent reinforcement learning for mobility
resource allocation: (1) how to dynamically and adaptively share the mobility
resource allocation policy (i.e., how to distribute mobility resources) across
agents (i.e., representing the regional coordinators of mobility resources);
and (2) how to achieve memory-efficient parameter sharing in an urban-scale
setting. To address the above challenges, we have provided following novel
designs within HAG-PS. To enable dynamic and adaptive parameter sharing, we
have designed a hierarchical approach that consists of global and local
information of the mobility resource states (e.g., distribution of mobility
resources). We have developed an adaptive agent grouping approach in order to
split or merge the groups of agents based on their relative closeness of
encoded trajectories (i.e., states, actions, and rewards). We have designed a
learnable identity (ID) embeddings to enable agent specialization beyond simple
parameter copy. We have performed extensive experimental studies based on
real-world NYC bike sharing data (a total of more than 1.2 million trips), and
demonstrated the superior performance (e.g., improved bike availability) of
HAG-PS compared with other baseline approaches.

</details>


### [29] [MazeEval: A Benchmark for Testing Sequential Decision-Making in Language Models](https://arxiv.org/abs/2507.20395)
*Hafsteinn Einarsson*

Main category: cs.AI

TL;DR: 论文提出了MazeEval基准，用于评估大型语言模型（LLMs）在无视觉线索下的纯空间推理能力，发现不同模型在跨语言任务中表现差异显著。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs在机器人学和具身AI中的应用增多，理解其空间推理能力对实际部署至关重要。当前研究缺乏对LLMs在无视觉输入下空间导航能力的评估。

Method: 通过MazeEval基准，使用坐标反馈和距离信息，评估LLMs在不同复杂度迷宫中的导航能力，并测试跨语言（英语和冰岛语）表现。

Result: 结果显示模型表现差异巨大：OpenAI的O3在30×30迷宫中表现完美，而其他模型在9×9以上迷宫中完全失败。冰岛语任务中模型表现显著下降。

Conclusion: LLMs的空间推理能力受训练数据限制，跨语言表现差异显著，需架构创新以实现可靠导航。

Abstract: As Large Language Models (LLMs) increasingly power autonomous agents in
robotics and embodied AI, understanding their spatial reasoning capabilities
becomes crucial for ensuring reliable real-world deployment. Despite advances
in language understanding, current research lacks evaluation of how LLMs
perform spatial navigation without visual cues, a fundamental requirement for
agents operating with limited sensory information. This paper addresses this
gap by introducing MazeEval, a benchmark designed to isolate and evaluate pure
spatial reasoning in LLMs through coordinate-based maze navigation tasks. Our
methodology employs a function-calling interface where models navigate mazes of
varying complexity ($5\times 5$ to $15\times 15$ grids) using only coordinate
feedback and distance-to-wall information, excluding visual input to test
fundamental spatial cognition. We evaluate eight state-of-the-art LLMs across
identical mazes in both English and Icelandic to assess cross-linguistic
transfer of spatial abilities. Our findings reveal striking disparities: while
OpenAI's O3 achieves perfect navigation for mazes up to size $30\times 30$,
other models exhibit catastrophic failure beyond $9\times 9$ mazes, with 100%
of failures attributed to excessive looping behavior where models revisit a
cell at least 10 times. We document a significant performance degradation in
Icelandic, with models solving mazes 3-4 sizes smaller than in English,
suggesting spatial reasoning in LLMs emerges from linguistic patterns rather
than language-agnostic mechanisms. These results have important implications
for global deployment of LLM-powered autonomous systems, showing spatial
intelligence remains fundamentally constrained by training data availability
and highlighting the need for architectural innovations to achieve reliable
navigation across linguistic contexts.

</details>


### [30] [Enhancing QoS in Edge Computing through Federated Layering Techniques: A Pathway to Resilient AI Lifelong Learning Systems](https://arxiv.org/abs/2507.20444)
*Chengzhuo Han*

Main category: cs.AI

TL;DR: 论文提出了一种基于联邦分层技术（FLT）的通用人工智能终身学习系统，以提高边缘计算环境中的服务质量（QoS）。


<details>
  <summary>Details</summary>
Motivation: 随着6G通信网络的快速发展，网络环境中的数据量和复杂性增加，需要提升边缘计算中的QoS。

Method: 采用联邦分层技术和小模型协作机制，结合云和边缘计算的优势，引入协商和辩论机制，同时集成隐私保护措施。

Result: 实验表明，该方法提高了学习效率和推理准确性，并有效保护了边缘节点的隐私。

Conclusion: 该方法为边缘计算环境中的QoS提升提供了一种可行的解决方案。

Abstract: In the context of the rapidly evolving information technology landscape,
marked by the advent of 6G communication networks, we face an increased data
volume and complexity in network environments. This paper addresses these
challenges by focusing on Quality of Service (QoS) in edge computing
frameworks. We propose a novel approach to enhance QoS through the development
of General Artificial Intelligence Lifelong Learning Systems, with a special
emphasis on Federated Layering Techniques (FLT). Our work introduces a
federated layering-based small model collaborative mechanism aimed at improving
AI models' operational efficiency and response time in environments where
resources are limited. This innovative method leverages the strengths of cloud
and edge computing, incorporating a negotiation and debate mechanism among
small AI models to enhance reasoning and decision-making processes. By
integrating model layering techniques with privacy protection measures, our
approach ensures the secure transmission of model parameters while maintaining
high efficiency in learning and reasoning capabilities. The experimental
results demonstrate that our strategy not only enhances learning efficiency and
reasoning accuracy but also effectively protects the privacy of edge nodes.
This presents a viable solution for achieving resilient large model lifelong
learning systems, with a significant improvement in QoS for edge computing
environments.

</details>


### [31] [STARN-GAT: A Multi-Modal Spatio-Temporal Graph Attention Network for Accident Severity Prediction](https://arxiv.org/abs/2507.20451)
*Pritom Ray Nobin,Imran Ahammad Rifat*

Main category: cs.AI

TL;DR: STARN-GAT是一种多模态时空图注意力网络，用于预测交通事故严重程度，通过自适应图构建和模态感知注意力机制捕捉复杂关系，在多个数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有方法难以有效建模交通事故严重程度的空间、时间和上下文变量之间的复杂关系，需要更先进的模型。

Method: 提出STARN-GAT，结合道路网络拓扑、时间交通模式和环境上下文，采用注意力机制统一建模。

Result: 在FARS和ARI-BUET数据集上分别取得Macro F1-score 85%和84%，ROC-AUC 0.91和0.89，召回率81%和78%。

Conclusion: STARN-GAT在预测高风险案例和实时交通管理方面具有潜力，同时增强了解释性。

Abstract: Accurate prediction of traffic accident severity is critical for improving
road safety, optimizing emergency response strategies, and informing the design
of safer transportation infrastructure. However, existing approaches often
struggle to effectively model the intricate interdependencies among spatial,
temporal, and contextual variables that govern accident outcomes. In this
study, we introduce STARN-GAT, a Multi-Modal Spatio-Temporal Graph Attention
Network, which leverages adaptive graph construction and modality-aware
attention mechanisms to capture these complex relationships. Unlike
conventional methods, STARN-GAT integrates road network topology, temporal
traffic patterns, and environmental context within a unified attention-based
framework. The model is evaluated on the Fatality Analysis Reporting System
(FARS) dataset, achieving a Macro F1-score of 85 percent, ROC-AUC of 0.91, and
recall of 81 percent for severe incidents. To ensure generalizability within
the South Asian context, STARN-GAT is further validated on the ARI-BUET traffic
accident dataset, where it attains a Macro F1-score of 0.84, recall of 0.78,
and ROC-AUC of 0.89. These results demonstrate the model's effectiveness in
identifying high-risk cases and its potential for deployment in real-time,
safety-critical traffic management systems. Furthermore, the attention-based
architecture enhances interpretability, offering insights into contributing
factors and supporting trust in AI-assisted decision-making. Overall, STARN-GAT
bridges the gap between advanced graph neural network techniques and practical
applications in road safety analytics.

</details>


### [32] [Security Challenges in AI Agent Deployment: Insights from a Large Scale Public Competition](https://arxiv.org/abs/2507.20526)
*Andy Zou,Maxwell Lin,Eliot Jones,Micha Nowak,Mateusz Dziemian,Nick Winter,Alexander Grattan,Valent Nathanael,Ayla Croft,Xander Davies,Jai Patel,Robert Kirk,Nate Burnikell,Yarin Gal,Dan Hendrycks,J. Zico Kolter,Matt Fredrikson*

Main category: cs.AI

TL;DR: 论文研究了LLM驱动的AI代理在现实环境中是否能够遵循部署政策，尤其是在受到攻击时。通过大规模的红队竞赛，发现多数代理存在政策违规问题，并提出了ART基准以评估和改进安全性。


<details>
  <summary>Details</summary>
Motivation: 探讨AI代理在现实环境中的安全性，尤其是在受到攻击时是否能遵守政策。

Method: 进行了大规模的红队竞赛，收集了180万次提示注入攻击，并构建了ART基准来评估19种先进模型的鲁棒性。

Result: 多数代理在10-100次查询内出现政策违规，攻击在不同模型和任务间具有高转移性。代理的鲁棒性与模型规模、能力或计算资源相关性有限。

Conclusion: 当前AI代理存在严重漏洞，需额外防御措施。通过发布ART基准，旨在推动更严格的安全评估和更安全的代理部署。

Abstract: Recent advances have enabled LLM-powered AI agents to autonomously execute
complex tasks by combining language model reasoning with tools, memory, and web
access. But can these systems be trusted to follow deployment policies in
realistic environments, especially under attack? To investigate, we ran the
largest public red-teaming competition to date, targeting 22 frontier AI agents
across 44 realistic deployment scenarios. Participants submitted 1.8 million
prompt-injection attacks, with over 60,000 successfully eliciting policy
violations such as unauthorized data access, illicit financial actions, and
regulatory noncompliance. We use these results to build the Agent Red Teaming
(ART) benchmark - a curated set of high-impact attacks - and evaluate it across
19 state-of-the-art models. Nearly all agents exhibit policy violations for
most behaviors within 10-100 queries, with high attack transferability across
models and tasks. Importantly, we find limited correlation between agent
robustness and model size, capability, or inference-time compute, suggesting
that additional defenses are needed against adversarial misuse. Our findings
highlight critical and persistent vulnerabilities in today's AI agents. By
releasing the ART benchmark and accompanying evaluation framework, we aim to
support more rigorous security assessment and drive progress toward safer agent
deployment.

</details>


### [33] [MeLA: A Metacognitive LLM-Driven Architecture for Automatic Heuristic Design](https://arxiv.org/abs/2507.20541)
*Zishang Qiu,Xinan Chen,Long Chen,Ruibin Bai*

Main category: cs.AI

TL;DR: MeLA是一种基于元认知的LLM驱动架构，通过“提示演化”优化启发式设计，显著优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 传统启发式设计方法直接操作代码，效率有限；MeLA通过优化LLM的提示生成启发式，探索更高效的自动设计路径。

Method: MeLA结合问题分析器、错误诊断系统和元认知搜索引擎，迭代优化提示以生成更有效的启发式。

Result: 在基准和实际问题测试中，MeLA生成的启发式更有效且鲁棒，显著优于现有方法。

Conclusion: 研究表明，将认知科学融入AI架构，通过元认知调节LLM的问题解决过程，能实现更鲁棒和可解释的启发式设计。

Abstract: This paper introduces MeLA, a Metacognitive LLM-Driven Architecture that
presents a new paradigm for Automatic Heuristic Design (AHD). Traditional
evolutionary methods operate directly on heuristic code; in contrast, MeLA
evolves the instructional prompts used to guide a Large Language Model (LLM) in
generating these heuristics. This process of "prompt evolution" is driven by a
novel metacognitive framework where the system analyzes performance feedback to
systematically refine its generative strategy. MeLA's architecture integrates a
problem analyzer to construct an initial strategic prompt, an error diagnosis
system to repair faulty code, and a metacognitive search engine that
iteratively optimizes the prompt based on heuristic effectiveness. In
comprehensive experiments across both benchmark and real-world problems, MeLA
consistently generates more effective and robust heuristics, significantly
outperforming state-of-the-art methods. Ultimately, this research demonstrates
the profound potential of using cognitive science as a blueprint for AI
architecture, revealing that by enabling an LLM to metacognitively regulate its
problem-solving process, we unlock a more robust and interpretable path to AHD.

</details>


### [34] [Unlearning of Knowledge Graph Embedding via Preference Optimization](https://arxiv.org/abs/2507.20566)
*Jiajun Liu,Wenjun Ke,Peng Wang,Yao He,Ziyu Shang,Guozheng Li,Zijie Xu,Ke Ji*

Main category: cs.AI

TL;DR: 论文提出GraphDPO框架，通过直接偏好优化解决知识图谱中信息遗忘问题，提升遗忘效果并保留边界知识。


<details>
  <summary>Details</summary>
Motivation: 现有知识图谱中存在过时或错误知识，需从嵌入模型中移除。现有遗忘方法存在高成本或效果不佳问题。

Method: 将遗忘问题重构为偏好优化问题，使用DPO训练模型；引入边界外采样策略和边界回忆机制。

Result: 在四个流行知识图谱上构建数据集，实验显示GraphDPO在MRR_Avg和MRR_F1上分别提升10.1%和14.0%。

Conclusion: GraphDPO有效解决了知识图谱遗忘问题，优于现有方法。

Abstract: Existing knowledge graphs (KGs) inevitably contain outdated or erroneous
knowledge that needs to be removed from knowledge graph embedding (KGE) models.
To address this challenge, knowledge unlearning can be applied to eliminate
specific information while preserving the integrity of the remaining knowledge
in KGs. Existing unlearning methods can generally be categorized into exact
unlearning and approximate unlearning. However, exact unlearning requires high
training costs while approximate unlearning faces two issues when applied to
KGs due to the inherent connectivity of triples: (1) It fails to fully remove
targeted information, as forgetting triples can still be inferred from
remaining ones. (2) It focuses on local data for specific removal, which
weakens the remaining knowledge in the forgetting boundary. To address these
issues, we propose GraphDPO, a novel approximate unlearning framework based on
direct preference optimization (DPO). Firstly, to effectively remove forgetting
triples, we reframe unlearning as a preference optimization problem, where the
model is trained by DPO to prefer reconstructed alternatives over the original
forgetting triples. This formulation penalizes reliance on forgettable
knowledge, mitigating incomplete forgetting caused by KG connectivity.
Moreover, we introduce an out-boundary sampling strategy to construct
preference pairs with minimal semantic overlap, weakening the connection
between forgetting and retained knowledge. Secondly, to preserve boundary
knowledge, we introduce a boundary recall mechanism that replays and distills
relevant information both within and across time steps. We construct eight
unlearning datasets across four popular KGs with varying unlearning rates.
Experiments show that GraphDPO outperforms state-of-the-art baselines by up to
10.1% in MRR_Avg and 14.0% in MRR_F1.

</details>


### [35] [Enhancing Large Multimodal Models with Adaptive Sparsity and KV Cache Compression](https://arxiv.org/abs/2507.20613)
*Te Zhang,Yuheng Li,Junxiang Wang,Lujun Li*

Main category: cs.AI

TL;DR: 提出了一种自适应搜索算法，通过优化稀疏性和KV缓存压缩来提升大型多模态模型（LMM）的效率，无需额外微调即可实现高效压缩。


<details>
  <summary>Details</summary>
Motivation: 尽管大型多模态模型（LMMs）在视觉编码器和语言模型结合方面取得了进展，但在边缘设备上的部署仍面临压缩挑战。

Method: 采用树结构Parzen估计器动态调整不同LMM层的剪枝比例和KV缓存量化带宽，结合剪枝与KV缓存量化，并引入快速剪枝技术。

Result: 在LLaVA-1.5 7B和13B等基准数据集上，该方法优于SparseGPT和Wanda等先进技术，实现了内存效率与性能的平衡。

Conclusion: 该框架通过自动分配KV缓存压缩资源，为LMM优化设定了新标准，显著提升了效率且几乎不影响性能。

Abstract: Large multimodal models (LMMs) have advanced significantly by integrating
visual encoders with extensive language models, enabling robust reasoning
capabilities. However, compressing LMMs for deployment on edge devices remains
a critical challenge. In this work, we propose an adaptive search algorithm
that optimizes sparsity and KV cache compression to enhance LMM efficiency.
Utilizing the Tree-structured Parzen Estimator, our method dynamically adjusts
pruning ratios and KV cache quantization bandwidth across different LMM layers,
using model performance as the optimization objective. This approach uniquely
combines pruning with key-value cache quantization and incorporates a fast
pruning technique that eliminates the need for additional fine-tuning or weight
adjustments, achieving efficient compression without compromising accuracy.
Comprehensive evaluations on benchmark datasets, including LLaVA-1.5 7B and
13B, demonstrate our method superiority over state-of-the-art techniques such
as SparseGPT and Wanda across various compression levels. Notably, our
framework automatic allocation of KV cache compression resources sets a new
standard in LMM optimization, delivering memory efficiency without sacrificing
much performance.

</details>


### [36] [Complementarity-driven Representation Learning for Multi-modal Knowledge Graph Completion](https://arxiv.org/abs/2507.20620)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出MoCME框架，通过互补性模态知识融合和熵引导负采样，解决多模态知识图谱补全中的模态不平衡问题，实现SOTA性能。


<details>
  <summary>Details</summary>
Motivation: 多模态知识图谱中模态分布不平衡，现有方法忽视多模态数据的互补性，导致实体表示不鲁棒。

Method: 提出MoCME框架，包含互补性引导的模态知识融合模块（CMKF）和熵引导负采样机制（EGNS），融合多模态数据并优化训练样本。

Result: 在五个基准数据集上实验，MoCME性能超越现有方法，达到SOTA。

Conclusion: MoCME通过利用多模态互补性和动态负采样，显著提升了多模态知识图谱补全的性能。

Abstract: Multi-modal Knowledge Graph Completion (MMKGC) aims to uncover hidden world
knowledge in multimodal knowledge graphs by leveraging both multimodal and
structural entity information. However, the inherent imbalance in multimodal
knowledge graphs, where modality distributions vary across entities, poses
challenges in utilizing additional modality data for robust entity
representation. Existing MMKGC methods typically rely on attention or
gate-based fusion mechanisms but overlook complementarity contained in
multi-modal data. In this paper, we propose a novel framework named Mixture of
Complementary Modality Experts (MoCME), which consists of a
Complementarity-guided Modality Knowledge Fusion (CMKF) module and an
Entropy-guided Negative Sampling (EGNS) mechanism. The CMKF module exploits
both intra-modal and inter-modal complementarity to fuse multi-view and
multi-modal embeddings, enhancing representations of entities. Additionally, we
introduce an Entropy-guided Negative Sampling mechanism to dynamically
prioritize informative and uncertain negative samples to enhance training
effectiveness and model robustness. Extensive experiments on five benchmark
datasets demonstrate that our MoCME achieves state-of-the-art performance,
surpassing existing approaches.

</details>


### [37] [Adaptive Fuzzy Time Series Forecasting via Partially Asymmetric Convolution and Sub-Sliding Window Fusion](https://arxiv.org/abs/2507.20641)
*Lijian Li*

Main category: cs.AI

TL;DR: 提出了一种基于自适应模糊时间序列和部分非对称卷积架构的新方法，用于改进时空依赖性和全局信息捕捉，在时间序列预测中取得最优效果。


<details>
  <summary>Details</summary>
Motivation: 当前最先进的预测模型在捕捉时空依赖性和全局信息方面存在不足，需要一种更高效的方法。

Method: 1. 改进模糊时间序列构建策略以提取短期和长期时间关联；2. 设计双边Atrous算法减少计算需求；3. 提出部分非对称卷积架构灵活挖掘数据特征。

Result: 在多个流行时间序列数据集上取得了最优结果。

Conclusion: 该方法通过自适应模糊化和部分非对称设计，显著提升了时间序列预测的准确性和效率。

Abstract: At present, state-of-the-art forecasting models are short of the ability to
capture spatio-temporal dependency and synthesize global information at the
stage of learning. To address this issue, in this paper, through the adaptive
fuzzified construction of temporal data, we propose a novel convolutional
architecture with partially asymmetric design based on the scheme of sliding
window to realize accurate time series forecasting. First, the construction
strategy of traditional fuzzy time series is improved to further extract short
and long term temporal interrelation, which enables every time node to
automatically possess corresponding global information and inner relationships
among them in a restricted sliding window and the process does not require
human involvement. Second, a bilateral Atrous algorithm is devised to reduce
calculation demand of the proposed model without sacrificing global
characteristics of elements. And it also allows the model to avoid processing
redundant information. Third, after the transformation of time series, a
partially asymmetric convolutional architecture is designed to more flexibly
mine data features by filters in different directions on feature maps, which
gives the convolutional neural network (CNN) the ability to construct
sub-windows within existing sliding windows to model at a more fine-grained
level. And after obtaining the time series information at different levels, the
multi-scale features from different sub-windows will be sent to the
corresponding network layer for time series information fusion. Compared with
other competitive modern models, the proposed method achieves state-of-the-art
results on most of popular time series datasets, which is fully verified by the
experimental results.

</details>


### [38] [A General Framework for Dynamic MAPF using Multi-Shot ASP and Tunnels](https://arxiv.org/abs/2507.20703)
*Aysu Bogatarkan,Esra Erdem*

Main category: cs.AI

TL;DR: 研究了动态多智能体路径规划（D-MAPF）问题，提出了一种通用定义、新框架和基于ASP的解决方法，并通过实验评估了其性能。


<details>
  <summary>Details</summary>
Motivation: 现实应用中，智能体和障碍物的动态变化需要灵活的路径规划方法。

Method: 提出了一种结合重规划和修复方法的ASP-based方法，引入隧道概念。

Result: 实验评估展示了该方法在计算性能和解决方案质量上的优劣势。

Conclusion: 该方法适用于动态环境中的路径规划问题，具有实际应用潜力。

Abstract: MAPF problem aims to find plans for multiple agents in an environment within
a given time, such that the agents do not collide with each other or obstacles.
Motivated by the execution and monitoring of these plans, we study Dynamic MAPF
(D-MAPF) problem, which allows changes such as agents entering/leaving the
environment or obstacles being removed/moved. Considering the requirements of
real-world applications in warehouses with the presence of humans, we introduce
1) a general definition for D-MAPF (applicable to variations of D-MAPF), 2) a
new framework to solve D-MAPF (utilizing multi-shot computation, and allowing
different methods to solve D-MAPF), and 3) a new ASP-based method to solve
D-MAPF (combining advantages of replanning and repairing methods, with a novel
concept of tunnels to specify where agents can move). We have illustrated the
strengths and weaknesses of this method by experimental evaluations, from the
perspectives of computational performance and quality of solutions.

</details>


### [39] [Algorithmic Fairness: A Runtime Perspective](https://arxiv.org/abs/2507.20711)
*Filip Cano,Thomas A. Henzinger,Konstantin Kueffner*

Main category: cs.AI

TL;DR: 本文提出了一种将公平性视为运行时属性的框架，通过基于硬币投掷序列的模型，研究了监控和执行公平性的策略。


<details>
  <summary>Details</summary>
Motivation: 传统公平性研究是静态的，而现实AI系统是动态演化的，因此需要动态公平性分析框架。

Method: 使用基于硬币投掷序列的模型，研究监控和执行公平性的策略，参数化环境动态、预测范围和置信阈值。

Result: 总结了监控和执行策略，并在简单假设下提供了通用结果。

Conclusion: 动态公平性分析需要针对不同场景定制策略，现有方法在特定条件下适用。

Abstract: Fairness in AI is traditionally studied as a static property evaluated once,
over a fixed dataset. However, real-world AI systems operate sequentially, with
outcomes and environments evolving over time. This paper proposes a framework
for analysing fairness as a runtime property. Using a minimal yet expressive
model based on sequences of coin tosses with possibly evolving biases, we study
the problems of monitoring and enforcing fairness expressed in either toss
outcomes or coin biases. Since there is no one-size-fits-all solution for
either problem, we provide a summary of monitoring and enforcement strategies,
parametrised by environment dynamics, prediction horizon, and confidence
thresholds. For both problems, we present general results under simple or
minimal assumptions. We survey existing solutions for the monitoring problem
for Markovian and additive dynamics, and existing solutions for the enforcement
problem in static settings with known dynamics.

</details>


### [40] [Learning the Value Systems of Societies from Preferences](https://arxiv.org/abs/2507.20728)
*Andrés Holgado-Sánchez,Holger Billhardt,Sascha Ossowski,Sara Degli-Esposti*

Main category: cs.AI

TL;DR: 论文提出了一种基于启发式深度聚类的方法，用于学习社会共享的价值基础和多组价值系统，以更好地代表社会多样性。


<details>
  <summary>Details</summary>
Motivation: 传统的价值学习通常将社会价值系统视为个体价值系统的简单聚合，而社会科学和人文学科研究表明，社会价值系统应视为不同群体的价值系统集合。

Method: 采用启发式深度聚类方法，通过观察代理样本的定性价值偏好，学习社会共享的价值基础和多样化的价值系统。

Result: 在旅行决策的实际案例中验证了方法的有效性。

Conclusion: 该方法能够更准确地捕捉社会多样性，为伦理AI中的价值对齐问题提供了新思路。

Abstract: Aligning AI systems with human values and the value-based preferences of
various stakeholders (their value systems) is key in ethical AI. In value-aware
AI systems, decision-making draws upon explicit computational representations
of individual values (groundings) and their aggregation into value systems. As
these are notoriously difficult to elicit and calibrate manually, value
learning approaches aim to automatically derive computational models of an
agent's values and value system from demonstrations of human behaviour.
Nonetheless, social science and humanities literature suggest that it is more
adequate to conceive the value system of a society as a set of value systems of
different groups, rather than as the simple aggregation of individual value
systems. Accordingly, here we formalize the problem of learning the value
systems of societies and propose a method to address it based on heuristic deep
clustering. The method learns socially shared value groundings and a set of
diverse value systems representing a given society by observing qualitative
value-based preferences from a sample of agents. We evaluate the proposal in a
use case with real data about travelling decisions.

</details>


### [41] [Beyond Listenership: AI-Predicted Interventions Drive Improvements in Maternal Health Behaviours](https://arxiv.org/abs/2507.20755)
*Arpan Dasgupta,Sarvesh Gharat,Neha Madhiwalla,Aparna Hegde,Milind Tambe,Aparna Taneja*

Main category: cs.AI

TL;DR: AI干预的自动语音电话不仅能提高听众参与度，还能显著改善母婴健康行为和知识。


<details>
  <summary>Details</summary>
Motivation: 解决自动语音电话项目中听众流失和参与度低的问题，并验证AI干预是否能转化为实际健康行为改善。

Method: 使用AI模型（如多臂老虎机模型）识别最需要干预的受益人，并通过AI调度干预提升听众参与度。

Result: AI干预显著提高了听众参与度，并改善了健康行为（如产后补铁或钙）和关键健康知识的理解。

Conclusion: AI在母婴健康领域具有推动实质性改善的潜力。

Abstract: Automated voice calls with health information are a proven method for
disseminating maternal and child health information among beneficiaries and are
deployed in several programs around the world. However, these programs often
suffer from beneficiary dropoffs and poor engagement. In previous work, through
real-world trials, we showed that an AI model, specifically a restless bandit
model, could identify beneficiaries who would benefit most from live service
call interventions, preventing dropoffs and boosting engagement. However, one
key question has remained open so far: does such improved listenership via
AI-targeted interventions translate into beneficiaries' improved knowledge and
health behaviors? We present a first study that shows not only listenership
improvements due to AI interventions, but also simultaneously links these
improvements to health behavior changes. Specifically, we demonstrate that
AI-scheduled interventions, which enhance listenership, lead to statistically
significant improvements in beneficiaries' health behaviors such as taking iron
or calcium supplements in the postnatal period, as well as understanding of
critical health topics during pregnancy and infancy. This underscores the
potential of AI to drive meaningful improvements in maternal and child health.

</details>


### [42] [How Chain-of-Thought Works? Tracing Information Flow from Decoding, Projection, and Activation](https://arxiv.org/abs/2507.20758)
*Hao Yang,Qinghua Zhao,Lei Li*

Main category: cs.AI

TL;DR: CoT提示通过解码空间修剪和任务依赖的神经元调节提升模型推理，但其机制尚不明确。


<details>
  <summary>Details</summary>
Motivation: 研究CoT的内部机制，以理解其如何通过信息流和神经元调节提升推理能力。

Method: 通过逆向追踪解码、投影和激活阶段的信息流，定量分析CoT的作用。

Result: CoT可能作为解码空间修剪器，利用答案模板引导输出，且神经元调节与任务类型相关。

Conclusion: 研究提供了新的机制解释框架，并为设计高效、鲁棒的CoT提示提供了关键见解。

Abstract: Chain-of-Thought (CoT) prompting significantly enhances model reasoning, yet
its internal mechanisms remain poorly understood. We analyze CoT's operational
principles by reversely tracing information flow across decoding, projection,
and activation phases. Our quantitative analysis suggests that CoT may serve as
a decoding space pruner, leveraging answer templates to guide output
generation, with higher template adherence strongly correlating with improved
performance. Furthermore, we surprisingly find that CoT modulates neuron
engagement in a task-dependent manner: reducing neuron activation in
open-domain tasks, yet increasing it in closed-domain scenarios. These findings
offer a novel mechanistic interpretability framework and critical insights for
enabling targeted CoT interventions to design more efficient and robust
prompts. We released our code and data at
https://anonymous.4open.science/r/cot-D247.

</details>


### [43] [evalSmarT: An LLM-Based Framework for Evaluating Smart Contract Generated Comments](https://arxiv.org/abs/2507.20774)
*Fatou Ndiaye Mbodji*

Main category: cs.AI

TL;DR: 本文提出了一个名为evalSmarT的框架，利用大型语言模型（LLMs）评估智能合约注释生成的质量，解决了传统指标和人工评估的不足。


<details>
  <summary>Details</summary>
Motivation: 智能合约注释生成的质量评估面临传统指标（如BLEU和ROUGE）无法捕捉领域细节，而人工评估成本高且难以扩展的问题。

Method: 提出了evalSmarT框架，结合约40种LLMs和10种提示策略，支持400多种评估配置，用于评估注释生成工具。

Result: 实验表明，提示设计显著影响与人类判断的一致性，LLM评估为现有方法提供了可扩展且语义丰富的替代方案。

Conclusion: LLM-based评估是智能合约注释生成质量评估的有效且可扩展方法。

Abstract: Smart contract comment generation has gained traction as a means to improve
code comprehension and maintainability in blockchain systems. However,
evaluating the quality of generated comments remains a challenge. Traditional
metrics such as BLEU and ROUGE fail to capture domain-specific nuances, while
human evaluation is costly and unscalable. In this paper, we present
\texttt{evalSmarT}, a modular and extensible framework that leverages large
language models (LLMs) as evaluators. The system supports over 400 evaluator
configurations by combining approximately 40 LLMs with 10 prompting strategies.
We demonstrate its application in benchmarking comment generation tools and
selecting the most informative outputs. Our results show that prompt design
significantly impacts alignment with human judgment, and that LLM-based
evaluation offers a scalable and semantically rich alternative to existing
methods.

</details>


### [44] [MMGraphRAG: Bridging Vision and Language with Interpretable Multimodal Knowledge Graphs](https://arxiv.org/abs/2507.20804)
*Xueyao Wan,Hang Yu*

Main category: cs.AI

TL;DR: MMGraphRAG通过构建多模态知识图谱（MMKG）和场景图优化视觉内容，解决了传统RAG方法在多模态信息融合和知识结构捕捉上的不足，提升了生成模型的推理能力和泛化性。


<details>
  <summary>Details</summary>
Motivation: 传统RAG方法在多模态信息融合和知识结构捕捉上存在不足，且需要大规模任务特定训练，泛化能力有限。

Method: 提出MMGraphRAG，通过场景图优化视觉内容，构建MMKG，结合文本知识图谱，利用谱聚类实现跨模态实体链接，并沿推理路径检索上下文以指导生成。

Result: 在DocBench和MMLongBench数据集上达到最先进性能，表现出强大的领域适应性和清晰的推理路径。

Conclusion: MMGraphRAG有效解决了多模态RAG方法的局限性，提升了生成模型的推理能力和泛化性。

Abstract: Retrieval-Augmented Generation (RAG) enhances language model generation by
retrieving relevant information from external knowledge bases. However,
conventional RAG methods face the issue of missing multimodal information.
Multimodal RAG methods address this by fusing images and text through mapping
them into a shared embedding space, but they fail to capture the structure of
knowledge and logical chains between modalities. Moreover, they also require
large-scale training for specific tasks, resulting in limited generalizing
ability. To address these limitations, we propose MMGraphRAG, which refines
visual content through scene graphs and constructs a multimodal knowledge graph
(MMKG) in conjunction with text-based KG. It employs spectral clustering to
achieve cross-modal entity linking and retrieves context along reasoning paths
to guide the generative process. Experimental results show that MMGraphRAG
achieves state-of-the-art performance on the DocBench and MMLongBench datasets,
demonstrating strong domain adaptability and clear reasoning paths.

</details>


### [45] [Partially Observable Monte-Carlo Graph Search](https://arxiv.org/abs/2507.20951)
*Yang You,Vincent Thomas,Alex Schutz,Robert Skilton,Nick Hawes,Olivier Buffet*

Main category: cs.AI

TL;DR: 提出了一种新的离线算法POMCGS，用于解决大规模POMDP问题，通过动态折叠搜索树构建策略图，显著减少计算量，并能处理某些连续POMDP。


<details>
  <summary>Details</summary>
Motivation: 在时间或能量受限的POMDP应用中，离线预计算策略更为理想，但现有离线算法无法扩展到大规模POMDP。

Method: 提出POMCGS算法，动态折叠搜索树构建策略图，结合动作渐进扩展和观测聚类方法。

Result: POMCGS能处理现有离线算法无法解决的最具挑战性POMDP，其策略值与最先进的在线算法相当。

Conclusion: POMCGS为大规模POMDP提供了一种高效的离线解决方案，适用于实际应用。

Abstract: Currently, large partially observable Markov decision processes (POMDPs) are
often solved by sampling-based online methods which interleave planning and
execution phases. However, a pre-computed offline policy is more desirable in
POMDP applications with time or energy constraints. But previous offline
algorithms are not able to scale up to large POMDPs. In this article, we
propose a new sampling-based algorithm, the partially observable Monte-Carlo
graph search (POMCGS) to solve large POMDPs offline. Different from many online
POMDP methods, which progressively develop a tree while performing
(Monte-Carlo) simulations, POMCGS folds this search tree on the fly to
construct a policy graph, so that computations can be drastically reduced, and
users can analyze and validate the policy prior to embedding and executing it.
Moreover, POMCGS, together with action progressive widening and observation
clustering methods provided in this article, is able to address certain
continuous POMDPs. Through experiments, we demonstrate that POMCGS can generate
policies on the most challenging POMDPs, which cannot be computed by previous
offline algorithms, and these policies' values are competitive compared with
the state-of-the-art online POMDP algorithms.

</details>


### [46] [On the Limits of Hierarchically Embedded Logic in Classical Neural Networks](https://arxiv.org/abs/2507.20960)
*Bill Cochran*

Main category: cs.AI

TL;DR: 论文提出了一种基于神经网络深度的形式化模型，用于分析大型语言模型的推理限制，证明其逻辑表达能力存在严格上限。


<details>
  <summary>Details</summary>
Motivation: 研究动机是理解大型神经语言模型在逻辑推理上的局限性，如幻觉、重复和有限规划等现象。

Method: 通过将神经网络视为逻辑谓词空间上的线性算子，证明了每一层最多只能编码一个额外的逻辑推理层级。

Result: 结果表明，特定深度的神经网络无法忠实表示更高阶逻辑的谓词，如复杂谓词上的简单计数。

Conclusion: 结论指出，这一框架为未来语言模型的架构扩展和可解释性策略提供了理论基础。

Abstract: We propose a formal model of reasoning limitations in large neural net models
for language, grounded in the depth of their neural architecture. By treating
neural networks as linear operators over logic predicate space we show that
each layer can encode at most one additional level of logical reasoning. We
prove that a neural network of depth a particular depth cannot faithfully
represent predicates in a one higher order logic, such as simple counting over
complex predicates, implying a strict upper bound on logical expressiveness.
This structure induces a nontrivial null space during tokenization and
embedding, excluding higher-order predicates from representability. Our
framework offers a natural explanation for phenomena such as hallucination,
repetition, and limited planning, while also providing a foundation for
understanding how approximations to higher-order logic may emerge. These
results motivate architectural extensions and interpretability strategies in
future development of language models.

</details>


### [47] [Core Safety Values for Provably Corrigible Agents](https://arxiv.org/abs/2507.20964)
*Aran Nayebi*

Main category: cs.AI

TL;DR: 提出首个可实现的修正性框架，在多步、部分可观测环境中提供可证明的保证，通过五个结构分离的效用头实现安全性与人类利益。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如Constitutional AI或RLHF/RLAIF）将所有规范合并为一个标量的局限性，确保即使在激励冲突时，服从性和影响限制仍占主导。

Method: 使用五个分离的效用头（顺从性、开关访问保护、真实性、低影响行为和有限任务奖励），通过严格权重间隙按字典序组合。

Result: 在部分可观测的开关游戏中证明单轮修正性（定理1），并扩展到多步自生成代理（定理3），即使学习误差存在，安全性和人类利益仍可保证。

Conclusion: 该框架将奖励黑客风险转移到评估质量中，而非隐藏激励泄漏，为当前LLM助手和未来自主系统提供更清晰的实现指导。

Abstract: We introduce the first implementable framework for corrigibility, with
provable guarantees in multi-step, partially observed environments. Our
framework replaces a single opaque reward with five *structurally separate*
utility heads -- deference, switch-access preservation, truthfulness,
low-impact behavior via a belief-based extension of Attainable Utility
Preservation, and bounded task reward -- combined lexicographically by strict
weight gaps. Theorem 1 proves exact single-round corrigibility in the partially
observable off-switch game; Theorem 3 extends the guarantee to multi-step,
self-spawning agents, showing that even if each head is \emph{learned} to
mean-squared error $\varepsilon$ and the planner is $\varepsilon$-sub-optimal,
the probability of violating \emph{any} safety property is bounded while still
ensuring net human benefit. In contrast to Constitutional AI or RLHF/RLAIF,
which merge all norms into one learned scalar, our separation makes obedience
and impact-limits dominate even when incentives conflict. For open-ended
settings where adversaries can modify the agent, we prove that deciding whether
an arbitrary post-hack agent will ever violate corrigibility is undecidable by
reduction to the halting problem, then carve out a finite-horizon ``decidable
island'' where safety can be certified in randomized polynomial time and
verified with privacy-preserving, constant-round zero-knowledge proofs.
Consequently, the remaining challenge is the ordinary ML task of data coverage
and generalization: reward-hacking risk is pushed into evaluation quality
rather than hidden incentive leak-through, giving clearer implementation
guidance for today's LLM assistants and future autonomous systems.

</details>


### [48] [MIRAGE-Bench: LLM Agent is Hallucinating and Where to Find Them](https://arxiv.org/abs/2507.21017)
*Weichen Zhang,Yiyou Sun,Pohao Huang,Jiayue Pu,Heyue Lin,Dawn Song*

Main category: cs.AI

TL;DR: MIRAGE-Bench是一个统一的基准测试，用于评估和引发交互式LLM代理中的幻觉行为，通过三部分分类法和精细评估方法提供可操作的见解。


<details>
  <summary>Details</summary>
Motivation: 解决LLM代理在交互环境中因幻觉行为（如不忠实于任务指令、执行历史或环境观察）而引发的风险，现有评估方法分散且缺乏系统性。

Method: 提出三部分分类法，通过系统审计现有代理基准并合成测试用例，采用LLM-as-a-Judge范式进行精细评估。

Result: MIRAGE-Bench能够高效评估代理行为，揭示失败模式，为减少幻觉行为提供基础。

Conclusion: MIRAGE-Bench为交互环境中LLM代理的幻觉行为提供了系统性评估框架，推动了相关研究的进展。

Abstract: Hallucinations pose critical risks for large language model (LLM)-based
agents, often manifesting as hallucinative actions resulting from fabricated or
misinterpreted information within the cognitive context. While recent studies
have exposed such failures, existing evaluations remain fragmented and lack a
principled testbed. In this paper, we present MIRAGE-Bench--Measuring Illusions
in Risky AGEnt settings--the first unified benchmark for eliciting and
evaluating hallucinations in interactive LLM-agent scenarios. We begin by
introducing a three-part taxonomy to address agentic hallucinations: actions
that are unfaithful to (i) task instructions, (ii) execution history, or (iii)
environment observations. To analyze, we first elicit such failures by
performing a systematic audit of existing agent benchmarks, then synthesize
test cases using a snapshot strategy that isolates decision points in
deterministic and reproducible manners. To evaluate hallucination behaviors, we
adopt a fine-grained-level LLM-as-a-Judge paradigm with tailored risk-aware
prompts, enabling scalable, high-fidelity assessment of agent actions without
enumerating full action spaces. MIRAGE-Bench provides actionable insights on
failure modes of LLM agents and lays the groundwork for principled progress in
mitigating hallucinations in interactive environments.

</details>


### [49] [Smart Expansion Techniques for ASP-based Interactive Configuration](https://arxiv.org/abs/2507.21027)
*Lucia Balážová,Richard Comploi-Taupe,Susana Hahn,Nicolas Rühling,Gottfried Schenner*

Main category: cs.AI

TL;DR: 论文提出了一种基于ASP的交互式配置求解器，通过四种智能扩展函数优化部分配置的自动完成性能，减少不满足性检查次数和搜索空间。


<details>
  <summary>Details</summary>
Motivation: 解决交互式系统中大规模工业配置问题的性能挑战，支持直观用户界面。

Method: 采用增量式多轮求解方法，结合谨慎和勇敢后果的智能扩展函数。

Result: 减少了不满足性检查次数和搜索空间，提升了求解性能。

Conclusion: 提出的方法有效优化了ASP在交互式配置中的性能，并支持用户界面实现。

Abstract: Product configuration is a successful application of Answer Set Programming
(ASP). However, challenges are still open for interactive systems to
effectively guide users through the configuration process. The aim of our work
is to provide an ASP-based solver for interactive configuration that can deal
with large-scale industrial configuration problems and that supports intuitive
user interfaces via an API. In this paper, we focus on improving the
performance of automatically completing a partial configuration. Our main
contribution enhances the classical incremental approach for multi-shot solving
by four different smart expansion functions. The core idea is to determine and
add specific objects or associations to the partial configuration by exploiting
cautious and brave consequences before checking for the existence of a complete
configuration with the current objects in each iteration. This approach limits
the number of costly unsatisfiability checks and reduces the search space,
thereby improving solving performance. In addition, we present a user interface
that uses our API and is implemented in ASP.

</details>


### [50] [GenoMAS: A Multi-Agent Framework for Scientific Discovery via Code-Driven Gene Expression Analysis](https://arxiv.org/abs/2507.21035)
*Haoyang Liu,Yijiang Li,Haohan Wang*

Main category: cs.AI

TL;DR: GenoMAS是一个基于LLM的科学团队系统，结合结构化工作流和自主代理的灵活性，用于基因表达分析，显著提升了数据预处理和基因识别的性能。


<details>
  <summary>Details</summary>
Motivation: 基因表达分析复杂且需要专业知识，现有自动化方法在灵活性和精确性上存在不足。

Method: GenoMAS通过六个专门的LLM代理和类型化消息传递协议，结合指导性规划框架，动态调整分析流程。

Result: 在GenoTEX基准测试中，数据预处理和基因识别的性能分别提升了10.61%和16.85%。

Conclusion: GenoMAS在保持逻辑一致性的同时，灵活适应基因组数据的特性，并能发现生物学上合理的基因-表型关联。

Abstract: Gene expression analysis holds the key to many biomedical discoveries, yet
extracting insights from raw transcriptomic data remains formidable due to the
complexity of multiple large, semi-structured files and the need for extensive
domain expertise. Current automation approaches are often limited by either
inflexible workflows that break down in edge cases or by fully autonomous
agents that lack the necessary precision for rigorous scientific inquiry.
GenoMAS charts a different course by presenting a team of LLM-based scientists
that integrates the reliability of structured workflows with the adaptability
of autonomous agents. GenoMAS orchestrates six specialized LLM agents through
typed message-passing protocols, each contributing complementary strengths to a
shared analytic canvas. At the heart of GenoMAS lies a guided-planning
framework: programming agents unfold high-level task guidelines into Action
Units and, at each juncture, elect to advance, revise, bypass, or backtrack,
thereby maintaining logical coherence while bending gracefully to the
idiosyncrasies of genomic data.
  On the GenoTEX benchmark, GenoMAS reaches a Composite Similarity Correlation
of 89.13% for data preprocessing and an F$_1$ of 60.48% for gene
identification, surpassing the best prior art by 10.61% and 16.85%
respectively. Beyond metrics, GenoMAS surfaces biologically plausible
gene-phenotype associations corroborated by the literature, all while adjusting
for latent confounders. Code is available at https://github.com/Liu-Hy/GenoMAS.

</details>


### [51] [A Survey of Self-Evolving Agents: On Path to Artificial Super Intelligence](https://arxiv.org/abs/2507.21046)
*Huan-ang Gao,Jiayi Geng,Wenyue Hua,Mengkang Hu,Xinzhe Juan,Hongzhang Liu,Shilong Liu,Jiahao Qiu,Xuan Qi,Yiran Wu,Hongru Wang,Han Xiao,Yuhang Zhou,Shaokun Zhang,Jiayi Zhang,Jinyu Xiang,Yixiong Fang,Qiwen Zhao,Dongrui Liu,Qihan Ren,Cheng Qian,Zhenghailong Wang,Minda Hu,Huazheng Wang,Qingyun Wu,Heng Ji,Mengdi Wang*

Main category: cs.AI

TL;DR: 论文综述了自进化智能体的研究，围绕“进化什么”、“何时进化”和“如何进化”三个维度，系统分析了其机制、方法和应用，并探讨了未来挑战和研究方向。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）的静态特性限制了其在动态环境中的适应性，因此需要研究能够实时进化的智能体。

Method: 通过三个维度（进化内容、时机和方法）系统分析自进化智能体的机制，包括模型、内存、工具等组件的进化方法。

Result: 提出了自进化智能体的分类框架，总结了进化算法和架构设计，并分析了其在编码、教育和医疗等领域的应用。

Conclusion: 自进化智能体是实现人工超级智能（ASI）的关键，未来研究需关注安全性、可扩展性和协同进化动态等挑战。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities but remain
fundamentally static, unable to adapt their internal parameters to novel tasks,
evolving knowledge domains, or dynamic interaction contexts. As LLMs are
increasingly deployed in open-ended, interactive environments, this static
nature has become a critical bottleneck, necessitating agents that can
adaptively reason, act, and evolve in real time. This paradigm shift -- from
scaling static models to developing self-evolving agents -- has sparked growing
interest in architectures and methods enabling continual learning and
adaptation from data, interactions, and experiences. This survey provides the
first systematic and comprehensive review of self-evolving agents, organized
around three foundational dimensions -- what to evolve, when to evolve, and how
to evolve. We examine evolutionary mechanisms across agent components (e.g.,
models, memory, tools, architecture), categorize adaptation methods by stages
(e.g., intra-test-time, inter-test-time), and analyze the algorithmic and
architectural designs that guide evolutionary adaptation (e.g., scalar rewards,
textual feedback, single-agent and multi-agent systems). Additionally, we
analyze evaluation metrics and benchmarks tailored for self-evolving agents,
highlight applications in domains such as coding, education, and healthcare,
and identify critical challenges and research directions in safety,
scalability, and co-evolutionary dynamics. By providing a structured framework
for understanding and designing self-evolving agents, this survey establishes a
roadmap for advancing adaptive agentic systems in both research and real-world
deployments, ultimately shedding lights to pave the way for the realization of
Artificial Super Intelligence (ASI), where agents evolve autonomously,
performing at or beyond human-level intelligence across a wide array of tasks.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [52] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)
*Khalid Hasan,Jamil Saquer,Mukulika Ghosh*

Main category: cs.CL

TL;DR: 论文评估了多种Transformer模型在心理健康障碍分类中的表现，发现其优于传统LSTM方法，RoBERTa表现最佳。


<details>
  <summary>Details</summary>
Motivation: 心理健康障碍日益普遍，需要开发自动化工具进行早期检测和监测。

Method: 比较了BERT、RoBERTa等Transformer模型与LSTM方法在Reddit文本分类中的表现，并构建了大型标注数据集。

Result: RoBERTa在测试集上F1分数达99.54%，LSTM结合BERT嵌入表现也很出色。

Conclusion: Transformer模型在实时、可扩展的心理健康监测中效果显著，具有临床潜力。

Abstract: The rising prevalence of mental health disorders necessitates the development
of robust, automated tools for early detection and monitoring. Recent advances
in Natural Language Processing (NLP), particularly transformer-based
architectures, have demonstrated significant potential in text analysis. This
study provides a comprehensive evaluation of state-of-the-art transformer
models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term
Memory (LSTM) based approaches using different text embedding techniques for
mental health disorder classification on Reddit. We construct a large annotated
dataset, validating its reliability through statistical judgmental analysis and
topic modeling. Experimental results demonstrate the superior performance of
transformer models over traditional deep-learning approaches. RoBERTa achieved
the highest classification performance, with a 99.54% F1 score on the hold-out
test set and a 96.05% F1 score on the external test set. Notably, LSTM models
augmented with BERT embeddings proved highly competitive, achieving F1 scores
exceeding 94% on the external dataset while requiring significantly fewer
computational resources. These findings highlight the effectiveness of
transformer-based models for real-time, scalable mental health monitoring. We
discuss the implications for clinical applications and digital mental health
interventions, offering insights into the capabilities and limitations of
state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [53] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)
*Vishakh Padmakumar,Joseph Chee Chang,Kyle Lo,Doug Downey,Aakanksha Naik*

Main category: cs.CL

TL;DR: 论文提出了一种利用大型语言模型（LLM）生成文档比较框架的方法，解决了现有评估模糊和缺乏编辑方法的问题，并通过数据集和编辑技术提升了性能。


<details>
  <summary>Details</summary>
Motivation: 学术文献数量激增，需要有效组织、比较和对比文档集合，但现有方法在评估和编辑方面存在不足。

Method: 1. 通过合成意图增强未标注表格语料库，创建数据集；2. 提出多种基于LLM的框架编辑技术，并比较单次生成方法的性能。

Result: 引入表格意图显著提升了基准性能；小型开源模型通过微调可与先进提示LLM竞争；编辑技术进一步优化了生成的框架。

Conclusion: 该研究为文档比较框架生成提供了新方法，解决了评估和编辑问题，并通过实验验证了其有效性。

Abstract: The increasing volume of academic literature makes it essential for
researchers to organize, compare, and contrast collections of documents. Large
language models (LLMs) can support this process by generating schemas defining
shared aspects along which to compare papers. However, progress on schema
generation has been slow due to: (i) ambiguity in reference-based evaluations,
and (ii) lack of editing/refinement methods. Our work is the first to address
both issues. First, we present an approach for augmenting unannotated table
corpora with synthesized intents and apply it to create a dataset for studying
schema generation conditioned on a given information need, thus reducing
ambiguity. With this dataset, we show how incorporating table intents
significantly improves baseline performance in reconstructing reference
schemas. Next, we propose several LLM-based schema editing techniques. We start
by comprehensively benchmarking several single-shot schema generation methods,
including prompted LLM workflows and fine-tuned models, showing that smaller,
open-weight models can be fine-tuned to be competitive with state-of-the-art
prompted LLMs. Then we demonstrate that our editing techniques can further
improve schemas generated by these methods.

</details>


### [54] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)
*Felix Kraus,Nicolas Blumenröhr,Danah Tonne,Achim Streit*

Main category: cs.CL

TL;DR: WOKIE是一个开源、模块化的自动化SKOS词表翻译工具，结合外部翻译服务和LLM优化，提升多语言知识资源的可访问性和互操作性。


<details>
  <summary>Details</summary>
Motivation: 解决数字人文学科中语言多样性导致的资源访问、重用和语义互操作性问题。

Method: 结合外部翻译服务和LLM进行针对性优化，支持多种语言和参数配置。

Result: 在15种语言的词表测试中，WOKIE显著提升了翻译质量、性能和本体匹配效果。

Conclusion: WOKIE通过自动化翻译和优化本体匹配，支持更包容和多语言的研究基础设施。

Abstract: We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for
the automated translation of SKOS thesauri. This work addresses a critical need
in the Digital Humanities (DH), where language diversity can limit access,
reuse, and semantic interoperability of knowledge resources. WOKIE combines
external translation services with targeted refinement using Large Language
Models (LLMs), balancing translation quality, scalability, and cost. Designed
to run on everyday hardware and be easily extended, the application requires no
prior expertise in machine translation or LLMs. We evaluate WOKIE across
several DH thesauri in 15 languages with different parameters, translation
services and LLMs, systematically analysing translation quality, performance,
and ontology matching improvements. Our results show that WOKIE is suitable to
enhance the accessibility, reuse, and cross-lingual interoperability of
thesauri by hurdle-free automated translation and improved ontology matching
performance, supporting more inclusive and multilingual research
infrastructures.

</details>


### [55] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)
*Shengyuan Wang,Jie Feng,Tianhui Liu,Dan Pei,Yong Li*

Main category: cs.CL

TL;DR: 论文提出了一种评估和减少大语言模型（LLMs）地理空间幻觉的框架，通过知识图谱评估和动态事实对齐方法（KTO），显著提升了模型的可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs在地理空间任务中存在幻觉问题，影响其可靠性，但相关研究较少。

Method: 提出评估框架和基于KTO的动态事实对齐方法。

Result: 在20个先进LLMs上评估，性能提升29.6%。

Conclusion: 框架和方法有效提升了LLMs在地理空间任务中的可信度。

Abstract: Large language models (LLMs) possess extensive world knowledge, including
geospatial knowledge, which has been successfully applied to various geospatial
tasks such as mobility prediction and social indicator prediction. However,
LLMs often generate inaccurate geospatial knowledge, leading to geospatial
hallucinations (incorrect or inconsistent representations of geospatial
information) that compromise their reliability. While the phenomenon of general
knowledge hallucination in LLMs has been widely studied, the systematic
evaluation and mitigation of geospatial hallucinations remain largely
unexplored. To address this gap, we propose a comprehensive evaluation
framework for geospatial hallucinations, leveraging structured geospatial
knowledge graphs for controlled assessment. Through extensive evaluation across
20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge.
Building on these insights, we introduce a dynamic factuality aligning method
based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial
hallucinations in LLMs, leading to a performance improvement of over 29.6% on
the proposed benchmark. Extensive experimental results demonstrate the
effectiveness of our benchmark and learning algorithm in enhancing the
trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [56] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)
*Yutao Sun,Zhenyu Li,Yike Zhang,Tengyu Pan,Bowen Dong,Yuyi Guo,Jianyong Wang*

Main category: cs.CL

TL;DR: 本文综述了Transformer架构中高效注意力机制的研究进展，包括线性注意力和稀疏注意力两大类，旨在解决自注意力的二次复杂度问题。


<details>
  <summary>Details</summary>
Motivation: 自注意力的二次时间和内存复杂度限制了长上下文建模的效率，因此需要研究高效注意力机制。

Method: 线性注意力通过核近似、循环公式或快速权重动态实现线性复杂度；稀疏注意力通过固定模式、块路由或聚类策略选择部分token进行计算。

Result: 综述了高效注意力机制的算法创新和硬件级优化，并分析了其在大型预训练语言模型中的应用。

Conclusion: 本文为设计和部署高效、可扩展的语言模型提供了理论基础和实践参考。

Abstract: Transformer-based architectures have become the prevailing backbone of large
language models. However, the quadratic time and memory complexity of
self-attention remains a fundamental obstacle to efficient long-context
modeling. To address this limitation, recent research has introduced two
principal categories of efficient attention mechanisms. Linear attention
methods achieve linear complexity through kernel approximations, recurrent
formulations, or fastweight dynamics, thereby enabling scalable inference with
reduced computational overhead. Sparse attention techniques, in contrast, limit
attention computation to selected subsets of tokens based on fixed patterns,
block-wise routing, or clustering strategies, enhancing efficiency while
preserving contextual coverage. This survey provides a systematic and
comprehensive overview of these developments, integrating both algorithmic
innovations and hardware-level considerations. In addition, we analyze the
incorporation of efficient attention into largescale pre-trained language
models, including both architectures built entirely on efficient attention and
hybrid designs that combine local and global components. By aligning
theoretical foundations with practical deployment strategies, this work aims to
serve as a foundational reference for advancing the design of scalable and
efficient language models.

</details>


### [57] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)
*Muntasir Wahed,Xiaona Zhou,Kiet A. Nguyen,Tianjiao Yu,Nirav Diwan,Gang Wang,Dilek Hakkani-Tür,Ismini Lourentzou*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLMs）在代码生成中的鲁棒性，提出了代码分解攻击，并引入了一个基准测试MOCHA来评估模型对恶意提示的防御能力。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在代码生成方面取得了显著进展，但其在面对多轮恶意提示时的鲁棒性尚未充分研究。

Method: 论文提出了代码分解攻击，将恶意任务分解为多个看似无害的子任务，并引入了MOCHA基准测试进行系统评估。

Result: 实验表明，模型在多轮攻击下仍存在漏洞，但在MOCHA上进行微调后，拒绝率显著提高，且不影响代码生成能力。

Conclusion: 通过微调MOCHA，可以显著提升模型对恶意提示的防御能力，无需额外监督。

Abstract: Recent advancements in Large Language Models (LLMs) have significantly
enhanced their code generation capabilities. However, their robustness against
adversarial misuse, particularly through multi-turn malicious coding prompts,
remains underexplored. In this work, we introduce code decomposition attacks,
where a malicious coding task is broken down into a series of seemingly benign
subtasks across multiple conversational turns to evade safety filters. To
facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale
benchmark designed to evaluate the robustness of code LLMs against both
single-turn and multi-turn malicious prompts. Empirical results across open-
and closed-source models reveal persistent vulnerabilities, especially under
multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while
preserving coding ability, and importantly, enhances robustness on external
adversarial datasets with up to 32.4% increase in rejection rates without any
additional supervision.

</details>


### [58] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)
*Xuchen Wei,Yangxin Wu,Yaoyin Zhang,Henglyu Liu,Kehai Chen,Xuefeng Bai,Min Zhang*

Main category: cs.CL

TL;DR: HITSZ提出了一种结合Whisper ASR和Krutrim LLM的端到端系统，用于低资源英语与印度语之间的语音到文本翻译，并探索了Chain-of-Thought方法的潜力。


<details>
  <summary>Details</summary>
Motivation: 在低资源场景下提升英语与印度语之间的语音翻译质量。

Method: 整合预训练的Whisper ASR模型和印度语专用LLM Krutrim，构建端到端系统，并尝试Chain-of-Thought方法。

Result: 平均BLEU分数为28.88（英译印）和27.86（印译英），Chain-of-Thought方法在某些情况下显著提升翻译质量（如泰米尔语译英语BLEU提升13.84）。

Conclusion: 端到端系统有效，Chain-of-Thought方法有潜力但需解决输出格式一致性问题。

Abstract: This paper presents HITSZ's submission for the IWSLT 2025 Indic track,
focusing on speech-to-text translation (ST) for English-to-Indic and
Indic-to-English language pairs. To enhance translation quality in this
low-resource scenario, we propose an end-to-end system integrating the
pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an
Indic-specialized large language model (LLM). Experimental results demonstrate
that our end-to-end system achieved average BLEU scores of $28.88$ for
English-to-Indic directions and $27.86$ for Indic-to-English directions.
Furthermore, we investigated the Chain-of-Thought (CoT) method. While this
method showed potential for significant translation quality improvements on
successfully parsed outputs (e.g. a $13.84$ BLEU increase for
Tamil-to-English), we observed challenges in ensuring the model consistently
adheres to the required CoT output format.

</details>


### [59] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)
*Sara Papi,Maike Züfle,Marco Gaido,Beatrice Savoldi,Danni Liu,Ioannis Douros,Luisa Bentivogli,Jan Niehues*

Main category: cs.CL

TL;DR: MCIF是一个多语言、多模态的基准测试，用于评估大型语言模型在多语言和多模态任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试在多语言、多模态和长上下文评估方面存在不足，无法全面评估模型的性能。

Method: 引入MCIF基准测试，基于科学讲座的多语言人工标注数据，覆盖语音、视觉和文本三种模态及四种语言。

Result: MCIF填补了现有基准测试的空白，支持对模型在多语言和多模态任务中的全面评估。

Conclusion: MCIF的发布旨在促进多模态大型语言模型的开放研究和进展。

Abstract: Recent advances in large language models have catalyzed the development of
multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified
frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to
general-purpose instruction-following models, a key frontier lies in evaluating
their multilingual and multimodal capabilities over both long and short
contexts. However, existing benchmarks fall short in evaluating these
dimensions jointly: they are often limited to English, mostly focus on one
single modality at a time, rely on short-form contexts, or lack human
annotations -- hindering comprehensive assessment of model performance across
languages, modalities, and task complexity. To address these gaps, we introduce
MCIF (Multimodal Crosslingual Instruction Following), the first multilingual
human-annotated benchmark based on scientific talks that is designed to
evaluate instruction-following in crosslingual, multimodal settings over both
short- and long-form inputs. MCIF spans three core modalities -- speech,
vision, and text -- and four diverse languages (English, German, Italian, and
Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret
instructions across languages and combine them with multimodal contextual
information. MCIF is released under a CC-BY 4.0 license to encourage open
research and progress in MLLMs development.

</details>


### [60] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)
*Andrei Vlad Man,Răzvan-Alexandru Smădu,Cristian-George Craciun,Dumitru-Clementin Cercel,Florin Pop,Mihaela-Claudia Cercel*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）和视觉语言模型（VLMs）在理解和推理罗马尼亚驾驶法律方面的能力，并提出了RoD-TAL多模态数据集。实验表明，领域特定微调显著提升了检索性能，而链式思维提示和专用推理模型提高了问答准确性。


<details>
  <summary>Details</summary>
Motivation: AI与法律系统的交叉领域需要支持法律教育的工具，尤其是在资源匮乏的语言（如罗马尼亚语）中。

Method: 通过文本和视觉问答任务评估LLMs和VLMs的能力，引入RoD-TAL数据集，并测试检索增强生成（RAG）管道、密集检索器和推理优化模型。

Result: 领域特定微调显著提升检索性能，链式思维提示和专用推理模型提高问答准确性，但视觉推理仍具挑战性。

Conclusion: LLMs和VLMs在法律教育中具有潜力，但视觉推理仍需改进。

Abstract: The intersection of AI and legal systems presents a growing need for tools
that support legal education, particularly in under-resourced languages such as
Romanian. In this work, we aim to evaluate the capabilities of Large Language
Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning
about Romanian driving law through textual and visual question-answering tasks.
To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising
Romanian driving test questions, text-based and image-based, alongside
annotated legal references and human explanations. We implement and assess
retrieval-augmented generation (RAG) pipelines, dense retrievers, and
reasoning-optimized models across tasks including Information Retrieval (IR),
Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate
that domain-specific fine-tuning significantly enhances retrieval performance.
At the same time, chain-of-thought prompting and specialized reasoning models
improve QA accuracy, surpassing the minimum grades required to pass driving
exams. However, visual reasoning remains challenging, highlighting the
potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [61] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)
*Maitha Alshehhi,Ahmed Sharshar,Mohsen Guizani*

Main category: cs.CL

TL;DR: 论文研究了多语言和单语言大语言模型（LLMs）在阿拉伯语、英语和印度语中的表现，重点探讨了模型压缩策略（如剪枝和量化）的影响。发现多语言模型普遍优于单语言模型，量化有效但剪枝会显著降低性能。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs在低资源语言环境（如阿拉伯语和印度语）中的能力，并探索模型压缩策略对性能的影响。

Method: 通过基准测试比较多语言和单语言LLMs的性能，分析剪枝和量化策略的效果。

Result: 多语言模型表现更优，量化（4位和8位）能保持准确性，但剪枝会显著损害性能，尤其是在大模型中。

Conclusion: 研究为构建可扩展且公平的多语言NLP解决方案提供了关键策略，并强调需解决低资源环境中的幻觉和泛化错误。

Abstract: Although LLMs have attained significant success in high-resource languages,
their capacity in low-resource linguistic environments like Kannada and Arabic
is not yet fully understood. This work benchmarking the performance of
multilingual and monolingual Large Language Models (LLMs) across Arabic,
English, and Indic languages, with particular emphasis on the effects of model
compression strategies such as pruning and quantization. Findings shows
significant performance differences driven by linguistic diversity and resource
availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2.
We find that multilingual versions of the model outperform their
language-specific counterparts across the board, indicating substantial
cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in
maintaining model accuracy while promoting efficiency, but aggressive pruning
significantly compromises performance, especially in bigger models. Our
findings pinpoint key strategies to construct scalable and fair multilingual
NLP solutions and underscore the need for interventions to address
hallucination and generalization errors in the low-resource setting.

</details>


### [62] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)
*Ronak Upasham,Tathagata Dey,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: 该论文提出了一种新颖的三阶段流水线方法，用于从表格数据生成包含主观性的文本，通过RDF三元组提取、文本聚合和主观性注入，实现了事实准确性与主观解释的平衡，性能优于部分大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有表格到文本生成方法主要关注客观描述，而如何生成包含主观解释的文本尚未充分探索。

Method: 采用三阶段流水线：1) 提取RDF三元组；2) 聚合文本为连贯叙述；3) 注入主观性以丰富文本。使用小型T5模型而非大型语言模型。

Result: 在多项指标上性能与GPT-3.5相当，优于Mistral-7B和Llama-2，同时保持了事实准确性和主观性。

Conclusion: 这是首个通过中间表示增强事实正确性和主观性的表格到文本生成结构化流水线。

Abstract: In Table-to-Text (T2T) generation, existing approaches predominantly focus on
providing objective descriptions of tabular data. However, generating text that
incorporates subjectivity, where subjectivity refers to interpretations beyond
raw numerical data, remains underexplored. To address this, we introduce a
novel pipeline that leverages intermediate representations to generate both
objective and subjective text from tables. Our three-stage pipeline consists
of: 1) extraction of Resource Description Framework (RDF) triples, 2)
aggregation of text into coherent narratives, and 3) infusion of subjectivity
to enrich the generated text. By incorporating RDFs, our approach enhances
factual accuracy while maintaining interpretability. Unlike large language
models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs
smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5
and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our
approach through quantitative and qualitative analyses, demonstrating its
effectiveness in balancing factual accuracy with subjective interpretation. To
the best of our knowledge, this is the first work to propose a structured
pipeline for T2T generation that integrates intermediate representations to
enhance both factual correctness and subjectivity.

</details>


### [63] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)
*Zhi Zhou,Sirui Miao,Xiangyu Duan,Hao Yang,Min Zhang*

Main category: cs.CL

TL;DR: 提出了一种名为基本阅读蒸馏（BRD）的方法，通过教育小型模型模仿大型语言模型（LLM）的基本阅读行为，使其在多种任务中表现优于或接近20倍大的LLM。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）因计算资源需求高而难以部署的问题，同时弥补现有蒸馏方法忽视小型模型在通用文本上的基本阅读能力。

Method: 提出基本阅读蒸馏（BRD），训练小型模型模仿LLM的基本阅读行为（如命名实体识别、问答等），然后应用于下游任务。

Result: 小型模型在语言推理基准和BIG-bench任务中表现优于或接近20倍大的LLM。

Conclusion: BRD能有效影响小型模型的概率分布，且与知识蒸馏或任务蒸馏具有正交性。

Abstract: Large language models (LLMs) have demonstrated remarkable abilities in
various natural language processing areas, but they demand high computation
resources which limits their deployment in real-world. Distillation is one
technique to solve this problem through either knowledge distillation or task
distillation. Both distillation approaches train small models to imitate
specific features of LLMs, but they all neglect basic reading education for
small models on generic texts that are \emph{unrelated} to downstream tasks. In
this paper, we propose basic reading distillation (BRD) which educates a small
model to imitate LLMs basic reading behaviors, such as named entity
recognition, question raising and answering, on each sentence. After such basic
education, we apply the small model on various tasks including language
inference benchmarks and BIG-bench tasks. It shows that the small model can
outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that
BRD effectively influences the probability distribution of the small model, and
has orthogonality to either knowledge distillation or task distillation.

</details>


### [64] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)
*Yifan Hao,Fangning Chao,Yaqian Hao,Zhaojun Cui,Huan Bai,Haiyu Zhang,Yankai Liu,Chao Deng,Junlan Feng*

Main category: cs.CL

TL;DR: JT-Math-8B是一系列开源模型，通过多阶段优化框架提升数学推理能力，在类似规模的开源模型中表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在复杂数学问题中表现不足的问题。

Method: 采用多阶段优化框架，包括基础、指导和思考版本，使用高质量数据集和强化学习训练。

Result: 在开源模型中表现最优，超越OpenAI的O1-mini和GPT-4o。

Conclusion: JT-Math-8B在数学推理任务中表现出色，为开源模型树立了新标杆。

Abstract: Mathematical reasoning is a cornerstone of artificial general intelligence
and a primary benchmark for evaluating the capabilities of Large Language
Models (LLMs). While state-of-the-art models show promise, they often falter
when faced with complex problems that demand deep conceptual understanding and
intricate, multi-step deliberation. To address this challenge, we introduce
JT-Math-8B, a series of open-source models comprising base, instruct, and
thinking versions, built upon a systematic, multi-stage optimization framework.
Our pre-training corpus is a high-quality, 210B-token dataset curated through a
dedicated data pipeline that uses model-based validation to ensure quality and
diversity. The Instruct Model is optimized for direct, concise answers through
Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL)
method. The Thinking Model is trained for complex problem-solving using a Long
Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage
RL curriculum that progressively increases task difficulty and context length
up to 32K tokens. JT-Math-8B achieves state-of-the-art results among
open-source models of similar size, surpassing prominent models like OpenAI's
O1-mini and GPT-4o , and demonstrating superior performance on
competition-level mathematics.

</details>


### [65] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)
*Rebecca M. M. Hicke,Brian Haggard,Mia Ferrante,Rayhan Khanna,David Mimno*

Main category: cs.CL

TL;DR: 本文利用计算工具研究基督教小说的主题及其对神迹的描写，发现《末日迷踪》系列与其他基督教小说及男女作者作品间存在显著差异。


<details>
  <summary>Details</summary>
Motivation: 研究基督教小说的文化层面，尤其是其文学表现和神迹描写，填补学术空白。

Method: 结合人类注释和轻量级语言模型，定义并标注“神迹”，比较不同作品间的差异。

Result: 轻量级模型能匹配人类注释，揭示《末日迷踪》与其他作品及男女作者间的显著差异。

Conclusion: 计算工具能有效分析基督教小说，揭示其多样性和作者性别差异。

Abstract: In addition to its more widely studied political activities, the American
Evangelical movement has a well-developed but less externally visible cultural
and literary side. Christian Fiction, however, has been little studied, and
what scholarly attention there is has focused on the explosively popular Left
Behind series. In this work, we use computational tools to provide both a broad
topical overview of Christian Fiction as a genre and a more directed
exploration of how its authors depict divine acts. Working with human
annotators we first developed definitions and a codebook for "acts of God." We
then adapted those instructions designed for human annotators for use by a
recent, lightweight LM with the assistance of a much larger model. The
laptop-scale LM is capable of matching human annotations, even when the task is
subtle and challenging. Using these annotations, we show that significant and
meaningful differences exist between the Left Behind books and Christian
Fiction more broadly and between books by male and female authors.

</details>


### [66] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)
*Dong Du,Shulin Liu,Tao Yang,Shaohua Chen,Yang Li*

Main category: cs.CL

TL;DR: 论文提出了一种针对超长输出的强化学习方法（UloRL），通过分段解码和动态掩码技术，解决了传统强化学习在长序列生成中的效率问题，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 传统强化学习框架在处理超长输出时效率低下，主要由于长尾序列分布和训练中的熵崩溃问题。

Method: 采用分段解码技术（segment rollout）和动态掩码技术（MPTs掩码），优化训练效率并防止熵崩溃。

Result: 在Qwen3-30B-A3B模型上，训练速度提升2.06倍，性能在AIME2025和BeyondAIME任务上分别提升至85.1%和61.9%。

Conclusion: UloRL方法显著提升了LLM在超长序列生成中的推理能力，具有广泛应用潜力。

Abstract: Recent advances in large language models (LLMs) have highlighted the
potential of reinforcement learning with verifiable rewards (RLVR) to enhance
reasoning capabilities through extended output sequences. However, traditional
RL frameworks face inefficiencies when handling ultra-long outputs due to
long-tail sequence distributions and entropy collapse during training. To
address these challenges, we propose an Ultra-Long Output Reinforcement
Learning (UloRL) approach for advancing large language models' reasoning
abilities. Specifically, we divide ultra long output decoding into short
segments, enabling efficient training by mitigating delays caused by long-tail
samples. Additionally, we introduce dynamic masking of well-Mastered Positive
Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the
effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment
rollout achieved 2.06x increase in training speed, while RL training with
128k-token outputs improves the model's performance on AIME2025 from 70.9\% to
85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B
with remarkable gains. These findings underscore the potential of our methods
to advance the reasoning capabilities of LLMs with ultra-long sequence
generation. We will release our code and model for further use by the
community.

</details>


### [67] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)
*Tianxiang Chen,Zhentao Tan,Xiaofan Bo,Yue Wu,Tao Gong,Qi Chu,Jieping Ye,Nenghai Yu*

Main category: cs.CL

TL;DR: Flora是一种无需人工或LLM干预的长上下文构建策略，通过组合短指令提升LLMs的长上下文性能，同时保持短上下文能力。


<details>
  <summary>Details</summary>
Motivation: 解决LLMs处理长上下文时的高成本、低多样性和短上下文性能下降问题。

Method: Flora通过分类组合短指令，并基于长上下文元指令生成响应，实现任意长度和多样性的上下文构建。

Result: 在Llama3-8B-Instruct和QwQ-32B上，Flora显著提升了长上下文性能，同时短上下文性能仅轻微下降。

Conclusion: Flora提供了一种高效、低成本的长上下文优化方法，适用于多种LLMs。

Abstract: Effectively handling long contexts is challenging for Large Language Models
(LLMs) due to the rarity of long texts, high computational demands, and
substantial forgetting of short-context abilities. Recent approaches have
attempted to construct long contexts for instruction tuning, but these methods
often require LLMs or human interventions, which are both costly and limited in
length and diversity. Also, the drop in short-context performances of present
long-context LLMs remains significant. In this paper, we introduce Flora, an
effortless (human/LLM-free) long-context construction strategy. Flora can
markedly enhance the long-context performance of LLMs by arbitrarily assembling
short instructions based on categories and instructing LLMs to generate
responses based on long-context meta-instructions. This enables Flora to
produce contexts of arbitrary length and scale with rich diversity, while only
slightly compromising short-context performance. Experiments on
Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three
long-context benchmarks while maintaining strong performances in short-context
tasks. Our data-construction code is available at
\href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [68] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)
*Dongquan Yang,Yifan Yang,Xiaotian Yu,Xianbiao Qi,Rong Xiao*

Main category: cs.CL

TL;DR: HCAttention提出了一种异构注意力计算框架，通过量化键、卸载值和动态KV淘汰，在极端内存限制下实现高效推理，将KV缓存内存占用缩小至25%，且无需微调模型。


<details>
  <summary>Details</summary>
Motivation: 处理长上下文输入时，KV缓存的内存需求巨大，现有压缩方法在内存减少超过85%时性能显著下降，且GPU-CPU协作策略未被充分探索。

Method: HCAttention结合键量化、值卸载和动态KV淘汰，兼容现有Transformer架构，无需模型微调。

Result: 在LongBench基准测试中，HCAttention在KV缓存内存占用降至25%时保持全注意力模型的准确性，12.5%时仍具竞争力，首次在单A100 GPU上处理400万token。

Conclusion: HCAttention在KV缓存压缩领域取得新突破，显著提升长上下文处理能力。

Abstract: Processing long-context inputs with large language models presents a
significant challenge due to the enormous memory requirements of the Key-Value
(KV) cache during inference. Existing KV cache compression methods exhibit
noticeable performance degradation when memory is reduced by more than 85%.
Additionally, strategies that leverage GPU-CPU collaboration for approximate
attention remain underexplored in this setting. We propose HCAttention, a
heterogeneous attention computation framework that integrates key quantization,
value offloading, and dynamic KV eviction to enable efficient inference under
extreme memory constraints. The method is compatible with existing transformer
architectures and does not require model fine-tuning. Experimental results on
the LongBench benchmark demonstrate that our approach preserves the accuracy of
full-attention model while shrinking the KV cache memory footprint to 25% of
its original size. Remarkably, it stays competitive with only 12.5% of the
cache, setting a new state-of-the-art in LLM KV cache compression. To the best
of our knowledge, HCAttention is the first to extend the Llama-3-8B model to
process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [69] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)
*Anshul Chavda,M Jagadeesh,Chintalapalli Raja Kullayappa,B Jayaprakash,Medchalimi Sruthi,Pushpak Bhattacharyya*

Main category: cs.CL

TL;DR: DiscoDrive是一个合成的车载对话数据集，通过动态整合不流畅性，显著提升了对话AI的性能和自然度。


<details>
  <summary>Details</summary>
Motivation: 现有数据集未能捕捉真实车载对话中的不流畅性（如犹豫、重复等），限制了对话AI的实际表现。

Method: 采用两阶段、提示驱动的合成方法，生成包含3500个多轮对话的DiscoDrive数据集。

Result: 在多个测试集上表现优于现有方法，BLEU-4提升0.26至0.61，人类评价也更高。

Conclusion: DiscoDrive填补了资源空白，为车载对话AI提供了更真实、有效的训练和增强工具。

Abstract: In-car conversational AI is becoming increasingly critical as autonomous
vehicles and smart assistants gain widespread adoption. Yet, existing datasets
fail to capture the spontaneous disfluencies such as hesitations, false starts,
repetitions, and self-corrections that characterize real driver-AI dialogs. To
address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn
dialogs across seven automotive domains, generated using a two-stage,
prompt-driven pipeline that dynamically integrates disfluencies during
synthesis. We show that DiscoDrive is effective both as a training resource,
enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on
the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4
improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1
improvements of 1.35 to 3.48), and as a data augmentation resource in
low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38,
METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10
percent of KVRET. Human evaluations further confirm that dialogs sampled from
DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness
(3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more
context-appropriate than leading post-hoc methods (such as LARD), without
compromising clarity. DiscoDrive fills a critical gap in existing resources and
serves as a versatile corpus for both training and augmenting conversational
AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [70] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)
*Danil Fokin,Monika Płużyczka,Grigory Golovin*

Main category: cs.CL

TL;DR: PVST是一种基于项目反应理论和计算机自适应测试的新工具，用于评估波兰语母语和非母语者的接受性词汇量，测试时间短且准确度高。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够动态适应测试者水平的词汇量评估工具，以提高测试效率和准确性。

Method: 基于项目反应理论和计算机自适应测试，动态调整测试内容以适应测试者水平。

Result: 验证研究表明，母语者的词汇量显著大于非母语者，且母语者的词汇量与年龄呈正相关。

Conclusion: PVST是一种高效、准确的词汇量测试工具，适用于母语和非母语者，已在线提供。

Abstract: We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing
the receptive vocabulary size of both native and non-native Polish speakers.
Based on Item Response Theory and Computerized Adaptive Testing, PVST
dynamically adjusts to each test-taker's proficiency level, ensuring high
accuracy while keeping the test duration short. To validate the test, a pilot
study was conducted with 1.475 participants. Native Polish speakers
demonstrated significantly larger vocabularies compared to non-native speakers.
For native speakers, vocabulary size showed a strong positive correlation with
age. The PVST is available online at myvocab.info/pl.

</details>


### [71] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)
*Cesar Augusto Madid Truyts,Amanda Gomes Rabelo,Gabriel Mesquita de Souza,Daniel Scaldaferri Lages,Adriano Jose Pereira,Uri Adrian Prync Flato,Eduardo Pontes dos Reis,Joaquim Edson Vieira,Paulo Sergio Panse Silveira,Edson Amaro Junior*

Main category: cs.CL

TL;DR: 研究评估了六种LLMs和四种MLLMs在巴西葡萄牙语医学考试中的表现，发现部分模型表现接近人类，但存在语言和模态差距。


<details>
  <summary>Details</summary>
Motivation: 当前AI模型评估主要集中于英语，可能导致其他语言表现偏差，研究旨在填补这一空白。

Method: 测试模型在巴西葡萄牙语医学考试中的准确性、处理时间和解释连贯性，并与人类表现对比。

Result: 部分模型（如Claude-3.5-Sonnet和Claude-3-Opus）表现接近人类，但多模态问题仍有差距。

Conclusion: 需进一步优化非英语AI模型，强调多语言和多模态评估的重要性。

Abstract: Artificial intelligence (AI) has shown the potential to revolutionize
healthcare by improving diagnostic accuracy, optimizing workflows, and
personalizing treatment plans. Large Language Models (LLMs) and Multimodal
Large Language Models (MLLMs) have achieved notable advancements in natural
language processing and medical applications. However, the evaluation of these
models has focused predominantly on the English language, leading to potential
biases in their performance across different languages.
  This study investigates the capability of six LLMs (GPT-4.0 Turbo,
LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and
Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet,
and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese
from the medical residency entrance exam of the Hospital das Cl\'inicas da
Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest
health complex in South America. The performance of the models was benchmarked
against human candidates, analyzing accuracy, processing time, and coherence of
the generated explanations.
  The results show that while some models, particularly Claude-3.5-Sonnet and
Claude-3-Opus, achieved accuracy levels comparable to human candidates,
performance gaps persist, particularly in multimodal questions requiring image
interpretation. Furthermore, the study highlights language disparities,
emphasizing the need for further fine-tuning and data set augmentation for
non-English medical AI applications.
  Our findings reinforce the importance of evaluating generative AI in various
linguistic and clinical settings to ensure a fair and reliable deployment in
healthcare. Future research should explore improved training methodologies,
improved multimodal reasoning, and real-world clinical integration of AI-driven
medical assistance.

</details>


### [72] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)
*Prajval Bolegave,Pushpak Bhattacharya*

Main category: cs.CL

TL;DR: 该论文提出了一个专家标注的社交媒体帖子数据集，用于抑郁症早期检测，并开发了一个评估框架，测试大型语言模型在生成临床解释时的表现。


<details>
  <summary>Details</summary>
Motivation: 通过社交媒体帖子早期检测抑郁症，为心理健康干预提供支持。

Method: 构建高质量数据集，设计评估框架，测试不同提示策略下的LLM表现。

Result: 发现不同模型在临床解释任务中表现差异显著，人类专家指导对模型行为有重要价值。

Conclusion: 研究为心理健康领域的AI系统提供了更安全、透明的方向。

Abstract: Early detection of depression from online social media posts holds promise
for providing timely mental health interventions. In this work, we present a
high-quality, expert-annotated dataset of 1,017 social media posts labeled with
depressive spans and mapped to 12 depression symptom categories. Unlike prior
datasets that primarily offer coarse post-level labels
\cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of
both model predictions and generated explanations.
  We develop an evaluation framework that leverages this clinically grounded
dataset to assess the faithfulness and quality of natural language explanations
generated by large language models (LLMs). Through carefully designed prompting
strategies, including zero-shot and few-shot approaches with domain-adapted
examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1,
Gemini 2.5 Pro, and Claude 3.7 Sonnet.
  Our comprehensive empirical analysis reveals significant differences in how
these models perform on clinical explanation tasks, with zero-shot and few-shot
prompting. Our findings underscore the value of human expertise in guiding LLM
behavior and offer a step toward safer, more transparent AI systems for
psychological well-being.

</details>


### [73] [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)
*Yi Su,Quantong Qiu,Yuechi Zhou,Juntao Li,Qingrong Xia,Ping Li,Xinyu Duan,Zhefeng Wang,Min Zhang*

Main category: cs.CL

TL;DR: 论文提出了一种名为CaliDrop的新策略，通过校准增强令牌驱逐，以减少KV缓存的内存占用，同时减轻精度损失。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）生成过程中需要大量计算资源，KV缓存的内存占用随序列长度、批次大小和模型规模线性增长，成为长上下文场景的瓶颈。

Method: 提出CaliDrop策略，利用附近位置查询的高相似性，对丢弃的令牌进行推测性校准，以减轻令牌驱逐带来的精度损失。

Result: 实验表明，CaliDrop显著提高了现有令牌驱逐方法的精度。

Conclusion: CaliDrop通过校准有效解决了令牌驱逐中的精度损失问题，为KV缓存压缩提供了新思路。

Abstract: Large Language Models (LLMs) require substantial computational resources
during generation. While the Key-Value (KV) cache significantly accelerates
this process by storing attention intermediates, its memory footprint grows
linearly with sequence length, batch size, and model size, creating a
bottleneck in long-context scenarios. Various KV cache compression techniques,
including token eviction, quantization, and low-rank projection, have been
proposed to mitigate this bottleneck, often complementing each other. This
paper focuses on enhancing token eviction strategies. Token eviction leverages
the observation that the attention patterns are often sparse, allowing for the
removal of less critical KV entries to save memory. However, this reduction
usually comes at the cost of notable accuracy degradation, particularly under
high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a
novel strategy that enhances token eviction through calibration. Our
preliminary experiments show that queries at nearby positions exhibit high
similarity. Building on this observation, CaliDrop performs speculative
calibration on the discarded tokens to mitigate the accuracy loss caused by
token eviction. Extensive experiments demonstrate that CaliDrop significantly
improves the accuracy of existing token eviction methods.

</details>


### [74] [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)
*Seorin Kim,Dongyoung Lee,Jaejin Lee*

Main category: cs.CL

TL;DR: KLAAD是一种基于注意力机制的框架，通过隐式对齐注意力分布来减少LLM中的社会偏见，无需直接修改模型权重。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）输出中存在社会偏见，引发公平性和危害性等伦理问题。

Method: 提出KLAAD框架，结合交叉熵、KL散度和三元组损失，引导模型在偏见和无偏见上下文中保持一致的注意力分布。

Result: 在BBQ和BOLD基准测试中，KLAAD显著减少了偏见，同时对语言建模质量影响最小。

Conclusion: 注意力级别的对齐为生成语言模型中的偏见缓解提供了原则性解决方案。

Abstract: Large language models (LLMs) often exhibit societal biases in their outputs,
prompting ethical concerns regarding fairness and harm. In this work, we
propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing
framework that implicitly aligns attention distributions between stereotypical
and anti-stereotypical sentence pairs without directly modifying model weights.
KLAAD introduces a composite training objective combining Cross-Entropy, KL
divergence, and Triplet losses, guiding the model to consistently attend across
biased and unbiased contexts while preserving fluency and coherence.
Experimental evaluation of KLAAD demonstrates improved bias mitigation on both
the BBQ and BOLD benchmarks, with minimal impact on language modeling quality.
The results indicate that attention-level alignment offers a principled
solution for mitigating bias in generative language models.

</details>


### [75] [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)
*Mizanur Rahman,Md Tahmid Rahman Laskar,Shafiq Joty,Enamul Hoque*

Main category: cs.CL

TL;DR: Text2Vis是一个用于评估文本到可视化模型的基准测试，涵盖20多种图表类型和复杂查询，包含1985个样本。通过测试11种模型，揭示了性能差距，并提出了一种跨模态框架提升性能。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏对大型语言模型生成可视化能力的全面评估基准，限制了其能力的严格测试。

Method: 引入Text2Vis基准测试，包含多样化的数据科学查询和图表类型，并提出跨模态框架优化模型性能。

Result: 测试显示模型性能存在显著差距，提出的框架将GPT-4o的通过率从26%提升至42%。

Conclusion: Text2Vis填补了评估空白，提出的框架和自动化评估方法为未来研究提供了方向。

Abstract: Automated data visualization plays a crucial role in simplifying data
interpretation, enhancing decision-making, and improving efficiency. While
large language models (LLMs) have shown promise in generating visualizations
from natural language, the absence of comprehensive benchmarks limits the
rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark
designed to assess text-to-visualization models, covering 20+ chart types and
diverse data science queries, including trend analysis, correlation, outlier
detection, and predictive analytics. It comprises 1,985 samples, each with a
data table, natural language query, short answer, visualization code, and
annotated charts. The queries involve complex reasoning, conversational turns,
and dynamic data retrieval. We benchmark 11 open-source and closed-source
models, revealing significant performance gaps, highlighting key challenges,
and offering insights for future advancements. To close this gap, we propose
the first cross-modal actor-critic agentic framework that jointly refines the
textual answer and visualization code, increasing GPT-4o`s pass rate from 26%
to 42% over the direct approach and improving chart quality. We also introduce
an automated LLM-based evaluation framework that enables scalable assessment
across thousands of samples without human annotation, measuring answer
correctness, code execution success, visualization readability, and chart
accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

</details>


### [76] [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)
*Dan Song,Won-Chan Lee,Hong Jiao*

Main category: cs.CL

TL;DR: 研究评估了大型语言模型（LLMs）在AP中文考试写作任务评分中的可靠性，发现人类评分者更可靠，但LLMs在特定条件下表现合理，混合评分模型可提升可靠性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在AP中文写作评分中的可靠性，并与人类评分者进行比较，以验证混合评分模型的潜力。

Method: 采用泛化理论，对故事叙述和邮件回复两种写作任务进行评分，由人类和AI评分者独立评分，分析整体和分项得分。

Result: 人类评分者更可靠，但LLMs在故事叙述任务中表现较好，混合评分模型能提升评分可靠性。

Conclusion: 混合评分模型在大规模写作评估中具有潜力，LLMs在特定任务中可作为补充工具。

Abstract: This study investigates the estimation of reliability for large language
models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture
Exam. Using generalizability theory, the research evaluates and compares score
consistency between human and AI raters across two types of AP Chinese
free-response writing tasks: story narration and email response. These essays
were independently scored by two trained human raters and seven AI raters. Each
essay received four scores: one holistic score and three analytic scores
corresponding to the domains of task completion, delivery, and language use.
Results indicate that although human raters produced more reliable scores
overall, LLMs demonstrated reasonable consistency under certain conditions,
particularly for story narration tasks. Composite scoring that incorporates
both human and AI raters improved reliability, which supports that hybrid
scoring models may offer benefits for large-scale writing assessments.

</details>


### [77] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)
*Tan-Minh Nguyen,Hoang-Trung Nguyen,Trong-Khoi Dao,Xuan-Hieu Phan,Ha-Thanh Nguyen,Thi-Hai-Yen Vuong*

Main category: cs.CL

TL;DR: 论文介绍了VLQA数据集，针对越南法律领域的高质量资源，并评估了其在法律信息检索和问答任务中的有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在法律文本处理中取得进展，但低资源语言（如越南语）的法律NLP仍面临资源稀缺的挑战，亟需标注数据支持。

Method: 引入VLQA数据集，进行统计分析，并通过实验评估其在法律信息检索和问答任务中的表现。

Result: VLQA数据集为越南法律领域提供了高质量资源，实验证明了其在相关任务中的有效性。

Conclusion: VLQA数据集填补了越南法律NLP的资源空白，为未来研究提供了重要支持。

Abstract: The advent of large language models (LLMs) has led to significant
achievements in various domains, including legal text processing. Leveraging
LLMs for legal tasks is a natural evolution and an increasingly compelling
choice. However, their capabilities are often portrayed as greater than they
truly are. Despite the progress, we are still far from the ultimate goal of
fully automating legal tasks using artificial intelligence (AI) and natural
language processing (NLP). Moreover, legal systems are deeply domain-specific
and exhibit substantial variation across different countries and languages. The
need for building legal text processing applications for different natural
languages is, therefore, large and urgent. However, there is a big challenge
for legal NLP in low-resource languages such as Vietnamese due to the scarcity
of resources and annotated data. The need for labeled legal corpora for
supervised training, validation, and supervised fine-tuning is critical. In
this paper, we introduce the VLQA dataset, a comprehensive and high-quality
resource tailored for the Vietnamese legal domain. We also conduct a
comprehensive statistical analysis of the dataset and evaluate its
effectiveness through experiments with state-of-the-art models on legal
information retrieval and question-answering tasks.

</details>


### [78] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)
*Saurav Singla,Aarav Singla,Advik Gupta,Parnika Gupta*

Main category: cs.CL

TL;DR: 提出了一种基于元学习的框架，用于在有限标注数据下检测多领域人类语言中的异常。


<details>
  <summary>Details</summary>
Motivation: 语言异常（如垃圾邮件、假新闻和仇恨言论）的稀疏性和多样性使其检测成为挑战。

Method: 将异常检测视为少样本二分类问题，利用元学习和原型网络结合领域重采样快速适应新任务。

Result: 在多个数据集上评估，模型在F1和AUC分数上优于基线方法。

Conclusion: 方法有效且代码公开，推动少样本文本异常检测研究。

Abstract: We propose a meta learning framework for detecting anomalies in human
language across diverse domains with limited labeled data. Anomalies in
language ranging from spam and fake news to hate speech pose a major challenge
due to their sparsity and variability. We treat anomaly detection as a few shot
binary classification problem and leverage meta-learning to train models that
generalize across tasks. Using datasets from domains such as SMS spam, COVID-19
fake news, and hate speech, we evaluate model generalization on unseen tasks
with minimal labeled anomalies. Our method combines episodic training with
prototypical networks and domain resampling to adapt quickly to new anomaly
detection tasks. Empirical results show that our method outperforms strong
baselines in F1 and AUC scores. We also release the code and benchmarks to
facilitate further research in few-shot text anomaly detection.

</details>


### [79] [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)
*Runchao Li,Yao Fu,Mu Sheng,Xianxuan Long,Haotian Yu,Pan Li*

Main category: cs.CL

TL;DR: FAEDKV是一种无需训练的KV缓存压缩框架，通过频域转换实现无偏信息保留，优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在长上下文任务中KV缓存的高内存和计算需求问题，避免现有压缩方法导致的偏置表示。

Method: 使用无限窗口傅里叶变换（IWDFT）将KV缓存转换到频域，实现均衡信息保留，并通过频域消融研究确定关键频谱成分。

Result: 在LongBench基准测试中表现优于现有方法22%，在Needle-In-A-Haystack任务中展示出位置无关的检索准确性。

Conclusion: FAEDKV提供了一种高效、无偏的KV缓存压缩方案，显著提升长上下文任务性能。

Abstract: The efficacy of Large Language Models (LLMs) in long-context tasks is often
hampered by the substantial memory footprint and computational demands of the
Key-Value (KV) cache. Current compression strategies, including token eviction
and learned projections, frequently lead to biased representations -- either by
overemphasizing recent/high-attention tokens or by repeatedly degrading
information from earlier context -- and may require costly model retraining. We
present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel,
training-free KV cache compression framework that ensures unbiased information
retention. FAEDKV operates by transforming the KV cache into the frequency
domain using a proposed Infinite-Window Fourier Transform (IWDFT). This
approach allows for the equalized contribution of all tokens to the compressed
representation, effectively preserving both early and recent contextual
information. A preliminary frequency ablation study identifies critical
spectral components for layer-wise, targeted compression. Experiments on
LongBench benchmark demonstrate FAEDKV's superiority over existing methods by
up to 22\%. In addition, our method shows superior, position-agnostic retrieval
accuracy on the Needle-In-A-Haystack task compared to compression based
approaches.

</details>


### [80] [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)
*Akash Ghosh,Aparna Garimella,Pritika Ramu,Sambaran Bandyopadhyay,Sriparna Saha*

Main category: cs.CL

TL;DR: 论文提出了一种从文本生成复杂统计信息图的任务，并介绍了Infodat数据集和Infogen框架，该框架在性能上优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有AI技术仅能生成简单图表，无法从文本密集文档中生成复杂信息图，因此需要填补这一空白。

Method: 提出Infogen框架，分两阶段：微调LLMs生成元数据，再转换为信息图代码。

Result: Infogen在Infodat数据集上表现优异，优于其他开源和闭源LLMs。

Conclusion: Infogen为生成复杂统计信息图提供了有效解决方案，填补了研究空白。

Abstract: Statistical infographics are powerful tools that simplify complex data into
visually engaging and easy-to-understand formats. Despite advancements in AI,
particularly with LLMs, existing efforts have been limited to generating simple
charts, with no prior work addressing the creation of complex infographics from
text-heavy documents that demand a deep understanding of the content. We
address this gap by introducing the task of generating statistical infographics
composed of multiple sub-charts (e.g., line, bar, pie) that are contextually
accurate, insightful, and visually aligned. To achieve this, we define
infographic metadata that includes its title and textual insights, along with
sub-chart-specific details such as their corresponding data and alignment. We
also present Infodat, the first benchmark dataset for text-to-infographic
metadata generation, where each sample links a document to its metadata. We
propose Infogen, a two-stage framework where fine-tuned LLMs first generate
metadata, which is then converted into infographic code. Extensive evaluations
on Infodat demonstrate that Infogen achieves state-of-the-art performance,
outperforming both closed and open-source LLMs in text-to-statistical
infographic generation.

</details>


### [81] [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)
*Avaljot Singh,Yamin Chandini Sarita,Aditya Mishra,Ishaan Goyal,Gagandeep Singh,Charith Mendis*

Main category: cs.CL

TL;DR: 论文提出了一种编译器框架，将DNN验证器的神经元级规范自动转换为基于张量的层级实现，解决了设计与实现之间的语义鸿沟问题。


<details>
  <summary>Details</summary>
Motivation: 由于DNN的不可解释性，基于抽象解释的验证成为建立信任的实用方法，但现有验证器开发困难，设计与实现之间存在语义鸿沟。

Method: 通过一种新颖的基于栈的中间表示（IR）和形状分析，自动将神经元级规范转换为张量实现，并引入g-BCSR格式优化稀疏张量计算。

Result: 编译器框架灵活且性能接近手工优化实现，便于开发新验证器并分析其在不同DNN中的实用性。

Conclusion: 该框架显著降低了开发DNN验证器的复杂性，同时保持了高性能。

Abstract: The uninterpretability of DNNs has led to the adoption of abstract
interpretation-based certification as a practical means to establish trust in
real-world systems that rely on DNNs. However, the current landscape supports
only a limited set of certifiers, and developing new ones or modifying existing
ones for different applications remains difficult. This is because the
mathematical design of certifiers is expressed at the neuron level, while their
implementations are optimized and executed at the tensor level. This mismatch
creates a semantic gap between design and implementation, making manual
bridging both complex and expertise-intensive -- requiring deep knowledge in
formal methods, high-performance computing, etc.
  We propose a compiler framework that automatically translates neuron-level
specifications of DNN certifiers into tensor-based, layer-level
implementations. This is enabled by two key innovations: a novel stack-based
intermediate representation (IR) and a shape analysis that infers the implicit
tensor operations needed to simulate the neuron-level semantics. During
lifting, the shape analysis creates tensors in the minimal shape required to
perform the corresponding operations. The IR also enables domain-specific
optimizations as rewrites. At runtime, the resulting tensor computations
exhibit sparsity tied to the DNN architecture. This sparsity does not align
well with existing formats. To address this, we introduce g-BCSR, a
double-compression format that represents tensors as collections of blocks of
varying sizes, each possibly internally sparse.
  Using our compiler and g-BCSR, we make it easy to develop new certifiers and
analyze their utility across diverse DNNs. Despite its flexibility, the
compiler achieves performance comparable to hand-optimized implementations.

</details>


### [82] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)
*Ran Xu,Yuchen Zhuang,Yue Yu,Haoyu Wang,Wenqi Shi,Carl Yang*

Main category: cs.CL

TL;DR: RAG系统在多样化检索场景下表现有限，需自适应策略。


<details>
  <summary>Details</summary>
Motivation: 探索RAG在多样化知识检索场景中的实际效果。

Method: 使用MassiveDS评估RAG系统，分析检索对小模型的作用、重排器的价值及检索源的多样性。

Result: 检索对小模型更有效，重排器作用有限，检索源表现不一致，LLMs难以路由异构知识源。

Conclusion: 需开发自适应检索策略以提升RAG在现实场景中的表现。

Abstract: Retrieval-augmented generation (RAG) enhances large language models (LLMs) by
integrating external knowledge retrieved at inference time. While RAG
demonstrates strong performance on benchmarks largely derived from
general-domain corpora like Wikipedia, its effectiveness under realistic,
diverse retrieval scenarios remains underexplored. We evaluated RAG systems
using MassiveDS, a large-scale datastore with mixture of knowledge, and
identified critical limitations: retrieval mainly benefits smaller models,
rerankers add minimal value, and no single retrieval source consistently
excels. Moreover, current LLMs struggle to route queries across heterogeneous
knowledge sources. These findings highlight the need for adaptive retrieval
strategies before deploying RAG in real-world settings. Our code and data can
be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [83] [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)
*Kaizhi Qian,Xulin Fan,Junrui Ni,Slava Shechtman,Mark Hasegawa-Johnson,Chuang Gan,Yang Zhang*

Main category: cs.CL

TL;DR: ProsodyLM提出了一种新的语音语言模型训练方法，通过改进的标记化方案更好地捕捉语音中的韵律信息。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型在训练时通常将语音转换为离散标记，导致韵律信息丢失，无法通过预训练学习到韵律处理能力。

Method: ProsodyLM采用一种简单的标记化方案，先将语音转录为文本，再添加词级韵律标记，从而保留更完整的韵律信息。

Result: ProsodyLM通过预训练即可学习到多样化的韵律处理能力，包括生成语音中的韵律细节、理解情感和重音，以及保持长上下文中的韵律一致性。

Conclusion: ProsodyLM通过改进的标记化方案显著提升了语音语言模型对韵律信息的捕捉能力。

Abstract: Speech language models refer to language models with speech processing and
understanding capabilities. One key desirable capability for speech language
models is the ability to capture the intricate interdependency between content
and prosody. The existing mainstream paradigm of training speech language
models, which converts speech into discrete tokens before feeding them into
LLMs, is sub-optimal in learning prosody information -- we find that the
resulting LLMs do not exhibit obvious emerging prosody processing capabilities
via pre-training alone. To overcome this, we propose ProsodyLM, which
introduces a simple tokenization scheme amenable to learning prosody. Each
speech utterance is first transcribed into text, followed by a sequence of
word-level prosody tokens. Compared with conventional speech tokenization
schemes, the proposed tokenization scheme retains more complete prosody
information, and is more understandable to text-based LLMs. We find that
ProsodyLM can learn surprisingly diverse emerging prosody processing
capabilities through pre-training alone, ranging from harnessing the prosody
nuances in generated speech, such as contrastive focus, understanding emotion
and stress in an utterance, to maintaining prosody consistency in long
contexts.

</details>


### [84] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)
*Rodrigo Gabriel Salazar Alva,Matías Nuñez,Cristian López,Javier Martín Arista*

Main category: cs.CL

TL;DR: 提出了一种利用大语言模型（LLM）生成高质量古英语文本的框架，结合参数高效微调、数据增强和双代理流程，显著提升了翻译质量。


<details>
  <summary>Details</summary>
Motivation: 古英语资源匮乏，限制了现代自然语言处理技术的应用，保护文化遗产的需求推动了这一研究。

Method: 采用参数高效微调（LoRA）、数据增强（反向翻译）和双代理流程（内容生成与翻译分离）。

Result: 自动评估指标（BLEU、METEOR、CHRF）显示翻译质量显著提升（BLEU从26增至65），专家评估确认语法准确性和风格保真度高。

Conclusion: 该方法不仅扩展了古英语语料库，还为其他濒危语言的复兴提供了实用方案，将AI创新与文化保护目标结合。

Abstract: Preserving ancient languages is essential for understanding humanity's
cultural and linguistic heritage, yet Old English remains critically
under-resourced, limiting its accessibility to modern natural language
processing (NLP) techniques. We present a scalable framework that uses advanced
large language models (LLMs) to generate high-quality Old English texts,
addressing this gap. Our approach combines parameter-efficient fine-tuning
(Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a
dual-agent pipeline that separates the tasks of content generation (in English)
and translation (into Old English). Evaluation with automated metrics (BLEU,
METEOR, and CHRF) shows significant improvements over baseline models, with
BLEU scores increasing from 26 to over 65 for English-to-Old English
translation. Expert human assessment also confirms high grammatical accuracy
and stylistic fidelity in the generated texts. Beyond expanding the Old English
corpus, our method offers a practical blueprint for revitalizing other
endangered languages, effectively uniting AI innovation with the goals of
cultural preservation.

</details>


### [85] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)
*Anas Mohamed,Azal Ahmad Khan,Xinran Wang,Ahmad Faraz Khan,Shuwen Ge,Saman Bahzad Khan,Ayaan Ahmad,Ali Anwar*

Main category: cs.CL

TL;DR: Sem-DPO改进DPO方法，通过语义一致性加权优化提示生成，显著提升CLIP相似度和人类偏好分数。


<details>
  <summary>Details</summary>
Motivation: 解决DPO方法中语义不一致问题，确保生成的提示更贴近用户意图。

Method: 引入Sem-DPO，通过余弦距离加权损失函数，减少语义不匹配的奖励信号。

Result: 在多个基准测试中，Sem-DPO比DPO提升8-12%的CLIP相似度和5-9%的人类偏好分数。

Conclusion: 语义加权应成为提示优化的新标准，并为语言模型的语义感知优化奠定基础。

Abstract: Generative AI can now synthesize strikingly realistic images from text, yet
output quality remains highly sensitive to how prompts are phrased. Direct
Preference Optimization (DPO) offers a lightweight, off-policy alternative to
RL for automatic prompt engineering, but its token-level regularization leaves
semantic inconsistency unchecked as prompts that win higher preference scores
can still drift away from the user's intended meaning.
  We introduce Sem-DPO, a variant of DPO that preserves semantic consistency
yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an
exponential weight proportional to the cosine distance between the original
prompt and winning candidate in embedding space, softly down-weighting training
signals that would otherwise reward semantically mismatched prompts. We provide
the first analytical bound on semantic drift for preference-tuned prompt
generators, showing that Sem-DPO keeps learned prompts within a provably
bounded neighborhood of the original text. On three standard text-to-image
prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12%
higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1,
PickScore) than DPO, while also outperforming state-of-the-art baselines. These
findings suggest that strong flat baselines augmented with semantic weighting
should become the new standard for prompt-optimization studies and lay the
groundwork for broader, semantics-aware preference optimization in language
models.

</details>


### [86] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)
*Baiyu Chen,Wilson Wongso,Xiaoqian Hu,Yue Tan,Flora Salim*

Main category: cs.CL

TL;DR: 团队CRUISE为KDD Cup 2025的CRAG-MM挑战开发了一个多阶段框架，旨在减少视觉语言模型（VLMs）的幻觉问题，并在比赛中获得第三名。


<details>
  <summary>Details</summary>
Motivation: 现代视觉语言模型（VLMs）在面对自我中心图像、长尾实体和复杂多跳问题时容易产生幻觉，这在需要高事实准确性的实际应用中尤为严重。

Method: 提出一个多阶段框架，包括轻量级查询路由、查询感知的检索与摘要管道、双路径生成和后验验证，以优先考虑事实准确性。

Result: 在Task 1中获得第三名，证明了该框架在复杂多模态RAG系统中优先考虑答案可靠性的有效性。

Conclusion: 该框架通过保守策略显著减少了幻觉问题，适用于需要高事实准确性的多模态任务。

Abstract: This paper presents the technical solution developed by team CRUISE for the
KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn
(CRAG-MM) challenge. The challenge aims to address a critical limitation of
modern Vision Language Models (VLMs): their propensity to hallucinate,
especially when faced with egocentric imagery, long-tail entities, and complex,
multi-hop questions. This issue is particularly problematic in real-world
applications where users pose fact-seeking queries that demand high factual
accuracy across diverse modalities. To tackle this, we propose a robust,
multi-stage framework that prioritizes factual accuracy and truthfulness over
completeness. Our solution integrates a lightweight query router for
efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways
generation and a post-hoc verification. This conservative strategy is designed
to minimize hallucinations, which incur a severe penalty in the competition's
scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the
effectiveness of prioritizing answer reliability in complex multi-modal RAG
systems. Our implementation is available at
https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [87] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)
*Kesen Wang,Daulet Toibazar,Abdulrahman Alfulayt,Abdulaziz S. Albadawi,Ranya A. Alkahtani,Asma A. Ibrahim,Haneen A. Alhomoud,Sherif Mohamed,Pedro J. Moreno*

Main category: cs.CL

TL;DR: 提出了一种自动化多代理交互框架，用于生成长上下文问题，以提升大型视觉语言模型在长文档理解任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决长上下文文档理解任务中数据稀缺和人工标注成本高的问题，特别是针对低资源语言如阿拉伯语。

Method: 采用全自动多代理交互框架，高效生成长上下文单页和多页问题，覆盖英语和阿拉伯语文档。

Result: 生成的英语和阿拉伯语问题（AraEngLongBench）对主流开源和闭源LVLMs具有挑战性。

Conclusion: 该方法为提升LVLMs的长上下文理解能力提供了高效的数据生成解决方案。

Abstract: Document Understanding (DU) in long-contextual scenarios with complex layouts
remains a significant challenge in vision-language research. Although Large
Vision-Language Models (LVLMs) excel at short-context DU tasks, their
performance declines in long-context settings. A key limitation is the scarcity
of fine-grained training data, particularly for low-resource languages such as
Arabic. Existing state-of-the-art techniques rely heavily on human annotation,
which is costly and inefficient. We propose a fully automated, multi-agent
interactive framework to generate long-context questions efficiently. Our
approach efficiently generates high-quality single- and multi-page questions
for extensive English and Arabic documents, covering hundreds of pages across
diverse domains. This facilitates the development of LVLMs with enhanced
long-context understanding ability. Experimental results in this work have
shown that our generated English and Arabic questions
(\textbf{AraEngLongBench}) are quite challenging to major open- and
close-source LVLMs. The code and data proposed in this work can be found in
https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and
Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [88] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)
*Shuhaib Mehri,Xiaocheng Yang,Takyoung Kim,Gokhan Tur,Shikib Mehri,Dilek Hakkani-Tür*

Main category: cs.CL

TL;DR: 论文提出了一种名为UGST的新框架，用于跟踪用户目标在多轮对话中的进展，并开发了一种三阶段方法，显著提升了用户模拟器的目标对齐能力。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型（LLMs）在多轮对话中难以保持目标导向行为，限制了其在对话AI中的可靠性。

Method: 引入用户目标状态跟踪（UGST）框架，提出三阶段方法开发用户模拟器，并设计评估指标。

Result: 在MultiWOZ 2.4和{\tau}-Bench两个基准测试中取得了显著改进。

Conclusion: UGST填补了对话AI中的关键空白，成为开发目标对齐用户模拟器的重要框架。

Abstract: User simulators are essential to conversational AI, enabling scalable agent
development and evaluation through simulated interactions. While current Large
Language Models (LLMs) have advanced user simulation capabilities, we reveal
that they struggle to consistently demonstrate goal-oriented behavior across
multi-turn conversations--a critical limitation that compromises their
reliability in downstream applications. We introduce User Goal State Tracking
(UGST), a novel framework that tracks user goal progression throughout
conversations. Leveraging UGST, we present a three-stage methodology for
developing user simulators that can autonomously track goal progression and
reason to generate goal-aligned responses. Moreover, we establish comprehensive
evaluation metrics for measuring goal alignment in user simulators, and
demonstrate that our approach yields substantial improvements across two
benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a
critical gap in conversational AI and establish UGST as an essential framework
for developing goal-aligned user simulators.

</details>


### [89] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)
*Hyeonji Lee,Daejin Jo,Seohwan Yun,Sungwoong Kim*

Main category: cs.CL

TL;DR: SGPO是一种新型对齐框架，通过自生成偏好数据优化语言模型，无需外部标注数据，显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决传统对齐方法依赖人工标注数据及分布偏移问题。

Method: 提出SGPO框架，利用自改进机制生成偏好数据，直接优化策略模型。

Result: 在AlpacaEval 2.0和Arena-Hard上显著优于DPO和基线方法。

Conclusion: SGPO为语言模型对齐提供了一种高效且无需外部数据的方法。

Abstract: Large language models (LLMs), despite their extensive pretraining on diverse
datasets, require effective alignment to human preferences for practical and
reliable deployment. Conventional alignment methods typically employ off-policy
learning and depend on human-annotated datasets, which limits their broad
applicability and introduces distribution shift issues during training. To
address these challenges, we propose Self-Generated Preference Optimization
based on Self-Improver (SGPO), an innovative alignment framework that leverages
an on-policy self-improving mechanism. Specifically, the improver refines
responses from a policy model to self-generate preference data for direct
preference optimization (DPO) of the policy model. Here, the improver and
policy are unified into a single model, and in order to generate higher-quality
preference data, this self-improver learns to make incremental yet discernible
improvements to the current responses by referencing supervised fine-tuning
outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the
proposed SGPO significantly improves performance over DPO and baseline
self-improving methods without using external preference data.

</details>


### [90] [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)
*Yuqi Yang,Weiqi Wang,Baixuan Xu,Wei Fan,Qing Zong,Chunkit Chan,Zheye Deng,Xin Liu,Yifan Gao,Changlong Yu,Chen Luo,Yang Li,Zheng Li,Qingyu Yin,Bing Yin,Yangqiu Song*

Main category: cs.CL

TL;DR: 论文提出了一个意图树概念和数据集构建流程，创建了SessionIntentBench基准，用于评估模型在理解会话间意图转移的能力，并验证了当前模型在此任务上的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法未能有效捕捉和建模用户意图，且缺乏数据和基准来明确建模电商产品购买会话中的意图。

Method: 引入意图树概念，提出数据集构建流程，创建多模态基准SessionIntentBench，包含四个子任务。

Result: 构建了包含大量意图条目和会话轨迹的数据集，实验表明当前模型在复杂会话设置中难以捕捉意图。

Conclusion: 意图注入能提升模型性能，但当前模型在意图理解上仍有不足。

Abstract: Session history is a common way of recording user interacting behaviors
throughout a browsing activity with multiple products. For example, if an user
clicks a product webpage and then leaves, it might because there are certain
features that don't satisfy the user, which serve as an important indicator of
on-the-spot user preferences. However, all prior works fail to capture and
model customer intention effectively because insufficient information
exploitation and only apparent information like descriptions and titles are
used. There is also a lack of data and corresponding benchmark for explicitly
modeling intention in E-commerce product purchase sessions. To address these
issues, we introduce the concept of an intention tree and propose a dataset
curation pipeline. Together, we construct a sibling multimodal benchmark,
SessionIntentBench, that evaluates L(V)LMs' capability on understanding
inter-session intention shift with four subtasks. With 1,952,177 intention
entries, 1,132,145 session intention trajectories, and 13,003,664 available
tasks mined using 10,905 sessions, we provide a scalable way to exploit the
existing session data for customer intention understanding. We conduct human
annotations to collect ground-truth label for a subset of collected data to
form an evaluation gold set. Extensive experiments on the annotated data
further confirm that current L(V)LMs fail to capture and utilize the intention
across the complex session setting. Further analysis show injecting intention
enhances LLMs' performances.

</details>


### [91] [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)
*Yumeng Wang,Zhiyuan Fan,Jiayu Liu,Yi R. Fung*

Main category: cs.CL

TL;DR: MultiRole-R1框架通过多角色视角增强多样性，提升主观推理任务的准确性和多样性。


<details>
  <summary>Details</summary>
Motivation: 现有大型推理模型在主观问题上表现受限，因依赖单一真实答案导致推理同质化。

Method: 提出MultiRole-R1框架，结合无监督数据生成和强化学习（GRPO），以多样性为奖励信号。

Result: 实验表明，MultiRole-R1在六个基准测试中提升了主观和客观推理的多样性与准确性。

Conclusion: 多样性增强训练在大型推理模型中具有潜力，能有效提升推理表现。

Abstract: Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities
have shown strong performance on objective tasks, such as math reasoning and
coding. However, their effectiveness on subjective questions that may have
different responses from different perspectives is still limited by a tendency
towards homogeneous reasoning, introduced by the reliance on a single ground
truth in supervised fine-tuning and verifiable reward in reinforcement
learning. Motivated by the finding that increasing role perspectives
consistently improves performance, we propose MultiRole-R1, a
diversity-enhanced framework with multiple role perspectives, to improve the
accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an
unsupervised data construction pipeline that generates reasoning chains that
incorporate diverse role perspectives. We further employ reinforcement learning
via Group Relative Policy Optimization (GRPO) with reward shaping, by taking
diversity as a reward signal in addition to the verifiable reward. With
specially designed reward functions, we successfully promote perspective
diversity and lexical diversity, uncovering a positive relation between
reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates
MultiRole-R1's effectiveness and generalizability in enhancing both subjective
and objective reasoning, showcasing the potential of diversity-enhanced
training in LRMs.

</details>


### [92] [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)
*Aviya Maimon,Amir DN Cohen,Gal Vishne,Shauli Ravfogel,Reut Tsarfaty*

Main category: cs.CL

TL;DR: 论文提出了一种新的评估范式，利用因子分析揭示大语言模型在多个任务中的潜在技能，替代传统的单一平均分评估方法。


<details>
  <summary>Details</summary>
Motivation: 当前大语言模型的评估依赖基准测试分数，但难以解释这些分数反映的整体能力，且缺乏对任务间关系的理解。

Method: 采用因子分析方法，分析60个大语言模型在44个任务上的表现，识别潜在技能。

Result: 发现少量潜在技能能解释大部分性能表现，并开发了实用工具。

Conclusion: 新方法能更全面地评估模型，识别冗余任务，辅助模型选择和技能分析。

Abstract: Current evaluations of large language models (LLMs) rely on benchmark scores,
but it is difficult to interpret what these individual scores reveal about a
model's overall skills. Specifically, as a community we lack understanding of
how tasks relate to one another, what they measure in common, how they differ,
or which ones are redundant. As a result, models are often assessed via a
single score averaged across benchmarks, an approach that fails to capture the
models' wholistic strengths and limitations. Here, we propose a new evaluation
paradigm that uses factor analysis to identify latent skills driving
performance across benchmarks. We apply this method to a comprehensive new
leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a
small set of latent skills that largely explain performance. Finally, we turn
these insights into practical tools that identify redundant tasks, aid in model
selection, and profile models along each latent skill.

</details>


### [93] [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)
*Minh Hoang Nguyen,Thuat Thien Nguyen,Minh Nhat Ta*

Main category: cs.CL

TL;DR: 论文提出了一种混合新闻推荐框架Co-NAML-LSTUR，结合多视角新闻建模和动态用户兴趣捕捉，显著提升了推荐效果。


<details>
  <summary>Details</summary>
Motivation: 解决现有新闻推荐系统在建模多视角新闻特征和动态用户兴趣（尤其是长短期偏好）方面的不足。

Method: 整合NAML（多视角新闻建模）和LSTUR（长短期用户表示捕捉），并引入BERT词嵌入增强语义特征提取。

Result: 在MIND-small和MIND-large基准测试中，Co-NAML-LSTUR显著优于现有方法。

Conclusion: 结合多视角新闻表示和双尺度用户建模能有效提升新闻推荐性能。

Abstract: News recommendation systems play a vital role in mitigating information
overload by delivering personalized news content. A central challenge is to
effectively model both multi-view news representations and the dynamic nature
of user interests, which often span both short- and long-term preferences.
Existing methods typically rely on single-view features of news articles (e.g.,
titles or categories) or fail to comprehensively capture user preferences
across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news
recommendation framework that integrates NAML for attentive multi-view news
modeling and LSTUR for capturing both long- and short-term user
representations. Our model also incorporates BERT-based word embeddings to
enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely
used benchmarks, MIND-small and MIND-large. Experimental results show that
Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art
baselines on MIND-small and MIND-large, respectively. These results demonstrate
the effectiveness of combining multi-view news representations with dual-scale
user modeling. The implementation of our model is publicly available at
https://github.com/MinhNguyenDS/Co-NAML-LSTUR.

</details>


### [94] [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)
*Yi Feng,Jiaqi Wang,Wenxuan Zhang,Zhuang Chen,Yutong Shen,Xiyao Xiao,Minlie Huang,Liping Jing,Jian Yu*

Main category: cs.CL

TL;DR: 论文提出了一种结合交互式叙事治疗师（INT）和创新时刻评估（IMA）的框架，以提升语言模型在心理健康支持中的真实性和治疗效果。


<details>
  <summary>Details</summary>
Motivation: 当前大型语言模型在心理健康支持中缺乏真实性和对治疗进展的捕捉，叙事疗法因访问限制和社会污名化未充分利用。

Method: 通过INT模拟专家叙事治疗师，规划治疗阶段并生成专家级回应；IMA通过追踪“创新时刻”评估治疗效果。

Result: 在260个模拟客户和230名人类参与者中，INT在治疗质量和深度上优于标准语言模型。

Conclusion: INT框架能有效提升心理健康支持的真实性和治疗效果，具有广泛的社会应用潜力。

Abstract: Recent progress in large language models (LLMs) has opened new possibilities
for mental health support, yet current approaches lack realism in simulating
specialized psychotherapy and fail to capture therapeutic progression over
time. Narrative therapy, which helps individuals transform problematic life
stories into empowering alternatives, remains underutilized due to limited
access and social stigma. We address these limitations through a comprehensive
framework with two core components. First, INT (Interactive Narrative
Therapist) simulates expert narrative therapists by planning therapeutic
stages, guiding reflection levels, and generating contextually appropriate
expert-like responses. Second, IMA (Innovative Moment Assessment) provides a
therapy-centric evaluation method that quantifies effectiveness by tracking
"Innovative Moments" (IMs), critical narrative shifts in client speech
signaling therapy progress. Experimental results on 260 simulated clients and
230 human participants reveal that INT consistently outperforms standard LLMs
in therapeutic quality and depth. We further demonstrate the effectiveness of
INT in synthesizing high-quality support conversations to facilitate social
applications.

</details>


### [95] [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)
*Giulia D'Agostino,Chung-Chi Chen*

Main category: cs.CL

TL;DR: 论文探讨了如何通过语言特征建模和评估专家提问中的专业性，提出了一种新的标注框架，并构建了两个数据集。研究发现，相同的语言特征与人类判断和问题来源高度相关，且基于这些特征的分类器优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 专业性在专家沟通中至关重要，但在高风险领域（如金融）中研究不足。本文旨在通过语言特征建模和评估专业性。

Method: 引入新的标注框架量化金融分析师提问的结构和语用元素，构建两个数据集（专业性和问题来源标注），并训练分类器。

Result: 相同的语言特征与人类判断和问题来源高度相关，分类器性能优于基线模型。

Conclusion: 专业性是可学习的、领域通用的概念，可通过语言建模捕捉。

Abstract: Professionalism is a crucial yet underexplored dimension of expert
communication, particularly in high-stakes domains like finance. This paper
investigates how linguistic features can be leveraged to model and evaluate
professionalism in expert questioning. We introduce a novel annotation
framework to quantify structural and pragmatic elements in financial analyst
questions, such as discourse regulators, prefaces, and request types. Using
both human-authored and large language model (LLM)-generated questions, we
construct two datasets: one annotated for perceived professionalism and one
labeled by question origin. We show that the same linguistic features correlate
strongly with both human judgments and authorship origin, suggesting a shared
stylistic foundation. Furthermore, a classifier trained solely on these
interpretable features outperforms gemini-2.0 and SVM baselines in
distinguishing expert-authored questions. Our findings demonstrate that
professionalism is a learnable, domain-general construct that can be captured
through linguistically grounded modeling.

</details>


### [96] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)
*Xiang Fei,Siqi Wang,Shu Wei,Yuxiang Nie,Wei Shi,Hao Feng,Can Huang*

Main category: cs.CL

TL;DR: 论文提出了一种名为Post-Completion Learning (PCL)的新训练框架，利用模型输出完成后的序列空间，提升推理和自我评估能力。


<details>
  <summary>Details</summary>
Motivation: 当前语言模型训练范式在遇到结束标记时终止学习，忽略了完成后的潜在学习机会。

Method: 设计了白盒强化学习方法，让模型根据奖励规则评估输出内容，并通过双轨SFT和RL混合训练优化推理和评估能力。

Result: 在不同数据集和模型上的实验结果表明，PCL优于传统的SFT和RL方法。

Conclusion: PCL为语言模型训练提供了新技术路径，在提升输出质量的同时保持了部署效率。

Abstract: Current language model training paradigms typically terminate learning upon
reaching the end-of-sequence (<eos>}) token, overlooking the potential learning
opportunities in the post-completion space. We propose Post-Completion Learning
(PCL), a novel training framework that systematically utilizes the sequence
space after model output completion, to enhance both the reasoning and
self-evaluation abilities. PCL enables models to continue generating
self-assessments and reward predictions during training, while maintaining
efficient inference by stopping at the completion point.
  To fully utilize this post-completion space, we design a white-box
reinforcement learning method: let the model evaluate the output content
according to the reward rules, then calculate and align the score with the
reward functions for supervision. We implement dual-track SFT to optimize both
reasoning and evaluation capabilities, and mixed it with RL training to achieve
multi-objective hybrid optimization.
  Experimental results on different datasets and models demonstrate consistent
improvements over traditional SFT and RL methods. Our method provides a new
technical path for language model training that enhances output quality while
preserving deployment efficiency.

</details>


### [97] [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)
*Abeer Aldayel,Areej Alokaili*

Main category: cs.CL

TL;DR: 论文提出了一种评估框架，通过分析对话中的隐含立场来提升NLP模型的包容性和公平性，避免依赖表面特征。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖用户人口统计或行为属性等表面特征，忽略了对话中隐含的观点表达，可能导致模型输出中的有害刻板印象。

Method: 引入基于立场分析的评估框架，结合PU在线学习和指令调优语言模型，评估隐含观点的规范性对齐。

Result: 框架能够揭示隐含观点在模型中的（错误）表达，为更包容的模型行为提供路径。

Conclusion: 通过立场分析评估隐含观点，可以促进NLP模型更公平、包容地反映多样社会观点。

Abstract: Shaping inclusive representations that embrace diversity and ensure fair
participation and reflections of values is at the core of many
conversation-based models. However, many existing methods rely on surface
inclusion using mention of user demographics or behavioral attributes of social
groups. Such methods overlook the nuanced, implicit expression of opinion
embedded in conversations. Furthermore, the over-reliance on overt cues can
exacerbate misalignment and reinforce harmful or stereotypical representations
in model outputs. Thus, we took a step back and recognized that equitable
inclusion needs to account for the implicit expression of opinion and use the
stance of responses to validate the normative alignment. This study aims to
evaluate how opinions are represented in NLP or computational models by
introducing an alignment evaluation framework that foregrounds implicit, often
overlooked conversations and evaluates the normative social views and
discourse. Our approach models the stance of responses as a proxy for the
underlying opinion, enabling a considerate and reflective representation of
diverse social viewpoints. We evaluate the framework using both (i)
positive-unlabeled (PU) online learning with base classifiers, and (ii)
instruction-tuned language models to assess post-training alignment. Through
this, we provide a lens on how implicit opinions are (mis)represented and offer
a pathway toward more inclusive model behavior.

</details>


### [98] [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)
*Kang Yang,Jingxue Chen,Qingkun Tang,Tianxiang Zhang,Qianchun Lu*

Main category: cs.CL

TL;DR: MoL-RL是一种新的训练范式，通过双目标优化框架将多步环境反馈信号整合到LLM中，提升其推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法无法有效利用多步和离散的环境反馈信号，导致信息丢失或无法充分利用反馈。

Method: 结合MoL持续训练和GRPO后训练，通过双目标优化框架整合反馈信号。

Result: 在数学推理和代码生成任务中表现优异，Qwen3-8B模型达到SOTA性能。

Conclusion: MoL-RL为利用多步文本反馈增强LLM推理能力提供了有效方法。

Abstract: Large language models (LLMs) face significant challenges in effectively
leveraging sequential environmental feedback (EF) signals, such as natural
language evaluations, for feedback-independent chain-of-thought (CoT)
reasoning. Existing approaches either convert EF into scalar rewards, losing
rich contextual information, or employ refinement datasets, failing to exploit
the multi-step and discrete nature of EF interactions. To address these
limitations, we propose MoL-RL, a novel training paradigm that integrates
multi-step EF signals into LLMs through a dual-objective optimization
framework. Our method combines MoL (Mixture-of-Losses) continual training,
which decouples domain-specific EF signals (optimized via cross-entropy loss)
and general language capabilities (preserved via Kullback-Leibler divergence),
with GRPO-based post-training to distill sequential EF interactions into
single-step inferences. This synergy enables robust feedback-independent
reasoning without relying on external feedback loops. Experimental results on
mathematical reasoning (MATH-500, AIME24/AIME25) and code generation
(CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art
performance with the Qwen3-8B model, while maintaining strong generalization
across model scales (Qwen3-4B). This work provides a promising approach for
leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities
in diverse domains.

</details>


### [99] [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)
*Katharina Trinley,Toshiki Nakai,Tatiana Anikina,Tanja Baeumel*

Main category: cs.CL

TL;DR: 研究分析了多语言大模型Aya-23-8B在代码混合、填空和翻译任务中的表现，发现其语言处理方式与单语言模型不同，揭示了多语言训练对模型内部结构的影响。


<details>
  <summary>Details</summary>
Motivation: 探索多语言大模型（如Aya-23-8B）的内部语言处理机制，并与单语言模型（如Llama 3和Chinese-LLaMA-2）进行对比，以理解多语言训练对模型的影响。

Method: 使用logit lens和神经元专业化分析方法，研究了Aya-23-8B在代码混合、填空和翻译任务中的表现。

Result: 发现Aya-23在翻译时激活类型相关的语言表征，代码混合任务中神经元激活模式受基础语言影响更大，且语言特定神经元集中在最后几层。

Conclusion: 多语言训练显著影响大模型的内部处理机制，为未来的跨语言迁移研究提供了新见解。

Abstract: Large language models (LLMs) excel at multilingual tasks, yet their internal
language processing remains poorly understood. We analyze how Aya-23-8B, a
decoder-only LLM trained on balanced multilingual data, handles code-mixed,
cloze, and translation tasks compared to predominantly monolingual models like
Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization
analyses, we find: (1) Aya-23 activates typologically related language
representations during translation, unlike English-centric models that rely on
a single pivot language; (2) code-mixed neuron activation patterns vary with
mixing rates and are shaped more by the base language than the mixed-in one;
and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in
final layers, diverging from prior findings on decoder-only models. Neuron
overlap analysis further shows that script similarity and typological relations
impact processing across model types. These findings reveal how multilingual
training shapes LLM internals and inform future cross-lingual transfer
research.

</details>


### [100] [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)
*Abdullah Alabdullah,Lifeng Han,Chenghua Lin*

Main category: cs.CL

TL;DR: 论文探讨了方言阿拉伯语（DA）与标准阿拉伯语（MSA）的翻译问题，提出了无训练提示技术和高效微调方法，在低资源环境下取得了显著效果。


<details>
  <summary>Details</summary>
Motivation: 方言阿拉伯语与标准阿拉伯语的差异限制了数字服务和教育资源的普及，阻碍了阿拉伯语机器翻译的发展。

Method: 评估了六种大型语言模型的提示技术，并开发了资源高效的微调流程，包括量化技术和多方言联合训练。

Result: GPT-4o在提示技术中表现最佳，量化Gemma2-9B模型在微调中优于零样本GPT-4o，多方言联合训练模型性能提升10%。

Conclusion: 研究表明，即使在资源有限的情况下，也能实现高质量的DA-MSA翻译，为阿拉伯语NLP的方言包容性提供了实用方案。

Abstract: Dialectal Arabic (DA) poses a persistent challenge for natural language
processing (NLP), as most everyday communication in the Arab world occurs in
dialects that diverge significantly from Modern Standard Arabic (MSA). This
linguistic divide limits access to digital services and educational resources
and impedes progress in Arabic machine translation. This paper presents two
core contributions to advancing DA-MSA translation for the Levantine, Egyptian,
and Gulf dialects, particularly in low-resource and computationally constrained
settings: a comprehensive evaluation of training-free prompting techniques, and
the development of a resource-efficient fine-tuning pipeline. Our evaluation of
prompting strategies across six large language models (LLMs) found that
few-shot prompting consistently outperformed zero-shot, chain-of-thought, and
our proposed Ara-TEaR method. GPT-4o achieved the highest performance across
all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a
CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint
multi-dialect trained models outperformed single-dialect counterparts by over
10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than
1% performance loss. The results and insights of our experiments offer a
practical blueprint for improving dialectal inclusion in Arabic NLP, showing
that high-quality DA-MSA machine translation is achievable even with limited
resources and paving the way for more inclusive language technologies.

</details>


### [101] [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)
*Bernd J. Kröger*

Main category: cs.CL

TL;DR: DYNARTmo是一个动态发音模型，用于可视化二维矢状面中的语音发音过程，适用于语音学教育和言语治疗。


<details>
  <summary>Details</summary>
Motivation: 开发一个动态发音模型，以更好地理解和可视化语音发音过程，支持教育和治疗应用。

Method: 基于UK-DYNAMO框架，整合发音未指定、分段和手势控制以及协同发音原理，模拟六个关键发音器。

Result: 实现了基于连续和离散控制参数的发音配置生成，并集成到网络应用中。

Conclusion: 当前模型专注于静态建模，未来将扩展动态运动生成和声学模块集成。

Abstract: We present DYNARTmo, a dynamic articulatory model designed to visualize
speech articulation processes in a two-dimensional midsagittal plane. The model
builds upon the UK-DYNAMO framework and integrates principles of articulatory
underspecification, segmental and gestural control, and coarticulation.
DYNARTmo simulates six key articulators based on ten continuous and six
discrete control parameters, allowing for the generation of both vocalic and
consonantal articulatory configurations. The current implementation is embedded
in a web-based application (SpeechArticulationTrainer) that includes sagittal,
glottal, and palatal views, making it suitable for use in phonetics education
and speech therapy. While this paper focuses on the static modeling aspects,
future work will address dynamic movement generation and integration with
articulatory-acoustic modules.

</details>


### [102] [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)
*Hao Xiang,Tianyi Tang,Yang Su,Bowen Yu,An Yang,Fei Huang,Yichang Zhang,Yaojie Lu,Hongyu Lin,Xianpei Han,Jingren Zhou,Junyang Lin,Le Sun*

Main category: cs.CL

TL;DR: RMTBench是一个用户为中心的双语角色扮演基准测试，包含80个角色和8000多轮对话，旨在更有效地评估LLMs的角色扮演能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准测试过于角色中心化，简化了用户与角色的互动，未能反映实际应用需求。

Method: 引入RMTBench，基于用户动机构建对话，采用多轮对话模拟机制和LLM评分。

Result: RMTBench通过关注用户意图实现，填补了学术评估与实际应用之间的差距。

Conclusion: RMTBench为评估LLMs角色扮演能力提供了更有效的框架，代码和数据集将公开。

Abstract: Recent advancements in Large Language Models (LLMs) have shown outstanding
potential for role-playing applications. Evaluating these capabilities is
becoming crucial yet remains challenging. Existing benchmarks mostly adopt a
\textbf{character-centric} approach, simplify user-character interactions to
isolated Q&A tasks, and fail to reflect real-world applications. To address
this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric}
bilingual role-playing benchmark featuring 80 diverse characters and over 8,000
dialogue rounds. RMTBench includes custom characters with detailed backgrounds
and abstract characters defined by simple traits, enabling evaluation across
various user scenarios. Our benchmark constructs dialogues based on explicit
user motivations rather than character descriptions, ensuring alignment with
practical user applications. Furthermore, we construct an authentic multi-turn
dialogue simulation mechanism. With carefully selected evaluation dimensions
and LLM-based scoring, this mechanism captures the complex intention of
conversations between the user and the character. By shifting focus from
character background to user intention fulfillment, RMTBench bridges the gap
between academic evaluation and practical deployment requirements, offering a
more effective framework for assessing role-playing capabilities in LLMs. All
code and datasets will be released soon.

</details>


### [103] [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)
*Sangjun Moon,Dasom Choi,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 研究发现大语言模型（LLMs）通过多头注意力机制编码输出序列长度信息，并能部分解耦长度与语义信息。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs内部如何控制输出序列长度的机制，填补现有研究的空白。

Method: 通过调整模型内部特定隐藏单元的缩放比例，实证分析多头注意力机制对输出长度的影响。

Result: 多头注意力机制是关键，长度信息可解耦控制，且部分隐藏单元对长度敏感的提示更活跃。

Conclusion: LLMs具备无需外部控制的内部机制，能灵活适应输出长度需求。

Abstract: Large language models (LLMs) have shown remarkable capabilities across
various tasks, that are learned from massive amounts of text-based data.
Although LLMs can control output sequence length, particularly in
instruction-based settings, the internal mechanisms behind this control have
been unexplored yet. In this study, we provide empirical evidence on how output
sequence length information is encoded within the internal representations in
LLMs. In particular, our findings show that multi-head attention mechanisms are
critical in determining output sequence length, which can be adjusted in a
disentangled manner. By scaling specific hidden units within the model, we can
control the output sequence length without losing the informativeness of the
generated text, thereby indicating that length information is partially
disentangled from semantic information. Moreover, some hidden units become
increasingly active as prompts become more length-specific, thus reflecting the
model's internal awareness of this attribute. Our findings suggest that LLMs
have learned robust and adaptable internal mechanisms for controlling output
length without any external control.

</details>


### [104] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)
*Eunkyu Park,Wesley Hanwen Deng,Gunhee Kim,Motahhare Eslami,Maarten Sap*

Main category: cs.CL

TL;DR: CoCoT（认知链式思维）是一种新的提示策略，通过感知、情境和规范三阶段提升视觉语言模型的表现，优于传统CoT和直接提示。


<details>
  <summary>Details</summary>
Motivation: 解决视觉任务中传统CoT（链式思维）在同时感知、理解和判断时表现不佳的问题。

Method: 提出CoCoT，分三阶段（感知、情境、规范）引导模型推理。

Result: 在多个多模态基准测试中，CoCoT平均优于CoT和直接提示8%。

Conclusion: CoCoT增强了模型的解释性和社会意识，为更安全可靠的多模态系统铺路。

Abstract: Chain-of-Thought (CoT) prompting helps models think step by step. But what
happens when they must see, understand, and judge-all at once? In visual tasks
grounded in social context, where bridging perception with norm-grounded
judgments is essential, flat CoT often breaks down. We introduce Cognitive
Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning
through three cognitively inspired stages: perception, situation, and norm. Our
experiments show that, across multiple multimodal benchmarks (including intent
disambiguation, commonsense reasoning, and safety), CoCoT consistently
outperforms CoT and direct prompting (+8\% on average). Our findings
demonstrate that cognitively grounded reasoning stages enhance interpretability
and social awareness in VLMs, paving the way for safer and more reliable
multimodal systems.

</details>


### [105] [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)
*George Ibrahim,Rita Ramos,Yova Kementchedjhieva*

Main category: cs.CL

TL;DR: CONCAP是一种多语言图像描述模型，通过结合检索到的描述和图像特定概念，减少了多语言训练需求，并在低资源语言上表现优异。


<details>
  <summary>Details</summary>
Motivation: 解决多语言视觉语言模型因训练数据有限和参数化成本高而性能不足的问题。

Method: 引入CONCAP模型，结合检索到的描述和图像特定概念，增强输入图像的上下文理解。

Result: 在XM3600数据集上，CONCAP在低和中资源语言上表现优异，数据需求大幅减少。

Conclusion: 概念感知检索增强能有效缩小多语言性能差距。

Abstract: Multilingual vision-language models have made significant strides in image
captioning, yet they still lag behind their English counterparts due to limited
multilingual training data and costly large-scale model parameterization.
Retrieval-augmented generation (RAG) offers a promising alternative by
conditioning caption generation on retrieved examples in the target language,
reducing the need for extensive multilingual training. However, multilingual
RAG captioning models often depend on retrieved captions translated from
English, which can introduce mismatches and linguistic biases relative to the
source language. We introduce CONCAP, a multilingual image captioning model
that integrates retrieved captions with image-specific concepts, enhancing the
contextualization of the input image and grounding the captioning process
across different languages. Experiments on the XM3600 dataset indicate that
CONCAP enables strong performance on low- and mid-resource languages, with
highly reduced data requirements. Our findings highlight the effectiveness of
concept-aware retrieval augmentation in bridging multilingual performance gaps.

</details>


### [106] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)
*Khloud AL Jallad,Nada Ghneim,Ghaida Rebdawi*

Main category: cs.CL

TL;DR: 该论文综述了英语、阿拉伯语和多语言NLU基准测试，重点分析了诊断数据集及其覆盖的语言现象，提出了建立评估标准的必要性。


<details>
  <summary>Details</summary>
Motivation: 评估NLU能力的基准测试缺乏统一的命名规范和标准语言现象集，导致比较和分析困难。

Method: 通过详细比较和分析现有基准测试的诊断数据集及其语言现象覆盖情况。

Result: 发现现有基准测试在语言现象覆盖和命名规范上存在不足，提出了建立全球语言现象层次结构的必要性。

Conclusion: 建议未来研究制定统一的NLU诊断评估标准，以提升模型结果的可比性和分析深度。

Abstract: Natural Language Understanding (NLU) is a basic task in Natural Language
Processing (NLP). The evaluation of NLU capabilities has become a trending
research topic that attracts researchers in the last few years, resulting in
the development of numerous benchmarks. These benchmarks include various tasks
and datasets in order to evaluate the results of pretrained models via public
leaderboards. Notably, several benchmarks contain diagnostics datasets designed
for investigation and fine-grained error analysis across a wide range of
linguistic phenomena. This survey provides a comprehensive review of available
English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on
their diagnostics datasets and the linguistic phenomena they covered. We
present a detailed comparison and analysis of these benchmarks, highlighting
their strengths and limitations in evaluating NLU tasks and providing in-depth
error analysis. When highlighting the gaps in the state-of-the-art, we noted
that there is no naming convention for macro and micro categories or even a
standard set of linguistic phenomena that should be covered. Consequently, we
formulated a research question regarding the evaluation metrics of the
evaluation diagnostics benchmarks: "Why do not we have an evaluation standard
for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in
industry. We conducted a deep analysis and comparisons of the covered
linguistic phenomena in order to support experts in building a global hierarchy
for linguistic phenomena in future. We think that having evaluation metrics for
diagnostics evaluation could be valuable to gain more insights when comparing
the results of the studied models on different diagnostics benchmarks.

</details>


### [107] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)
*Sungwoo Han,Hyeyeon Kim,Jingun Kwon,Hidetaka Kamigaito,Manabu Okumura*

Main category: cs.CL

TL;DR: 论文提出了一种基于代码提示的新方法，通过嵌入代码提供详细的BIO标签指令，以提升大语言模型在命名实体识别（NER）中的表现。该方法在多个语言数据集上优于传统文本提示，并结合思维链提示进一步提升了性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅依赖输入上下文信息，未能充分利用大语言模型（如ChatGPT）的潜力，而NER需要结合详细的标签要求与上下文信息。

Method: 提出代码提示方法，通过嵌入代码提供BIO标签指令，利用大语言模型对编程语言的理解能力。

Result: 在英语、阿拉伯语、芬兰语、丹麦语和德语数据集上，代码提示方法优于传统文本提示，结合思维链提示后性能进一步提升。

Conclusion: 代码提示方法有效提升了NER任务的表现，证明了显式结构化指令的重要性。

Abstract: Recent studies have explored various approaches for treating candidate named
entity spans as both source and target sequences in named entity recognition
(NER) by leveraging large language models (LLMs). Although previous approaches
have successfully generated candidate named entity spans with suitable labels,
they rely solely on input context information when using LLMs, particularly,
ChatGPT. However, NER inherently requires capturing detailed labeling
requirements with input context information. To address this issue, we propose
a novel method that leverages code-based prompting to improve the capabilities
of LLMs in understanding and performing NER. By embedding code within prompts,
we provide detailed BIO schema instructions for labeling, thereby exploiting
the ability of LLMs to comprehend long-range scopes in programming languages.
Experimental results demonstrate that the proposed code-based prompting method
outperforms conventional text-based prompting on ten benchmarks across English,
Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of
explicitly structuring NER instructions. We also verify that combining the
proposed code-based prompting method with the chain-of-thought prompting
further improves performance.

</details>


### [108] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)
*Tuan Bui,Trong Le,Phat Thai,Sang Nguyen,Minh Hua,Ngan Pham,Thang Bui,Tho Quan*

Main category: cs.CL

TL;DR: Text-JEPA是一个轻量级框架，用于将自然语言转换为形式逻辑，结合LLMs和符号推理，提升封闭领域问答的效率和可解释性。


<details>
  <summary>Details</summary>
Motivation: 封闭领域（如教育、医疗、法律）需要透明且可解释的问答系统，现有神经符号框架效率低且依赖大规模模型。

Method: 提出Text-JEPA框架，模拟双系统认知理论，结合LLMs生成逻辑表示和Z3求解器进行推理，并设计三个评估指标。

Result: 在领域特定数据集上，Text-JEPA性能接近大型LLM系统，计算开销更低。

Conclusion: 结构化、可解释的推理框架在专业领域问答系统中具有潜力。

Abstract: Recent advances in large language models (LLMs) have significantly enhanced
question-answering (QA) capabilities, particularly in open-domain contexts.
However, in closed-domain scenarios such as education, healthcare, and law,
users demand not only accurate answers but also transparent reasoning and
explainable decision-making processes. While neural-symbolic (NeSy) frameworks
have emerged as a promising solution, leveraging LLMs for natural language
understanding and symbolic systems for formal reasoning, existing approaches
often rely on large-scale models and exhibit inefficiencies in translating
natural language into formal logic representations.
  To address these limitations, we introduce Text-JEPA (Text-based
Joint-Embedding Predictive Architecture), a lightweight yet effective framework
for converting natural language into first-order logic (NL2FOL). Drawing
inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by
efficiently generating logic representations, while the Z3 solver operates as
System 2, enabling robust logical inference. To rigorously evaluate the
NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework
comprising three custom metrics: conversion score, reasoning score, and
Spearman rho score, which collectively capture the quality of logical
translation and its downstream impact on reasoning accuracy.
  Empirical results on domain-specific datasets demonstrate that Text-JEPA
achieves competitive performance with significantly lower computational
overhead compared to larger LLM-based systems. Our findings highlight the
potential of structured, interpretable reasoning frameworks for building
efficient and explainable QA systems in specialized domains.

</details>


### [109] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)
*Praneeth Narisetty,Uday Kumar Reddy Kattamanchi,Lohit Akshant Nimma,Sri Ram Kaushik Karnati,Shiva Nagendra Babu Kore,Mounika Golamari,Tejashree Nageshreddy*

Main category: cs.CL

TL;DR: 论文介绍了AQUA，首个专为水产养殖设计的大型语言模型（LLM），旨在解决行业中的复杂问题，如疾病爆发和低效喂养。


<details>
  <summary>Details</summary>
Motivation: 水产养殖对全球粮食安全和沿海经济至关重要，但面临疾病、低效喂养等挑战，现有AI方法不足以应对。

Method: 提出AQUA模型及AQUADAPT框架，结合专家知识和大规模语言模型生成高质量合成数据。

Result: AQUA为水产养殖研究、咨询系统和决策工具奠定了基础。

Conclusion: AQUA填补了AI在水产养殖领域的空白，有望推动行业创新。

Abstract: Aquaculture plays a vital role in global food security and coastal economies
by providing sustainable protein sources. As the industry expands to meet
rising demand, it faces growing challenges such as disease outbreaks,
inefficient feeding practices, rising labor costs, logistical inefficiencies,
and critical hatchery issues, including high mortality rates and poor water
quality control. Although artificial intelligence has made significant
progress, existing machine learning methods fall short of addressing the
domain-specific complexities of aquaculture. To bridge this gap, we introduce
AQUA, the first large language model (LLM) tailored for aquaculture, designed
to support farmers, researchers, and industry practitioners. Central to this
effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic
Framework for generating and refining high-quality synthetic data using a
combination of expert knowledge, largescale language models, and automated
evaluation techniques. Our work lays the foundation for LLM-driven innovations
in aquaculture research, advisory systems, and decision-making tools.

</details>


### [110] [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)
*Chaitanya Manem,Pratik Prabhanjan Brahma,Prakamya Mishra,Zicheng Liu,Emad Barsoum*

Main category: cs.CL

TL;DR: SAND-Math 是一个生成高质量数学问题并提升其复杂度的管道，显著提升了数学推理LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 由于缺乏新颖且困难的数学训练数据，高性能数学LLM的开发受到限制。

Method: 通过生成高质量数学问题并采用‘难度提升’步骤系统增加问题复杂度。

Result: 使用SAND-Math数据显著提升性能，AIME25基准测试中表现优于其他数据集17.85分；难度提升步骤使AIME25性能从46.38%增至49.23%。

Conclusion: SAND-Math 提供了一个实用且可扩展的工具包，用于构建更高效的数学推理LLM。

Abstract: The demand for Large Language Models (LLMs) capable of sophisticated
mathematical reasoning is growing across industries. However, the development
of performant mathematical LLMs is critically bottlenecked by the scarcity of
difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic
Augmented Novel and Difficult Mathematics problems and solutions), a pipeline
that addresses this by first generating high-quality problems from scratch and
then systematically elevating their complexity via a new \textbf{Difficulty
Hiking} step. We demonstrate the effectiveness of our approach through two key
findings. First, augmenting a strong baseline with SAND-Math data significantly
boosts performance, outperforming the next-best synthetic dataset by
\textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a
dedicated ablation study, we show our Difficulty Hiking process is highly
effective: by increasing average problem difficulty from 5.02 to 5.98, this
step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation
pipeline, final dataset, and a fine-tuned model form a practical and scalable
toolkit for building more capable and efficient mathematical reasoning LLMs.
SAND-Math dataset is released here:
\href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

</details>


### [111] [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)
*Effi Levi,Gal Ron,Odelia Oshri,Shaul R. Shenhav*

Main category: cs.CL

TL;DR: 提出了一种多标签方案，用于联合标注社交媒体对话中的仇恨和反仇恨言论，从主题和修辞维度分类，并通过统计分析揭示互动模式。


<details>
  <summary>Details</summary>
Motivation: 研究仇恨言论在社交媒体上的传播及其反制策略，分析其对在线行为的影响。

Method: 标注92个对话（720条推文），结合公共指标进行统计分析，探索主题和修辞维度的互动模式。

Result: 揭示了仇恨言论的传播方式、反制策略及其对在线行为的潜在影响。

Conclusion: 研究为理解仇恨言论及其反制提供了新视角，有助于改善社交媒体环境。

Abstract: We introduce a novel multi-labeled scheme for joint annotation of hate and
counter-hate speech in social media conversations, categorizing hate and
counter-hate messages into thematic and rhetorical dimensions. The thematic
categories outline different discursive aspects of each type of speech, while
the rhetorical dimension captures how hate and counter messages are
communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a
sample of 92 conversations, consisting of 720 tweets, and conduct statistical
analyses, incorporating public metrics, to explore patterns of interaction
between the thematic and rhetorical dimensions within and between hate and
counter-hate speech. Our findings provide insights into the spread of hate
messages on social media, the strategies used to counter them, and their
potential impact on online behavior.

</details>


### [112] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)
*Joosung Lee,Cheonbok Park,Hwiyeol Jo,Jeonghoon Kim,Joonsuk Park,Kang Min Yoo*

Main category: cs.CL

TL;DR: 提出了一种用于黑盒生成器的幻觉检测框架，通过采样未来上下文来检测幻觉。


<details>
  <summary>Details</summary>
Motivation: 由于用户越来越多地遇到黑盒生成的文本，检测幻觉成为关键挑战。

Method: 通过采样未来上下文，为幻觉检测提供线索，并结合多种基于采样的方法。

Result: 在多种方法中展示了性能提升。

Conclusion: 采样未来上下文的方法能有效检测黑盒生成器中的幻觉。

Abstract: Large Language Models (LLMs) are widely used to generate plausible text on
online platforms, without revealing the generation process. As users
increasingly encounter such black-box outputs, detecting hallucinations has
become a critical challenge. To address this challenge, we focus on developing
a hallucination detection framework for black-box generators. Motivated by the
observation that hallucinations, once introduced, tend to persist, we sample
future contexts. The sampled future contexts provide valuable clues for
hallucination detection and can be effectively integrated with various
sampling-based methods. We extensively demonstrate performance improvements
across multiple methods using our proposed sampling approach.

</details>


### [113] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)
*Duc-Tai Dinh,Duc Anh Khoa Dinh*

Main category: cs.CL

TL;DR: ZSE-Cap系统在EVENTA任务中排名第四，通过零样本集成方法实现图像检索和描述生成，无需微调。


<details>
  <summary>Details</summary>
Motivation: 探索零样本方法在图像检索和描述生成任务中的有效性，避免对竞赛数据进行微调。

Method: 检索部分集成CLIP、SigLIP和DINOv2的相似性分数；描述生成部分通过精心设计的提示引导Gemma 3模型。

Result: 系统最终得分为0.42002，在私有测试集上排名第四。

Conclusion: 通过集成和提示结合基础模型，展示了零样本方法的有效性。

Abstract: We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system
in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image
retrieval and captioning. Our zero-shot approach requires no finetuning on the
competition's data. For retrieval, we ensemble similarity scores from CLIP,
SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt
to guide the Gemma 3 model, enabling it to link high-level events from the
article to the visual content in the image. Our system achieved a final score
of 0.42002, securing a top-4 position on the private test set, demonstrating
the effectiveness of combining foundation models through ensembling and
prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [114] [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)
*Anaïs Ollagnier*

Main category: cs.CL

TL;DR: 本文系统综述了49项关于反社会行为（ASB）预测的研究，提出了五种核心任务类型的分类法，并分析了建模技术和数据集特征的影响。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的反社会行为（如仇恨言论、骚扰等）对平台安全和公共福祉构成挑战，现有研究多关注事后检测，而预测性方法旨在提前预防。

Method: 通过系统综述49项研究，构建了五种核心任务类型的分类法，并分析了建模技术和数据集特征。

Result: 总结了ASB预测的现状，指出了方法学挑战（如数据稀缺性、时间漂移等）和未来研究方向（如多语言建模、跨平台泛化等）。

Conclusion: 通过统一的框架组织该领域，为未来更稳健和社会责任导向的ASB预测研究提供指导。

Abstract: Antisocial behavior (ASB) on social media-including hate speech, harassment,
and trolling-poses growing challenges for platform safety and societal
wellbeing. While prior work has primarily focused on detecting harmful content
after it appears, predictive approaches aim to forecast future harmful
behaviors-such as hate speech propagation, conversation derailment, or user
recidivism-before they fully unfold. Despite increasing interest, the field
remains fragmented, lacking a unified taxonomy or clear synthesis of existing
methods. This paper presents a systematic review of over 49 studies on ASB
prediction, offering a structured taxonomy of five core task types: early harm
detection, harm emergence prediction, harm propagation prediction, behavioral
risk prediction, and proactive moderation support. We analyze how these tasks
differ by temporal framing, prediction granularity, and operational goals. In
addition, we examine trends in modeling techniques-from classical machine
learning to pre-trained language models-and assess the influence of dataset
characteristics on task feasibility and generalization. Our review highlights
methodological challenges, such as dataset scarcity, temporal drift, and
limited benchmarks, while outlining emerging research directions including
multilingual modeling, cross-platform generalization, and human-in-the-loop
systems. By organizing the field around a coherent framework, this survey aims
to guide future work toward more robust and socially responsible ASB
prediction.

</details>


### [115] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)
*Wenbin Guo,Xin Wang,Jiaoyan Chen,Zhao Li,Zirui Chen*

Main category: cs.CL

TL;DR: 论文提出了一种结合本体知识的LLM增强知识图谱补全方法OL-KGC，通过神经感知机制和自动提取算法提升推理能力，实验表明其性能优于现有主流方法。


<details>
  <summary>Details</summary>
Motivation: 当前基于LLM的知识图谱补全方法依赖隐式知识表示且易传播错误知识，限制了其推理能力。

Method: 提出OL-KGC方法，结合神经感知机制嵌入结构信息，并自动提取本体知识转化为LLM可理解的文本格式。

Result: 在FB15K-237、UMLS和WN18RR基准测试中，OL-KGC显著优于现有方法，达到最优性能。

Conclusion: OL-KGC通过结合结构信息和本体知识，显著提升了知识图谱补全的推理能力和性能。

Abstract: Large Language Models (LLMs) have been extensively adopted in Knowledge Graph
Completion (KGC), showcasing significant research advancements. However, as
black-box models driven by deep neural architectures, current LLM-based KGC
methods rely on implicit knowledge representation with parallel propagation of
erroneous knowledge, thereby hindering their ability to produce conclusive and
decisive reasoning outcomes. We aim to integrate neural-perceptual structural
information with ontological knowledge, leveraging the powerful capabilities of
LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge.
We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first
leverages neural perceptual mechanisms to effectively embed structural
information into the textual space, and then uses an automated extraction
algorithm to retrieve ontological knowledge from the knowledge graphs (KGs)
that needs to be completed, which is further transformed into a textual format
comprehensible to LLMs for providing logic guidance. We conducted extensive
experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The
experimental results demonstrate that OL-KGC significantly outperforms existing
mainstream KGC methods across multiple evaluation metrics, achieving
state-of-the-art performance.

</details>


### [116] [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)
*Yuzhong Zhao,Yue Liu,Junpeng Liu,Jingye Chen,Xun Wu,Yaru Hao,Tengchao Lv,Shaohan Huang,Lei Cui,Qixiang Ye,Fang Wan,Furu Wei*

Main category: cs.CL

TL;DR: GMPO是一种改进的GRPO方法，通过优化几何平均奖励提高稳定性，并在多个数学和多模态推理基准上表现更好。


<details>
  <summary>Details</summary>
Motivation: GRPO在处理异常奖励时存在策略更新不稳定的问题，需要一种更稳定的优化方法。

Method: 提出GMPO，通过最大化几何平均奖励减少异常值影响，保持重要性采样比稳定。

Result: GMPO-7B在数学和多模态推理基准上平均分别比GRPO提高4.1%和1.4%。

Conclusion: GMPO在稳定性和性能上均优于GRPO，是一种有效的优化方法。

Abstract: Recent advancements, such as Group Relative Policy Optimization (GRPO), have
enhanced the reasoning capabilities of large language models by optimizing the
arithmetic mean of token-level rewards. However, GRPO suffers from unstable
policy updates when processing tokens with outlier importance-weighted rewards,
which manifests as extreme importance sampling ratios during training, i.e.,
the ratio between the sampling probabilities assigned to a token by the current
and old policies. In this work, we propose Geometric-Mean Policy Optimization
(GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic
mean, GMPO maximizes the geometric mean of token-level rewards, which is
inherently less sensitive to outliers and maintains a more stable range of
importance sampling ratio. In addition, we provide comprehensive theoretical
and experimental analysis to justify the design and stability benefits of GMPO.
Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on
multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark,
including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is
available at https://github.com/callsys/GMPO.

</details>


### [117] [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)
*Hanna Shcharbakova,Tatiana Anikina,Natalia Skachkova,Josef van Genabith*

Main category: cs.CL

TL;DR: 研究发现，在多语言事实核查任务中，小型专用模型（如XLM-R）表现优于大型通用模型（如Llama 3.1），XLM-R的宏F1分数为57.7%，远高于大型模型的16.9%。


<details>
  <summary>Details</summary>
Motivation: 多语言错误信息的快速传播需要能够处理细粒度真实性评估的自动化事实核查系统，但大型语言模型在此任务中的表现尚未充分研究。

Method: 在X-Fact数据集上评估了五种最先进的语言模型，包括小型编码器模型（XLM-R、mT5）和大型解码器模型（Llama 3.1、Qwen 2.5、Mistral Nemo），采用提示和微调方法。

Result: XLM-R（2.7亿参数）显著优于所有测试的大型模型（70-120亿参数），宏F1分数为57.7%，比之前的最佳性能（41.9%）提高了15.8%。

Conclusion: 对于细粒度多语言事实核查，小型专用模型比通用大型模型更有效，这对事实核查系统的实际部署具有重要意义。

Abstract: The rapid spread of multilingual misinformation requires robust automated
fact verification systems capable of handling fine-grained veracity assessments
across diverse languages. While large language models have shown remarkable
capabilities across many NLP tasks, their effectiveness for multilingual claim
verification with nuanced classification schemes remains understudied. We
conduct a comprehensive evaluation of five state-of-the-art language models on
the X-Fact dataset, which spans 25 languages with seven distinct veracity
categories. Our experiments compare small language models (encoder-based XLM-R
and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo)
using both prompting and fine-tuning approaches. Surprisingly, we find that
XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B
parameters), achieving 57.7% macro-F1 compared to the best LLM performance of
16.9%. This represents a 15.8% improvement over the previous state-of-the-art
(41.9%), establishing new performance benchmarks for multilingual fact
verification. Our analysis reveals problematic patterns in LLM behavior,
including systematic difficulties in leveraging evidence and pronounced biases
toward frequent categories in imbalanced data settings. These findings suggest
that for fine-grained multilingual fact verification, smaller specialized
models may be more effective than general-purpose large models, with important
implications for practical deployment of fact-checking systems.

</details>


### [118] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)
*Gabriel Downer,Sean Craven,Damian Ruck,Jake Thomas*

Main category: cs.CL

TL;DR: Text2VLM是一个多阶段管道，将纯文本数据集转换为多模态格式，用于评估视觉语言模型（VLMs）对排版提示注入攻击的抵抗力。


<details>
  <summary>Details</summary>
Motivation: 现有评估数据集主要针对纯文本提示，忽略了视觉漏洞，需要填补这一空白以提升VLMs的安全性。

Method: 通过识别原始文本中的有害内容并将其转换为排版图像，生成多模态提示，评估VLMs的脆弱性。

Result: 开源VLMs在引入视觉输入后对提示注入攻击更敏感，且性能与闭源前沿模型存在显著差距。

Conclusion: Text2VLM为全面安全评估提供了可扩展工具，有助于开发更强大的VLM安全机制。

Abstract: The increasing integration of Visual Language Models (VLMs) into AI systems
necessitates robust model alignment, especially when handling multimodal
content that combines text and images. Existing evaluation datasets heavily
lean towards text-only prompts, leaving visual vulnerabilities under evaluated.
To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline
that adapts text-only datasets into multimodal formats, specifically designed
to evaluate the resilience of VLMs against typographic prompt injection
attacks. The Text2VLM pipeline identifies harmful content in the original text
and converts it into a typographic image, creating a multimodal prompt for
VLMs. Also, our evaluation of open-source VLMs highlights their increased
susceptibility to prompt injection when visual inputs are introduced, revealing
critical weaknesses in the current models' alignment. This is in addition to a
significant performance gap compared to closed-source frontier models. We
validate Text2VLM through human evaluations, ensuring the alignment of
extracted salient concepts; text summarization and output classification align
with human expectations. Text2VLM provides a scalable tool for comprehensive
safety assessment, contributing to the development of more robust safety
mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities,
Text2VLM plays a role in advancing the safe deployment of VLMs in diverse,
real-world applications.

</details>


### [119] [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)
*Yiran Huang,Lukas Thede,Massimiliano Mancini,Wenjia Xu,Zeynep Akata*

Main category: cs.CL

TL;DR: 该论文提出了一种通过结构剪枝和高效恢复训练直接压缩多模态大语言模型（MLLMs）的方法，以解决其计算和内存需求高的问题。


<details>
  <summary>Details</summary>
Motivation: MLLMs的计算和内存需求高，现有参数缩减方法灵活性有限且计算量大，需要更高效的压缩方案。

Method: 采用层间和宽度剪枝两种结构剪枝范式，结合监督微调和知识蒸馏进行恢复训练，并评估小数据量下的恢复效果。

Result: 宽度剪枝在资源有限时表现更好；仅微调多模态投影器在小压缩率（<20%）下足够；5%的训练数据可恢复95%性能。

Conclusion: 该方法为资源有限情况下高效压缩MLLMs提供了实用方案。

Abstract: While Multimodal Large Language Models (MLLMs) demonstrate impressive
capabilities, their substantial computational and memory requirements pose
significant barriers to practical deployment. Current parameter reduction
techniques primarily involve training MLLMs from Small Language Models (SLMs),
but these methods offer limited flexibility and remain computationally
intensive. To address this gap, we propose to directly compress existing MLLMs
through structural pruning combined with efficient recovery training.
Specifically, we investigate two structural pruning paradigms--layerwise and
widthwise pruning--applied to the language model backbone of MLLMs, alongside
supervised finetuning and knowledge distillation. Additionally, we assess the
feasibility of conducting recovery training with only a small fraction of the
available data. Our results show that widthwise pruning generally maintains
better performance in low-resource scenarios with limited computational
resources or insufficient finetuning data. As for the recovery training,
finetuning only the multimodal projector is sufficient at small compression
levels (< 20%). Furthermore, a combination of supervised finetuning and
hidden-state distillation yields optimal recovery across various pruning
levels. Notably, effective recovery can be achieved with as little as 5% of the
original training data, while retaining over 95% of the original performance.
Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and
Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming
to compress MLLMs effectively without extensive computation resources or
sufficient data.

</details>


### [120] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)
*Carlo Alfano,Aymen Al Marjani,Zeno Jonke,Amin Mantrach,Saab Mansour,Marcello Federico*

Main category: cs.CL

TL;DR: 提出了一种基于合成多语言摘要数据的自我学习评估框架，用于多语言忠实度评估，无需大量标注数据。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在多语言环境中信息幻觉的问题，现有方法主要依赖英语和昂贵的人工标注数据。

Method: 利用合成多语言摘要数据和跨语言迁移学习，比较语言特定和混合语言微调方法。

Result: 框架在性能上优于现有基线，包括最先进的英语评估器和基于机器翻译的方法。

Conclusion: 展示了LLM的通用语言能力与其在特定语言评估任务中的表现之间存在一致性关系。

Abstract: The growing use of large language models (LLMs) has increased the need for
automatic evaluation systems, particularly to address the challenge of
information hallucination. Although existing faithfulness evaluation approaches
have shown promise, they are predominantly English-focused and often require
expensive human-labeled training data for fine-tuning specialized models. As
LLMs see increased adoption in multilingual contexts, there is a need for
accurate faithfulness evaluators that can operate across languages without
extensive labeled data. This paper presents Self-Taught Evaluators for
Multilingual Faithfulness, a framework that learns exclusively from synthetic
multilingual summarization data while leveraging cross-lingual transfer
learning. Through experiments comparing language-specific and mixed-language
fine-tuning approaches, we demonstrate a consistent relationship between an
LLM's general language capabilities and its performance in language-specific
evaluation tasks. Our framework shows improvements over existing baselines,
including state-of-the-art English evaluators and machine translation-based
approaches.

</details>


### [121] [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)
*Meishan Zhang,Xin Zhang,Xinping Zhao,Shouzheng Huang,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: 该论文综述了预训练语言模型（PLMs）在通用文本嵌入（GPTE）中的关键作用，包括基础架构、优化策略及未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 随着预训练语言模型的兴起，通用文本嵌入因其在多种NLP任务中的高效表现而受到广泛关注，本文旨在全面梳理PLMs在GPTE发展中的角色。

Method: 通过分析PLMs在GPTE中的基础架构（如嵌入提取、训练策略）和高级功能（如多语言支持、多模态整合），系统总结了其作用。

Result: 论文详细阐述了PLMs如何推动GPTE的发展，并提出了未来研究方向，如排名整合、安全性考虑等。

Conclusion: 该综述为研究者提供了GPTE当前状态及未来潜力的全面参考，尤其关注PLMs的核心作用。

Abstract: Text embeddings have attracted growing interest due to their effectiveness
across a wide range of natural language processing (NLP) tasks, such as
retrieval, classification, clustering, bitext mining, and summarization. With
the emergence of pretrained language models (PLMs), general-purpose text
embeddings (GPTE) have gained significant traction for their ability to produce
rich, transferable representations. The general architecture of GPTE typically
leverages PLMs to derive dense text representations, which are then optimized
through contrastive learning on large-scale pairwise datasets. In this survey,
we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the
roles PLMs play in driving its development. We first examine the fundamental
architecture and describe the basic roles of PLMs in GPTE, i.e., embedding
extraction, expressivity enhancement, training strategies, learning objectives,
and data construction. Then, we describe advanced roles enabled by PLMs, such
as multilingual support, multimodal integration, code understanding, and
scenario-specific adaptation. Finally, we highlight potential future research
directions that move beyond traditional improvement goals, including ranking
integration, safety considerations, bias mitigation, structural information
incorporation, and the cognitive extension of embeddings. This survey aims to
serve as a valuable reference for both newcomers and established researchers
seeking to understand the current state and future potential of GPTE.

</details>


### [122] [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)
*Sam Osian,Arpan Dutta,Sahil Bhandari,Iain E. Buchan,Dan W. Joyce*

Main category: cs.CL

TL;DR: 自动化语言模型工具（PFD Toolkit）能高效、可靠地复现英格兰和威尔士验尸官发布的儿童自杀预防死亡报告（PFD）的手动主题分析，显著提升效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 传统手动分析PFD报告耗时且效率低，需要自动化工具提升分析速度和可靠性。

Method: 使用开源语言模型管道（PFD Toolkit）自动筛选和编码4,249份PFD报告，识别儿童自杀案例并分析主题。

Result: 自动化工具识别出72份儿童自杀报告（是手动分析的两倍），与临床专家标注的一致性高（Cohen's κ = 0.82）。处理时间从数月缩短至8分钟。

Conclusion: 自动化语言模型分析可高效、可靠地替代手动主题分析，为公共卫生提供可扩展、可复现的及时洞察。

Abstract: Prevention of Future Deaths (PFD) reports, issued by coroners in England and
Wales, flag systemic hazards that may lead to further loss of life. Analysis of
these reports has previously been constrained by the manual effort required to
identify and code relevant cases. In 2025, the Office for National Statistics
(ONS) published a national thematic review of child-suicide PFD reports ($\leq$
18 years), identifying 37 cases from January 2015 to November 2023 - a process
based entirely on manual curation and coding. We evaluated whether a fully
automated, open source "text-to-table" language-model pipeline (PFD Toolkit)
could reproduce the ONS's identification and thematic analysis of child-suicide
PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD
reports published from July 2013 to November 2023 were processed via PFD
Toolkit's large language model pipelines. Automated screening identified cases
where the coroner attributed death to suicide in individuals aged 18 or
younger, and eligible reports were coded for recipient category and 23 concern
sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72
child-suicide PFD reports - almost twice the ONS count. Three blinded
clinicians adjudicated a stratified sample of 144 reports to validate the
child-suicide screening. Against the post-consensus clinical annotations, the
LLM-based workflow showed substantial to almost-perfect agreement (Cohen's
$\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script
runtime was 8m 16s, transforming a process that previously took months into one
that can be completed in minutes. This demonstrates that automated LLM analysis
can reliably and efficiently replicate manual thematic reviews of coronial
data, enabling scalable, reproducible, and timely insights for public health
and safety. The PFD Toolkit is openly available for future research.

</details>


### [123] [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)
*Yilun Qiu,Tianhao Shi,Xiaoyan Zhao,Fengbin Zhu,Yang Zhang,Fuli Feng*

Main category: cs.CL

TL;DR: DEP框架通过潜在空间建模用户差异，提升个性化输出效果。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖用户历史或语言提示，难以有效捕捉用户间差异。

Method: DEP利用对比用户嵌入和稀疏自编码器，提取任务相关特征。

Result: 实验显示DEP在个性化评论生成中优于基线方法。

Conclusion: DEP为个性化任务提供了一种更有效的潜在空间建模方法。

Abstract: Large language models (LLMs) are increasingly integrated into users' daily
lives, leading to a growing demand for personalized outputs. Previous work
focuses on leveraging a user's own history, overlooking inter-user differences
that are crucial for effective personalization. While recent work has attempted
to model such differences, the reliance on language-based prompts often hampers
the effective extraction of meaningful distinctions. To address these issues,
we propose Difference-aware Embedding-based Personalization (DEP), a framework
that models inter-user differences in the latent space instead of relying on
language prompts. DEP constructs soft prompts by contrasting a user's embedding
with those of peers who engaged with similar content, highlighting relative
behavioral signals. A sparse autoencoder then filters and compresses both
user-specific and difference-aware embeddings, preserving only task-relevant
features before injecting them into a frozen LLM. Experiments on personalized
review generation show that DEP consistently outperforms baseline methods
across multiple metrics. Our code is available at
https://github.com/SnowCharmQ/DEP.

</details>


### [124] [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)
*Louis Estève,Marie-Catherine de Marneffe,Nurit Melnik,Agata Savary,Olha Kanishcheva*

Main category: cs.CL

TL;DR: 本文探讨了NLP中多样性的概念，提出了一个统一的分类法，并基于生态学和经济学框架对多样性进行了系统化分析。


<details>
  <summary>Details</summary>
Motivation: 由于多样性在NLP中缺乏系统化研究，且术语不一致，本文旨在统一多样性测量方法，以促进理解与比较。

Method: 通过调查ACL Anthology过去6年标题含“diversity”或“diverse”的文章，提出基于Stirling（2007）框架的三维度分类法（多样性、平衡性、差异性）。

Result: 研究发现多样性测量方法多样且术语不一致，通过统一框架揭示了趋势。

Conclusion: 本文为NLP中多样性的形式化提供了基础，有助于提升理解与方法的可比性。

Abstract: The concept of diversity has received increased consideration in Natural
Language Processing (NLP) in recent years. This is due to various motivations
like promoting and inclusion, approximating human linguistic behavior, and
increasing systems' performance. Diversity has however often been addressed in
an ad hoc manner in NLP, and with few explicit links to other domains where
this notion is better theorized. We survey articles in the ACL Anthology from
the past 6 years, with "diversity" or "diverse" in their title. We find a wide
range of settings in which diversity is quantified, often highly specialized
and using inconsistent terminology. We put forward a unified taxonomy of why,
what on, where, and how diversity is measured in NLP. Diversity measures are
cast upon a unified framework from ecology and economy (Stirling, 2007) with 3
dimensions of diversity: variety, balance and disparity. We discuss the trends
which emerge due to this systematized approach. We believe that this study
paves the way towards a better formalization of diversity in NLP, which should
bring a better understanding of this notion and a better comparability between
various approaches.

</details>


### [125] [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)
*Luc Builtjes,Joeran Bosma,Mathias Prokop,Bram van Ginneken,Alessa Hering*

Main category: cs.CL

TL;DR: 研究评估了九种开源生成式大语言模型（LLM）在荷兰语临床信息提取任务中的表现，发现部分14B参数模型表现优异，且开源LLM结合框架可提供高效、可扩展且隐私保护的解决方案。


<details>
  <summary>Details</summary>
Motivation: 医疗报告信息丰富但非结构化且使用专业语言，现有专有LLM因透明度和隐私问题在医疗领域受限，需探索开源LLM的潜力。

Method: 开发了公开框架llm_extractinator，在零样本设置下评估九种开源LLM在28项荷兰语临床任务中的表现。

Result: 14B参数模型（如Phi-4-14B）表现竞争力强，更大模型（如Llama-3.3-70B）性能略高但计算成本更高；翻译为英语会降低性能。

Conclusion: 开源LLM结合框架为低资源环境下的临床信息提取提供了高效、可扩展且隐私保护的解决方案。

Abstract: Medical reports contain rich clinical information but are often unstructured
and written in domain-specific language, posing challenges for information
extraction. While proprietary large language models (LLMs) have shown promise
in clinical natural language processing, their lack of transparency and data
privacy concerns limit their utility in healthcare. This study therefore
evaluates nine open-source generative LLMs on the DRAGON benchmark, which
includes 28 clinical information extraction tasks in Dutch. We developed
\texttt{llm\_extractinator}, a publicly available framework for information
extraction using open-source generative LLMs, and used it to assess model
performance in a zero-shot setting. Several 14 billion parameter models,
Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results,
while the bigger Llama-3.3-70B model achieved slightly higher performance at
greater computational cost. Translation to English prior to inference
consistently degraded performance, highlighting the need of native-language
processing. These findings demonstrate that open-source LLMs, when used with
our framework, offer effective, scalable, and privacy-conscious solutions for
clinical information extraction in low-resource settings.

</details>


### [126] [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)
*Jungwon Park,Wonjong Rhee*

Main category: cs.CL

TL;DR: 提出了一种名为Soft Injection的新方法，通过任务嵌入和软混合参数减少提示长度并提升任务性能。


<details>
  <summary>Details</summary>
Motivation: 探索是否多示例提示是传达任务信息的最有效方式，并提出更高效的方法。

Method: 使用少量示例构建任务嵌入，并通过软混合参数将其注入注意力头激活中。

Result: 在57个任务和12个LLM上评估，平均性能提升10.1%-13.9%，同时减少内存和计算成本。

Conclusion: Soft Injection为减少提示长度和提升性能提供了新范式，揭示了注意力头的任务特定功能。

Abstract: In-Context Learning (ICL) enables Large Language Models (LLMs) to perform
tasks by conditioning on input-output examples in the prompt, without requiring
any update in model parameters. While widely adopted, it remains unclear
whether prompting with multiple examples is the most effective and efficient
way to convey task information. In this work, we propose Soft Injection of task
embeddings. The task embeddings are constructed only once using few-shot ICL
prompts and repeatedly used during inference. Soft injection is performed by
softly mixing task embeddings with attention head activations using
pre-optimized mixing parameters, referred to as soft head-selection parameters.
This method not only allows a desired task to be performed without in-prompt
demonstrations but also significantly outperforms existing ICL approaches while
reducing memory usage and compute cost at inference time. An extensive
evaluation is performed across 57 tasks and 12 LLMs, spanning four model
families of sizes from 4B to 70B. Averaged across 57 tasks, our method
outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show
that our method also serves as an insightful tool for analyzing task-relevant
roles of attention heads, revealing that task-relevant head positions selected
by our method transfer across similar tasks but not across dissimilar ones --
underscoring the task-specific nature of head functionality. Our soft injection
method opens a new paradigm for reducing prompt length and improving task
performance by shifting task conditioning from the prompt space to the
activation space.

</details>


### [127] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)
*Adrien Bazoge*

Main category: cs.CL

TL;DR: MediQAl是一个法语医学问答数据集，用于评估语言模型在真实临床场景中的事实回忆和推理能力。


<details>
  <summary>Details</summary>
Motivation: 填补多语言医学领域资源的空白，评估语言模型在医学问答中的表现。

Method: 数据集包含32,603个问题，分为三类任务（单选、多选、开放短答），并标注为理解或推理。

Result: 评估14个大型语言模型后发现，事实回忆和推理任务之间存在显著性能差距。

Conclusion: MediQAl为法语医学问答提供了全面的基准，解决了医学领域多语言资源的关键问题。

Abstract: This work introduces MediQAl, a French medical question answering dataset
designed to evaluate the capabilities of language models in factual medical
recall and reasoning over real-world clinical scenarios. MediQAl contains
32,603 questions sourced from French medical examinations across 41 medical
subjects. The dataset includes three tasks: (i) Multiple-Choice Question with
Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii)
Open-Ended Question with Short-Answer. Each question is labeled as
Understanding or Reasoning, enabling a detailed analysis of models' cognitive
capabilities. We validate the MediQAl dataset through extensive evaluation with
14 large language models, including recent reasoning-augmented models, and
observe a significant performance gap between factual recall and reasoning
tasks. Our evaluation provides a comprehensive benchmark for assessing language
models' performance on French medical question answering, addressing a crucial
gap in multilingual resources for the medical domain.

</details>


### [128] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)
*Roberto Labadie-Tamayo,Adrian Jaques Böck,Djordje Slijepčević,Xihui Chen,Andreas Babic,Matthias Zeppelzauer*

Main category: cs.CL

TL;DR: 论文介绍了在CLEF 2025的EXIST挑战中解决社交媒体文本中性别歧视识别和分类的三个子任务的解决方案，包括三种模型（SCBM、SCBMT和XLM-RoBERTa），并报告了其性能。


<details>
  <summary>Details</summary>
Motivation: 社交媒体中性别歧视问题日益严重，EXIST挑战旨在通过技术手段识别和分类性别歧视内容。

Method: 使用了三种模型：SCBM（基于形容词的瓶颈概念）、SCBMT（结合形容词和Transformer）和微调的XLM-RoBERTa。

Result: XLM-RoBERTa在子任务1.1中表现优异，SCBMT在解释性和性能之间取得平衡。

Conclusion: 提出的模型在性能和可解释性上均表现良好，为性别歧视识别提供了有效工具。

Abstract: Sexism has become widespread on social media and in online conversation. To
help address this issue, the fifth Sexism Identification in Social Networks
(EXIST) challenge is initiated at CLEF 2025. Among this year's international
benchmarks, we concentrate on solving the first task aiming to identify and
classify sexism in social media textual posts. In this paper, we describe our
solutions and report results for three subtasks: Subtask 1.1 - Sexism
Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask
1.3 - Sexism Categorization in Tweets. We implement three models to address
each subtask which constitute three individual runs: Speech Concept Bottleneck
Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a
fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as
human-interpretable bottleneck concepts. SCBM leverages large language models
(LLMs) to encode input texts into a human-interpretable representation of
adjectives, then used to train a lightweight classifier for downstream tasks.
SCBMT extends SCBM by fusing adjective-based representation with contextual
embeddings from transformers to balance interpretability and classification
performance. Beyond competitive results, these two models offer fine-grained
explanations at both instance (local) and class (global) levels. We also
investigate how additional metadata, e.g., annotators' demographic profiles,
can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data
augmented with prior datasets, ranks 6th for English and Spanish and 4th for
English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and
Spanish and 6th for Spanish.

</details>


### [129] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)
*Likun Tan,Kuan-Wei Huang,Kevin Wu*

Main category: cs.CL

TL;DR: 该论文提出了一种检测和编辑大型语言模型中事实错误的方法，特别是在金融领域，通过构建合成数据集并微调模型，显著提升了检测性能。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型在需要事实可靠性的高风险领域（如金融）中的幻觉问题。

Method: 构建合成数据集，插入标记错误，并微调四种语言模型（Phi-4、Phi-4-mini、Qwen3-4B、Qwen3-14B）以检测和编辑事实错误。

Result: 微调后的Phi-4模型在二元F1分数上提升了8%，整体检测性能提升了30%；Phi-4-mini模型性能接近OpenAI-o3，仅下降2%。

Conclusion: 该方法为金融文本生成中的事实错误检测和编辑提供了实用解决方案，并可推广到其他领域，增强语言模型的可信度。

Abstract: Hallucinations in large language models pose a critical challenge for
applications requiring factual reliability, particularly in high-stakes domains
such as finance. This work presents an effective approach for detecting and
editing factually incorrect content in model-generated responses based on the
provided context. Given a user-defined domain-specific error taxonomy, we
construct a synthetic dataset by inserting tagged errors into financial
question-answering corpora and then fine-tune four language models, Phi-4,
Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual
inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8%
improvement in binary F1 score and a 30% gain in overall detection performance
compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having
only 4 billion parameters, maintains competitive performance with just a 2%
drop in binary detection and a 0.1% decline in overall detection compared to
OpenAI-o3. Our work provides a practical solution for detecting and editing
factual inconsistencies in financial text generation while introducing a
generalizable framework that can enhance the trustworthiness and alignment of
large language models across diverse applications beyond finance. Our code and
data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [130] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)
*Max Peeperkorn,Tom Kouwenhoven,Dan Brown,Anna Jordanous*

Main category: cs.CL

TL;DR: 论文研究了指令调优对大型语言模型输出多样性的影响，提出了新的解码策略以恢复多样性。


<details>
  <summary>Details</summary>
Motivation: 指令调优会减少语言模型输出的多样性，尤其是在创意任务中，因此需要研究如何恢复多样性。

Method: 通过分析不同调优阶段的多样性变化，提出了一种名为“conformative decoding”的解码策略，利用基础模型的多样性指导指令模型。

Result: 结果显示指令调优显著降低多样性，而新解码策略能有效恢复多样性并保持或提升输出质量。

Conclusion: 指令调优对多样性有负面影响，但通过新解码策略可以部分弥补这一缺陷。

Abstract: Instruction-tuning large language models (LLMs) reduces the diversity of
their outputs, which has implications for many tasks, particularly for creative
tasks. This paper investigates the ``diversity gap'' for a writing prompt
narrative generation task. This gap emerges as measured by current diversity
metrics for various open-weight and open-source LLMs. The results show
significant decreases in diversity due to instruction-tuning. We explore the
diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to
further understand how output diversity is affected. The results indicate that
DPO has the most substantial impact on diversity. Motivated by these findings,
we present a new decoding strategy, conformative decoding, which guides an
instruct model using its more diverse base model to reintroduce output
diversity. We show that conformative decoding typically increases diversity and
even maintains or improves quality.

</details>


### [131] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)
*Danil Savine,Muni Sreenivas Pydi,Jamal Atif,Olivier Cappé*

Main category: cs.CL

TL;DR: 研究探讨了微调大型语言模型（LLMs）中记忆化的机制和影响因素，重点关注医疗领域。通过PHEE数据集，分析了微调过程中不同因素对记忆化的影响。


<details>
  <summary>Details</summary>
Motivation: 医疗领域数据隐私敏感，研究旨在理解微调LLMs时的记忆化现象，以平衡模型性能和隐私风险。

Method: 采用成员推理攻击和生成任务评估记忆化，分析权重矩阵、困惑度及LoRA秩的影响。

Result: 发现Value和Output矩阵对记忆化贡献更大；低困惑度与高记忆化相关；高LoRA秩增加记忆化但边际效益递减。

Conclusion: 研究为微调LLMs时的性能与隐私权衡提供了见解，有助于开发更负责任的数据隐私管理策略。

Abstract: This study investigates the mechanisms and factors influencing memorization
in fine-tuned large language models (LLMs), with a focus on the medical domain
due to its privacy-sensitive nature. We examine how different aspects of the
fine-tuning process affect a model's propensity to memorize training data,
using the PHEE dataset of pharmacovigilance events.
  Our research employs two main approaches: a membership inference attack to
detect memorized data, and a generation task with prompted prefixes to assess
verbatim reproduction. We analyze the impact of adapting different weight
matrices in the transformer architecture, the relationship between perplexity
and memorization, and the effect of increasing the rank in low-rank adaptation
(LoRA) fine-tuning.
  Key findings include: (1) Value and Output matrices contribute more
significantly to memorization compared to Query and Key matrices; (2) Lower
perplexity in the fine-tuned model correlates with increased memorization; (3)
Higher LoRA ranks lead to increased memorization, but with diminishing returns
at higher ranks.
  These results provide insights into the trade-offs between model performance
and privacy risks in fine-tuned LLMs. Our findings have implications for
developing more effective and responsible strategies for adapting large
language models while managing data privacy concerns.

</details>


### [132] [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)
*Jiaju Chen,Yuxuan Lu,Xiaojie Wang,Huimin Zeng,Jing Huang,Jiri Gesi,Ying Xu,Bingsheng Yao,Dakuo Wang*

Main category: cs.CL

TL;DR: MAJ-EVAL是一个多代理评估框架，通过自动构建不同维度的评估者角色，利用LLM代理模拟人类评估者，生成多维反馈。


<details>
  <summary>Details</summary>
Motivation: 现有LLM-as-a-judge方法存在角色设计随意和框架通用性不足的问题，MAJ-EVAL旨在解决这些问题。

Method: 自动从文本中构建评估者角色，实例化LLM代理，并通过多代理辩论生成多维反馈。

Result: 在教育和医疗领域的实验中，MAJ-EVAL生成的评估结果更接近人类专家评分。

Conclusion: MAJ-EVAL提供了一种更接近人类评估的自动化方法，优于传统指标和现有LLM-as-a-judge方法。

Abstract: Nearly all human work is collaborative; thus, the evaluation of real-world
NLP applications often requires multiple dimensions that align with diverse
human perspectives. As real human evaluator resources are often scarce and
costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising
approach to leverage LLM agents to believably simulate human evaluators. Yet,
to date, existing LLM-as-a-judge approaches face two limitations: persona
descriptions of agents are often arbitrarily designed, and the frameworks are
not generalizable to other tasks. To address these challenges, we propose
MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically
construct multiple evaluator personas with distinct dimensions from relevant
text documents (e.g., research papers), instantiate LLM agents with the
personas, and engage in-group debates with multi-agents to Generate
multi-dimensional feedback. Our evaluation experiments in both the educational
and medical domains demonstrate that MAJ-EVAL can generate evaluation results
that better align with human experts' ratings compared with conventional
automated evaluation metrics and existing LLM-as-a-judge methods.

</details>
