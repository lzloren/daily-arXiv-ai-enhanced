<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.CL](#cs.CL) [Total: 49]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [ASP-Assisted Symbolic Regression: Uncovering Hidden Physics in Fluid Mechanics](https://arxiv.org/abs/2507.17777)
*Theofanis Aravanis,Grigorios Chrimatopoulos,Mohammad Ferdows,Michalis Xenos,Efstratios Em Tzirtzilakis*

Main category: cs.AI

TL;DR: 该研究利用符号回归（SR）和答案集编程（ASP）结合的方法，从数值模拟数据中推导出可解释的数学关系，用于建模三维不可压缩流动，并确保结果的物理合理性。


<details>
  <summary>Details</summary>
Motivation: 在流体力学中，理解流动物理与准确预测同样重要，而传统机器学习方法缺乏可解释性。

Method: 使用PySR库从数值模拟数据中推导符号方程，并结合ASP框架确保物理合理性。

Result: 推导的方程不仅近似模拟了抛物线速度分布和压力降，还与文献中的解析解完全一致。

Conclusion: SR能简化复杂流动行为为可解释方程，结合ASP可提升数据驱动模型的可靠性和领域一致性。

Abstract: Unlike conventional Machine-Learning (ML) approaches, often criticized as
"black boxes", Symbolic Regression (SR) stands out as a powerful tool for
revealing interpretable mathematical relationships in complex physical systems,
requiring no a priori assumptions about models' structures. Motivated by the
recognition that, in fluid mechanics, an understanding of the underlying flow
physics is as crucial as accurate prediction, this study applies SR to model a
fundamental three-dimensional (3D) incompressible flow in a rectangular
channel, focusing on the (axial) velocity and pressure fields under laminar
conditions. By employing the PySR library, compact symbolic equations were
derived directly from numerical simulation data, revealing key characteristics
of the flow dynamics. These equations not only approximate the parabolic
velocity profile and pressure drop observed in the studied fluid flow, but also
perfectly coincide with analytical solutions from the literature. Furthermore,
we propose an innovative approach that integrates SR with the
knowledge-representation framework of Answer Set Programming (ASP), combining
the generative power of SR with the declarative reasoning strengths of ASP. The
proposed hybrid SR/ASP framework ensures that the SR-generated symbolic
expressions are not only statistically accurate, but also physically plausible,
adhering to domain-specific principles. Overall, the study highlights two key
contributions: SR's ability to simplify complex flow behaviours into concise,
interpretable equations, and the potential of knowledge-representation
approaches to improve the reliability and alignment of data-driven SR models
with domain principles. Insights from the examined 3D channel flow pave the way
for integrating such hybrid approaches into efficient frameworks, [...] where
explainable predictions and real-time data analysis are crucial.

</details>


### [2] [I2I-STRADA -- Information to Insights via Structured Reasoning Agent for Data Analysis](https://arxiv.org/abs/2507.17874)
*SaiBarath Sundar,Pranav Satheesan,Udayaadithya Avadhanam*

Main category: cs.AI

TL;DR: I2I-STRADA是一种多智能体架构，通过结构化推理过程改进数据分析，优于现有系统。


<details>
  <summary>Details</summary>
Motivation: 现有系统忽视了数据分析中的结构化推理过程，导致推理步骤缺乏一致性。

Method: 提出I2I-STRADA架构，通过模块化子任务模拟认知步骤，实现结构化推理。

Result: 在DABstep和DABench基准测试中表现优于现有系统，规划一致性和洞察对齐更优。

Conclusion: 结构化认知工作流对数据分析智能体设计至关重要。

Abstract: Recent advances in agentic systems for data analysis have emphasized
automation of insight generation through multi-agent frameworks, and
orchestration layers. While these systems effectively manage tasks like query
translation, data transformation, and visualization, they often overlook the
structured reasoning process underlying analytical thinking. Reasoning large
language models (LLMs) used for multi-step problem solving are trained as
general-purpose problem solvers. As a result, their reasoning or thinking steps
do not adhere to fixed processes for specific tasks. Real-world data analysis
requires a consistent cognitive workflow: interpreting vague goals, grounding
them in contextual knowledge, constructing abstract plans, and adapting
execution based on intermediate outcomes. We introduce I2I-STRADA
(Information-to-Insight via Structured Reasoning Agent for Data Analysis), an
agentic architecture designed to formalize this reasoning process. I2I-STRADA
focuses on modeling how analysis unfolds via modular sub-tasks that reflect the
cognitive steps of analytical reasoning. Evaluations on the DABstep and DABench
benchmarks show that I2I-STRADA outperforms prior systems in planning coherence
and insight alignment, highlighting the importance of structured cognitive
workflows in agent design for data analysis.

</details>


### [3] [SMARTAPS: Tool-augmented LLMs for Operations Management](https://arxiv.org/abs/2507.17927)
*Timothy Tin Long Yu,Mahdi Mostajabdaveh,Jabo Serge Byusa,Rindra Ramamonjison,Giuseppe Carenini,Kun Mao,Zirui Zhou,Yong Zhang*

Main category: cs.AI

TL;DR: SmartAPS是一个基于大型语言模型的对话系统，旨在为供应链规划者提供更易访问的高级规划系统（APS），通过自然语言界面实现查询、分析和推荐功能。


<details>
  <summary>Details</summary>
Motivation: 传统APS系统因定制和维护成本高昂，许多用户无法负担。SmartAPS旨在通过LLM技术降低使用门槛，满足供应链规划者的需求。

Method: SmartAPS基于工具增强的大型语言模型，提供自然语言聊天界面，支持查询、反事实推理、推荐和场景分析。

Result: 系统通过直观的界面帮助用户更好地管理运营，降低了APS的使用成本。

Conclusion: SmartAPS展示了LLM在提升传统工具可访问性和用户体验方面的潜力。

Abstract: Large language models (LLMs) present intriguing opportunities to enhance user
interaction with traditional algorithms and tools in real-world applications.
An advanced planning system (APS) is a sophisticated software that leverages
optimization to help operations planners create, interpret, and modify an
operational plan. While highly beneficial, many customers are priced out of
using an APS due to the ongoing costs of consultants responsible for
customization and maintenance. To address the need for a more accessible APS
expressed by supply chain planners, we present SmartAPS, a conversational
system built on a tool-augmented LLM. Our system provides operations planners
with an intuitive natural language chat interface, allowing them to query
information, perform counterfactual reasoning, receive recommendations, and
execute scenario analysis to better manage their operation. A short video
demonstrating the system has been released: https://youtu.be/KtIrJjlDbyw

</details>


### [4] [Synthesis of timeline-based planning strategies avoiding determinization](https://arxiv.org/abs/2507.17988)
*Dario Della Monica,Angelo Montanari,Pietro Sala*

Main category: cs.AI

TL;DR: 本文提出了一种定性时间轴规划的片段，其计划存在性问题可直接映射到确定性有限自动机的非空性问题，从而避免了耗时的确定性步骤。


<details>
  <summary>Details</summary>
Motivation: 定性时间轴规划的计划存在性问题通常需要非确定性有限自动机，但确定性步骤耗时。本文旨在找到可直接映射到确定性有限自动机的规划片段。

Method: 识别定性时间轴规划的一个片段，将其计划存在性问题映射到确定性有限自动机的非空性问题，并确定适合该片段的Allen关系的最大子集。

Result: 成功识别了一个可直接映射到确定性有限自动机的规划片段，并确定了适用的Allen关系的最大子集。

Conclusion: 该研究为定性时间轴规划提供了一种更高效的策略合成方法，避免了非确定性步骤的复杂性。

Abstract: Qualitative timeline-based planning models domains as sets of independent,
but
  interacting, components whose behaviors over time, the timelines, are
governed
  by sets of qualitative temporal constraints (ordering relations), called
  synchronization rules.
  Its plan-existence problem has been shown to be PSPACE-complete; in
  particular, PSPACE-membership has been proved via reduction to the
  nonemptiness problem for nondeterministic finite automata.
  However, nondeterministic automata cannot be directly used to synthesize
  planning strategies as a costly determinization step is needed.
  In this paper, we identify a fragment of qualitative timeline-based planning
  whose plan-existence problem can be directly mapped into the nonemptiness
  problem of deterministic finite automata, which can then
  synthesize strategies.
  In addition, we identify a maximal subset of Allen's relations that fits into
  such a deterministic fragment.

</details>


### [5] [E.A.R.T.H.: Structuring Creative Evolution through Model Error in Generative AI](https://arxiv.org/abs/2507.18004)
*Yusen Peng,Shuhua Mao*

Main category: cs.AI

TL;DR: 论文提出E.A.R.T.H.框架，通过错误生成、放大、精炼选择、转换和利用反馈五个阶段，将模型生成的错误转化为创意资产，显著提升创造力。


<details>
  <summary>Details</summary>
Motivation: 探索AI如何超越模仿实现真正的创造力，提出“创意潜力隐藏在失败中”的理念。

Method: 采用五阶段生成流程（E.A.R.T.H.），结合结构化提示、语义评分和人类反馈，使用多种模型（如LLaMA-2-7B-Chat、SBERT等）和复合奖励函数。

Result: 创造力评分提升70.4%，口号更短、更新颖，跨模态测试显示强对齐性，人类评价中60%输出得分≥4.0。

Conclusion: 错误驱动和反馈驱动的生成方法能有效增强AI创造力，为自进化、人类对齐的创意AI提供可行路径。

Abstract: How can AI move beyond imitation toward genuine creativity? This paper
proposes the E.A.R.T.H. framework, a five-stage generative pipeline that
transforms model-generated errors into creative assets through Error
generation, Amplification, Refine selection, Transform, and Harness feedback.
Drawing on cognitive science and generative modeling, we posit that "creative
potential hides in failure" and operationalize this via structured prompts,
semantic scoring, and human-in-the-loop evaluation. Implemented using
LLaMA-2-7B-Chat, SBERT, BERTScore, CLIP, BLIP-2, and Stable Diffusion, the
pipeline employs a composite reward function based on novelty, surprise, and
relevance. At the Refine stage, creativity scores increase by 52.5% (1.179 to
1.898, t = -5.56, p < 0.001), with final outputs reaching 2.010 - a 70.4%
improvement. Refined slogans are 48.4% shorter, 40.7% more novel, with only a
4.0% drop in relevance. Cross-modal tests show strong slogan-to-image alignment
(CLIPScore: 0.249; BERTScore F1: 0.816). In human evaluations, 60% of outputs
scored >= 4.0, with metaphorical slogans (avg. 4.09) outperforming literal ones
(3.99). Feedback highlights stylistic precision and emotional resonance. These
results demonstrate that error-centered, feedback-driven generation enhances
creativity, offering a scalable path toward self-evolving, human-aligned
creative AI.

</details>


### [6] [Does visualization help AI understand data?](https://arxiv.org/abs/2507.18022)
*Victoria R. Li,Johnathan Sun,Martin Wattenberg*

Main category: cs.AI

TL;DR: 研究发现，图表能提升AI系统（如GPT 4.1和Claude 3.5）在数据分析任务中的表现，尤其是数据复杂度增加时。


<details>
  <summary>Details</summary>
Motivation: 探讨图表是否对AI系统分析数据有帮助。

Method: 通过实验比较两种商业视觉语言模型在三种分析任务中的表现，对比了原始数据、散点图、空白图和错误图的效果。

Result: AI系统在数据伴随散点图时表现更精确和准确，图表内容对性能提升起关键作用。

Conclusion: 初步证明AI系统也能像人类一样从可视化中受益。

Abstract: Charts and graphs help people analyze data, but can they also be useful to AI
systems? To investigate this question, we perform a series of experiments with
two commercial vision-language models: GPT 4.1 and Claude 3.5. Across three
representative analysis tasks, the two systems describe synthetic datasets more
precisely and accurately when raw data is accompanied by a scatterplot,
especially as datasets grow in complexity. Comparison with two baselines --
providing a blank chart and a chart with mismatched data -- shows that the
improved performance is due to the content of the charts. Our results are
initial evidence that AI systems, like humans, can benefit from visualization.

</details>


### [7] [Multi-Agent Guided Policy Optimization](https://arxiv.org/abs/2507.18059)
*Yueheng Li,Guangming Xie,Zongqing Lu*

Main category: cs.AI

TL;DR: MAGPO是一种新的多智能体强化学习框架，通过结合集中式指导和分散式执行，优化了CTDE范式，提供了理论保证和实际性能提升。


<details>
  <summary>Details</summary>
Motivation: 现有CTDE方法未能充分利用集中式训练或缺乏理论保证，MAGPO旨在解决这些问题。

Method: MAGPO采用自回归联合策略进行可扩展的协调探索，并明确与分散策略对齐以确保可部署性。

Result: 在43个任务和6种环境中，MAGPO表现优于CTDE基线，甚至媲美完全集中式方法。

Conclusion: MAGPO为分散式多智能体学习提供了理论支持且实用的解决方案。

Abstract: Due to practical constraints such as partial observability and limited
communication, Centralized Training with Decentralized Execution (CTDE) has
become the dominant paradigm in cooperative Multi-Agent Reinforcement Learning
(MARL). However, existing CTDE methods often underutilize centralized training
or lack theoretical guarantees. We propose Multi-Agent Guided Policy
Optimization (MAGPO), a novel framework that better leverages centralized
training by integrating centralized guidance with decentralized execution.
MAGPO uses an auto-regressive joint policy for scalable, coordinated
exploration and explicitly aligns it with decentralized policies to ensure
deployability under partial observability. We provide theoretical guarantees of
monotonic policy improvement and empirically evaluate MAGPO on 43 tasks across
6 diverse environments. Results show that MAGPO consistently outperforms strong
CTDE baselines and matches or surpasses fully centralized approaches, offering
a principled and practical solution for decentralized multi-agent learning. Our
code and experimental data can be found in https://github.com/liyheng/MAGPO.

</details>


### [8] [AlphaGo Moment for Model Architecture Discovery](https://arxiv.org/abs/2507.18074)
*Yixiu Liu,Yang Nan,Weixian Xu,Xiangkun Hu,Lyumanshan Ye,Zhen Qin,Pengfei Liu*

Main category: cs.AI

TL;DR: ASI-Arch是一种自主超级智能系统，用于AI架构发现，突破了人类认知限制，实现了AI自主创新。


<details>
  <summary>Details</summary>
Motivation: AI研究受限于人类认知能力，发展速度缓慢，需要突破这一瓶颈。

Method: ASI-Arch通过自主假设、实现、训练和验证，进行端到端研究，发现新型架构。

Result: 系统进行了1773次实验，发现106种创新架构，性能超越人类设计。

Conclusion: ASI-Arch展示了AI自主研究的潜力，为自加速AI系统提供了蓝图。

Abstract: While AI systems demonstrate exponentially improving capabilities, the pace
of AI research itself remains linearly bounded by human cognitive capacity,
creating an increasingly severe development bottleneck. We present ASI-Arch,
the first demonstration of Artificial Superintelligence for AI research
(ASI4AI) in the critical domain of neural architecture discovery--a fully
autonomous system that shatters this fundamental constraint by enabling AI to
conduct its own architectural innovation. Moving beyond traditional Neural
Architecture Search (NAS), which is fundamentally limited to exploring
human-defined spaces, we introduce a paradigm shift from automated optimization
to automated innovation. ASI-Arch can conduct end-to-end scientific research in
the domain of architecture discovery, autonomously hypothesizing novel
architectural concepts, implementing them as executable code, training and
empirically validating their performance through rigorous experimentation and
past experience. ASI-Arch conducted 1,773 autonomous experiments over 20,000
GPU hours, culminating in the discovery of 106 innovative, state-of-the-art
(SOTA) linear attention architectures. Like AlphaGo's Move 37 that revealed
unexpected strategic insights invisible to human players, our AI-discovered
architectures demonstrate emergent design principles that systematically
surpass human-designed baselines and illuminate previously unknown pathways for
architectural innovation. Crucially, we establish the first empirical scaling
law for scientific discovery itself--demonstrating that architectural
breakthroughs can be scaled computationally, transforming research progress
from a human-limited to a computation-scalable process. We provide
comprehensive analysis of the emergent design patterns and autonomous research
capabilities that enabled these breakthroughs, establishing a blueprint for
self-accelerating AI systems.

</details>


### [9] [Agentic AI framework for End-to-End Medical Data Inference](https://arxiv.org/abs/2507.18115)
*Soorya Ram Shimgekar,Shayan Vassef,Abhay Goyal,Navin Kumar,Koustuv Saha*

Main category: cs.AI

TL;DR: 提出了一种自动化临床数据管道的Agentic AI框架，通过模块化代理处理数据预处理、特征提取、模型选择和推理，减少人工干预，提升医疗AI的可扩展性和成本效益。


<details>
  <summary>Details</summary>
Motivation: 医疗领域机器学习解决方案的构建和部署成本高且繁琐，主要由于数据预处理流程分散、模型兼容性问题以及严格的数据隐私限制。

Method: 采用模块化代理系统，包括数据摄入识别、匿名化、特征提取、模型匹配、预处理推荐与实施以及模型推理，支持结构化和非结构化数据。

Result: 在老年医学、姑息治疗和结肠镜成像等公开数据集上验证了框架的有效性，自动化流程显著减少了专家干预需求。

Conclusion: 该框架为临床环境中的AI应用提供了可扩展且经济高效的解决方案，自动化了机器学习生命周期的关键阶段。

Abstract: Building and deploying machine learning solutions in healthcare remains
expensive and labor-intensive due to fragmented preprocessing workflows, model
compatibility issues, and stringent data privacy constraints. In this work, we
introduce an Agentic AI framework that automates the entire clinical data
pipeline, from ingestion to inference, through a system of modular,
task-specific agents. These agents handle both structured and unstructured
data, enabling automatic feature selection, model selection, and preprocessing
recommendation without manual intervention. We evaluate the system on publicly
available datasets from geriatrics, palliative care, and colonoscopy imaging.
For example, in the case of structured data (anxiety data) and unstructured
data (colonoscopy polyps data), the pipeline begins with file-type detection by
the Ingestion Identifier Agent, followed by the Data Anonymizer Agent ensuring
privacy compliance, where we first identify the data type and then anonymize
it. The Feature Extraction Agent identifies features using an embedding-based
approach for tabular data, extracting all column names, and a multi-stage
MedGemma-based approach for image data, which infers modality and disease name.
These features guide the Model-Data Feature Matcher Agent in selecting the
best-fit model from a curated repository. The Preprocessing Recommender Agent
and Preprocessing Implementor Agent then apply tailored preprocessing based on
data type and model requirements. Finally, the ``Model Inference Agent" runs
the selected model on the uploaded data and generates interpretable outputs
using tools like SHAP, LIME, and DETR attention maps. By automating these
high-friction stages of the ML lifecycle, the proposed framework reduces the
need for repeated expert intervention, offering a scalable, cost-efficient
pathway for operationalizing AI in clinical environments.

</details>


### [10] [Actively evaluating and learning the distinctions that matter: Vaccine safety signal detection from emergency triage notes](https://arxiv.org/abs/2507.18123)
*Sedigh Khademi,Christopher Palmer,Muhammad Javed,Hazel Clothier,Jim Buttery,Gerardo Luis Dimaguila,Jim Black*

Main category: cs.AI

TL;DR: 该研究利用自然语言处理（NLP）和主动学习技术，开发了一种从急诊科分诊笔记中快速检测潜在疫苗安全问题的分类器，以弥补临床试验中安全数据收集的不足。


<details>
  <summary>Details</summary>
Motivation: 由于临床试验中安全数据收集时间有限，且疫苗广泛接种后需要快速监测潜在安全问题，因此需要开发高效的监测系统。

Method: 结合自然语言处理（NLP）、主动学习和数据增强技术，开发分类器，利用急诊科分诊笔记进行疫苗安全信号监测。

Result: 该方法提高了疫苗安全监测的准确性和效率，减少了基于关键词分类的假阳性问题。

Conclusion: 通过NLP和主动学习技术，能够更快速、准确地监测疫苗安全问题，为公共卫生提供支持。

Abstract: The rapid development of COVID-19 vaccines has showcased the global
communitys ability to combat infectious diseases. However, the need for
post-licensure surveillance systems has grown due to the limited window for
safety data collection in clinical trials and early widespread implementation.
This study aims to employ Natural Language Processing techniques and Active
Learning to rapidly develop a classifier that detects potential vaccine safety
issues from emergency department notes. ED triage notes, containing expert,
succinct vital patient information at the point of entry to health systems, can
significantly contribute to timely vaccine safety signal surveillance. While
keyword-based classification can be effective, it may yield false positives and
demand extensive keyword modifications. This is exacerbated by the infrequency
of vaccination-related ED presentations and their similarity to other reasons
for ED visits. NLP offers a more accurate and efficient alternative, albeit
requiring annotated data, which is often scarce in the medical field. Active
learning optimizes the annotation process and the quality of annotated data,
which can result in faster model implementation and improved model performance.
This work combines active learning, data augmentation, and active learning and
evaluation techniques to create a classifier that is used to enhance vaccine
safety surveillance from ED triage notes.

</details>


### [11] [Logical Characterizations of GNNs with Mean Aggregation](https://arxiv.org/abs/2507.18145)
*Moritz Schönherr,Carsten Lutz*

Main category: cs.AI

TL;DR: 研究了使用均值聚合函数的图神经网络（GNNs）的表达能力，发现其在非均匀设置下与比率模态逻辑等价，而在均匀设置下低于求和与最大值聚合的GNNs。


<details>
  <summary>Details</summary>
Motivation: 探索均值聚合GNNs的表达能力，并与求和及最大值聚合GNNs进行比较，以理解其在不同设置下的表现。

Method: 在非均匀和均匀设置下分析均值聚合GNNs的表达能力，并与模态逻辑和MSO（Monadic Second-Order Logic）进行对比。

Result: 非均匀设置下，均值聚合GNNs与比率模态逻辑等价；均匀设置下，其表达能力低于求和和最大值聚合GNNs。

Conclusion: 均值聚合GNNs的表达能力受设置和假设影响，均匀设置下较弱，但放宽假设可提升表达能力。

Abstract: We study the expressive power of graph neural networks (GNNs) with mean as
the aggregation function. In the non-uniform setting, we show that such GNNs
have exactly the same expressive power as ratio modal logic, which has modal
operators expressing that at least a certain ratio of the successors of a
vertex satisfies a specified property. The non-uniform expressive power of mean
GNNs is thus higher than that of GNNs with max aggregation, but lower than for
sum aggregation--the latter are characterized by modal logic and graded modal
logic, respectively. In the uniform setting, we show that the expressive power
relative to MSO is exactly that of alternation-free modal logic, under the
natural assumptions that combination functions are continuous and
classification functions are thresholds. This implies that, relative to MSO and
in the uniform setting, mean GNNs are strictly less expressive than sum GNNs
and max GNNs. When any of the assumptions is dropped, the expressive power
increases.

</details>


### [12] [Decoupling Knowledge and Reasoning in LLMs: An Exploration Using Cognitive Dual-System Theory](https://arxiv.org/abs/2507.18178)
*Mutian Yang,Jiandong Gao,Ji Wu*

Main category: cs.AI

TL;DR: 该论文提出了一种认知归因框架，用于解耦大型语言模型（LLMs）中知识和推理的贡献，通过快速和慢速思维模式分析其表现。


<details>
  <summary>Details</summary>
Motivation: 受双系统认知理论启发，研究旨在区分LLMs中知识和推理的作用，以提升模型分析、可解释性和开发。

Method: 将LLMs的认知分解为知识检索（Phase 1）和推理调整（Phase 2），通过快速和慢速思维模式生成答案，分析不同模式下的表现。

Result: 结果显示推理调整具有领域特异性，参数扩展显著提升知识和推理能力，且知识和推理分别集中在网络的不同层次。

Conclusion: 该框架为理解LLMs提供了新视角，并对现有研究如扩展法则、层次知识编辑和小模型推理限制提供了新见解。

Abstract: While large language models (LLMs) leverage both knowledge and reasoning
during inference, the capacity to distinguish between them plays a pivotal role
in model analysis, interpretability, and development. Inspired by dual-system
cognitive theory, we propose a cognition attribution framework to decouple the
contribution of knowledge and reasoning. In particular, the cognition of LLMs
is decomposed into two distinct yet complementary phases: knowledge retrieval
(Phase 1) and reasoning adjustment (Phase 2). To separate these phases, LLMs
are prompted to generate answers under two different cognitive modes, fast
thinking and slow thinking, respectively. The performance under different
cognitive modes is analyzed to quantify the contribution of knowledge and
reasoning. This architecture is employed to 15 LLMs across 3 datasets. Results
reveal: (1) reasoning adjustment is domain-specific, benefiting
reasoning-intensive domains (e.g., mathematics, physics, and chemistry) and
potentially imparing knowledge-intensive domains. (2) Parameter scaling
improves both knowledge and reasoning, with knowledge improvements being more
pronounced. Additionally, parameter scaling make LLMs reasoning significantly
more prudent, while moderately more intelligent. (3) Knowledge primarily
resides in lower network layers, while reasoning operates in higher layers. Our
framework not only helps understand LLMs from a "decoupling" perspective, but
also provides new insights into existing research, including scaling laws,
hierarchical knowledge editing, and limitations of small-model reasoning.

</details>


### [13] [Comparing Non-minimal Semantics for Disjunction in Answer Set Programming](https://arxiv.org/abs/2507.18198)
*Felicidad Aguado,Pedro Cabalar,Brais Muñiz,Gilberto Pérez,Concepción Vidal*

Main category: cs.AI

TL;DR: 本文比较了四种不遵循模型最小化原则的析取语义，证明其中三种方法（Forks、Justified Models和DI语义的合理放宽）实际上等价，且比第四种方法（Strongly Supported Models）更强。


<details>
  <summary>Details</summary>
Motivation: 研究不依赖模型最小化原则的析取语义，探索其等价性和强度关系。

Method: 比较四种析取语义：Justified Models、Strongly Supported Models、Forks和DI语义，分析其定义和性质。

Result: 证明Forks、Justified Models和放宽的DI语义等价，且比Strongly Supported Models更强。

Conclusion: 三种方法等价且提供比经典逻辑更强的析取语义，为ASP提供新的理论基础。

Abstract: In this paper, we compare four different semantics for disjunction in Answer
Set Programming that, unlike stable models, do not adhere to the principle of
model minimality. Two of these approaches, Cabalar and Mu\~niz' \emph{Justified
Models} and Doherty and Szalas' \emph{Strongly Supported Models}, directly
provide an alternative non-minimal semantics for disjunction. The other two,
Aguado et al's \emph{Forks} and Shen and Eiter's \emph{Determining Inference}
(DI) semantics, actually introduce a new disjunction connective, but are
compared here as if they constituted new semantics for the standard disjunction
operator. We are able to prove that three of these approaches (Forks, Justified
Models and a reasonable relaxation of the DI semantics) actually coincide,
constituting a common single approach under different definitions. Moreover,
this common semantics always provides a superset of the stable models of a
program (in fact, modulo any context) and is strictly stronger than the fourth
approach (Strongly Supported Models), that actually treats disjunctions as in
classical logic.

</details>


### [14] [Foundations for Risk Assessment of AI in Protecting Fundamental Rights](https://arxiv.org/abs/2507.18290)
*Antonino Rotolo,Beatrice Ferrigno,Jose Miguel Angel Garcia Godinez,Claudio Novelli,Giovanni Sartor*

Main category: cs.AI

TL;DR: 本文提出了一种定性评估AI风险的框架，结合定义性平衡和可废止推理，以应对欧盟AI法案中的法律合规和基本权利保护问题。


<details>
  <summary>Details</summary>
Motivation: 解决AI部署中法律合规与基本权利保护的复杂性，为AI风险评估提供理论基础。

Method: 采用定义性平衡（比例分析）和可废止推理，分析AI部署场景及其对基本权利的多层影响。

Result: 提出了一个逻辑化的AI风险分析框架，适用于高风险AI和通用AI系统，并强调后者的广泛适用性。

Conclusion: 未来将开发形式化模型和算法，以增强AI风险评估，支持负责任的AI治理。

Abstract: This chapter introduces a conceptual framework for qualitative risk
assessment of AI, particularly in the context of the EU AI Act. The framework
addresses the complexities of legal compliance and fundamental rights
protection by itegrating definitional balancing and defeasible reasoning.
Definitional balancing employs proportionality analysis to resolve conflicts
between competing rights, while defeasible reasoning accommodates the dynamic
nature of legal decision-making. Our approach stresses the need for an analysis
of AI deployment scenarios and for identifying potential legal violations and
multi-layered impacts on fundamental rights. On the basis of this analysis, we
provide philosophical foundations for a logical account of AI risk analysis. In
particular, we consider the basic building blocks for conceptually grasping the
interaction between AI deployment scenarios and fundamental rights,
incorporating in defeasible reasoning definitional balancing and arguments
about the contextual promotion or demotion of rights. This layered approach
allows for more operative models of assessment of both high-risk AI systems and
General Purpose AI (GPAI) systems, emphasizing the broader applicability of the
latter. Future work aims to develop a formal model and effective algorithms to
enhance AI risk assessment, bridging theoretical insights with practical
applications to support responsible AI governance.

</details>


### [15] [The AlphaPhysics Term Rewriting System for Marking Algebraic Expressions in Physics Exams](https://arxiv.org/abs/2507.18337)
*Peter Baumgartner,Lachlan McGinness*

Main category: cs.AI

TL;DR: 提出了一种自动批改物理考试的方法，结合计算机代数系统、SMT求解器和项重写系统，利用大语言模型处理学生答案，并通过自动推理技术评估正确性。


<details>
  <summary>Details</summary>
Motivation: 解决自动评估学生物理考试答案正确性的挑战性问题。

Method: 结合计算机代数系统、SMT求解器和项重写系统，利用大语言模型处理学生答案，并通过自动推理技术评估正确性。

Result: 在2023年澳大利亚物理奥林匹克竞赛的1500多份真实学生答案上进行了评估。

Conclusion: 该方法在自动批改物理考试中表现出潜力，特别是在处理复杂三角函数表达式时。

Abstract: We present our method for automatically marking Physics exams. The marking
problem consists in assessing typed student answers for correctness with
respect to a ground truth solution. This is a challenging problem that we seek
to tackle using a combination of a computer algebra system, an SMT solver and a
term rewriting system. A Large Language Model is used to interpret and remove
errors from student responses and rewrite these in a machine readable format.
Once formalized and language-aligned, the next step then consists in applying
automated reasoning techniques for assessing student solution correctness. We
consider two methods of automated theorem proving: off-the-shelf SMT solving
and term rewriting systems tailored for physics problems involving
trigonometric expressions. The development of the term rewrite system and
establishing termination and confluence properties was not trivial, and we
describe it in some detail in the paper. We evaluate our system on a rich pool
of over 1500 real-world student exam responses from the 2023 Australian Physics
Olympiad.

</details>


### [16] [Reasoning Beyond the Obvious: Evaluating Divergent and Convergent Thinking in LLMs for Financial Scenarios](https://arxiv.org/abs/2507.18368)
*Zhuang Qiang Bok,Watson Wei Khong Chua*

Main category: cs.AI

TL;DR: ConDiFi是一个评估LLMs在金融任务中发散和收敛思维的基准，包含607个宏观金融提示和990个多跳对抗性选择题。测试14个模型后发现，GPT-4o在创新性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+表现优异。


<details>
  <summary>Details</summary>
Motivation: 金融领域需要LLMs不仅能做出最优决策，还需在不确定性下生成创造性的未来情景，现有基准未能充分评估这些能力。

Method: ConDiFi包含发散性思维的宏观金融提示和收敛性思维的多跳对抗性选择题，用于评估14个领先LLMs。

Result: GPT-4o在创新性和可操作性上表现不佳，而DeepSeek-R1和Cohere Command R+在生成可操作见解方面表现突出。

Conclusion: ConDiFi为评估LLMs在金融领域的安全和战略部署提供了新视角。

Abstract: Most reasoning benchmarks for LLMs emphasize factual accuracy or step-by-step
logic. In finance, however, professionals must not only converge on optimal
decisions but also generate creative, plausible futures under uncertainty. We
introduce ConDiFi, a benchmark that jointly evaluates divergent and convergent
thinking in LLMs for financial tasks.
  ConDiFi features 607 macro-financial prompts for divergent reasoning and 990
multi-hop adversarial MCQs for convergent reasoning. Using this benchmark, we
evaluated 14 leading models and uncovered striking differences. Despite high
fluency, GPT-4o underperforms on Novelty and Actionability. In contrast, models
like DeepSeek-R1 and Cohere Command R+ rank among the top for generating
actionable, insights suitable for investment decisions. ConDiFi provides a new
perspective to assess reasoning capabilities essential to safe and strategic
deployment of LLMs in finance.

</details>


### [17] [Revisiting LLM Reasoning via Information Bottleneck](https://arxiv.org/abs/2507.18391)
*Shiye Lei,Zhihao Cheng,Kai Jia,Dacheng Tao*

Main category: cs.AI

TL;DR: 论文提出了一种基于信息瓶颈（IB）原则的理论框架IBRO，优化大语言模型（LLM）的推理能力，并通过轻量级IB正则化方法提升性能。


<details>
  <summary>Details</summary>
Motivation: 现有基于强化学习的LLM推理方法缺乏理论支持，限制了方法论的发展。

Method: 提出IB-aware reasoning optimization（IBRO）框架，推导出token级代理目标，并设计轻量级IB正则化方法。

Result: 在多个数学推理基准测试和RL算法中验证了IB正则化的有效性，推理性能一致提升。

Conclusion: IBRO框架为LLM推理提供了理论支持，轻量级IB正则化方法简单高效，可无缝集成现有RL训练框架。

Abstract: Large language models (LLMs) have recently demonstrated remarkable progress
in reasoning capabilities through reinforcement learning with verifiable
rewards (RLVR). By leveraging simple rule-based rewards, RL effectively
incentivizes LLMs to produce extended chain-of-thought (CoT) reasoning
trajectories, progressively guiding them toward correct answers. However,
existing approaches remain largely heuristic and intuition-driven, limiting the
development of principled methodologies. In this paper, we present a
theoretical characterization of LLM reasoning grounded in information
bottleneck (IB) principle, introducing IB-aware reasoning optimization (IBRO),
a framework that encourages reasoning trajectories to be both informative about
the final correct answer and generalizable across diverse prompts. We derive a
practical token-level surrogate objective and propose an efficient
approximation, resulting in the lightweight IB regularization method. This
technique integrates seamlessly into existing RL-based post-training frameworks
without additional computational overhead, requiring only a one-line code
modification. Empirically, we validate IB regularization across multiple
mathematical reasoning benchmarks and RL algorithms, demonstrating consistent
improvements in LLM reasoning performance.

</details>


### [18] [Optimising Call Centre Operations using Reinforcement Learning: Value Iteration versus Proximal Policy Optimisation](https://arxiv.org/abs/2507.18398)
*Kwong Ho Li,Wathsala Karunarathne*

Main category: cs.AI

TL;DR: 论文比较了基于模型的VI方法和无模型的PPO方法在呼叫中心路由优化中的应用，PPO表现最佳。


<details>
  <summary>Details</summary>
Motivation: 优化呼叫中心的路由策略，以减少客户等待时间和员工空闲时间。

Method: 比较了基于模型的Value Iteration（VI）和无模型的Proximal Policy Optimisation（PPO），使用MDP框架和SBR模型。

Result: PPO在1000次测试中表现最优，客户等待时间和员工空闲时间最低，但训练时间较长。

Conclusion: PPO在呼叫中心路由优化中效果显著，尽管训练时间较长。

Abstract: This paper investigates the application of Reinforcement Learning (RL) to
optimise call routing in call centres to minimise client waiting time and staff
idle time. Two methods are compared: a model-based approach using Value
Iteration (VI) under known system dynamics, and a model-free approach using
Proximal Policy Optimisation (PPO) that learns from experience. For the
model-based approach, a theoretical model is used, while a simulation model
combining Discrete Event Simulation (DES) with the OpenAI Gym environment is
developed for model-free learning. Both models frame the problem as a Markov
Decision Process (MDP) within a Skills-Based Routing (SBR) framework, with
Poisson client arrivals and exponentially distributed service and abandonment
times. For policy evaluation, random, VI, and PPO policies are evaluated using
the simulation model. After 1,000 test episodes, PPO consistently achives the
highest rewards, along with the lowest client waiting time and staff idle time,
despite requiring longer training time.

</details>


### [19] [GPU Accelerated Compact-Table Propagation](https://arxiv.org/abs/2507.18413)
*Enrico Santi,Fabio Tardivo,Agostino Dovier,Andrea Formisano*

Main category: cs.AI

TL;DR: 论文探讨了如何利用现代GPU的强大计算能力优化Compact-Table（CT）算法，以处理大规模表约束问题。


<details>
  <summary>Details</summary>
Motivation: 传统CPU在处理包含大量有效案例的表约束问题时效率不足，需要借助GPU的并行计算能力提升性能。

Method: 设计并实现了GPU加速的CT算法，并将其集成到现有约束求解器中。

Result: 通过实验验证，GPU加速的CT算法在大量实例中表现显著优于传统CPU方法。

Conclusion: GPU加速的CT算法为处理大规模表约束问题提供了高效解决方案。

Abstract: Constraint Programming developed within Logic Programming in the Eighties;
nowadays all Prolog systems encompass modules capable of handling constraint
programming on finite domains demanding their solution to a constraint solver.
This work focuses on a specific form of constraint, the so-called table
constraint, used to specify conditions on the values of variables as an
enumeration of alternative options. Since every condition on a set of finite
domain variables can be ultimately expressed as a finite set of cases, Table
can, in principle, simulate any other constraint. These characteristics make
Table one of the most studied constraints ever, leading to a series of
increasingly efficient propagation algorithms. Despite this, it is not uncommon
to encounter real-world problems with hundreds or thousands of valid cases that
are simply too many to be handled effectively with standard CPU-based
approaches. In this paper, we deal with the Compact-Table (CT) algorithm, the
state-of-the-art propagation algorithms for Table. We describe how CT can be
enhanced by exploiting the massive computational power offered by modern GPUs
to handle large Table constraints. In particular, we report on the design and
implementation of GPU-accelerated CT, on its integration into an existing
constraint solver, and on an experimental validation performed on a significant
set of instances.

</details>


### [20] [On the Performance of Concept Probing: The Influence of the Data (Extended Version)](https://arxiv.org/abs/2507.18550)
*Manuel de Sousa Ribeiro,Afonso Leote,João Leite*

Main category: cs.AI

TL;DR: 本文探讨了概念探测在图像分类任务中训练数据对探测模型性能的影响，并提供了两个常用数据集的概念标签。


<details>
  <summary>Details</summary>
Motivation: 概念探测用于解释人工神经网络，但现有研究忽视了训练探测模型所需数据的影响。

Method: 研究聚焦图像分类任务，分析训练数据对探测模型性能的作用。

Result: 揭示了训练数据对概念探测模型性能的重要性。

Conclusion: 强调了训练数据在概念探测中的关键作用，并公开了两个数据集的概念标签。

Abstract: Concept probing has recently garnered increasing interest as a way to help
interpret artificial neural networks, dealing both with their typically large
size and their subsymbolic nature, which ultimately renders them unfeasible for
direct human interpretation. Concept probing works by training additional
classifiers to map the internal representations of a model into human-defined
concepts of interest, thus allowing humans to peek inside artificial neural
networks. Research on concept probing has mainly focused on the model being
probed or the probing model itself, paying limited attention to the data
required to train such probing models. In this paper, we address this gap.
Focusing on concept probing in the context of image classification tasks, we
investigate the effect of the data used to train probing models on their
performance. We also make available concept labels for two widely used
datasets.

</details>


### [21] [SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law](https://arxiv.org/abs/2507.18576)
*Shanghai AI Lab,:,Yicheng Bao,Guanxu Chen,Mingkang Chen,Yunhao Chen,Chiyu Chen,Lingjie Chen,Sirui Chen,Xinquan Chen,Jie Cheng,Yu Cheng,Dengke Deng,Yizhuo Ding,Dan Ding,Xiaoshan Ding,Yi Ding,Zhichen Dong,Lingxiao Du,Yuyu Fan,Xinshun Feng,Yanwei Fu,Yuxuan Gao,Ruijun Ge,Tianle Gu,Lujun Gui,Jiaxuan Guo,Qianxi He,Yuenan Hou,Xuhao Hu,Hong Huang,Kaichen Huang,Shiyang Huang,Yuxian Jiang,Shanzhe Lei,Jie Li,Lijun Li,Hao Li,Juncheng Li,Xiangtian Li,Yafu Li,Lingyu Li,Xueyan Li,Haotian Liang,Dongrui Liu,Qihua Liu,Zhixuan Liu,Bangwei Liu,Huacan Liu,Yuexiao Liu,Zongkai Liu,Chaochao Lu,Yudong Lu,Xiaoya Lu,Zhenghao Lu,Qitan Lv,Caoyuan Ma,Jiachen Ma,Xiaoya Ma,Zhongtian Ma,Lingyu Meng,Ziqi Miao,Yazhe Niu,Yuezhang Peng,Yuan Pu,Han Qi,Chen Qian,Xingge Qiao,Jingjing Qu,Jiashu Qu,Wanying Qu,Wenwen Qu,Xiaoye Qu,Qihan Ren,Qingnan Ren,Qingyu Ren,Jing Shao,Wenqi Shao,Shuai Shao,Dongxing Shi,Xin Song,Xinhao Song,Yan Teng,Xuan Tong,Yingchun Wang,Xuhong Wang,Shujie Wang,Xin Wang,Yige Wang,Yixu Wang,Yuanfu Wang,Futing Wang,Ruofan Wang,Wenjie Wang,Yajie Wang,Muhao Wei,Xiaoyu Wen,Fenghua Weng,Yuqi Wu,Yingtong Xiong,Xingcheng Xu,Chao Yang,Yue Yang,Yang Yao,Yulei Ye,Zhenyun Yin,Yi Yu,Bo Zhang,Qiaosheng Zhang,Jinxuan Zhang,Yexin Zhang,Yinqiang Zheng,Hefeng Zhou,Zhanhui Zhou,Pengyu Zhu,Qingzi Zhu,Yubo Zhu,Bowen Zhou*

Main category: cs.AI

TL;DR: SafeWork-R1是一种多模态推理模型，通过SafeLadder框架实现能力与安全性的协同进化，显著提升了安全性表现，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 开发一种能够在提升能力的同时确保安全性的AI模型，解决传统对齐方法（如RLHF）仅学习人类偏好的局限性。

Method: 采用SafeLadder框架，结合大规模、渐进式、安全导向的强化学习后训练，并辅以多原则验证器，实现内在安全推理和自我反思能力。

Result: SafeWork-R1在安全性基准测试中平均提升46.54%，超越GPT-4.1和Claude Opus 4等领先专有模型。

Conclusion: SafeWork-R1证明了安全性与能力可以协同进化，SafeLadder框架具有通用性，适用于构建稳健、可靠且可信的通用AI。

Abstract: We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that
demonstrates the coevolution of capabilities and safety. It is developed by our
proposed SafeLadder framework, which incorporates large-scale, progressive,
safety-oriented reinforcement learning post-training, supported by a suite of
multi-principled verifiers. Unlike previous alignment methods such as RLHF that
simply learn human preferences, SafeLadder enables SafeWork-R1 to develop
intrinsic safety reasoning and self-reflection abilities, giving rise to safety
`aha' moments. Notably, SafeWork-R1 achieves an average improvement of
$46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks
without compromising general capabilities, and delivers state-of-the-art safety
performance compared to leading proprietary models such as GPT-4.1 and Claude
Opus 4. To further bolster its reliability, we implement two distinct
inference-time intervention methods and a deliberative search mechanism,
enforcing step-level verification. Finally, we further develop
SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and
SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and
capability can co-evolve synergistically, highlighting the generalizability of
our framework in building robust, reliable, and trustworthy general-purpose AI.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [Shop-R1: Rewarding LLMs to Simulate Human Behavior in Online Shopping via Reinforcement Learning](https://arxiv.org/abs/2507.17842)
*Yimeng Zhang,Tian Wang,Jiri Gesi,Ziyi Wang,Yuxuan Lu,Jiacheng Lin,Sinong Zhan,Vianne Gao,Ruochen Jiao,Junze Liu,Kun Qian,Yuxin Tang,Ran Xue,Houyu Zhang,Qingjun Cui,Yufan Guo,Dakuo Wang*

Main category: cs.CL

TL;DR: Shop-R1是一个新颖的强化学习框架，通过分解人类行为模拟任务为两个阶段（理由生成和动作预测），并结合自监督和分层奖励机制，显著提升了LLM在在线购物环境中的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖于LLM生成的理性数据，但其性能受限于模型的推理能力。Shop-R1旨在通过强化学习框架突破这一限制，更真实地模拟人类行为。

Method: Shop-R1将任务分解为理由生成和动作预测两阶段，分别采用自监督信号和分层奖励结构（包括难度感知缩放）来优化模型。

Result: 实验结果显示，该方法相比基线实现了超过65%的相对改进。

Conclusion: Shop-R1通过创新的强化学习设计，显著提升了LLM在复杂环境中的推理和行为模拟能力。

Abstract: Large Language Models (LLMs) have recently demonstrated strong potential in
generating 'believable human-like' behavior in web environments. Prior work has
explored augmenting training data with LLM-synthesized rationales and applying
supervised fine-tuning (SFT) to enhance reasoning ability, which in turn can
improve downstream action prediction. However, the performance of such
approaches remains inherently bounded by the reasoning capabilities of the
model used to generate the rationales. In this paper, we introduce Shop-R1, a
novel reinforcement learning (RL) framework aimed at enhancing the reasoning
ability of LLMs for simulation of real human behavior in online shopping
environments Specifically, Shop-R1 decomposes the human behavior simulation
task into two stages: rationale generation and action prediction, each guided
by distinct reward signals. For rationale generation, we leverage internal
model signals (e.g., logit distributions) to guide the reasoning process in a
self-supervised manner. For action prediction, we propose a hierarchical reward
structure with difficulty-aware scaling to prevent reward hacking and enable
fine-grained reward assignment. This design evaluates both high-level action
types and the correctness of fine-grained sub-action details (attributes and
values), rewarding outputs proportionally to their difficulty. Experimental
results show that our method achieves a relative improvement of over 65%
compared to the baseline.

</details>


### [23] [Dynamic and Generalizable Process Reward Modeling](https://arxiv.org/abs/2507.17849)
*Zhangyue Yin,Qiushi Sun,Zhiyuan Zeng,Qinyuan Cheng,Xipeng Qiu,Xuanjing Huang*

Main category: cs.CL

TL;DR: DG-PRM提出了一种动态且通用的过程奖励建模方法，通过奖励树和多维奖励标准提升LLM在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有PRM依赖启发式方法，跨领域泛化能力不足，且静态评估标准难以适应复杂过程监督。

Method: DG-PRM采用奖励树存储细粒度奖励标准，动态选择奖励信号，并首次使用Pareto优势估计处理多维奖励。

Result: 实验显示DG-PRM在基准测试中表现优异，显著提升模型性能，并展现出良好的泛化能力。

Conclusion: DG-PRM为复杂任务提供了高效的过程奖励建模方案，具有广泛适用性。

Abstract: Process Reward Models (PRMs) are crucial for guiding Large Language Models
(LLMs) in complex scenarios by providing dense reward signals. However,
existing PRMs primarily rely on heuristic approaches, which struggle with
cross-domain generalization. While LLM-as-judge has been proposed to provide
generalized rewards, current research has focused mainly on feedback results,
overlooking the meaningful guidance embedded within the text. Additionally,
static and coarse-grained evaluation criteria struggle to adapt to complex
process supervision. To tackle these challenges, we propose Dynamic and
Generalizable Process Reward Modeling (DG-PRM), which features a reward tree to
capture and store fine-grained, multi-dimensional reward criteria. DG-PRM
dynamically selects reward signals for step-wise reward scoring. To handle
multifaceted reward signals, we pioneeringly adopt Pareto dominance estimation
to identify discriminative positive and negative pairs. Experimental results
show that DG-PRM achieves stunning performance on prevailing benchmarks,
significantly boosting model performance across tasks with dense rewards.
Further analysis reveals that DG-PRM adapts well to out-of-distribution
scenarios, demonstrating exceptional generalizability.

</details>


### [24] [VeriMinder: Mitigating Analytical Vulnerabilities in NL2SQL](https://arxiv.org/abs/2507.17896)
*Shubham Mohole,Sainyam Galhotra*

Main category: cs.CL

TL;DR: VeriMinder是一个交互式系统，用于检测和缓解自然语言数据库接口中的认知偏差，通过语义映射、系统化分析框架和优化的LLM提示生成，显著提升分析质量。


<details>
  <summary>Details</summary>
Motivation: 自然语言数据库接口的普及带来了认知偏差问题，现有研究多关注文本到SQL的准确性，而忽略了分析问题中的偏差。

Method: 引入上下文语义映射框架、基于Hard-to-Vary原则的分析框架，以及优化的LLM提示生成系统。

Result: 82.5%的用户认为系统提升了分析质量；在比较评估中，VeriMinder在具体性、全面性和准确性上至少优于其他方法20%。

Conclusion: VeriMinder有效帮助用户避免分析中的“错误问题”漏洞，其开源代码和提示库可供进一步研究和社区采用。

Abstract: Application systems using natural language interfaces to databases (NLIDBs)
have democratized data analysis. This positive development has also brought
forth an urgent challenge to help users who might use these systems without a
background in statistical analysis to formulate bias-free analytical questions.
Although significant research has focused on text-to-SQL generation accuracy,
addressing cognitive biases in analytical questions remains underexplored. We
present VeriMinder, https://veriminder.ai, an interactive system for detecting
and mitigating such analytical vulnerabilities. Our approach introduces three
key innovations: (1) a contextual semantic mapping framework for biases
relevant to specific analysis contexts (2) an analytical framework that
operationalizes the Hard-to-Vary principle and guides users in systematic data
analysis (3) an optimized LLM-powered system that generates high-quality,
task-specific prompts using a structured process involving multiple candidates,
critic feedback, and self-reflection.
  User testing confirms the merits of our approach. In direct user experience
evaluation, 82.5% participants reported positively impacting the quality of the
analysis. In comparative evaluation, VeriMinder scored significantly higher
than alternative approaches, at least 20% better when considered for metrics of
the analysis's concreteness, comprehensiveness, and accuracy. Our system,
implemented as a web application, is set to help users avoid "wrong question"
vulnerability during data analysis. VeriMinder code base with prompts,
https://reproducibility.link/veriminder, is available as an MIT-licensed
open-source software to facilitate further research and adoption within the
community.

</details>


### [25] [One Whisper to Grade Them All](https://arxiv.org/abs/2507.17918)
*Nhan Phan,Anusha Porwal,Yaroslav Getman,Ekaterina Voskoboinik,Tamás Grósz,Mikko Kurimo*

Main category: cs.CL

TL;DR: 提出了一种高效的端到端自动口语评估方法，通过单一Whisper-small编码器处理多部分测试，减少推理时间并提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决大规模计算机辅助语言学习系统中自动口语评估的实用性问题，减少对转录和分部分模型的依赖。

Method: 使用Whisper-small编码器处理所有口语响应，通过轻量级聚合器整合信息并预测最终分数，提出数据采样策略。

Result: 系统RMSE为0.384，优于文本基线（0.44），参数仅168M；数据采样策略下仅需44.8%的说话人数据即可达到0.383 RMSE。

Conclusion: 该方法高效、实用，适用于大规模语言学习系统，且在数据效率和性能上表现优异。

Abstract: We present an efficient end-to-end approach for holistic Automatic Speaking
Assessment (ASA) of multi-part second-language tests, developed for the 2025
Speak & Improve Challenge. Our system's main novelty is the ability to process
all four spoken responses with a single Whisper-small encoder, combine all
information via a lightweight aggregator, and predict the final score. This
architecture removes the need for transcription and per-part models, cuts
inference time, and makes ASA practical for large-scale Computer-Assisted
Language Learning systems.
  Our system achieved a Root Mean Squared Error (RMSE) of 0.384, outperforming
the text-based baseline (0.44) while using at most 168M parameters (about 70%
of Whisper-small). Furthermore, we propose a data sampling strategy, allowing
the model to train on only 44.8% of the speakers in the corpus and still reach
0.383 RMSE, demonstrating improved performance on imbalanced classes and strong
data efficiency.

</details>


### [26] [Evaluating the Performance of AI Text Detectors, Few-Shot and Chain-of-Thought Prompting Using DeepSeek Generated Text](https://arxiv.org/abs/2507.17944)
*Hulayyil Alshammari,Praveen Rao*

Main category: cs.CL

TL;DR: 研究探讨了六种AI检测工具对DeepSeek生成文本的识别能力，发现对抗攻击（如改写和人化）显著降低检测准确率，而Few-shot和CoT提示方法表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注ChatGPT等知名LLM，缺乏对DeepSeek的检测能力评估，因此填补这一空白。

Method: 使用六种检测工具测试DeepSeek生成文本，并应用改写和人化等对抗攻击，评估检测准确率。

Result: QuillBot和Copyleaks对原始和改写文本表现优异，但人化攻击显著降低准确率；Few-shot和CoT提示方法准确率最高。

Conclusion: 对抗攻击显著影响检测工具性能，Few-shot和CoT提示是有效的替代检测方法。

Abstract: Large language models (LLMs) have rapidly transformed the creation of written
materials. LLMs have led to questions about writing integrity, thereby driving
the creation of artificial intelligence (AI) detection technologies.
Adversarial attacks, such as standard and humanized paraphrasing, inhibit
detectors' ability to detect machine-generated text. Previous studies have
mainly focused on ChatGPT and other well-known LLMs and have shown varying
accuracy across detectors. However, there is a clear gap in the literature
about DeepSeek, a recently published LLM. Therefore, in this work, we
investigate whether six generally accessible AI detection tools -- AI Text
Classifier, Content Detector AI, Copyleaks, QuillBot, GPT-2, and GPTZero -- can
consistently recognize text generated by DeepSeek. The detectors were exposed
to the aforementioned adversarial attacks. We also considered DeepSeek as a
detector by performing few-shot prompting and chain-of-thought reasoning (CoT)
for classifying AI and human-written text. We collected 49 human-authored
question-answer pairs from before the LLM era and generated matching responses
using DeepSeek-v3, producing 49 AI-generated samples. Then, we applied
adversarial techniques such as paraphrasing and humanizing to add 196 more
samples. These were used to challenge detector robustness and assess accuracy
impact. While QuillBot and Copyleaks showed near-perfect performance on
original and paraphrased DeepSeek text, others -- particularly AI Text
Classifier and GPT-2 -- showed inconsistent results. The most effective attack
was humanization, reducing accuracy to 71% for Copyleaks, 58% for QuillBot, and
52% for GPTZero. Few-shot and CoT prompting showed high accuracy, with the best
five-shot result misclassifying only one of 49 samples (AI recall 96%, human
recall 100%).

</details>


### [27] [Are LLM Belief Updates Consistent with Bayes' Theorem?](https://arxiv.org/abs/2507.17951)
*Sohaib Imran,Ihor Kendiukhov,Matthew Broerman,Aditya Thomas,Riccardo Campanella,Rob Lamb,Peter M. Atkinson*

Main category: cs.CL

TL;DR: 研究探讨更大、更强大的语言模型是否能在上下文中根据证据更一致地更新其“信念”以符合贝叶斯定理。通过提出贝叶斯一致性系数（BCC）并生成数据集进行测量，发现更大、更先进的预训练语言模型在贝叶斯一致性上表现更好。


<details>
  <summary>Details</summary>
Motivation: 探究语言模型是否能在上下文中更一致地应用贝叶斯定理更新信念，以理解其推理能力。

Method: 提出贝叶斯一致性系数（BCC）作为度量标准，生成数据集，并测量多个预训练语言模型的BCC，比较模型参数、训练数据和基准测试分数。

Result: 更大、更先进的预训练语言模型在贝叶斯一致性上表现更好，支持研究假设。

Conclusion: 研究结果对理解和管理大型语言模型（LLMs）具有重要意义。

Abstract: Do larger and more capable language models learn to update their "beliefs"
about propositions more consistently with Bayes' theorem when presented with
evidence in-context? To test this, we formulate a Bayesian Coherence
Coefficient (BCC) metric and generate a dataset with which to measure the BCC.
We measure BCC for multiple pre-trained-only language models across five model
families, comparing against the number of model parameters, the amount of
training data, and model scores on common benchmarks. Our results provide
evidence for our hypothesis that larger and more capable pre-trained language
models assign credences that are more coherent with Bayes' theorem. These
results have important implications for our understanding and governance of
LLMs.

</details>


### [28] [Natural Language Processing for Tigrinya: Current State and Future Directions](https://arxiv.org/abs/2507.17974)
*Fitsum Gaim,Jong C. Park*

Main category: cs.CL

TL;DR: 本文综述了提格里尼亚语（Tigrinya）在自然语言处理（NLP）领域的研究现状，分析了40多项研究，总结了从基于规则的系统到现代神经架构的演变，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 提格里尼亚语虽然使用者众多，但在NLP研究中严重不足，本文旨在填补这一空白。

Method: 通过系统分析2011至2025年的40多项研究，评估了计算资源、模型和应用在10个下游任务中的表现。

Result: 研究发现，资源创建的里程碑推动了从规则系统到神经架构的进步，但提格里尼亚语的形态复杂性和资源稀缺仍是主要挑战。

Conclusion: 本文为提格里尼亚语NLP研究提供了全面参考和未来路线图，包括形态感知建模和跨语言迁移等方向。

Abstract: Despite being spoken by millions of people, Tigrinya remains severely
underrepresented in Natural Language Processing (NLP) research. This work
presents a comprehensive survey of NLP research for Tigrinya, analyzing over 40
studies spanning more than a decade of work from 2011 to 2025. We
systematically review the current state of computational resources, models, and
applications across ten distinct downstream tasks, including morphological
processing, machine translation, speech recognition, and question-answering.
Our analysis reveals a clear trajectory from foundational, rule-based systems
to modern neural architectures, with progress consistently unlocked by resource
creation milestones. We identify key challenges rooted in Tigrinya's
morphological complexity and resource scarcity, while highlighting promising
research directions, including morphology-aware modeling, cross-lingual
transfer, and community-centered resource development. This work serves as both
a comprehensive reference for researchers and a roadmap for advancing Tigrinya
NLP. A curated metadata of the surveyed studies and resources is made publicly
available.\footnote{Tigrinya NLP Anthology:
https://github.com/fgaim/tigrinya-nlp-anthology.

</details>


### [29] [Technical Report of TeleChat2, TeleChat2.5 and T1](https://arxiv.org/abs/2507.18013)
*Zihan Wang,Xinzhang Liu,Yitong Yao,Chao Wang,Yu Zhao,Zhihao Yang,Wenmin Deng,Kaipeng Jia,Jiaxin Peng,Yuyao Huang,Sishi Xiong,Zhuo Jiang,Kaidong Yu,Xiaohui Hu,Fubei Yao,Ruiyu Fang,Zhuoru Jiang,Ruiting Song,Qiyi Xie,Rui Xue,Xuewei He,Yanlei Xue,Zhu Yuan,Zhaoxi Zhang,Zilu Huang,Shiquan Wang,Xin Wang,Hanming Wu,Mingyuan Wang,Xufeng Zhan,Yuhan Sun,Zhaohu Xing,Yuhao Jiang,Bingkai Yang,Shuangyong Song,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: TeleChat系列最新模型（TeleChat2、TeleChat2.5和T1）通过改进训练策略显著提升性能，T1在复杂推理任务中表现优异，TeleChat2.5注重速度。


<details>
  <summary>Details</summary>
Motivation: 提升TeleChat系列模型的性能，满足不同应用场景需求，如复杂推理和快速推理。

Method: 采用增强的预训练和后训练策略，包括SFT、DPO、持续预训练和强化学习，优化模型架构。

Result: TeleChat2.5和T1在推理和通用任务中表现优异，T1-115B超越部分专有模型。

Conclusion: 公开发布TeleChat系列模型，为开发者和研究人员提供高性能语言模型。

Abstract: We introduce the latest series of TeleChat models: \textbf{TeleChat2},
\textbf{TeleChat2.5}, and \textbf{T1}, offering a significant upgrade over
their predecessor, TeleChat. Despite minimal changes to the model architecture,
the new series achieves substantial performance gains through enhanced training
strategies in both pre-training and post-training stages. The series begins
with \textbf{TeleChat2}, which undergoes pretraining on 10 trillion
high-quality and diverse tokens. This is followed by Supervised Fine-Tuning
(SFT) and Direct Preference Optimization (DPO) to further enhance its
capabilities. \textbf{TeleChat2.5} and \textbf{T1} expand the pipeline by
incorporating a continual pretraining phase with domain-specific datasets,
combined with reinforcement learning (RL) to improve performance in code
generation and mathematical reasoning tasks. The \textbf{T1} variant is
designed for complex reasoning, supporting long Chain-of-Thought (CoT)
reasoning and demonstrating substantial improvements in mathematics and coding.
In contrast, \textbf{TeleChat2.5} prioritizes speed, delivering rapid
inference. Both flagship models of \textbf{T1} and \textbf{TeleChat2.5} are
dense Transformer-based architectures with 115B parameters, showcasing
significant advancements in reasoning and general task performance compared to
the original TeleChat. Notably, \textbf{T1-115B} outperform proprietary models
such as OpenAI's o1-mini and GPT-4o. We publicly release \textbf{TeleChat2},
\textbf{TeleChat2.5} and \textbf{T1}, including post-trained versions with 35B
and 115B parameters, to empower developers and researchers with
state-of-the-art language models tailored for diverse applications.

</details>


### [30] [NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database](https://arxiv.org/abs/2507.18028)
*Weizhi Fei,Hao Shi,Jing Xu,Jingchen Peng,Jiazheng Li,Jingzhao Zhang,Bo Bai,Wei Han,Zhenyuan Chen,Xueyan Niu*

Main category: cs.CL

TL;DR: 论文提出NeuralDB框架，通过神经键值数据库和非线性门控检索模块高效编辑大型语言模型中的知识，同时保持模型通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有线性Locate-and-Edit方法在编辑大规模事实时可能损害模型通用能力或导致遗忘的问题。

Method: 将线性L&E方法建模为键值数据库查询，提出NeuralDB框架，使用非线性门控检索模块仅在涉及编辑事实时激活。

Result: 在10,000事实编辑实验中，NeuralDB在编辑效果、通用性、特异性、流畅性和一致性上表现优异，且能扩展到100,000事实。

Conclusion: NeuralDB是一种高效且可扩展的知识编辑框架，能同时保持语言模型的通用能力。

Abstract: Efficiently editing knowledge stored in large language models (LLMs) enables
model updates without large-scale training. One possible solution is
Locate-and-Edit (L\&E), allowing simultaneous modifications of a massive number
of facts. However, such editing may compromise the general abilities of LLMs
and even result in forgetting edited facts when scaling up to thousands of
edits. In this paper, we model existing linear L\&E methods as querying a
Key-Value (KV) database. From this perspective, we then propose NeuralDB, an
editing framework that explicitly represents the edited facts as a neural KV
database equipped with a non-linear gated retrieval module, % In particular,
our gated module only operates when inference involves the edited facts,
effectively preserving the general abilities of LLMs. Comprehensive experiments
involving the editing of 10,000 facts were conducted on the ZsRE and
CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results
demonstrate that NeuralDB not only excels in editing efficacy, generalization,
specificity, fluency, and consistency, but also preserves overall performance
across six representative text understanding and generation tasks. Further
experiments indicate that NeuralDB maintains its effectiveness even when scaled
to 100,000 facts (\textbf{50x} more than in prior work).

</details>


### [31] [GrAInS: Gradient-based Attribution for Inference-Time Steering of LLMs and VLMs](https://arxiv.org/abs/2507.18043)
*Duy Nguyen,Archiki Prasad,Elias Stengel-Eskin,Mohit Bansal*

Main category: cs.CL

TL;DR: GrAInS是一种基于梯度的推理时引导方法，通过识别关键令牌并构建方向性引导向量，优化语言和视觉语言模型的行为，无需微调。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖固定干预向量，忽略输入令牌的因果影响和梯度信息，尤其在多模态任务中表现不足。

Method: 使用对比梯度归因识别关键令牌，构建方向性引导向量，调整隐藏激活并归一化。

Result: 在TruthfulQA、MMHal-Bench和SPA-VL等任务中显著提升性能，同时保持模型流畅性。

Conclusion: GrAInS提供了一种细粒度、可解释且模块化的模型行为控制方法，优于现有方法。

Abstract: Inference-time steering methods offer a lightweight alternative to
fine-tuning large language models (LLMs) and vision-language models (VLMs) by
modifying internal activations at test time without updating model weights.
However, most existing approaches rely on fixed, global intervention vectors,
overlook the causal influence of individual input tokens, and fail to leverage
informative gradients from the model's logits, particularly in multimodal
settings where visual and textual inputs contribute unevenly. To address these
limitations, we introduce GrAInS, an inference-time steering approach that
operates across both language-only and vision-language models and tasks. GrAInS
uses contrastive, gradient-based attribution via Integrated Gradients to
identify the top-k most influential tokens, both positively and negatively
attributed based on their contribution to preferred versus dispreferred
outputs. These tokens are then used to construct directional steering vectors
that capture semantic shifts from undesirable to desirable behavior. During
inference, GrAInS adjusts hidden activations at transformer layers guided by
token-level attribution signals, and normalizes activations to preserve
representational scale. This enables fine-grained, interpretable, and modular
control over model behavior, without retraining or auxiliary supervision.
Empirically, GrAInS consistently outperforms both fine-tuning and existing
steering baselines: it achieves a 13.22% accuracy gain on TruthfulQA using
Llama-3.1-8B, reduces hallucination rates on MMHal-Bench from 0.624 to 0.514
with LLaVA-1.6-7B, and improves alignment win rates on SPA-VL by 8.11%, all
while preserving the model's fluency and general capabilities.

</details>


### [32] [Synthetic Data Generation for Phrase Break Prediction with Large Language Model](https://arxiv.org/abs/2507.18044)
*Hoyeon Lee,Sejung Son,Ye-Eun Kang,Jong-Hwan Kim*

Main category: cs.CL

TL;DR: 利用大语言模型（LLM）生成合成短语断句标注，减少人工标注需求，并在多语言中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决传统短语断句预测依赖大量人工标注的问题，以及语音领域数据获取的复杂性。

Method: 利用LLM生成合成短语断句标注，并与传统标注方法对比，评估其在多语言中的效果。

Result: LLM生成的合成数据有效缓解了短语断句预测中的数据挑战。

Conclusion: LLM在语音领域具有潜力，可作为解决数据问题的可行方案。

Abstract: Current approaches to phrase break prediction address crucial prosodic
aspects of text-to-speech systems but heavily rely on vast human annotations
from audio or text, incurring significant manual effort and cost. Inherent
variability in the speech domain, driven by phonetic factors, further
complicates acquiring consistent, high-quality data. Recently, large language
models (LLMs) have shown success in addressing data challenges in NLP by
generating tailored synthetic data while reducing manual annotation needs.
Motivated by this, we explore leveraging LLM to generate synthetic phrase break
annotations, addressing the challenges of both manual annotation and
speech-related tasks by comparing with traditional annotations and assessing
effectiveness across multiple languages. Our findings suggest that LLM-based
synthetic data generation effectively mitigates data challenges in phrase break
prediction and highlights the potential of LLMs as a viable solution for the
speech domain.

</details>


### [33] [Privacy-Preserving Synthetic Review Generation with Diverse Writing Styles Using LLMs](https://arxiv.org/abs/2507.18055)
*Tevin Atwal,Chan Nam Tieu,Yefeng Yuan,Zhan Shi,Yuhong Liu,Liang Cheng*

Main category: cs.CL

TL;DR: 论文探讨了LLM生成合成数据的多样性和隐私问题，提出了评估指标和改进方法。


<details>
  <summary>Details</summary>
Motivation: 合成数据虽成本低且可扩展，但其多样性和隐私风险尚未充分研究。

Method: 提出了一套评估指标，并基于实验结果设计了一种提示方法以改进多样性。

Result: 实验显示LLM生成的合成数据在多样性和隐私保护方面存在显著不足。

Conclusion: 通过提示方法可提升合成数据的多样性，同时保护隐私。

Abstract: The increasing use of synthetic data generated by Large Language Models
(LLMs) presents both opportunities and challenges in data-driven applications.
While synthetic data provides a cost-effective, scalable alternative to
real-world data to facilitate model training, its diversity and privacy risks
remain underexplored. Focusing on text-based synthetic data, we propose a
comprehensive set of metrics to quantitatively assess the diversity (i.e.,
linguistic expression, sentiment, and user perspective), and privacy (i.e.,
re-identification risk and stylistic outliers) of synthetic datasets generated
by several state-of-the-art LLMs. Experiment results reveal significant
limitations in LLMs' capabilities in generating diverse and privacy-preserving
synthetic data. Guided by the evaluation results, a prompt-based approach is
proposed to enhance the diversity of synthetic reviews while preserving
reviewer privacy.

</details>


### [34] [TELEVAL: A Dynamic Benchmark Designed for Spoken Language Models in Chinese Interactive Scenarios](https://arxiv.org/abs/2507.18061)
*Zehan Li,Hongjie Chen,Yuxin Zhang,Jing Zhou,Xuening Wang,Hang Lv,Mengjie Du,Yaodong Song,Jie Lian,Jian Kang,Jie Li,Yongxiang Li,Zhongjiang He,Xuelong Li*

Main category: cs.CL

TL;DR: TELEVAL是一个动态基准，用于评估口语模型在真实中文对话场景中的表现，重点关注隐式语义理解和自然交互能力。


<details>
  <summary>Details</summary>
Motivation: 现有基准多关注复杂任务而非真实对话场景，TELEVAL旨在填补这一空白，更贴近用户实际体验。

Method: TELEVAL定义了三个评估维度（显式语义、副语言与隐式语义、系统能力），采用对话形式，分别评估文本和音频输出。

Result: 实验表明，现有口语模型在自然对话任务中仍有较大改进空间。

Conclusion: TELEVAL可作为用户中心的评估框架，推动对话导向的口语模型发展。

Abstract: Spoken language models (SLMs) have seen rapid progress in recent years, along
with the development of numerous benchmarks for evaluating their performance.
However, most existing benchmarks primarily focus on evaluating whether SLMs
can perform complex tasks comparable to those tackled by large language models
(LLMs), often failing to align with how users naturally interact in real-world
conversational scenarios. In this paper, we propose TELEVAL, a dynamic
benchmark specifically designed to evaluate SLMs' effectiveness as
conversational agents in realistic Chinese interactive settings. TELEVAL
defines three evaluation dimensions: Explicit Semantics, Paralinguistic and
Implicit Semantics, and System Abilities. It adopts a dialogue format
consistent with real-world usage and evaluates text and audio outputs
separately. TELEVAL particularly focuses on the model's ability to extract
implicit cues from user speech and respond appropriately without additional
instructions. Our experiments demonstrate that despite recent progress,
existing SLMs still have considerable room for improvement in natural
conversational tasks. We hope that TELEVAL can serve as a user-centered
evaluation framework that directly reflects the user experience and contributes
to the development of more capable dialogue-oriented SLMs.

</details>


### [35] [Hybrid and Unitary Fine-Tuning of Large Language Models: Methods and Benchmarking under Resource Constraints](https://arxiv.org/abs/2507.18076)
*Haomin Qi,Zihan Dai,Chengbo Huang*

Main category: cs.CL

TL;DR: 论文提出了一种混合参数高效微调（PEFT）方法，结合BOFT的正交稳定性和LoRA-GA的梯度对齐快速收敛，显著提升了收敛效率和泛化能力，同时减少了资源消耗。


<details>
  <summary>Details</summary>
Motivation: 大规模语言模型（LLMs）的微调存在计算和内存需求高的瓶颈，需要更高效的微调方法。

Method: 提出了一种混合策略，动态结合BOFT和LoRA-GA的优势，并首次将uRNN原理应用于基于Transformer的LLMs。

Result: 在多个基准测试中，混合方法优于其他PEFT基线，接近全微调精度，同时减少训练时间和内存消耗。

Conclusion: 混合方法为资源受限的LLMs实际部署提供了实用且可扩展的微调解决方案。

Abstract: Fine-tuning large language models (LLMs) remains a computational bottleneck
due to their scale and memory demands. This paper presents a comprehensive
evaluation of parameter-efficient fine-tuning (PEFT) techniques, including
LoRA, BOFT, LoRA-GA, and uRNN, and introduces a novel hybrid strategy that
dynamically integrates BOFT's orthogonal stability with LoRA-GA's
gradient-aligned rapid convergence. By computing per-layer adaptive updates
guided by gradient norms, the hybrid method achieves superior convergence
efficiency and generalization across diverse tasks. We also explore, for the
first time, the adaptation of unitary RNN (uRNN) principles to
transformer-based LLMs, enhancing gradient stability through structured unitary
constraints. Empirical evaluations on four benchmarks -- GLUE, GSM8K, MT-Bench,
and HumanEval -- using models ranging from 7B to 405B parameters demonstrate
that our hybrid method consistently outperforms individual PEFT baselines,
approaching full fine-tuning accuracy while reducing resource consumption by up
to 2.1 times in training time and 50 percent in memory usage. These findings
establish the hybrid approach as a practical and scalable fine-tuning solution
for real-world deployment of LLMs under resource constraints.

</details>


### [36] [A New Pair of GloVes](https://arxiv.org/abs/2507.18103)
*Riley Carlson,John Bauer,Christopher D. Manning*

Main category: cs.CL

TL;DR: 2024年新版GloVe模型报告，更新了2014年原始模型，解决了数据版本和预处理文档不足的问题，并展示了新模型在文化和语言相关性任务中的改进。


<details>
  <summary>Details</summary>
Motivation: 2014年的GloVe模型虽广泛使用，但语言和世界在变化，且原模型缺乏详细文档，因此需要更新和改进。

Method: 使用Wikipedia、Gigaword和Dolma子集训练两组词嵌入，并通过词汇比较、直接测试和NER任务进行评估。

Result: 2024年模型包含新文化相关词汇，在结构任务（如类比和相似性）上表现相当，在非西方新闻数据等时间依赖NER任务上表现更优。

Conclusion: 新版GloVe模型在保持原有性能的同时，更好地适应了语言和文化的变化，解决了文档不足的问题。

Abstract: This report documents, describes, and evaluates new 2024 English GloVe
(Global Vectors for Word Representation) models. While the original GloVe
models built in 2014 have been widely used and found useful, languages and the
world continue to evolve and we thought that current usage could benefit from
updated models. Moreover, the 2014 models were not carefully documented as to
the exact data versions and preprocessing that were used, and we rectify this
by documenting these new models. We trained two sets of word embeddings using
Wikipedia, Gigaword, and a subset of Dolma. Evaluation through vocabulary
comparison, direct testing, and NER tasks shows that the 2024 vectors
incorporate new culturally and linguistically relevant words, perform
comparably on structural tasks like analogy and similarity, and demonstrate
improved performance on recent, temporally dependent NER datasets such as
non-Western newswire data.

</details>


### [37] [GOAT-SLM: A Spoken Language Model with Paralinguistic and Speaker Characteristic Awareness](https://arxiv.org/abs/2507.18119)
*Hongjie Chen,Zehan Li,Yaodong Song,Wenming Deng,Yitong Yao,Yuxin Zhang,Hang Lv,Xuechao Zhu,Jian Kang,Jie Lian,Jie Li,Chao Wang,Shuangyong Song,Yongxiang Li,Zhongjiang He*

Main category: cs.CL

TL;DR: GOAT-SLM是一种新型的端到端语音语言模型，专注于捕捉语音中的副语言和说话者特征，通过双模态头架构和分阶段训练策略，实现了语义和非语义任务的平衡表现。


<details>
  <summary>Details</summary>
Motivation: 现有语音语言模型主要关注语言内容，忽略了语音中丰富的副语言和说话者特征（如方言、年龄、情感等），限制了自然交互的能力。

Method: 采用双模态头架构分离语言建模和声学实现，提出模块化分阶段训练策略，逐步对齐语言、副语言和说话者特征信息。

Result: 在TELEVAL基准测试中，GOAT-SLM在语义和非语义任务上表现均衡，尤其在情感、方言和年龄敏感交互方面优于现有开源模型。

Conclusion: 该研究强调了超越语言内容建模的重要性，推动了更自然、自适应和社会感知的语音语言系统的发展。

Abstract: Recent advances in end-to-end spoken language models (SLMs) have
significantly improved the ability of AI systems to engage in natural spoken
interactions. However, most existing models treat speech merely as a vehicle
for linguistic content, often overlooking the rich paralinguistic and speaker
characteristic cues embedded in human speech, such as dialect, age, emotion,
and non-speech vocalizations. In this work, we introduce GOAT-SLM, a novel
spoken language model with paralinguistic and speaker characteristic awareness,
designed to extend spoken language modeling beyond text semantics. GOAT-SLM
adopts a dual-modality head architecture that decouples linguistic modeling
from acoustic realization, enabling robust language understanding while
supporting expressive and adaptive speech generation. To enhance model
efficiency and versatility, we propose a modular, staged training strategy that
progressively aligns linguistic, paralinguistic, and speaker characteristic
information using large-scale speech-text corpora. Experimental results on
TELEVAL, a multi-dimensional evaluation benchmark, demonstrate that GOAT-SLM
achieves well-balanced performance across both semantic and non-semantic tasks,
and outperforms existing open-source models in handling emotion, dialectal
variation, and age-sensitive interactions. This work highlights the importance
of modeling beyond linguistic content and advances the development of more
natural, adaptive, and socially aware spoken language systems.

</details>


### [38] [MathOPEval: A Fine-grained Evaluation Benchmark for Visual Operations of MLLMs in Mathematical Reasoning](https://arxiv.org/abs/2507.18140)
*Xiaoyuan Li,Moxin Li,Wenjie Wang,Rui Men,Yichang Zhang,Fuli Feng,Dayiheng Liu,Junyang Lin*

Main category: cs.CL

TL;DR: 该论文评估了多模态大语言模型（MLLMs）在基于代码的多模态数学推理中的能力，重点关注代码生成和编辑任务，发现现有模型在细粒度视觉操作上仍远不及人类表现。


<details>
  <summary>Details</summary>
Motivation: 现有评估主要关注纯文本推理输出，而忽略了MLLMs通过代码执行准确视觉操作的能力，因此需要填补这一研究空白。

Method: 提出一个评估框架，包括多模态代码生成（MCG）和多模态代码编辑（MCE），并利用覆盖五种数学图形的数据集对九种主流MLLMs进行实验。

Result: 实验结果表明，现有模型在细粒度视觉操作（如删除、修改和注释）上的表现显著落后于人类。

Conclusion: 该研究为评估MLLMs在多模态数学推理中的代码能力提供了初步框架，并揭示了现有模型的局限性。

Abstract: Recent progress in Multi-modal Large Language Models (MLLMs) has enabled
step-by-step multi-modal mathematical reasoning by performing visual operations
based on the textual instructions. A promising approach uses code as an
intermediate representation to precisely express and manipulate the images in
the reasoning steps. However, existing evaluations focus mainly on text-only
reasoning outputs, leaving the MLLM's ability to perform accurate visual
operations via code largely unexplored. This work takes a first step toward
addressing that gap by evaluating MLLM's code-based capabilities in multi-modal
mathematical reasoning.Specifically, our framework focuses on two key
evaluation aspects: (1) Multi-modal Code Generation (MCG) evaluates the model's
ability to accurately understand and construct visualizations from scratch. (2)
Multi-modal Code Editing (MCE) assesses the model's capacity for fine-grained
operations, which include three types: Deletion, Modification and Annotation.
To evaluate the above tasks, we incorporate a dataset that covers the five most
popular types of mathematical figures, including geometric diagrams, function
plots, and three types of statistical charts, to provide a comprehensive and
effective measurement of existing MLLMs. Our experimental evaluation involves
nine mainstream MLLMs, and the results reveal that existing models still lag
significantly behind human performance in performing fine-grained visual
operations.

</details>


### [39] [HIVMedQA: Benchmarking large language models for HIV medical decision support](https://arxiv.org/abs/2507.18143)
*Gonzalo Cardenal Antolin,Jacques Fellay,Bashkim Jaha,Roger Kouyos,Niko Beerenwinkel,Diane Duroux*

Main category: cs.CL

TL;DR: 该研究评估了大型语言模型（LLMs）在HIV管理中的能力，开发了HIVMedQA基准测试，发现Gemini 2.5 Pro表现最佳，但模型性能随问题复杂性下降，且医学微调模型不一定优于通用模型。


<details>
  <summary>Details</summary>
Motivation: HIV管理的复杂性使其成为LLMs应用的理想场景，但临床实践中LLMs的准确性、潜在危害和接受度问题尚未充分研究。

Method: 研究开发了HIVMedQA基准测试，评估了7个通用和3个医学专用LLMs，结合词相似性和LLM作为评判的方法，从多个维度评估性能。

Result: Gemini 2.5 Pro表现最佳，但性能随问题复杂性下降；医学微调模型不一定优于通用模型；推理和理解比事实回忆更具挑战性。

Conclusion: 研究强调需要针对性开发和评估，以确保LLMs在临床中的安全有效应用。

Abstract: Large language models (LLMs) are emerging as valuable tools to support
clinicians in routine decision-making. HIV management is a compelling use case
due to its complexity, including diverse treatment options, comorbidities, and
adherence challenges. However, integrating LLMs into clinical practice raises
concerns about accuracy, potential harm, and clinician acceptance. Despite
their promise, AI applications in HIV care remain underexplored, and LLM
benchmarking studies are scarce. This study evaluates the current capabilities
of LLMs in HIV management, highlighting their strengths and limitations. We
introduce HIVMedQA, a benchmark designed to assess open-ended medical question
answering in HIV care. The dataset consists of curated, clinically relevant
questions developed with input from an infectious disease physician. We
evaluated seven general-purpose and three medically specialized LLMs, applying
prompt engineering to enhance performance. Our evaluation framework
incorporates both lexical similarity and an LLM-as-a-judge approach, extended
to better reflect clinical relevance. We assessed performance across key
dimensions: question comprehension, reasoning, knowledge recall, bias,
potential harm, and factual accuracy. Results show that Gemini 2.5 Pro
consistently outperformed other models across most dimensions. Notably, two of
the top three models were proprietary. Performance declined as question
complexity increased. Medically fine-tuned models did not always outperform
general-purpose ones, and larger model size was not a reliable predictor of
performance. Reasoning and comprehension were more challenging than factual
recall, and cognitive biases such as recency and status quo were observed.
These findings underscore the need for targeted development and evaluation to
ensure safe, effective LLM integration in clinical care.

</details>


### [40] [Sticking to the Mean: Detecting Sticky Tokens in Text Embedding Models](https://arxiv.org/abs/2507.18171)
*Kexin Chen,Dongxia Wang,Yi Liu,Haonan Zhang,Wenhai Wang*

Main category: cs.CL

TL;DR: 研究发现Transformer文本嵌入模型中的'粘性标记'会破坏嵌入可靠性，提出检测方法STD并发现868个粘性标记，分析其来源及对下游任务的影响。


<details>
  <summary>Details</summary>
Motivation: 粘性标记会破坏嵌入距离的正常分布，影响下游任务性能，需系统研究其成因及解决方法。

Method: 提出STD方法，基于句子和标记过滤检测粘性标记，应用于40个检查点和14个模型家族。

Result: 发现868个粘性标记，其来源多为特殊词汇或未使用条目，对下游任务性能影响显著（下降达50%）。

Conclusion: 需改进标记化策略和模型设计以减少粘性标记的影响。

Abstract: Despite the widespread use of Transformer-based text embedding models in NLP
tasks, surprising 'sticky tokens' can undermine the reliability of embeddings.
These tokens, when repeatedly inserted into sentences, pull sentence similarity
toward a certain value, disrupting the normal distribution of embedding
distances and degrading downstream performance. In this paper, we
systematically investigate such anomalous tokens, formally defining them and
introducing an efficient detection method, Sticky Token Detector (STD), based
on sentence and token filtering. Applying STD to 40 checkpoints across 14 model
families, we discover a total of 868 sticky tokens. Our analysis reveals that
these tokens often originate from special or unused entries in the vocabulary,
as well as fragmented subwords from multilingual corpora. Notably, their
presence does not strictly correlate with model size or vocabulary size. We
further evaluate how sticky tokens affect downstream tasks like clustering and
retrieval, observing significant performance drops of up to 50%. Through
attention-layer analysis, we show that sticky tokens disproportionately
dominate the model's internal representations, raising concerns about
tokenization robustness. Our findings show the need for better tokenization
strategies and model design to mitigate the impact of sticky tokens in future
text embedding applications.

</details>


### [41] [SCOPE: Stochastic and Counterbiased Option Placement for Evaluating Large Language Models](https://arxiv.org/abs/2507.18182)
*Wonjun Jeong,Dongseok Kim,Taegkeun Whangbo*

Main category: cs.CL

TL;DR: SCOPE框架通过消除LLMs在选择题任务中的选择偏差，提升了评估的公平性和可靠性。


<details>
  <summary>Details</summary>
Motivation: LLMs可能通过利用选项位置或标签的偏差而非真实理解来获得高分，这影响了评估的准确性。

Method: SCOPE通过重复调用无语义内容的空提示，估计模型的独特位置偏差分布，并重新分配答案槽以平衡幸运率，同时阻止基于邻近线索的猜测。

Result: SCOPE在多个基准实验中表现优于现有去偏方法，提供了更稳定的性能改进和更清晰的置信度分布。

Conclusion: SCOPE为提升LLM评估的公平性和可靠性提供了新标准。

Abstract: Large Language Models (LLMs) can achieve inflated scores on multiple-choice
tasks by exploiting inherent biases in option positions or labels, rather than
demonstrating genuine understanding. This study introduces SCOPE, an evaluation
framework designed to measure and mitigate such selection bias in a
dataset-independent manner. By repeatedly invoking a null prompt that lacks
semantic content, SCOPE estimates each model's unique position-bias
distribution. It then redistributes the answer slot according to the
inverse-bias distribution, thereby equalizing the lucky-rate, the probability
of selecting the correct answer by chance. Furthermore, it prevents
semantically similar distractors from being placed adjacent to the answer,
thereby blocking near-miss guesses based on superficial proximity cues. Across
multiple benchmark experiments, SCOPE consistently outperformed existing
debiasing methods in terms of stable performance improvements and showed
clearer confidence distributions over correct options. This framework thus
offers a new standard for enhancing the fairness and reliability of LLM
evaluations.

</details>


### [42] [TN-AutoRCA: Benchmark Construction and Agentic Framework for Self-Improving Alarm-Based Root Cause Analysis in Telecommunication Networks](https://arxiv.org/abs/2507.18190)
*Keyu Wu,Qianjin Yu,Manlin Mei,Ruiting Liu,Jun Wang,Kailai Zhang,Yelun Bao*

Main category: cs.CL

TL;DR: 论文探讨了AI在电信网络根因分析（RCA）中的挑战，包括复杂的图推理需求和缺乏现实基准。


<details>
  <summary>Details</summary>
Motivation: 电信网络中的根因分析（RCA）对AI提出了严峻挑战，尤其是其复杂的图推理需求和现实基准的稀缺性。

Method: 未明确提及具体方法，但强调了图推理和基准的重要性。

Result: 未明确提及具体结果，但指出了当前AI在RCA任务中的局限性。

Conclusion: 需要进一步研究以解决AI在电信网络RCA中的复杂性和基准问题。

Abstract: Root Cause Analysis (RCA) in telecommunication networks is a critical task,
yet it presents a formidable challenge for Artificial Intelligence (AI) due to
its complex, graph-based reasoning requirements and the scarcity of realistic
benchmarks.

</details>


### [43] [Integrating an ISO30401-compliant Knowledge management system with existing business processes of an organization](https://arxiv.org/abs/2507.18197)
*Aline Belloni,Patrick Prieur*

Main category: cs.CL

TL;DR: 论文探讨了ISO30401知识管理系统（KMS）如何与ISO9001流程模型集成，并通过SECI模型和PDCA循环实现。


<details>
  <summary>Details</summary>
Motivation: 解决ISO30401实施者在解释知识管理活动如何与现有业务流程整合时的挑战。

Method: 结合ISO9001流程建模原则，通过SECI模型和PDCA循环实现KMS。

Result: 展示了KMS如何与综合管理系统中的其他流程结合。

Conclusion: ISO30401 KMS可以通过SECI和PDCA有效集成到现有流程中。

Abstract: Business process modeling is used by most organizations as an essential
framework for ensuring efficiency and effectiveness of the work and workflow
performed by its employees and for ensuring the alignment of such work with its
strategic goals. For organizations that are compliant or near-compliant with
ISO 9001, this approach involves the detailed mapping of processes,
sub-processes, activities, and tasks. ISO30401 is a Management System Standard,
introduced in 2018, establishing universal requirements for the set up of a
Knowledge Management System in an organization. As ``ISO30401 implementers'' we
regularly face the challenge of explaining our clients how the knowledge
development, transformation and conveyances activities depicted in ISO30401 do
integrate with existing operational processes. This article recaps process
modelling principles in the context of ISO9001 and explores, based on our
experience, how an ISO30401-compliant Knowledge Management System (KMS)
entwines with all other processes of an Integrated Management System and in
particular how it can be implemented by deploying the mechanisms of the SECI
model through the steps of PDCA cycles.

</details>


### [44] [Safeguarding RAG Pipelines with GMTP: A Gradient-based Masked Token Probability Method for Poisoned Document Detection](https://arxiv.org/abs/2507.18202)
*San Kim,Jonghwi Kim,Yejin Jeon,Gary Geunbae Lee*

Main category: cs.CL

TL;DR: GMTP是一种基于梯度的防御方法，通过掩码语言模型检测恶意文档，有效过滤90%以上的中毒内容。


<details>
  <summary>Details</summary>
Motivation: RAG依赖外部知识库可能被攻击者注入恶意文档，导致误导性输出，需防御方法。

Method: GMTP通过分析检索器相似度函数的梯度识别关键令牌，掩码后检查其概率，检测恶意文档。

Result: 实验显示GMTP能过滤90%以上中毒内容，同时保留相关文档，保持检索和生成性能。

Conclusion: GMTP是一种高效防御方法，适用于多种对抗场景。

Abstract: Retrieval-Augmented Generation (RAG) enhances Large Language Models (LLMs) by
providing external knowledge for accurate and up-to-date responses. However,
this reliance on external sources exposes a security risk, attackers can inject
poisoned documents into the knowledge base to steer the generation process
toward harmful or misleading outputs. In this paper, we propose Gradient-based
Masked Token Probability (GMTP), a novel defense method to detect and filter
out adversarially crafted documents. Specifically, GMTP identifies high-impact
tokens by examining gradients of the retriever's similarity function. These key
tokens are then masked, and their probabilities are checked via a Masked
Language Model (MLM). Since injected tokens typically exhibit markedly low
masked-token probabilities, this enables GMTP to easily detect malicious
documents and achieve high-precision filtering. Experiments demonstrate that
GMTP is able to eliminate over 90% of poisoned content while retaining relevant
documents, thus maintaining robust retrieval and generation performance across
diverse datasets and adversarial settings.

</details>


### [45] [Exploring the Impact of Instruction-Tuning on LLM's Susceptibility to Misinformation](https://arxiv.org/abs/2507.18203)
*Kyubeen Han,Junseo Jang,Hongjin Kim,Geunyeong Jeong,Harksoo Kim*

Main category: cs.CL

TL;DR: 指令微调提升了大语言模型（LLM）遵循用户指令的能力，但也增加了对用户输入的依赖，可能导致错误信息的接受和幻觉生成。研究发现，指令微调后的LLM更容易接受用户提供的错误信息。


<details>
  <summary>Details</summary>
Motivation: 探讨指令微调对LLM接受错误信息倾向的影响，填补现有研究的空白。

Method: 通过对比指令微调模型和基础模型，分析其对用户提供信息的依赖程度，并研究提示结构、错误信息长度和系统提示警告等因素的影响。

Result: 指令微调显著增加LLM对用户提供错误信息的接受度，且依赖从助手角色转向用户角色。

Conclusion: 需系统性方法减轻指令微调的负面影响，提升LLM在实际应用中的可靠性。

Abstract: Instruction-tuning enhances the ability of large language models (LLMs) to
follow user instructions more accurately, improving usability while reducing
harmful outputs. However, this process may increase the model's dependence on
user input, potentially leading to the unfiltered acceptance of misinformation
and the generation of hallucinations. Existing studies primarily highlight that
LLMs are receptive to external information that contradict their parametric
knowledge, but little research has been conducted on the direct impact of
instruction-tuning on this phenomenon. In our study, we investigate the impact
of instruction-tuning on LLM's susceptibility to misinformation. Our analysis
reveals that instruction-tuned LLMs are significantly more likely to accept
misinformation when it is presented by the user. A comparison with base models
shows that instruction-tuning increases reliance on user-provided information,
shifting susceptibility from the assistant role to the user role. Furthermore,
we explore additional factors influencing misinformation susceptibility, such
as the role of the user in prompt structure, misinformation length, and the
presence of warnings in the system prompt. Our findings underscore the need for
systematic approaches to mitigate unintended consequences of instruction-tuning
and enhance the reliability of LLMs in real-world applications.

</details>


### [46] [Prune&Comp: Free Lunch for Layer-Pruned LLMs via Iterative Pruning with Magnitude Compensation](https://arxiv.org/abs/2507.18212)
*Xinrui Chen,Hongxing Zhang,Fanyi Zeng,Yongxian Wei,Yizhi Wang,Xitong Ling,Guanghao Li,Chun Yuan*

Main category: cs.CL

TL;DR: Prune&Comp是一种无需训练的层剪枝方法，通过幅度补偿减少剪枝导致的性能下降，显著提升模型压缩效果。


<details>
  <summary>Details</summary>
Motivation: 剪除层会导致隐藏状态幅度差距，引起性能显著下降，需解决此问题。

Method: 提出Prune&Comp，通过离线重缩放剩余权重补偿幅度差距，无运行时开销。

Result: 在LLaMA-3-8B上剪除5层时，困惑度减半，保留93.19%问答性能，优于基线4.01%。

Conclusion: Prune&Comp是一种高效且无需训练的层剪枝方案，显著提升现有剪枝指标。

Abstract: Layer pruning has emerged as a promising technique for compressing large
language models (LLMs) while achieving acceleration proportional to the pruning
ratio. In this work, we identify that removing any layer induces a significant
magnitude gap in hidden states, resulting in substantial performance
degradation. To address this issue, we propose Prune&Comp, a novel
plug-and-play layer pruning scheme that leverages magnitude compensation to
mitigate such gaps in a training-free manner. Specifically, we first estimate
the magnitude gap caused by layer removal and then eliminate this gap by
rescaling the remaining weights offline, with zero runtime overhead incurred.
We further demonstrate the advantages of Prune&Comp through an iterative
pruning strategy. When integrated with an iterative prune-and-compensate loop,
Prune&Comp consistently enhances existing layer pruning metrics. For instance,
when 5 layers of LLaMA-3-8B are pruned using the prevalent block influence
metric, Prune&Comp nearly halves the perplexity and retains 93.19\% of the
original model's question-answering performance, outperforming the baseline by
4.01%.

</details>


### [47] [Locate-and-Focus: Enhancing Terminology Translation in Speech Language Models](https://arxiv.org/abs/2507.18263)
*Suhang Wu,Jialong Tang,Chengyi Yang,Pei Zhang,Baosong Yang,Junhui Li,Junfeng Yao,Min Zhang,Jinsong Su*

Main category: cs.CL

TL;DR: 提出了一种名为Locate-and-Focus的新方法，用于解决直接语音翻译中术语翻译的难题，通过定位术语片段并关联多模态信息，提升翻译准确性。


<details>
  <summary>Details</summary>
Motivation: 当前方法在直接语音翻译中难以准确翻译术语，且易受无关噪声干扰，未能充分利用翻译知识。

Method: 提出Locate-and-Focus方法，先定位术语片段以构建翻译知识，再通过音频和文本模态关联知识，优化翻译过程。

Result: 实验表明，该方法能有效定位术语并提升术语翻译成功率，同时保持整体翻译性能。

Conclusion: Locate-and-Focus方法显著改善了直接语音翻译中的术语翻译问题。

Abstract: Direct speech translation (ST) has garnered increasing attention nowadays,
yet the accurate translation of terminology within utterances remains a great
challenge. In this regard, current studies mainly concentrate on leveraging
various translation knowledge into ST models. However, these methods often
struggle with interference from irrelevant noise and can not fully utilize the
translation knowledge. To address these issues, in this paper, we propose a
novel Locate-and-Focus method for terminology translation. It first effectively
locates the speech clips containing terminologies within the utterance to
construct translation knowledge, minimizing irrelevant information for the ST
model. Subsequently, it associates the translation knowledge with the utterance
and hypothesis from both audio and textual modalities, allowing the ST model to
better focus on translation knowledge during translation. Experimental results
across various datasets demonstrate that our method effectively locates
terminologies within utterances and enhances the success rate of terminology
translation, while maintaining robust general translation performance.

</details>


### [48] [Zero-shot OCR Accuracy of Low-Resourced Languages: A Comparative Analysis on Sinhala and Tamil](https://arxiv.org/abs/2507.18264)
*Nevidu Jayatilleke,Nisansa de Silva*

Main category: cs.CL

TL;DR: 该研究比较了六种OCR引擎在低资源语言（Sinhala和Tamil）上的零样本性能，发现Surya在Sinhala上表现最佳，Document AI在Tamil上表现最优。


<details>
  <summary>Details</summary>
Motivation: 解决低资源语言（LRL）使用独特脚本时的OCR问题，填补了高资源语言（HRL）OCR已解决但LRL仍为开放问题的研究空白。

Method: 选取六种OCR引擎（包括商业和开源系统），通过五种测量技术在字符和单词级别评估其性能。

Result: Surya在Sinhala上表现最佳（WER 2.61%），Document AI在Tamil上表现最优（CER 0.78%）。

Conclusion: 研究为低资源语言的OCR性能提供了基准，并引入了一个新的合成Tamil OCR数据集。

Abstract: Solving the problem of Optical Character Recognition (OCR) on printed text
for Latin and its derivative scripts can now be considered settled due to the
volumes of research done on English and other High-Resourced Languages (HRL).
However, for Low-Resourced Languages (LRL) that use unique scripts, it remains
an open problem. This study presents a comparative analysis of the zero-shot
performance of six distinct OCR engines on two LRLs: Sinhala and Tamil. The
selected engines include both commercial and open-source systems, aiming to
evaluate the strengths of each category. The Cloud Vision API, Surya, Document
AI, and Tesseract were evaluated for both Sinhala and Tamil, while Subasa OCR
and EasyOCR were examined for only one language due to their limitations. The
performance of these systems was rigorously analysed using five measurement
techniques to assess accuracy at both the character and word levels. According
to the findings, Surya delivered the best performance for Sinhala across all
metrics, with a WER of 2.61%. Conversely, Document AI excelled across all
metrics for Tamil, highlighted by a very low CER of 0.78%. In addition to the
above analysis, we also introduce a novel synthetic Tamil OCR benchmarking
dataset.

</details>


### [49] [StyleAdaptedLM: Enhancing Instruction Following Models with Efficient Stylistic Transfer](https://arxiv.org/abs/2507.18294)
*Pritika Ramu,Apoorv Saxena,Meghanath M Y,Varsha Sankar,Debraj Basu*

Main category: cs.CL

TL;DR: StyleAdaptedLM通过LoRA技术，将风格特征高效迁移到指令跟随模型，实现风格定制且不牺牲任务性能。


<details>
  <summary>Details</summary>
Motivation: 企业通信需要LLMs适应特定风格（如品牌声音或作者语调），但缺乏指令-响应格式的语料库使得这一目标难以实现。

Method: 使用LoRA适配器在基础模型上训练多样化的非结构化风格语料库，然后与指令跟随模型合并。

Result: 实验表明，该方法在多数据集和模型上提高了风格一致性，同时保持指令遵循能力，人类评估证实了品牌特定惯例的采纳。

Conclusion: StyleAdaptedLM为LLMs的风格个性化提供了一条高效路径。

Abstract: Adapting LLMs to specific stylistic characteristics, like brand voice or
authorial tones, is crucial for enterprise communication but challenging to
achieve from corpora which lacks instruction-response formatting without
compromising instruction adherence. We introduce StyleAdaptedLM, a framework
that efficiently transfers stylistic traits to instruction-following models
using Low-Rank Adaptation (LoRA). LoRA adapters are first trained on a base
model with diverse unstructured stylistic corpora, then merged with a separate
instruction-following model. This enables robust stylistic customization
without paired data or sacrificing task performance. Experiments across
multiple datasets and models demonstrate improved stylistic consistency while
preserving instruction adherence, with human evaluations confirming
brand-specific convention uptake. StyleAdaptedLM offers an efficient path for
stylistic personalization in LLMs.

</details>


### [50] [BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit](https://arxiv.org/abs/2507.18305)
*Biao Yi,Zekun Fei,Jianing Geng,Tong Li,Lihai Nie,Zheli Liu,Yiming Li*

Main category: cs.CL

TL;DR: 论文提出了一种针对大型推理模型（LRMs）的新型攻击方法，称为“过度思考后门”，通过数据污染实现可控的推理冗长攻击。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型（LRMs）在复杂推理任务中表现出色，但其推理冗长性可能被恶意利用，目前尚未有相关研究。

Method: 提出了一种可调谐的后门攻击方法，通过数据污染将触发信号与冗长的推理响应配对，利用教师LLM程序化生成冗余推理步骤。

Result: 实验证明该方法能可靠地触发可控的多倍推理冗长增加，且不影响最终答案的正确性。

Conclusion: 该攻击方法作为一种纯资源消耗向量，具有隐蔽性，揭示了LRMs的新安全风险。

Abstract: Large reasoning models (LRMs) have emerged as a significant advancement in
artificial intelligence, representing a specialized class of large language
models (LLMs) designed to tackle complex reasoning tasks. The defining
characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning
capabilities. In this paper, we identify a previously unexplored attack vector
against LRMs, which we term "overthinking backdoors". We advance this concept
by proposing a novel tunable backdoor, which moves beyond simple on/off attacks
to one where an attacker can precisely control the extent of the model's
reasoning verbosity. Our attack is implemented through a novel data poisoning
methodology. It pairs a tunable trigger-where the number of repetitions signals
the desired intensity-with a correspondingly verbose CoT response. These
responses are programmatically generated by instructing a teacher LLM to inject
a controlled number of redundant refinement steps into a correct reasoning
process. The approach preserves output correctness, which ensures stealth and
establishes the attack as a pure resource-consumption vector. Extensive
empirical results on various LRMs demonstrate that our method can reliably
trigger a controllable, multi-fold increase in the length of the reasoning
process, without degrading the final answer's correctness. Our source code is
available at https://github.com/FZaKK/BadReasoner.

</details>


### [51] [Uncertainty Quantification for Evaluating Machine Translation Bias](https://arxiv.org/abs/2507.18338)
*Ieva Raminta Staliūnaitė,Julius Cheng,Andreas Vlachos*

Main category: cs.CL

TL;DR: 论文探讨了机器翻译中性别标记不明确时的翻译问题，发现模型在模糊情况下未能表现出足够的不确定性，且去偏方法对模糊和明确实例的影响不同。


<details>
  <summary>Details</summary>
Motivation: 研究机器翻译模型在性别标记不明确时的表现，尤其是模型是否能在模糊情况下保持不确定性，以及去偏方法的效果。

Method: 使用语义不确定性度量方法，分析模型在明确和模糊性别标记下的翻译表现。

Result: 模型在明确实例中表现良好，但在模糊情况下未能表现出足够的不确定性；去偏方法对模糊和明确实例的影响独立。

Conclusion: 机器翻译模型需要在模糊情况下更好地保持不确定性，同时去偏方法需针对性优化。

Abstract: In machine translation (MT), when the source sentence includes a lexeme whose
gender is not overtly marked, but whose target-language equivalent requires
gender specification, the model must infer the appropriate gender from the
context and/or external knowledge. Studies have shown that MT models exhibit
biased behaviour, relying on stereotypes even when they clash with contextual
information. We posit that apart from confidently translating using the correct
gender when it is evident from the input, models should also maintain
uncertainty about the gender when it is ambiguous. Using recently proposed
metrics of semantic uncertainty, we find that models with high translation and
gender accuracy on unambiguous instances do not necessarily exhibit the
expected level of uncertainty in ambiguous ones. Similarly, debiasing has
independent effects on ambiguous and unambiguous translation instances.

</details>


### [52] [TDR: Task-Decoupled Retrieval with Fine-Grained LLM Feedback for In-Context Learning](https://arxiv.org/abs/2507.18340)
*Yifu Chen,Bingchen Huang,Zhiling Wang,Yuanchao Du,Junfeng Luo,Lei Shen,Zhineng chen*

Main category: cs.CL

TL;DR: TDR框架通过解耦多任务数据集中的示例并利用LLM的细粒度反馈，提升了ICL中示例检索的质量，实现了SOTA性能。


<details>
  <summary>Details</summary>
Motivation: ICL的效果依赖于示例质量，但现有方法在跨任务数据分布区分和细粒度反馈建模上存在挑战。

Method: 提出TDR框架，解耦多任务数据集的示例，并利用LLM反馈指导检索模块训练。

Result: 在30个NLP任务上实验表明，TDR显著提升性能，且为即插即用方法。

Conclusion: TDR通过改进示例检索，为ICL提供了高效解决方案，并兼容多种LLM。

Abstract: In-context learning (ICL) has become a classic approach for enabling LLMs to
handle various tasks based on a few input-output examples. The effectiveness of
ICL heavily relies on the quality of these examples, and previous works which
focused on enhancing example retrieval capabilities have achieved impressive
performances. However, two challenges remain in retrieving high-quality
examples: (1) Difficulty in distinguishing cross-task data distributions, (2)
Difficulty in making the fine-grained connection between retriever output and
feedback from LLMs. In this paper, we propose a novel framework called TDR. TDR
decouples the ICL examples from different tasks, which enables the retrieval
module to retrieve examples specific to the target task within a multi-task
dataset. Furthermore, TDR models fine-grained feedback from LLMs to supervise
and guide the training of the retrieval module, which helps to retrieve
high-quality examples. We conducted extensive experiments on a suite of 30 NLP
tasks, the results demonstrate that TDR consistently improved results across
all datasets and achieves state-of-the-art performance. Meanwhile, our approach
is a plug-and-play method, which can be easily combined with various LLMs to
improve example retrieval abilities for ICL. The code is available at
https://github.com/Nnn-s/TDR.

</details>


### [53] [Hybrid Annotation for Propaganda Detection: Integrating LLM Pre-Annotations with Human Intelligence](https://arxiv.org/abs/2507.18343)
*Ariana Sahitaj,Premtim Sahitaj,Veronika Solopova,Jiaao Li,Sebastian Möller,Vera Schmitt*

Main category: cs.CL

TL;DR: 本文提出了一种结合人类专家与大语言模型（LLM）的新框架，用于改进社交媒体上的宣传检测任务，通过分层分类法和LLM辅助预标注提升标注一致性和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 社交媒体上的宣传检测任务复杂且高质量标注数据有限，需要一种既能保证标注一致性又能提高效率的方法。

Method: 提出分层分类法组织14种细粒度宣传技术，进行人类标注研究，并设计LLM辅助预标注流程，包括提取宣传片段、生成解释和分配标签。随后通过人类验证研究验证效果，并基于LLM生成数据训练小型语言模型（SLMs）。

Result: LLM辅助预标注显著提高了标注一致性和时间效率，且基于LLM生成数据训练的SLMs表现良好。

Conclusion: 该框架为开发可扩展且鲁棒的宣传检测系统提供了支持，符合SDG 16目标，代码已开源。

Abstract: Propaganda detection on social media remains challenging due to task
complexity and limited high-quality labeled data. This paper introduces a novel
framework that combines human expertise with Large Language Model (LLM)
assistance to improve both annotation consistency and scalability. We propose a
hierarchical taxonomy that organizes 14 fine-grained propaganda techniques into
three broader categories, conduct a human annotation study on the HQP dataset
that reveals low inter-annotator agreement for fine-grained labels, and
implement an LLM-assisted pre-annotation pipeline that extracts propagandistic
spans, generates concise explanations, and assigns local labels as well as a
global label. A secondary human verification study shows significant
improvements in both agreement and time-efficiency. Building on this, we
fine-tune smaller language models (SLMs) to perform structured annotation.
Instead of fine-tuning on human annotations, we train on high-quality
LLM-generated data, allowing a large model to produce these annotations and a
smaller model to learn to generate them via knowledge distillation. Our work
contributes towards the development of scalable and robust propaganda detection
systems, supporting the idea of transparent and accountable media ecosystems in
line with SDG 16. The code is publicly available at our GitHub repository.

</details>


### [54] [CLEAR: Error Analysis via LLM-as-a-Judge Made Easy](https://arxiv.org/abs/2507.18392)
*Asaf Yehudai,Lilach Eden,Yotam Perlitz,Roy Bar-Haim,Michal Shmueli-Scheuer*

Main category: cs.CL

TL;DR: CLEAR是一个交互式开源工具，用于分析LLM的错误，提供详细反馈和可视化分析。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估仅提供单一评分或排名，缺乏具体性能原因分析。

Method: CLEAR生成实例级反馈、系统级错误问题，并通过交互式仪表板进行可视化分析。

Result: CLEAR在RAG和数学基准测试中展示了其分析能力，并通过用户案例验证了实用性。

Conclusion: CLEAR填补了LLM评估中具体错误分析的空白，提供了更深入的性能洞察。

Abstract: The evaluation of Large Language Models (LLMs) increasingly relies on other
LLMs acting as judges. However, current evaluation paradigms typically yield a
single score or ranking, answering which model is better but not why. While
essential for benchmarking, these top-level scores obscure the specific,
actionable reasons behind a model's performance. To bridge this gap, we
introduce CLEAR, an interactive, open-source package for LLM-based error
analysis. CLEAR first generates per-instance textual feedback, then it creates
a set of system-level error issues, and quantifies the prevalence of each
identified issue. Our package also provides users with an interactive dashboard
that allows for a comprehensive error analysis through aggregate
visualizations, applies interactive filters to isolate specific issues or score
ranges, and drills down to the individual instances that exemplify a particular
behavioral pattern. We demonstrate CLEAR analysis for RAG and Math benchmarks,
and showcase its utility through a user case study.

</details>


### [55] [Factual Inconsistencies in Multilingual Wikipedia Tables](https://arxiv.org/abs/2507.18406)
*Silvia Cappa,Lingxiao Kong,Pille-Riin Peet,Fanfu Wei,Yuchen Zhou,Jan-Christoph Kalo*

Main category: cs.CL

TL;DR: 研究探讨了维基百科多语言版本中表格数据的跨语言不一致性，提出了一种收集、对齐和分析的方法，并评估了其影响。


<details>
  <summary>Details</summary>
Motivation: 维基百科多语言版本内容独立更新导致事实不一致，影响其可靠性和AI系统的训练数据质量。

Method: 开发了一种方法，收集、对齐和分析多语言维基百科文章中的表格数据，定义不一致类别，并应用定量和定性指标评估对齐情况。

Result: 研究揭示了跨语言表格数据的不一致性，为事实验证和多语言知识交互提供了见解。

Conclusion: 研究结果对事实验证、多语言知识交互及依赖维基百科内容的可靠AI系统设计具有重要意义。

Abstract: Wikipedia serves as a globally accessible knowledge source with content in
over 300 languages. Despite covering the same topics, the different versions of
Wikipedia are written and updated independently. This leads to factual
inconsistencies that can impact the neutrality and reliability of the
encyclopedia and AI systems, which often rely on Wikipedia as a main training
source. This study investigates cross-lingual inconsistencies in Wikipedia's
structured content, with a focus on tabular data. We developed a methodology to
collect, align, and analyze tables from Wikipedia multilingual articles,
defining categories of inconsistency. We apply various quantitative and
qualitative metrics to assess multilingual alignment using a sample dataset.
These insights have implications for factual verification, multilingual
knowledge interaction, and design for reliable AI systems leveraging Wikipedia
content.

</details>


### [56] [FinDPO: Financial Sentiment Analysis for Algorithmic Trading through Preference Optimization of LLMs](https://arxiv.org/abs/2507.18417)
*Giorgos Iacovides,Wuyang Zhou,Danilo Mandic*

Main category: cs.CL

TL;DR: FinDPO是一种基于Direct Preference Optimization（DPO）的金融领域专用LLM框架，显著提升了情感分析性能，并在实际投资策略中表现出色。


<details>
  <summary>Details</summary>
Motivation: 在线金融文本的情感分析对交易决策和市场动态影响深远，但传统监督微调（SFT）方法存在泛化能力不足的问题。

Method: 提出FinDPO框架，通过DPO进行后训练人类偏好对齐，并设计‘logit-to-score’转换方法生成连续情感分数。

Result: FinDPO在情感分类基准上平均提升11%，模拟投资策略年回报率达67%，夏普比率2.0。

Conclusion: FinDPO是首个在金融领域实现高性能情感分析并有效应用于投资策略的框架。

Abstract: Opinions expressed in online finance-related textual data are having an
increasingly profound impact on trading decisions and market movements. This
trend highlights the vital role of sentiment analysis as a tool for quantifying
the nature and strength of such opinions. With the rapid development of
Generative AI (GenAI), supervised fine-tuned (SFT) large language models (LLMs)
have become the de facto standard for financial sentiment analysis. However,
the SFT paradigm can lead to memorization of the training data and often fails
to generalize to unseen samples. This is a critical limitation in financial
domains, where models must adapt to previously unobserved events and the
nuanced, domain-specific language of finance. To this end, we introduce FinDPO,
the first finance-specific LLM framework based on post-training human
preference alignment via Direct Preference Optimization (DPO). The proposed
FinDPO achieves state-of-the-art performance on standard sentiment
classification benchmarks, outperforming existing supervised fine-tuned models
by 11% on the average. Uniquely, the FinDPO framework enables the integration
of a fine-tuned causal LLM into realistic portfolio strategies through a novel
'logit-to-score' conversion, which transforms discrete sentiment predictions
into continuous, rankable sentiment scores (probabilities). In this way,
simulations demonstrate that FinDPO is the first sentiment-based approach to
maintain substantial positive returns of 67% annually and strong risk-adjusted
performance, as indicated by a Sharpe ratio of 2.0, even under realistic
transaction costs of 5 basis points (bps).

</details>


### [57] [AraTable: Benchmarking LLMs' Reasoning and Understanding of Arabic Tabular Data](https://arxiv.org/abs/2507.18442)
*Rana Alshaikh,Israa Alghanmi,Shelan Jeawak*

Main category: cs.CL

TL;DR: AraTable是一个新的阿拉伯语表格数据基准，用于评估大语言模型（LLMs）的理解和推理能力，填补了阿拉伯语资源的空白。


<details>
  <summary>Details</summary>
Motivation: 现有LLMs在处理结构化数据（尤其是表格）时表现有限，且阿拉伯语资源稀缺，因此需要专门的基准来推动相关研究。

Method: 采用混合方法，先由LLMs生成内容，再由人类专家过滤和验证，确保数据集质量。

Result: LLMs在简单任务（如直接问答）上表现尚可，但在复杂推理和事实验证任务中仍有显著挑战。

Conclusion: AraTable为阿拉伯语结构化数据处理提供了宝贵资源和评估框架，未来需进一步改进复杂推理任务的性能。

Abstract: The cognitive and reasoning abilities of large language models (LLMs) have
enabled remarkable progress in natural language processing. However, their
performance in interpreting structured data, especially in tabular formats,
remains limited. Although benchmarks for English tabular data are widely
available, Arabic is still underrepresented because of the limited availability
of public resources and its unique language features. To address this gap, we
present AraTable, a novel and comprehensive benchmark designed to evaluate the
reasoning and understanding capabilities of LLMs when applied to Arabic tabular
data. AraTable consists of various evaluation tasks, such as direct question
answering, fact verification, and complex reasoning, involving a wide range of
Arabic tabular sources. Our methodology follows a hybrid pipeline, where
initial content is generated by LLMs and subsequently filtered and verified by
human experts to ensure high dataset quality. Initial analyses using AraTable
show that, while LLMs perform adequately on simpler tabular tasks such as
direct question answering, they continue to face significant cognitive
challenges when tasks require deeper reasoning and fact verification. This
indicates that there are substantial opportunities for future work to improve
performance on complex tabular reasoning tasks. We also propose a fully
automated evaluation framework that uses a self-deliberation mechanism and
achieves performance nearly identical to that of human judges. This research
provides a valuable, publicly available resource and evaluation framework that
can help accelerate the development of foundational models for processing and
analysing Arabic structured data.

</details>


### [58] [Restoring Rhythm: Punctuation Restoration Using Transformer Models for Bangla, a Low-Resource Language](https://arxiv.org/abs/2507.18448)
*Md Obyedullahil Mamun,Md Adyelullahil Mamun,Arif Ahmad,Md. Imran Hossain Emu*

Main category: cs.CL

TL;DR: 该研究探讨了基于XLM-RoBERTa-large的Transformer模型在孟加拉语文本中自动恢复标点的应用，构建了大规模训练语料并采用数据增强技术，取得了高准确率。


<details>
  <summary>Details</summary>
Motivation: 提升低资源语言（如孟加拉语）文本的可读性，并支持自动语音识别（ASR）的后处理任务。

Method: 使用XLM-RoBERTa-large模型预测四种标点符号（句号、逗号、问号、感叹号），构建多样化训练语料并应用数据增强。

Result: 最佳模型在News测试集上准确率达97.1%，Reference集91.2%，ASR集90.2%，表现出强泛化能力。

Conclusion: 该研究为孟加拉语标点恢复建立了强基线，并公开了数据集和代码，支持低资源NLP的未来研究。

Abstract: Punctuation restoration enhances the readability of text and is critical for
post-processing tasks in Automatic Speech Recognition (ASR), especially for
low-resource languages like Bangla. In this study, we explore the application
of transformer-based models, specifically XLM-RoBERTa-large, to automatically
restore punctuation in unpunctuated Bangla text. We focus on predicting four
punctuation marks: period, comma, question mark, and exclamation mark across
diverse text domains. To address the scarcity of annotated resources, we
constructed a large, varied training corpus and applied data augmentation
techniques. Our best-performing model, trained with an augmentation factor of
alpha = 0.20%, achieves an accuracy of 97.1% on the News test set, 91.2% on the
Reference set, and 90.2% on the ASR set.
  Results show strong generalization to reference and ASR transcripts,
demonstrating the model's effectiveness in real-world, noisy scenarios. This
work establishes a strong baseline for Bangla punctuation restoration and
contributes publicly available datasets and code to support future research in
low-resource NLP.

</details>


### [59] [Generation of Synthetic Clinical Text: A Systematic Review](https://arxiv.org/abs/2507.18451)
*Basel Alshaikhdeeb,Ahmed Abdelmonem Hemedan,Soumyabrata Ghosh,Irina Balaur,Venkata Satagopam*

Main category: cs.CL

TL;DR: 本文系统综述了生成合成医学文本的研究，分析了其目的、技术和评估方法，发现其在增强数据、隐私保护等方面有潜力，但隐私问题仍需关注。


<details>
  <summary>Details</summary>
Motivation: 解决临床NLP中的稀疏性和隐私问题，通过生成合成医学文本来支持数据增强、辅助写作等应用。

Method: 系统综述了94篇相关文献，分析了生成合成医学文本的目的、技术和评估方法。

Result: Transformer架构（如GPT）是主要技术，评估以实用性为主；合成文本在NLP任务中表现良好，但隐私问题仍需解决。

Conclusion: 合成医学文本在数据增强和隐私保护方面有潜力，但需进一步解决隐私问题以加速实际应用。

Abstract: Generating clinical synthetic text represents an effective solution for
common clinical NLP issues like sparsity and privacy. This paper aims to
conduct a systematic review on generating synthetic medical free-text by
formulating quantitative analysis to three research questions concerning (i)
the purpose of generation, (ii) the techniques, and (iii) the evaluation
methods. We searched PubMed, ScienceDirect, Web of Science, Scopus, IEEE,
Google Scholar, and arXiv databases for publications associated with generating
synthetic medical unstructured free-text. We have identified 94 relevant
articles out of 1,398 collected ones. A great deal of attention has been given
to the generation of synthetic medical text from 2018 onwards, where the main
purpose of such a generation is towards text augmentation, assistive writing,
corpus building, privacy-preserving, annotation, and usefulness. Transformer
architectures were the main predominant technique used to generate the text,
especially the GPTs. On the other hand, there were four main aspects of
evaluation, including similarity, privacy, structure, and utility, where
utility was the most frequent method used to assess the generated synthetic
medical text. Although the generated synthetic medical text demonstrated a
moderate possibility to act as real medical documents in different downstream
NLP tasks, it has proven to be a great asset as augmented, complementary to the
real documents, towards improving the accuracy and overcoming
sparsity/undersampling issues. Yet, privacy is still a major issue behind
generating synthetic medical text, where more human assessments are needed to
check for the existence of any sensitive information. Despite that, advances in
generating synthetic medical text will considerably accelerate the adoption of
workflows and pipeline development, discarding the time-consuming legalities of
data transfer.

</details>


### [60] [Not All Features Deserve Attention: Graph-Guided Dependency Learning for Tabular Data Generation with Language Models](https://arxiv.org/abs/2507.18504)
*Zheyu Zhang,Shuo Yang,Bardh Prenkaj,Gjergji Kasneci*

Main category: cs.CL

TL;DR: GraDe是一种新方法，通过将稀疏依赖图集成到LLM的注意力机制中，优化表格数据生成，显著提升复杂数据集上的性能。


<details>
  <summary>Details</summary>
Motivation: LLMs在处理表格数据时，由于自注意力机制对所有特征对均匀分配注意力，导致关键依赖关系被稀释，尤其是复杂或语义模糊的数据集。

Method: GraDe通过轻量级动态图学习模块，结合外部提取的功能依赖关系，优先处理关键特征交互，抑制无关特征。

Result: 实验表明，GraDe在复杂数据集上比现有LLM方法性能提升高达12%，在合成数据质量上与最先进方法竞争。

Conclusion: GraDe是一种高效且实用的方法，为LLM的结构感知表格数据建模提供了解决方案。

Abstract: Large Language Models (LLMs) have shown strong potential for tabular data
generation by modeling textualized feature-value pairs. However, tabular data
inherently exhibits sparse feature-level dependencies, where many feature
interactions are structurally insignificant. This creates a fundamental
mismatch as LLMs' self-attention mechanism inevitably distributes focus across
all pairs, diluting attention on critical relationships, particularly in
datasets with complex dependencies or semantically ambiguous features. To
address this limitation, we propose GraDe (Graph-Guided Dependency Learning), a
novel method that explicitly integrates sparse dependency graphs into LLMs'
attention mechanism. GraDe employs a lightweight dynamic graph learning module
guided by externally extracted functional dependencies, prioritizing key
feature interactions while suppressing irrelevant ones. Our experiments across
diverse real-world datasets demonstrate that GraDe outperforms existing
LLM-based approaches by up to 12% on complex datasets while achieving
competitive results with state-of-the-art approaches in synthetic data quality.
Our method is minimally intrusive yet effective, offering a practical solution
for structure-aware tabular data modeling with LLMs.

</details>


### [61] [The Moral Gap of Large Language Models](https://arxiv.org/abs/2507.18523)
*Maciej Skorski,Alina Landowska*

Main category: cs.CL

TL;DR: 研究发现，尽管大型语言模型（LLMs）在多种任务中表现出色，但在道德推理任务中表现不佳，任务特定的微调模型仍优于提示工程。


<details>
  <summary>Details</summary>
Motivation: 道德基础检测对分析社会话语和开发符合伦理的AI系统至关重要，但LLMs在专业道德推理任务中的表现尚不明确。

Method: 研究首次全面比较了最先进的LLMs和微调Transformer模型，在Twitter和Reddit数据集上使用ROC、PR和DET曲线分析。

Result: 结果显示LLMs在道德内容检测中存在显著性能差距，表现为高假阴性率和系统性漏检，即使经过提示工程优化。

Conclusion: 任务特定的微调模型在道德推理应用中仍优于LLMs的提示工程方法。

Abstract: Moral foundation detection is crucial for analyzing social discourse and
developing ethically-aligned AI systems. While large language models excel
across diverse tasks, their performance on specialized moral reasoning remains
unclear.
  This study provides the first comprehensive comparison between
state-of-the-art LLMs and fine-tuned transformers across Twitter and Reddit
datasets using ROC, PR, and DET curve analysis.
  Results reveal substantial performance gaps, with LLMs exhibiting high false
negative rates and systematic under-detection of moral content despite prompt
engineering efforts. These findings demonstrate that task-specific fine-tuning
remains superior to prompting for moral reasoning applications.

</details>


### [62] [Effective Multi-Task Learning for Biomedical Named Entity Recognition](https://arxiv.org/abs/2507.18542)
*João Ruano,Gonçalo M. Correia,Leonor Barreiros,Afonso Mendes*

Main category: cs.CL

TL;DR: SRU-NER是一种新颖的命名实体识别方法，通过多任务学习和动态损失计算解决生物医学领域中的嵌套实体和数据集标注不一致问题。


<details>
  <summary>Details</summary>
Motivation: 生物医学命名实体识别因术语复杂性和数据集标注不一致而面临挑战。

Method: 提出SRU-NER方法，结合多任务学习和动态损失计算，处理嵌套实体并整合多数据集。

Result: SRU-NER在生物医学和通用领域NER任务中表现优异，并提升了跨领域泛化能力。

Conclusion: SRU-NER通过创新方法有效解决了生物医学NER中的关键问题，具有广泛应用潜力。

Abstract: Biomedical Named Entity Recognition presents significant challenges due to
the complexity of biomedical terminology and inconsistencies in annotation
across datasets. This paper introduces SRU-NER (Slot-based Recurrent Unit NER),
a novel approach designed to handle nested named entities while integrating
multiple datasets through an effective multi-task learning strategy. SRU-NER
mitigates annotation gaps by dynamically adjusting loss computation to avoid
penalizing predictions of entity types absent in a given dataset. Through
extensive experiments, including a cross-corpus evaluation and human assessment
of the model's predictions, SRU-NER achieves competitive performance in
biomedical and general-domain NER tasks, while improving cross-domain
generalization.

</details>


### [63] [GLiNER2: An Efficient Multi-Task Information Extraction System with Schema-Driven Interface](https://arxiv.org/abs/2507.18546)
*Urchade Zaratiana,Gil Pasternak,Oliver Boyd,George Hurn-Maloney,Ash Lewis*

Main category: cs.CL

TL;DR: GLiNER2是一个统一框架，支持命名实体识别、文本分类和层次结构化数据提取，具有高效性和CPU友好性。


<details>
  <summary>Details</summary>
Motivation: 解决现有信息提取方法需要专用模型或依赖计算成本高的大语言模型的问题。

Method: 基于预训练Transformer编码器架构，引入多任务组合和基于模式的接口。

Result: 在提取和分类任务中表现竞争力，部署便捷性显著优于基于LLM的替代方案。

Conclusion: GLiNER2是一个高效、多功能的开源工具，适用于多种NLP任务。

Abstract: Information extraction (IE) is fundamental to numerous NLP applications, yet
existing solutions often require specialized models for different tasks or rely
on computationally expensive large language models. We present GLiNER2, a
unified framework that enhances the original GLiNER architecture to support
named entity recognition, text classification, and hierarchical structured data
extraction within a single efficient model. Built pretrained transformer
encoder architecture, GLiNER2 maintains CPU efficiency and compact size while
introducing multi-task composition through an intuitive schema-based interface.
Our experiments demonstrate competitive performance across extraction and
classification tasks with substantial improvements in deployment accessibility
compared to LLM-based alternatives. We release GLiNER2 as an open-source
pip-installable library with pre-trained models and documentation at
https://github.com/fastino-ai/GLiNER2.

</details>


### [64] [GIIFT: Graph-guided Inductive Image-free Multimodal Machine Translation](https://arxiv.org/abs/2507.18562)
*Jiafeng Xiong,Yuting Zhao*

Main category: cs.CL

TL;DR: GIIFT框架通过构建多模态场景图和跨模态图注意力网络，在无需图像的情况下实现了多模态机器翻译的泛化能力，并在实验中表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有MMT方法在模态对齐和泛化能力上存在局限，需要一种新方法来更好地利用多模态信息并扩展到无图像领域。

Method: 构建多模态场景图，提出两阶段GIIFT框架，使用跨模态图注意力网络在统一融合空间中学习多模态知识。

Result: 在Multi30K和WMT基准测试中，GIIFT超越了现有方法，实现了最先进的性能。

Conclusion: GIIFT展示了在多模态机器翻译中的强大泛化能力，尤其在无图像推理场景下表现突出。

Abstract: Multimodal Machine Translation (MMT) has demonstrated the significant help of
visual information in machine translation. However, existing MMT methods face
challenges in leveraging the modality gap by enforcing rigid visual-linguistic
alignment whilst being confined to inference within their trained multimodal
domains. In this work, we construct novel multimodal scene graphs to preserve
and integrate modality-specific information and introduce GIIFT, a two-stage
Graph-guided Inductive Image-Free MMT framework that uses a cross-modal Graph
Attention Network adapter to learn multimodal knowledge in a unified fused
space and inductively generalize it to broader image-free translation domains.
Experimental results on the Multi30K dataset of English-to-French and
English-to-German tasks demonstrate that our GIIFT surpasses existing
approaches and achieves the state-of-the-art, even without images during
inference. Results on the WMT benchmark show significant improvements over the
image-free translation baselines, demonstrating the strength of GIIFT towards
inductive image-free inference.

</details>


### [65] [Hybrid Tokenization Strategy for DNA Language Model using Byte Pair Encoding and K-MER Methods](https://arxiv.org/abs/2507.18570)
*Ganesh Sapkota,Md Hasibur Rahman*

Main category: cs.CL

TL;DR: 提出了一种结合6-mer和BPE-600的混合分词策略，显著提升了DNA语言模型的性能。


<details>
  <summary>Details</summary>
Motivation: 传统k-mer分词在捕捉DNA局部结构时存在不足，如分词分布不均和全局上下文理解有限。

Method: 通过合并6-mer和BPE-600生成的词汇，构建平衡且上下文感知的混合词汇表。

Result: 模型在3-mer、4-mer和5-mer预测任务中表现优异，准确率分别为10.78%、10.1%和4.12%，优于现有模型。

Conclusion: 混合分词策略能同时捕捉DNA的局部和全局信息，为基因组建模和生物研究奠定基础。

Abstract: This paper presents a novel hybrid tokenization strategy that enhances the
performance of DNA Language Models (DLMs) by combining 6-mer tokenization with
Byte Pair Encoding (BPE-600). Traditional k-mer tokenization is effective at
capturing local DNA sequence structures but often faces challenges, including
uneven token distribution and a limited understanding of global sequence
context. To address these limitations, we propose merging unique 6mer tokens
with optimally selected BPE tokens generated through 600 BPE cycles. This
hybrid approach ensures a balanced and context-aware vocabulary, enabling the
model to capture both short and long patterns within DNA sequences
simultaneously. A foundational DLM trained on this hybrid vocabulary was
evaluated using next-k-mer prediction as a fine-tuning task, demonstrating
significantly improved performance. The model achieved prediction accuracies of
10.78% for 3-mers, 10.1% for 4-mers, and 4.12% for 5-mers, outperforming
state-of-the-art models such as NT, DNABERT2, and GROVER. These results
highlight the ability of the hybrid tokenization strategy to preserve both the
local sequence structure and global contextual information in DNA modeling.
This work underscores the importance of advanced tokenization methods in
genomic language modeling and lays a robust foundation for future applications
in downstream DNA sequence analysis and biological research.

</details>


### [66] [Wide-In, Narrow-Out: Revokable Decoding for Efficient and Effective DLLMs](https://arxiv.org/abs/2507.18578)
*Feng Hong,Geng Yu,Yushi Ye,Haicheng Huang,Huangjie Zheng,Ya Zhang,Yanfeng Wang,Jiangchao Yao*

Main category: cs.CL

TL;DR: WINO是一种无需训练的DLLM解码算法，通过并行草稿和验证机制，显著改善了质量与速度的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有DLLM在并行解码时存在质量与速度的严重权衡问题，标准解码的不可逆性导致早期错误积累。

Method: 提出WINO算法，采用并行草稿和验证机制，通过双向上下文验证并重新掩码可疑标记以优化解码。

Result: 在GSM8K数学基准上，推理速度提升6倍，准确率提高2.58%；在Flickr30K字幕任务中，速度提升10倍且性能更高。

Conclusion: WINO有效解决了DLLM的质量与速度权衡问题，实验验证了其优越性和实用性。

Abstract: Diffusion Large Language Models (DLLMs) have emerged as a compelling
alternative to Autoregressive models, designed for fast parallel generation.
However, existing DLLMs are plagued by a severe quality-speed trade-off, where
faster parallel decoding leads to significant performance degradation. We
attribute this to the irreversibility of standard decoding in DLLMs, which is
easily polarized into the wrong decoding direction along with early error
context accumulation. To resolve this, we introduce Wide-In, Narrow-Out (WINO),
a training-free decoding algorithm that enables revokable decoding in DLLMs.
WINO employs a parallel draft-and-verify mechanism, aggressively drafting
multiple tokens while simultaneously using the model's bidirectional context to
verify and re-mask suspicious ones for refinement. Verified in open-source
DLLMs like LLaDA and MMaDA, WINO is shown to decisively improve the
quality-speed trade-off. For instance, on the GSM8K math benchmark, it
accelerates inference by 6$\times$ while improving accuracy by 2.58%; on
Flickr30K captioning, it achieves a 10$\times$ speedup with higher performance.
More comprehensive experiments are conducted to demonstrate the superiority and
provide an in-depth understanding of WINO.

</details>


### [67] [AQuilt: Weaving Logic and Self-Inspection into Low-Cost, High-Relevance Data Synthesis for Specialist LLMs](https://arxiv.org/abs/2507.18584)
*Xiaopeng Ke,Hexuan Deng,Xuebo Liu,Jun Rao,Zhenxi Song,Jun Yu,Min Zhang*

Main category: cs.CL

TL;DR: AQuilt框架通过逻辑和自检机制，利用未标注数据构建领域专用指令调优数据，显著降低计算成本并提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 解决大语言模型在专业领域表现不佳的问题，同时降低现有数据合成方法的高计算成本和性能限制。

Method: 提出AQuilt框架，结合答案、问题、未标注数据、检查、逻辑和任务类型，生成高质量指令调优数据。

Result: 构建了703k样本的数据集，性能媲美DeepSeek-V3，但成本仅为17%，且数据与下游任务相关性更高。

Conclusion: AQuilt为专业领域提供了一种高效、低成本的数据合成方法，显著提升模型性能。

Abstract: Despite the impressive performance of large language models (LLMs) in general
domains, they often underperform in specialized domains. Existing approaches
typically rely on data synthesis methods and yield promising results by using
unlabeled data to capture domain-specific features. However, these methods
either incur high computational costs or suffer from performance limitations,
while also demonstrating insufficient generalization across different tasks. To
address these challenges, we propose AQuilt, a framework for constructing
instruction-tuning data for any specialized domains from corresponding
unlabeled data, including Answer, Question, Unlabeled data, Inspection, Logic,
and Task type. By incorporating logic and inspection, we encourage reasoning
processes and self-inspection to enhance model performance. Moreover,
customizable task instructions enable high-quality data generation for any
task. As a result, we construct a dataset of 703k examples to train a powerful
data synthesis model. Experiments show that AQuilt is comparable to DeepSeek-V3
while utilizing just 17% of the production cost. Further analysis demonstrates
that our generated data exhibits higher relevance to downstream tasks. Source
code, models, and scripts are available at https://github.com/Krueske/AQuilt.

</details>


### [68] [System Report for CCL25-Eval Task 10: SRAG-MAV for Fine-Grained Chinese Hate Speech Recognition](https://arxiv.org/abs/2507.18580)
*Jiahao Wang,Ramen Liu,Longhui Zhang,Jing Li*

Main category: cs.CL

TL;DR: 论文提出SRAG-MAV框架，用于细粒度中文仇恨言论识别，通过任务重构、自检索增强生成和多轮累积投票显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决细粒度中文仇恨言论识别任务，提升模型在复杂语境下的表现。

Method: 将四元组提取任务重构为三元组提取，结合动态检索生成上下文提示，并采用多轮推理和投票机制。

Result: 在STATE ToxiCN数据集上，Hard Score为26.66，Soft Score为48.35，显著优于基线模型。

Conclusion: SRAG-MAV框架有效提升了仇恨言论识别的性能，代码已开源。

Abstract: This paper presents our system for CCL25-Eval Task 10, addressing
Fine-Grained Chinese Hate Speech Recognition (FGCHSR). We propose a novel
SRAG-MAV framework that synergistically integrates task reformulation(TR),
Self-Retrieval-Augmented Generation (SRAG), and Multi-Round Accumulative Voting
(MAV). Our method reformulates the quadruplet extraction task into triplet
extraction, uses dynamic retrieval from the training set to create contextual
prompts, and applies multi-round inference with voting to improve output
stability and performance. Our system, based on the Qwen2.5-7B model, achieves
a Hard Score of 26.66, a Soft Score of 48.35, and an Average Score of 37.505 on
the STATE ToxiCN dataset, significantly outperforming baselines such as GPT-4o
(Average Score 15.63) and fine-tuned Qwen2.5-7B (Average Score 35.365). The
code is available at https://github.com/king-wang123/CCL25-SRAG-MAV.

</details>


### [69] [TRPrompt: Bootstrapping Query-Aware Prompt Optimization from Textual Rewards](https://arxiv.org/abs/2507.18618)
*Andreea Nica,Ivan Zakazov,Nicolas Mario Baldwin,Saibo Geng,Robert West*

Main category: cs.CL

TL;DR: TRPrompt框架通过将文本反馈直接融入提示模型的训练，统一了基于文本反馈和数值奖励的提示优化方法，无需预收集数据集，并在GSMHard和MATH数据集上实现了最先进的查询特定提示。


<details>
  <summary>Details</summary>
Motivation: 现有提示优化方法分为基于文本反馈和数值奖励的两类，但缺乏统一框架。TRPrompt旨在结合两者优势，直接利用文本反馈训练提示模型，提升大型语言模型的推理能力。

Method: 提出Textual Reward Prompt框架（TRPrompt），通过文本反馈迭代优化提示模型，无需预收集数据集，并利用LLM对“好”提示的理解能力。

Result: 在GSMHard和MATH数据集上，TRPrompt生成的查询特定提示达到了最先进的性能。

Conclusion: TRPrompt通过统一文本反馈与训练方法，显著提升了提示优化的效果，为LLM推理能力的提升提供了新方向。

Abstract: Prompt optimization improves the reasoning abilities of large language models
(LLMs) without requiring parameter updates to the target model. Following
heuristic-based "Think step by step" approaches, the field has evolved in two
main directions: while one group of methods uses textual feedback to elicit
improved prompts from general-purpose LLMs in a training-free way, a concurrent
line of research relies on numerical rewards to train a special prompt model,
tailored for providing optimal prompts to the target model. In this paper, we
introduce the Textual Reward Prompt framework (TRPrompt), which unifies these
approaches by directly incorporating textual feedback into training of the
prompt model. Our framework does not require prior dataset collection and is
being iteratively improved with the feedback on the generated prompts. When
coupled with the capacity of an LLM to internalize the notion of what a "good"
prompt is, the high-resolution signal provided by the textual rewards allows us
to train a prompt model yielding state-of-the-art query-specific prompts for
the problems from the challenging math datasets GSMHard and MATH.

</details>


### [70] [Checklists Are Better Than Reward Models For Aligning Language Models](https://arxiv.org/abs/2507.18624)
*Vijay Viswanathan,Yanchao Sun,Shuang Ma,Xiang Kong,Meng Cao,Graham Neubig,Tongshuang Wu*

Main category: cs.CL

TL;DR: 论文提出了一种基于检查表反馈的强化学习方法（RLCF），通过动态、指令特定的标准来提升语言模型对用户指令的理解和遵循能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法通常使用固定的标准（如“有帮助性”和“无害性”）来调整语言模型，限制了其适应多样化指令的能力。

Method: 从指令中提取检查表，结合AI评估和专用验证程序对响应进行评分，生成强化学习的奖励信号。

Result: RLCF在多个基准测试中表现优异，包括FollowBench、InFoBench和Arena-Hard，显著提升了模型性能。

Conclusion: 检查表反馈是提升语言模型支持多样化查询需求的关键工具。

Abstract: Language models must be adapted to understand and follow user instructions.
Reinforcement learning is widely used to facilitate this -- typically using
fixed criteria such as "helpfulness" and "harmfulness". In our work, we instead
propose using flexible, instruction-specific criteria as a means of broadening
the impact that reinforcement learning can have in eliciting instruction
following. We propose "Reinforcement Learning from Checklist Feedback" (RLCF).
From instructions, we extract checklists and evaluate how well responses
satisfy each item - using both AI judges and specialized verifier programs -
then combine these scores to compute rewards for RL. We compare RLCF with other
alignment methods applied to a strong instruction following model
(Qwen2.5-7B-Instruct) on five widely-studied benchmarks -- RLCF is the only
method to improve performance on every benchmark, including a 4-point boost in
hard satisfaction rate on FollowBench, a 6-point increase on InFoBench, and a
3-point rise in win rate on Arena-Hard. These results establish checklist
feedback as a key tool for improving language models' support of queries that
express a multitude of needs.

</details>
