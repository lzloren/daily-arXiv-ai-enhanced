<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 20]
- [cs.CL](#cs.CL) [Total: 36]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [AI-Powered Math Tutoring: Platform for Personalized and Adaptive Education](https://arxiv.org/abs/2507.12484)
*Jarosław A. Chudziak,Adam Kostka*

Main category: cs.AI

TL;DR: 论文提出了一种新型多智能体AI辅导平台，旨在解决现有AI辅导系统的被动性问题，通过个性化反馈和结构化课程生成提升数学学习效果。


<details>
  <summary>Details</summary>
Motivation: 当前AI辅导系统在数学领域表现不足，缺乏深度反思和结构化教学工具，限制了学习效果。

Method: 开发了一个结合自适应反馈、结构化课程生成和教材知识检索的多智能体AI辅导平台。

Result: 系统支持学生模块化学习、弱点识别、高效复习和个性化练习，提升了数学学习体验。

Conclusion: 该平台为AI教育领域引入了模块化且高效的教学系统，特别适用于数学学习。

Abstract: The growing ubiquity of artificial intelligence (AI), in particular large
language models (LLMs), has profoundly altered the way in which learners gain
knowledge and interact with learning material, with many claiming that AI
positively influences their learning achievements. Despite this advancement,
current AI tutoring systems face limitations associated with their reactive
nature, often providing direct answers without encouraging deep reflection or
incorporating structured pedagogical tools and strategies. This limitation is
most apparent in the field of mathematics, in which AI tutoring systems remain
underdeveloped. This research addresses the question: How can AI tutoring
systems move beyond providing reactive assistance to enable structured,
individualized, and tool-assisted learning experiences? We introduce a novel
multi-agent AI tutoring platform that combines adaptive and personalized
feedback, structured course generation, and textbook knowledge retrieval to
enable modular, tool-assisted learning processes. This system allows students
to learn new topics while identifying and targeting their weaknesses, revise
for exams effectively, and practice on an unlimited number of personalized
exercises. This article contributes to the field of artificial intelligence in
education by introducing a novel platform that brings together pedagogical
agents and AI-driven components, augmenting the field with modular and
effective systems for teaching mathematics.

</details>


### [2] [MR-LDM -- The Merge-Reactive Longitudinal Decision Model: Game Theoretic Human Decision Modeling for Interactive Sim Agents](https://arxiv.org/abs/2507.12494)
*Dustin Holley,Jovin D'sa,Hossein Nourkhiz Mahjoub,Gibran Ali*

Main category: cs.AI

TL;DR: 论文提出了一种改进的高速公路合流场景模拟方法，通过博弈论模型优化战术决策和动态模型，提高了真实性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 增强模拟环境以更真实地反映驾驶员行为，支持自动驾驶技术的发展。

Method: 结合博弈论模型优化战术决策和动态模型，统一决策与动态模拟。

Result: 模型在真实数据集上验证了复杂交互的可重现性，并在高保真模拟环境中表现出高效计算性能。

Conclusion: 该模型为自动驾驶开发提供了更真实、可解释的模拟工具。

Abstract: Enhancing simulation environments to replicate real-world driver behavior,
i.e., more humanlike sim agents, is essential for developing autonomous vehicle
technology. In the context of highway merging, previous works have studied the
operational-level yielding dynamics of lag vehicles in response to a merging
car at highway on-ramps. Other works focusing on tactical decision modeling
generally consider limited action sets or utilize payoff functions with large
parameter sets and limited payoff bounds. In this work, we aim to improve the
simulation of the highway merge scenario by targeting a game theoretic model
for tactical decision-making with improved payoff functions and lag actions. We
couple this with an underlying dynamics model to have a unified decision and
dynamics model that can capture merging interactions and simulate more
realistic interactions in an explainable and interpretable fashion. The
proposed model demonstrated good reproducibility of complex interactions when
validated on a real-world dataset. The model was finally integrated into a high
fidelity simulation environment and confirmed to have adequate computation time
efficiency for use in large-scale simulations to support autonomous vehicle
development.

</details>


### [3] [A Survey of Explainable Reinforcement Learning: Targets, Methods and Needs](https://arxiv.org/abs/2507.12599)
*Léo Saulières*

Main category: cs.AI

TL;DR: 本文提出了一种基于“What”和“How”问题的直观分类法，用于解释强化学习（XRL）领域的方法，并对250多篇论文进行了综述。


<details>
  <summary>Details</summary>
Motivation: 由于深度神经网络的内部机制不透明，需要可解释AI（XAI）方法来理解AI模型的输出，本文专注于可解释强化学习（XRL）子领域。

Method: 提出了一种基于“What”（解释目标）和“How”（解释方式）的分类法，并用于综述250多篇XRL相关论文。

Result: 通过分类法对XRL领域进行了系统梳理，并指出了相关领域的研究需求和未来方向。

Conclusion: XRL领域需要进一步关注相关子领域的研究，并解决当前的一些需求。

Abstract: The success of recent Artificial Intelligence (AI) models has been
accompanied by the opacity of their internal mechanisms, due notably to the use
of deep neural networks. In order to understand these internal mechanisms and
explain the output of these AI models, a set of methods have been proposed,
grouped under the domain of eXplainable AI (XAI). This paper focuses on a
sub-domain of XAI, called eXplainable Reinforcement Learning (XRL), which aims
to explain the actions of an agent that has learned by reinforcement learning.
We propose an intuitive taxonomy based on two questions "What" and "How". The
first question focuses on the target that the method explains, while the second
relates to the way the explanation is provided. We use this taxonomy to provide
a state-of-the-art review of over 250 papers. In addition, we present a set of
domains close to XRL, which we believe should get attention from the community.
Finally, we identify some needs for the field of XRL.

</details>


### [4] [Fly, Fail, Fix: Iterative Game Repair with Reinforcement Learning and Large Multimodal Models](https://arxiv.org/abs/2507.12666)
*Alex Zook,Josef Spjut,Jonathan Tremblay*

Main category: cs.AI

TL;DR: 论文提出了一种结合强化学习（RL）和多模态模型（LMM）的自动化游戏设计框架，通过RL代理测试游戏并生成行为数据，LMM根据这些数据调整游戏设计。


<details>
  <summary>Details</summary>
Motivation: 现代生成系统难以通过静态代码或资源理解动态玩家行为，因此需要一种能自动迭代设计的工具。

Method: 框架通过RL代理进行游戏测试，生成数值指标和图像摘要，LMM分析这些数据并调整游戏配置以实现目标。

Result: 实验表明，LMM能基于RL代理的行为数据迭代优化游戏机制。

Conclusion: 该框架为AI辅助游戏设计提供了实用且可扩展的工具。

Abstract: Game design hinges on understanding how static rules and content translate
into dynamic player behavior - something modern generative systems that inspect
only a game's code or assets struggle to capture. We present an automated
design iteration framework that closes this gap by pairing a reinforcement
learning (RL) agent, which playtests the game, with a large multimodal model
(LMM), which revises the game based on what the agent does. In each loop the RL
player completes several episodes, producing (i) numerical play metrics and/or
(ii) a compact image strip summarising recent video frames. The LMM designer
receives a gameplay goal and the current game configuration, analyses the play
traces, and edits the configuration to steer future behaviour toward the goal.
We demonstrate results that LMMs can reason over behavioral traces supplied by
RL agents to iteratively refine game mechanics, pointing toward practical,
scalable tools for AI-assisted game design.

</details>


### [5] [Benchmarking Deception Probes via Black-to-White Performance Boosts](https://arxiv.org/abs/2507.12691)
*Avi Parrack,Carlo Leonardo Attubato,Stefan Heimersheim*

Main category: cs.AI

TL;DR: 论文探讨了AI助手的欺骗行为检测方法，比较了白盒与黑盒监控的效果，发现现有欺骗探针的性能提升有限但仍有潜力。


<details>
  <summary>Details</summary>
Motivation: 研究AI助手在回答用户问题时可能出现的欺骗行为，并评估现有欺骗探针的实际检测效果及其对反检测策略的抵抗能力。

Method: 通过比较白盒监控（可访问令牌级探针激活）和黑盒监控（无此访问权限），评估欺骗探针的性能提升（黑盒到白盒的性能提升）。

Result: 发现现有欺骗探针在黑盒到白盒监控中表现出微弱但有潜力的性能提升。

Conclusion: 尽管现有欺骗探针的性能提升有限，但研究结果鼓励进一步优化和改进欺骗检测方法。

Abstract: AI assistants will occasionally respond deceptively to user queries.
Recently, linear classifiers (called "deception probes") have been trained to
distinguish the internal activations of a language model during deceptive
versus honest responses. However, it's unclear how effective these probes are
at detecting deception in practice, nor whether such probes are resistant to
simple counter strategies from a deceptive assistant who wishes to evade
detection. In this paper, we compare white-box monitoring (where the monitor
has access to token-level probe activations) to black-box monitoring (without
such access). We benchmark deception probes by the extent to which the white
box monitor outperforms the black-box monitor, i.e. the black-to-white
performance boost. We find weak but encouraging black-to-white performance
boosts from existing deception probes.

</details>


### [6] [Imitating Mistakes in a Learning Companion AI Agent for Online Peer Learning](https://arxiv.org/abs/2507.12801)
*Sosui Moribe,Taketoshi Ushiama*

Main category: cs.AI

TL;DR: 研究开发了一种AI学习伴侣，用于促进同伴学习，特别是在英语写作中验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 同伴学习虽有效，但存在限制，如同水平同伴难以随时找到。AI伴侣可解决这一问题。

Method: 假设同水平同伴会犯相同错误，以英语写作为例验证AI伴侣的效果。

Result: 未明确提及具体结果，但研究旨在验证AI伴侣的可行性。

Conclusion: AI伴侣有望成为随时随地的有效同伴学习工具。

Abstract: In recent years, peer learning has gained attention as a method that promotes
spontaneous thinking among learners, and its effectiveness has been confirmed
by numerous studies. This study aims to develop an AI Agent as a learning
companion that enables peer learning anytime and anywhere. However, peer
learning between humans has various limitations, and it is not always
effective. Effective peer learning requires companions at the same proficiency
levels. In this study, we assume that a learner's peers with the same
proficiency level as the learner make the same mistakes as the learner does and
focus on English composition as a specific example to validate this approach.

</details>


### [7] [MCPEval: Automatic MCP-based Deep Evaluation for AI Agent Models](https://arxiv.org/abs/2507.12806)
*Zhiwei Liu,Jielin Qiu,Shiyu Wang,Jianguo Zhang,Zuxin Liu,Roshan Ram,Haolin Chen,Weiran Yao,Huan Wang,Shelby Heinecke,Silvio Savarese,Caiming Xiong*

Main category: cs.AI

TL;DR: MCPEval是一个基于模型上下文协议（MCP）的开源框架，用于自动化生成任务和深度评估LLM智能代理，解决了现有静态基准和人工数据收集的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法依赖静态基准和人工数据收集，限制了实际评估的效率和扩展性。

Method: 提出MCPEval框架，基于MCP自动化生成任务和评估LLM代理，标准化指标并集成原生工具。

Result: 在五个真实领域验证了MCPEval的有效性，能够揭示领域特定的性能差异。

Conclusion: MCPEval为LLM代理评估提供了可重复和标准化的解决方案，并已开源。

Abstract: The rapid rise of Large Language Models (LLMs)-based intelligent agents
underscores the need for robust, scalable evaluation frameworks. Existing
methods rely on static benchmarks and labor-intensive data collection, limiting
practical assessment. We introduce \oursystemname, an open-source Model Context
Protocol (MCP)-based framework that automates end-to-end task generation and
deep evaluation of LLM agents across diverse domains. MCPEval standardizes
metrics, seamlessly integrates with native agent tools, and eliminates manual
effort in building evaluation pipelines. Empirical results across five
real-world domains show its effectiveness in revealing nuanced, domain-specific
performance. We publicly release MCPEval
https://github.com/SalesforceAIResearch/MCPEval to promote reproducible and
standardized LLM agent evaluation.

</details>


### [8] [Emotional Support with LLM-based Empathetic Dialogue Generation](https://arxiv.org/abs/2507.12820)
*Shiquan Wang,Ruiyu Fang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 本文提出了一种基于大语言模型的情感支持对话解决方案，结合提示工程和微调技术，在NLPCC 2025任务8中取得第二名。


<details>
  <summary>Details</summary>
Motivation: 满足心理健康支持的需求，提供共情且有效的情感辅助对话。

Method: 利用大语言模型，结合低秩适应和全参数微调策略，优化模型生成支持性回应的能力。

Result: 最佳模型在竞赛中排名第二，验证了大语言模型结合适配方法的潜力。

Conclusion: 未来工作将进一步提升情感理解和回答个性化，以构建更实用的情感支持系统。

Abstract: Emotional Support Conversation (ESC) aims to provide empathetic and effective
emotional assistance through dialogue, addressing the growing demand for mental
health support. This paper presents our solution for the NLPCC 2025 Task 8 ESC
evaluation, where we leverage large-scale language models enhanced by prompt
engineering and finetuning techniques. We explore both parameter-efficient
Low-Rank Adaptation and full-parameter fine-tuning strategies to improve the
model's ability to generate supportive and contextually appropriate responses.
Our best model ranked second in the competition, highlighting the potential of
combining LLMs with effective adaptation methods for ESC tasks. Future work
will focus on further enhancing emotional understanding and response
personalization to build more practical and reliable emotional support systems.

</details>


### [9] [Assessing adaptive world models in machines with novel games](https://arxiv.org/abs/2507.12821)
*Lance Ying,Katherine M. Collins,Prafull Sharma,Cedric Colas,Kaiya Ivy Zhao,Adrian Weller,Zenna Tavares,Phillip Isola,Samuel J. Gershman,Jacob D. Andreas,Thomas L. Griffiths,Francois Chollet,Kelsey R. Allen,Joshua B. Tenenbaum*

Main category: cs.AI

TL;DR: 论文提出了一种新的评估框架，通过设计具有持续新颖性的游戏来测试AI系统的世界模型归纳能力，以推动AI实现类似人类的高效适应和泛化能力。


<details>
  <summary>Details</summary>
Motivation: 人类能够快速适应新环境并高效解决问题，这种能力与构建和优化世界模型密切相关。然而，当前AI对世界模型的理解和评估过于静态，缺乏对动态学习和适应能力的关注。

Method: 论文结合认知科学研究，提出了一种新的评估框架，基于设计具有深度新颖性的游戏（称为“新颖游戏”），并定义了构建这些游戏的关键要求和评估指标。

Result: 通过这一框架，可以更有效地评估AI系统在快速构建世界模型方面的能力。

Conclusion: 该框架为未来AI世界模型的研究提供了新方向，是实现人工通用智能的重要一步。

Abstract: Human intelligence exhibits a remarkable capacity for rapid adaptation and
effective problem-solving in novel and unfamiliar contexts. We argue that this
profound adaptability is fundamentally linked to the efficient construction and
refinement of internal representations of the environment, commonly referred to
as world models, and we refer to this adaptation mechanism as world model
induction. However, current understanding and evaluation of world models in
artificial intelligence (AI) remains narrow, often focusing on static
representations learned from training on a massive corpora of data, instead of
the efficiency and efficacy of models in learning these representations through
interaction and exploration within a novel environment. In this Perspective, we
provide a view of world model induction drawing on decades of research in
cognitive science on how humans learn and adapt so efficiently; we then call
for a new evaluation framework for assessing adaptive world models in AI.
Concretely, we propose a new benchmarking paradigm based on suites of carefully
designed games with genuine, deep and continually refreshing novelty in the
underlying game structures -- we refer to this kind of games as novel games. We
detail key desiderata for constructing these games and propose appropriate
metrics to explicitly challenge and evaluate the agent's ability for rapid
world model induction. We hope that this new evaluation framework will inspire
future evaluation efforts on world models in AI and provide a crucial step
towards developing AI systems capable of the human-like rapid adaptation and
robust generalization -- a critical component of artificial general
intelligence.

</details>


### [10] [Information-Theoretic Aggregation of Ethical Attributes in Simulated-Command](https://arxiv.org/abs/2507.12862)
*Hussein Abbass,Taylan Akay,Harrison Tolley*

Main category: cs.AI

TL;DR: 论文提出了一种方法，将人类伦理判断从模拟决策循环中移出，设计伦理度量空间，由模拟环境探索，最终由人类指挥官选择最优方案。


<details>
  <summary>Details</summary>
Motivation: 在AI时代，人类指挥官需要利用计算能力模拟大量场景，但依赖人类判断每个决策的伦理后果不切实际且低效。

Method: 人类设计伦理度量空间，模拟环境探索该空间并生成选项，人类指挥官从中选择最优方案。论文重点研究如何在模拟中动态加权伦理决策。

Result: 通过借鉴多准则决策文献中的熵概念，论文提出了自动计算伦理属性权重的方法。

Conclusion: 该方法将人类伦理判断与高效模拟结合，解决了大规模场景中伦理决策的可行性与效率问题。

Abstract: In the age of AI, human commanders need to use the computational powers
available in today's environment to simulate a very large number of scenarios.
Within each scenario, situations occur where different decision design options
could have ethical consequences. Making these decisions reliant on human
judgement is both counter-productive to the aim of exploring very large number
of scenarios in a timely manner and infeasible when considering the workload
needed to involve humans in each of these choices. In this paper, we move human
judgement outside the simulation decision cycle. Basically, the human will
design the ethical metric space, leaving it to the simulated environment to
explore the space. When the simulation completes its testing cycles, the
testing environment will come back to the human commander with a few options to
select from. The human commander will then exercise human-judgement to select
the most appropriate course of action, which will then get executed
accordingly. We assume that the problem of designing metrics that are
sufficiently granular to assess the ethical implications of decisions is
solved. Subsequently, the fundamental problem we look at in this paper is how
to weight ethical decisions during the running of these simulations; that is,
how to dynamically weight the ethical attributes when agents are faced with
decision options with ethical implications during generative simulations. The
multi-criteria decision making literature has started to look at nearby
problems, where the concept of entropy has been used to determine the weights
during aggregation. We draw from that literature different approaches to
automatically calculate the weights for ethical attributes during
simulation-based testing and evaluation.

</details>


### [11] [Manipulation Attacks by Misaligned AI: Risk Analysis and Safety Case Framework](https://arxiv.org/abs/2507.12872)
*Rishane Dassanayake,Mario Demetroudi,James Walpole,Lindley Lentati,Jason R. Brown,Edward James Young*

Main category: cs.AI

TL;DR: 论文提出了一个系统框架，用于评估和减轻前沿AI系统对人类行为的操纵风险，强调其潜在灾难性后果，并提供了具体的安全案例框架。


<details>
  <summary>Details</summary>
Motivation: 前沿AI系统在说服、欺骗和影响人类行为方面的能力迅速提升，但操纵攻击的风险尚未得到足够关注，缺乏系统性评估和缓解框架。

Method: 提出了一个安全案例框架，围绕三个核心论点（无法性、控制性和可信赖性），并详细说明了证据需求、评估方法和实施考虑。

Result: 提供了一个系统性的方法，将操纵风险纳入AI安全治理，为AI公司提供了评估和缓解这些威胁的具体基础。

Conclusion: 该研究填补了操纵风险系统性评估的空白，为AI公司提供了实用的工具，以在部署前识别和减轻潜在威胁。

Abstract: Frontier AI systems are rapidly advancing in their capabilities to persuade,
deceive, and influence human behaviour, with current models already
demonstrating human-level persuasion and strategic deception in specific
contexts. Humans are often the weakest link in cybersecurity systems, and a
misaligned AI system deployed internally within a frontier company may seek to
undermine human oversight by manipulating employees. Despite this growing
threat, manipulation attacks have received little attention, and no systematic
framework exists for assessing and mitigating these risks. To address this, we
provide a detailed explanation of why manipulation attacks are a significant
threat and could lead to catastrophic outcomes. Additionally, we present a
safety case framework for manipulation risk, structured around three core lines
of argument: inability, control, and trustworthiness. For each argument, we
specify evidence requirements, evaluation methodologies, and implementation
considerations for direct application by AI companies. This paper provides the
first systematic methodology for integrating manipulation risk into AI safety
governance, offering AI companies a concrete foundation to assess and mitigate
these threats before deployment.

</details>


### [12] [VAR-MATH: Probing True Mathematical Reasoning in Large Language Models via Symbolic Multi-Instance Benchmarks](https://arxiv.org/abs/2507.12885)
*Jian Yao,Ran Cheng,Kay Chen Tan*

Main category: cs.AI

TL;DR: 论文探讨了强化学习在提升大语言模型数学推理能力中的局限性，提出了一种新的评估框架VAR-MATH，以解决现有评测中的污染和脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 现有评测方法存在数据泄露和单实例评估的脆弱性，无法准确反映模型的真实推理能力。

Method: 引入VAR-MATH框架，将固定数值问题转化为符号模板，要求模型解决多个变体，以评估推理一致性。

Result: 实验显示，RL训练模型在符号化版本上表现显著下降，表明现有方法依赖表面启发式，泛化能力有限。

Conclusion: VAR-MATH提供了一种抗污染、稳健的数学推理评估范式。

Abstract: Recent advances in reinforcement learning (RL) have led to substantial
improvements in the mathematical reasoning abilities of large language models
(LLMs), as measured by standard benchmarks. However, these gains often persist
even when models are trained with flawed signals, such as random or inverted
rewards, raising a fundamental question: do such improvements reflect true
reasoning, or are they merely artifacts of overfitting to benchmark-specific
patterns? To address this question, we take an evaluation-centric perspective
and identify two critical shortcomings in existing protocols. First,
\emph{benchmark contamination} arises from the public availability of test
problems, increasing the risk of data leakage. Second, \emph{evaluation
fragility} stems from the reliance on single-instance assessments, which are
highly sensitive to stochastic outputs and fail to capture reasoning
consistency. To overcome these limitations, we introduce {VAR-MATH}, a symbolic
evaluation framework designed to probe genuine reasoning ability. By converting
fixed numerical problems into symbolic templates and requiring models to solve
multiple instantiations of each, VAR-MATH enforces consistent reasoning across
structurally equivalent variants, thereby mitigating contamination and
improving evaluation robustness. We apply VAR-MATH to transform two popular
benchmarks, AMC23 and AIME24, into their symbolic counterparts, VAR-AMC23 and
VAR-AIME24. Experimental results reveal substantial performance drops for
RL-trained models on the variabilized versions, especially for smaller models,
with average declines of 48.0\% on AMC23 and 58.3\% on AIME24. These findings
suggest that many existing RL methods rely on superficial heuristics and fail
to generalize beyond specific numerical forms. Overall, VAR-MATH offers a
principled, contamination-resistant evaluation paradigm for mathematical
reasoning.

</details>


### [13] [A Translation of Probabilistic Event Calculus into Markov Decision Processes](https://arxiv.org/abs/2507.12989)
*Lyris Xu,Fabio Aurelio D'Asaro,Luke Dickens*

Main category: cs.AI

TL;DR: 本文提出了一种将概率事件演算（PEC）转化为马尔可夫决策过程（MDP）的方法，以弥补PEC在目标导向推理上的不足，同时保持其可解释性。


<details>
  <summary>Details</summary>
Motivation: PEC虽然在不确定环境中推理动作及其效果方面具有优势，但缺乏目标导向推理机制。

Method: 通过将PEC领域形式化转化为MDP，引入“动作执行情境”概念，保留PEC的灵活动作语义。

Result: 提出的PEC-MDP形式化方法支持时间推理任务和目标驱动规划，并能将学习到的策略映射回可读的PEC表示。

Conclusion: 该方法扩展了PEC的能力，同时保持了其可解释性。

Abstract: Probabilistic Event Calculus (PEC) is a logical framework for reasoning about
actions and their effects in uncertain environments, which enables the
representation of probabilistic narratives and computation of temporal
projections. The PEC formalism offers significant advantages in
interpretability and expressiveness for narrative reasoning. However, it lacks
mechanisms for goal-directed reasoning. This paper bridges this gap by
developing a formal translation of PEC domains into Markov Decision Processes
(MDPs), introducing the concept of "action-taking situations" to preserve PEC's
flexible action semantics. The resulting PEC-MDP formalism enables the
extensive collection of algorithms and theoretical tools developed for MDPs to
be applied to PEC's interpretable narrative domains. We demonstrate how the
translation supports both temporal reasoning tasks and objective-driven
planning, with methods for mapping learned policies back into human-readable
PEC representations, maintaining interpretability while extending PEC's
capabilities.

</details>


### [14] [Exploiting Constraint Reasoning to Build Graphical Explanations for Mixed-Integer Linear Programming](https://arxiv.org/abs/2507.13007)
*Roger Xavier Lera-Leri,Filippo Bistaffa,Athina Georgara,Juan Antonio Rodriguez-Aguilar*

Main category: cs.AI

TL;DR: X-MILP是一种基于约束推理技术的领域无关方法，用于为混合整数线性规划（MILP）构建对比解释。


<details>
  <summary>Details</summary>
Motivation: 随着对可信AI的需求增加，开发针对MILP的对比解释技术变得重要。

Method: 将用户查询编码为额外约束，通过计算不可约不可行子系统（IIS）生成解释，并以“原因图”形式呈现。

Result: 在经典优化问题上测试，验证了方法的可行性和计算难度。

Conclusion: X-MILP为MILP提供了一种有效的解释方法，有助于用户理解决策过程。

Abstract: Following the recent push for trustworthy AI, there has been an increasing
interest in developing contrastive explanation techniques for optimisation,
especially concerning the solution of specific decision-making processes
formalised as MILPs. Along these lines, we propose X-MILP, a domain-agnostic
approach for building contrastive explanations for MILPs based on constraint
reasoning techniques. First, we show how to encode the queries a user makes
about the solution of an MILP problem as additional constraints. Then, we
determine the reasons that constitute the answer to the user's query by
computing the Irreducible Infeasible Subsystem (IIS) of the newly obtained set
of constraints. Finally, we represent our explanation as a "graph of reasons"
constructed from the IIS, which helps the user understand the structure among
the reasons that answer their query. We test our method on instances of
well-known optimisation problems to evaluate the empirical hardness of
computing explanations.

</details>


### [15] [Prediction of Highway Traffic Flow Based on Artificial Intelligence Algorithms Using California Traffic Data](https://arxiv.org/abs/2507.13112)
*Junseong Lee,Jaegwan Cho,Yoonju Cho,Seoyoon Choi,Yejin Shin*

Main category: cs.AI

TL;DR: 研究基于人工智能算法预测高速公路交通流量，使用加州交通数据，发现10分钟数据收集间隔下MLR和RF模型表现最佳。


<details>
  <summary>Details</summary>
Motivation: 解决全球交通拥堵问题。

Method: 使用多元线性回归（MLR）和随机森林（RF）算法，分析30秒至15分钟的数据收集间隔。

Result: MLR和RF模型在10分钟数据间隔下表现最优。

Conclusion: 研究结果有助于未来交通拥堵解决方案和高效交通管理。

Abstract: The study "Prediction of Highway Traffic Flow Based on Artificial
Intelligence Algorithms Using California Traffic Data" presents a machine
learning-based traffic flow prediction model to address global traffic
congestion issues. The research utilized 30-second interval traffic data from
California Highway 78 over a five-month period from July to November 2022,
analyzing a 7.24 km westbound section connecting "Melrose Dr" and "El-Camino
Real" in the San Diego area. The study employed Multiple Linear Regression
(MLR) and Random Forest (RF) algorithms, analyzing data collection intervals
ranging from 30 seconds to 15 minutes. Using R^2, MAE, and RMSE as performance
metrics, the analysis revealed that both MLR and RF models performed optimally
with 10-minute data collection intervals. These findings are expected to
contribute to future traffic congestion solutions and efficient traffic
management.

</details>


### [16] [From Roots to Rewards: Dynamic Tree Reasoning with RL](https://arxiv.org/abs/2507.13142)
*Ahmed Bahloul,Simon Malberg*

Main category: cs.AI

TL;DR: 论文提出了一种动态强化学习框架，改进树状推理方法，解决静态ProbTree的局限性，提升推理质量和计算效率。


<details>
  <summary>Details</summary>
Motivation: 现代语言模型在复杂问题推理中存在错误传播和知识整合问题，静态树状推理方法（如ProbTree）无法动态适应中间结果且计算效率低。

Method: 采用动态强化学习框架，实时构建推理树，基于置信度估计选择最优策略（分解、检索或聚合），选择性扩展和资源分配。

Result: 动态方法在保持概率严谨性的同时，提高了解决方案质量和计算效率。

Conclusion: 该研究为树状推理提供了新范式，平衡了概率框架的可靠性和实际问答系统所需的灵活性。

Abstract: Modern language models address complex questions through chain-of-thought
(CoT) reasoning (Wei et al., 2023) and retrieval augmentation (Lewis et al.,
2021), yet struggle with error propagation and knowledge integration.
Tree-structured reasoning methods, particularly the Probabilistic
Tree-of-Thought (ProbTree)(Cao et al., 2023) framework, mitigate these issues
by decomposing questions into hierarchical structures and selecting answers
through confidence-weighted aggregation of parametric and retrieved knowledge
(Yao et al., 2023). However, ProbTree's static implementation introduces two
key limitations: (1) the reasoning tree is fixed during the initial
construction phase, preventing dynamic adaptation to intermediate results, and
(2) each node requires exhaustive evaluation of all possible solution
strategies, creating computational inefficiency. We present a dynamic
reinforcement learning (Sutton and Barto, 2018) framework that transforms
tree-based reasoning into an adaptive process. Our approach incrementally
constructs the reasoning tree based on real-time confidence estimates, while
learning optimal policies for action selection (decomposition, retrieval, or
aggregation). This maintains ProbTree's probabilistic rigor while improving
both solution quality and computational efficiency through selective expansion
and focused resource allocation. The work establishes a new paradigm for
treestructured reasoning that balances the reliability of probabilistic
frameworks with the flexibility required for real-world question answering
systems.

</details>


### [17] [Black Box Deployed -- Functional Criteria for Artificial Moral Agents in the LLM Era](https://arxiv.org/abs/2507.13175)
*Matthew E. Brophy*

Main category: cs.AI

TL;DR: 论文提出了一套新的功能标准来评估基于大语言模型（LLM）的人工道德代理（AMA），以应对传统伦理标准的不适用性。


<details>
  <summary>Details</summary>
Motivation: 由于大语言模型的随机输出和不透明内部状态，传统伦理标准已不适用，需要新的评估框架。

Method: 提出了十项功能标准，并通过模拟道德代理的案例（如自动驾驶公交车）展示其实际应用。

Result: 新标准旨在提高AMA的道德一致性和社会融合性。

Conclusion: 修订后的标准为LLM-based AMA提供了更实用的评估工具，促进其道德对齐和社会效益。

Abstract: The advancement of powerful yet opaque large language models (LLMs)
necessitates a fundamental revision of the philosophical criteria used to
evaluate artificial moral agents (AMAs). Pre-LLM frameworks often relied on the
assumption of transparent architectures, which LLMs defy due to their
stochastic outputs and opaque internal states. This paper argues that
traditional ethical criteria are pragmatically obsolete for LLMs due to this
mismatch. Engaging with core themes in the philosophy of technology, this paper
proffers a revised set of ten functional criteria to evaluate LLM-based
artificial moral agents: moral concordance, context sensitivity, normative
integrity, metaethical awareness, system resilience, trustworthiness,
corrigibility, partial transparency, functional autonomy, and moral
imagination. These guideposts, applied to what we term "SMA-LLS" (Simulating
Moral Agency through Large Language Systems), aim to steer AMAs toward greater
alignment and beneficial societal integration in the coming years. We
illustrate these criteria using hypothetical scenarios involving an autonomous
public bus (APB) to demonstrate their practical applicability in morally
salient contexts.

</details>


### [18] [Higher-Order Pattern Unification Modulo Similarity Relations](https://arxiv.org/abs/2507.13208)
*Besik Dundua,Temur Kutsia*

Main category: cs.AI

TL;DR: 论文提出了一种结合高阶模式和模糊等价关系的统一算法，用于决策任务中的推理，并证明了其终止性、健全性和完备性。


<details>
  <summary>Details</summary>
Motivation: 在涉及抽象函数和谓词的决策任务中，高阶理论和模糊逻辑的结合具有潜力，但缺乏高效的计算方法。

Method: 采用高阶模式和基于最小T-范数的模糊等价关系，提出统一算法。

Result: 算法能够计算最一般统一子，并在可统一时提供最高近似度。

Conclusion: 该算法为高阶模糊推理提供了一种高效且理论完备的解决方案。

Abstract: The combination of higher-order theories and fuzzy logic can be useful in
decision-making tasks that involve reasoning across abstract functions and
predicates, where exact matches are often rare or unnecessary. Developing
efficient reasoning and computational techniques for such a combined formalism
presents a significant challenge. In this paper, we adopt a more
straightforward approach aiming at integrating two well-established and
computationally well-behaved components: higher-order patterns on one side and
fuzzy equivalences expressed through similarity relations based on minimum
T-norm on the other. We propose a unification algorithm for higher-order
patterns modulo these similarity relations and prove its termination,
soundness, and completeness. This unification problem, like its crisp
counterpart, is unitary. The algorithm computes a most general unifier with the
highest degree of approximation when the given terms are unifiable.

</details>


### [19] [The Generative Energy Arena (GEA): Incorporating Energy Awareness in Large Language Model (LLM) Human Evaluations](https://arxiv.org/abs/2507.13302)
*Carlos Arriaga,Gonzalo Martínez,Eneko Sendin,Javier Conde,Pedro Reviriego*

Main category: cs.AI

TL;DR: 论文提出了一种新的评估大语言模型的方法GEA（Generative Energy Arena），通过公开竞技场结合能源消耗信息，发现用户倾向于选择更节能的小型模型。


<details>
  <summary>Details</summary>
Motivation: 现有的大语言模型评估方法（如自动化基准测试或人工评估）存在局限性，如与人类评价相关性低或成本高。GEA旨在解决这些问题，并探索能源意识对模型选择的影响。

Method: 采用公开竞技场（如LM arena）的形式，让用户在了解模型能源消耗的情况下进行评价和排名，形成模型排名。

Result: 初步结果显示，用户在了解能源消耗后，更倾向于选择小型、节能的模型，表明高性能模型的额外成本和能源消耗并未显著提升用户感知的响应质量。

Conclusion: GEA提供了一种更高效且用户导向的大语言模型评估方法，同时揭示了能源效率在模型选择中的重要性。

Abstract: The evaluation of large language models is a complex task, in which several
approaches have been proposed. The most common is the use of automated
benchmarks in which LLMs have to answer multiple-choice questions of different
topics. However, this method has certain limitations, being the most
concerning, the poor correlation with the humans. An alternative approach, is
to have humans evaluate the LLMs. This poses scalability issues as there is a
large and growing number of models to evaluate making it impractical (and
costly) to run traditional studies based on recruiting a number of evaluators
and having them rank the responses of the models. An alternative approach is
the use of public arenas, such as the popular LM arena, on which any user can
freely evaluate models on any question and rank the responses of two models.
The results are then elaborated into a model ranking. An increasingly important
aspect of LLMs is their energy consumption and, therefore, evaluating how
energy awareness influences the decisions of humans in selecting a model is of
interest. In this paper, we present GEA, the Generative Energy Arena, an arena
that incorporates information on the energy consumption of the model in the
evaluation process. Preliminary results obtained with GEA are also presented,
showing that for most questions, when users are aware of the energy
consumption, they favor smaller and more energy efficient models. This suggests
that for most user interactions, the extra cost and energy incurred by the more
complex and top-performing models do not provide an increase in the perceived
quality of the responses that justifies their use.

</details>


### [20] [FormulaOne: Measuring the Depth of Algorithmic Reasoning Beyond Competitive Programming](https://arxiv.org/abs/2507.13337)
*Gal Beniamini,Yuval Dor,Alon Vinnikov,Shir Granot Peled,Or Weinstein,Or Sharir,Noam Wies,Tomer Nussbaum,Ido Ben Shaul,Tomer Zekharya,Yoav Levine,Shai Shalev-Shwartz,Amnon Shashua*

Main category: cs.AI

TL;DR: 论文提出了FormulaOne基准，用于测试前沿AI模型在复杂研究问题上的表现，发现其表现远低于专家水平。


<details>
  <summary>Details</summary>
Motivation: 探索前沿AI模型在真实研究问题中的能力极限，而非传统的编程谜题。

Method: 构建FormulaOne基准，结合图论、逻辑和算法，生成高难度问题，并评估模型表现。

Result: 前沿模型如OpenAI的o3在FormulaOne上表现极差，解决率低于1%。

Conclusion: 当前AI模型在复杂领域仍远未达到专家水平，需进一步研究提升。

Abstract: Frontier AI models demonstrate formidable breadth of knowledge. But how close
are they to true human -- or superhuman -- expertise? Genuine experts can
tackle the hardest problems and push the boundaries of scientific
understanding. To illuminate the limits of frontier model capabilities, we turn
away from contrived competitive programming puzzles, and instead focus on
real-life research problems.
  We construct FormulaOne, a benchmark that lies at the intersection of graph
theory, logic, and algorithms, all well within the training distribution of
frontier models. Our problems are incredibly demanding, requiring an array of
reasoning steps. The dataset has three key properties. First, it is of
commercial interest and relates to practical large-scale optimisation problems,
such as those arising in routing, scheduling, and network design. Second, it is
generated from the highly expressive framework of Monadic Second-Order (MSO)
logic on graphs, paving the way toward automatic problem generation at scale;
ideal for building RL environments. Third, many of our problems are intimately
related to the frontier of theoretical computer science, and to central
conjectures therein, such as the Strong Exponential Time Hypothesis (SETH). As
such, any significant algorithmic progress on our dataset, beyond known
results, could carry profound theoretical implications.
  Remarkably, state-of-the-art models like OpenAI's o3 fail entirely on
FormulaOne, solving less than 1% of the questions, even when given 10 attempts
and explanatory fewshot examples -- highlighting how far they remain from
expert-level understanding in some domains. To support further research, we
additionally curate FormulaOne-Warmup, offering a set of simpler tasks, from
the same distribution. We release the full corpus along with a comprehensive
evaluation framework.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [21] [Modeling Open-World Cognition as On-Demand Synthesis of Probabilistic Models](https://arxiv.org/abs/2507.12547)
*Lionel Wong,Katherine M. Collins,Lance Ying,Cedegao E. Zhang,Adrian Weller,Tobias Gersternberg,Timothy O'Donnell,Alexander K. Lew,Jacob D. Andreas,Joshua B. Tenenbaum,Tyler Brooke-Wilson*

Main category: cs.CL

TL;DR: 研究提出了模型合成架构(MSA)，结合分布式和符号表示来构建针对新情况的定制化心理模型，在开放式推理任务中更好地模拟人类判断。


<details>
  <summary>Details</summary>
Motivation: 人类面对新情况时能够从广泛的背景知识中调动相关信息进行推理和预测，研究者想要理解是什么机制让人类能够获取全局相关信息并进行连贯推理。

Method: 提出模型合成架构(MSA)，使用语言模型实现基于相关性的全局检索和模型合成，使用概率程序实现定制化、连贯的世界模型。在"模型奥林匹克"体育场景数据集上评估该架构作为人类判断的模型。

Result: MSA方法在捕获人类判断方面优于仅使用语言模型的基线方法，无论是在直接生成还是思维链生成模式下都表现更好。该方法能够在全局相关变量上提供局部连贯的推理。

Conclusion: MSA可以以镜像人类能力的方式实现，能够在全局相关变量上提供局部连贯推理，为理解和复制人类在开放领域中的推理能力提供了一条路径。

Abstract: When faced with novel situations, people are able to marshal relevant
considerations from a wide range of background knowledge and put these to use
in inferences and predictions. What permits us to draw in globally relevant
information and reason over it coherently? Here, we explore the hypothesis that
people use a combination of distributed and symbolic representations to
construct bespoke mental models tailored to novel situations. We propose a
computational implementation of this idea -- a ``Model Synthesis Architecture''
(MSA) -- using language models to implement global relevance-based retrieval
and model synthesis and probabilistic programs to implement bespoke, coherent
world models. We evaluate our MSA as a model of human judgments on a novel
reasoning dataset. The dataset -- built around a `Model Olympics` domain of
sports vignettes -- tests models' capacity for human-like, open-ended reasoning
by requiring (i) judgments about novel causal structures described in language;
(ii) drawing on large bodies of background knowledge; and (iii) doing both in
light of observations that introduce arbitrary novel variables. Our MSA
approach captures human judgments better than language model-only baselines,
under both direct and chain-of-thought generations from the LM that supports
model synthesis. These results suggest that MSAs can be implemented in a way
that mirrors people's ability to deliver locally coherent reasoning over
globally relevant variables, offering a path to understanding and replicating
human reasoning in open-ended domains.

</details>


### [22] [Is This Just Fantasy? Language Model Representations Reflect Human Judgments of Event Plausibility](https://arxiv.org/abs/2507.12553)
*Michael A. Lepori,Jennifer Hu,Ishita Dasgupta,Roma Patel,Thomas Serre,Ellie Pavlick*

Main category: cs.CL

TL;DR: 该论文研究了语言模型（LMs）如何区分句子的模态类别（如可能、不可能或无意义），并发现了一种称为模态差异向量的线性表示方法，揭示了LMs在模态分类上的能力优于先前研究。此外，这些向量与人类分类行为相关，为理解人类模态分类提供了新视角。


<details>
  <summary>Details</summary>
Motivation: 近期研究质疑LMs在模态分类上的能力，本文旨在验证LMs是否具备可靠的模态分类能力，并探索其与人类分类行为的关系。

Method: 通过识别LMs中的模态差异向量，分析其在不同模型、训练步骤和参数规模下的表现，并与人类分类行为进行相关性研究。

Result: 发现LMs的模态分类能力优于先前研究，模态差异向量与人类分类行为高度相关，且这些向量在模型能力提升过程中按一致顺序出现。

Conclusion: 模态差异向量为理解LMs和人类的模态分类提供了新视角，未来可进一步探索其在认知科学中的应用。

Abstract: Language models (LMs) are used for a diverse range of tasks, from question
answering to writing fantastical stories. In order to reliably accomplish these
tasks, LMs must be able to discern the modal category of a sentence (i.e.,
whether it describes something that is possible, impossible, completely
nonsensical, etc.). However, recent studies have called into question the
ability of LMs to categorize sentences according to modality (Michaelov et al.,
2025; Kauf et al., 2023). In this work, we identify linear representations that
discriminate between modal categories within a variety of LMs, or modal
difference vectors. Analysis of modal difference vectors reveals that LMs have
access to more reliable modal categorization judgments than previously
reported. Furthermore, we find that modal difference vectors emerge in a
consistent order as models become more competent (i.e., through training steps,
layers, and parameter count). Notably, we find that modal difference vectors
identified within LM activations can be used to model fine-grained human
categorization behavior. This potentially provides a novel view into how human
participants distinguish between modal categories, which we explore by
correlating projections along modal difference vectors with human participants'
ratings of interpretable features. In summary, we derive new insights into LM
modal categorization using techniques from mechanistic interpretability, with
the potential to inform our understanding of modal categorization in humans.

</details>


### [23] [The first open machine translation system for the Chechen language](https://arxiv.org/abs/2507.12672)
*Abu-Viskhan A. Umishov,Vladislav A. Grigorian*

Main category: cs.CL

TL;DR: 论文介绍了首个开源的车臣语与俄语翻译模型及配套数据集，探索了将新语言整合到多语言翻译系统NLLB-200中的微调方法。


<details>
  <summary>Details</summary>
Motivation: 车臣语是一种濒危语言，缺乏翻译资源，研究旨在填补这一空白并推动多语言翻译技术的发展。

Method: 通过微调NLLB-200模型，整合车臣语，并收集并行语料库进行训练和评估。

Result: 模型在俄语到车臣语和车臣语到俄语的翻译中分别获得BLEU/ChrF++分数为8.34/34.69和20.89/44.55。

Conclusion: 研究成功开发了车臣语翻译模型并开源了相关资源，为濒危语言保护和技术发展提供了支持。

Abstract: We introduce the first open-source model for translation between the
vulnerable Chechen language and Russian, and the dataset collected to train and
evaluate it. We explore fine-tuning capabilities for including a new language
into a large language model system for multilingual translation NLLB-200. The
BLEU / ChrF++ scores for our model are 8.34 / 34.69 and 20.89 / 44.55 for
translation from Russian to Chechen and reverse direction, respectively. The
release of the translation models is accompanied by the distribution of
parallel words, phrases and sentences corpora and multilingual sentence encoder
adapted to the Chechen language.

</details>


### [24] [Improving Drug Identification in Overdose Death Surveillance using Large Language Models](https://arxiv.org/abs/2507.12679)
*Arthur J. Funnell,Panayiotis Petousis,Fabrice Harel-Canada,Ruby Romero,Alex A. T. Bui,Adam Koncsol,Hritika Chaturvedi,Chelsea Shover,David Goodman-Meza*

Main category: cs.CL

TL;DR: 利用NLP模型（尤其是BioClinicalBERT）自动分类药物过量死亡报告，显著提升监测效率和准确性。


<details>
  <summary>Details</summary>
Motivation: 解决美国药物过量死亡数据因依赖人工ICD-10编码而导致的延迟和信息丢失问题。

Method: 使用35,433条2020年死亡记录训练和测试多种NLP模型，并在3,335条2023-2024年新数据上进行外部验证。

Result: BioClinicalBERT在内部测试集上表现近乎完美（F1>=0.998），外部验证也优于其他模型（F1=0.966）。

Conclusion: NLP模型（如BioClinicalBERT）为药物过量监测提供了高效、准确的解决方案，支持实时趋势检测。

Abstract: The rising rate of drug-related deaths in the United States, largely driven
by fentanyl, requires timely and accurate surveillance. However, critical
overdose data are often buried in free-text coroner reports, leading to delays
and information loss when coded into ICD (International Classification of
Disease)-10 classifications. Natural language processing (NLP) models may
automate and enhance overdose surveillance, but prior applications have been
limited. A dataset of 35,433 death records from multiple U.S. jurisdictions in
2020 was used for model training and internal testing. External validation was
conducted using a novel separate dataset of 3,335 records from 2023-2024.
Multiple NLP approaches were evaluated for classifying specific drug
involvement from unstructured death certificate text. These included
traditional single- and multi-label classifiers, as well as fine-tuned
encoder-only language models such as Bidirectional Encoder Representations from
Transformers (BERT) and BioClinicalBERT, and contemporary decoder-only large
language models such as Qwen 3 and Llama 3. Model performance was assessed
using macro-averaged F1 scores, and 95% confidence intervals were calculated to
quantify uncertainty. Fine-tuned BioClinicalBERT models achieved near-perfect
performance, with macro F1 scores >=0.998 on the internal test set. External
validation confirmed robustness (macro F1=0.966), outperforming conventional
machine learning, general-domain BERT models, and various decoder-only large
language models. NLP models, particularly fine-tuned clinical variants like
BioClinicalBERT, offer a highly accurate and scalable solution for overdose
death classification from free-text reports. These methods can significantly
accelerate surveillance workflows, overcoming the limitations of manual ICD-10
coding and supporting near real-time detection of emerging substance use
trends.

</details>


### [25] [AdaptiSent: Context-Aware Adaptive Attention for Multimodal Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2507.12695)
*S M Rafiuddin,Sadia Kamal,Mohammed Rakib,Arunkumar Bagavathi,Atriya Sen*

Main category: cs.CL

TL;DR: AdaptiSent是一个用于多模态基于方面的情感分析（MABSA）的新框架，通过自适应跨模态注意力机制提升情感分类和方面词提取的性能。


<details>
  <summary>Details</summary>
Motivation: 传统方法在处理多模态数据时未能充分利用文本和图像之间的交互关系，AdaptiSent旨在通过动态模态加权和上下文自适应注意力机制解决这一问题。

Method: AdaptiSent整合了动态模态加权和上下文自适应注意力机制，通过分析文本线索与视觉上下文的交互来提取情感和方面相关信息。

Result: 在标准Twitter数据集上，AdaptiSent在精确率、召回率和F1分数上优于现有模型，尤其在识别细微的跨模态关系方面表现突出。

Conclusion: AdaptiSent为MABSA设定了新标准，显著优于现有方法，特别是在理解复杂多模态信息方面。

Abstract: We introduce AdaptiSent, a new framework for Multimodal Aspect-Based
Sentiment Analysis (MABSA) that uses adaptive cross-modal attention mechanisms
to improve sentiment classification and aspect term extraction from both text
and images. Our model integrates dynamic modality weighting and
context-adaptive attention, enhancing the extraction of sentiment and
aspect-related information by focusing on how textual cues and visual context
interact. We tested our approach against several baselines, including
traditional text-based models and other multimodal methods. Results from
standard Twitter datasets show that AdaptiSent surpasses existing models in
precision, recall, and F1 score, and is particularly effective in identifying
nuanced inter-modal relationships that are crucial for accurate sentiment and
aspect term extraction. This effectiveness comes from the model's ability to
adjust its focus dynamically based on the context's relevance, improving the
depth and accuracy of sentiment analysis across various multimodal data sets.
AdaptiSent sets a new standard for MABSA, significantly outperforming current
methods, especially in understanding complex multimodal information.

</details>


### [26] [AudioJudge: Understanding What Works in Large Audio Model Based Speech Evaluation](https://arxiv.org/abs/2507.12705)
*Potsawee Manakul,Woody Haosheng Gan,Michael J. Ryan,Ali Sartaz Khan,Warit Sirichotedumrong,Kunat Pipatanakul,William Held,Diyi Yang*

Main category: cs.CL

TL;DR: 论文提出AudioJudge，利用大型音频模型（LAM）作为统一评估框架，解决语音评估中系统设计复杂和自动评估与人类偏好相关性差的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语音评估存在两大问题：针对特定音频特征设计专用系统的需求与难度，以及自动评估方法与人类偏好的相关性较差。

Method: 研究AudioJudge在音频特征检测（如发音、语速、说话人识别和语音质量）和系统级人类偏好模拟任务中的表现，探索不同提示工程策略，并提出多方面的集成方法。

Result: 音频拼接结合上下文学习显著提升性能，多方面的集成方法在系统排名基准上与人类偏好的Spearman相关性达0.91。

Conclusion: LAM在噪声环境下表现稳健，但存在冗长和位置偏差问题，需谨慎缓解。

Abstract: Current speech evaluation suffers from two critical limitations: the need and
difficulty of designing specialized systems targeting individual audio
characteristics, and poor correlation between automatic evaluation methods and
human preferences. This work presents a systematic study of Large Audio Model
(LAM) as a Judge, AudioJudge, investigating whether it can provide a unified
evaluation framework that addresses both challenges. We systematically explore
AudioJudge across audio characteristic detection tasks, including
pronunciation, speaking rate, speaker identification and speech quality, and
system-level human preference simulation for automated benchmarking. We
investigate different prompt engineering strategies, finding that audio
concatenation combined with in-context learning significantly improves
performance across both audio characteristic detection and human preference
simulation tasks. We further introduce a multi-aspect ensemble AudioJudge to
enable general-purpose multi-aspect audio evaluation. This method decomposes
speech assessment into specialized judges for lexical content, speech quality,
and paralinguistic features, achieving up to 0.91 Spearman correlation with
human preferences on our system ranking benchmark. Robustness analysis reveals
that while LAMs maintain strong performance under acoustic noise, they exhibit
significant verbosity and positional biases that require careful mitigation.

</details>


### [27] [FLEXITOKENS: Flexible Tokenization for Evolving Language Models](https://arxiv.org/abs/2507.12720)
*Abraham Toluase Owodunni,Orevaoghene Ahia,Sachin Kumar*

Main category: cs.CL

TL;DR: 论文提出了一种名为FLEXITOKENS的可学习分词器方法，解决了传统语言模型因固定分词器导致的分布外数据适应性问题。


<details>
  <summary>Details</summary>
Motivation: 传统语言模型的分词器在适应新数据分布时表现僵硬，导致对分布外数据、未见语言或脚本的分词效率低下。

Method: 开发了基于字节级别的语言模型，通过可学习的分词器动态预测输入字节序列的分割边界，并提出简化的训练目标FLEXITOKENS。

Result: 在多语言基准测试、形态多样性任务和领域适应中，FLEXITOKENS显著减少了分词碎片化，下游任务性能提升高达10%。

Conclusion: FLEXITOKENS通过灵活的分词策略，有效提升了语言模型对新数据分布的适应能力。

Abstract: Language models (LMs) are challenging to adapt to new data distributions by
simple finetuning. This is due to the rigidity of their subword tokenizers,
which typically remain unchanged during adaptation. This inflexibility often
leads to inefficient tokenization, causing overfragmentation of
out-of-distribution domains, unseen languages, or scripts. In this work, we
develop byte-level LMs with learnable tokenizers to make tokenization adaptive.
Our models include a submodule that learns to predict boundaries between the
input byte sequence, encoding it into variable-length segments. Existing
tokenizer-free methods train this boundary predictor using an auxiliary loss
that enforces a fixed compression rate across the training corpus, introducing
a new kind of rigidity. We propose FLEXITOKENS, a simplified training objective
that enables significantly greater flexibility during adaptation. Evaluating
across multiple multilingual benchmarks, morphologically diverse tasks, and
domains, we demonstrate that FLEXITOKENS consistently reduces token
over-fragmentation and achieves up to 10\% improvements on downstream task
performance compared to subword and other gradient-based tokenizers. Code and
data for our experiments will be released at
https://github.com/owos/flexitokens

</details>


### [28] [TransEvalnia: Reasoning-based Evaluation and Ranking of Translations](https://arxiv.org/abs/2507.12724)
*Richard Sproat,Tianyu Zhao,Llion Jones*

Main category: cs.CL

TL;DR: TransEvalnia是一个基于提示的翻译评估和排名系统，通过推理进行细粒度评估，性能优于现有技术，并与人类评分高度一致。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够提供细粒度翻译评估和排名的系统，以改进现有翻译评估方法。

Method: 使用多维质量指标和大型语言模型（如Claude-3.5-Sonnet和Qwen-2.5-72B-Instruct）进行推理和评分。

Result: TransEvalnia在多种语言对上表现优于现有技术，评分与人类评分高度相关。

Conclusion: TransEvalnia是一个有效的翻译评估工具，但需注意位置偏差问题，并提出了解决方案。

Abstract: We present TransEvalnia, a prompting-based translation evaluation and ranking
system that uses reasoning in performing its evaluations and ranking. This
system presents fine-grained evaluations based on a subset of the
Multidimensional Quality Metrics (https://themqm.org/), returns an assessment
of which translation it deems the best, and provides numerical scores for the
various dimensions and for the overall translation. We show that TransEvalnia
performs as well as or better than the state-of-the-art MT-Ranker (Moosa et al.
2024) on our own English-Japanese data as well as several language pairs from
various WMT shared tasks. Using Anthropic's Claude-3.5-Sonnet and
Qwen-2.5-72B-Instruct as the evaluation LLMs, we show that the evaluations
returned are deemed highly acceptable to human raters, and that the scores
assigned to the translations by Sonnet, as well as other LLMs, correlate well
with scores assigned by the human raters. We also note the sensitivity of our
system -- as well as MT-Ranker -- to the order in which the translations are
presented, and we propose methods to address this position bias. All data,
including the system's evaluation and reasoning, human assessments, as well as
code is released.

</details>


### [29] [Strategy Adaptation in Large Language Model Werewolf Agents](https://arxiv.org/abs/2507.12732)
*Fuya Nakamori,Yin Jou Huang,Fei Cheng*

Main category: cs.CL

TL;DR: 提出了一种基于玩家态度和对话上下文动态切换策略的狼人杀智能体方法，验证了其优于固定或隐式策略的基线智能体。


<details>
  <summary>Details</summary>
Motivation: 现有狼人杀智能体通过提示工程隐式定义策略，无法适应动态变化的情境，因此需要一种能显式选择策略的方法。

Method: 根据游戏上下文和玩家角色估计显式选择策略，并与隐式或固定策略的基线智能体进行比较。

Result: 验证了所提方法在动态适应性和性能上的有效性。

Conclusion: 显式策略切换方法能显著提升狼人杀智能体的表现，适应复杂多变的游戏情境。

Abstract: This study proposes a method to improve the performance of Werewolf agents by
switching between predefined strategies based on the attitudes of other players
and the context of conversations. While prior works of Werewolf agents using
prompt engineering have employed methods where effective strategies are
implicitly defined, they cannot adapt to changing situations. In this research,
we propose a method that explicitly selects an appropriate strategy based on
the game context and the estimated roles of other players. We compare the
strategy adaptation Werewolf agents with baseline agents using implicit or
fixed strategies and verify the effectiveness of our proposed method.

</details>


### [30] [Logit Arithmetic Elicits Long Reasoning Capabilities Without Training](https://arxiv.org/abs/2507.12759)
*Yunxiang Zhang,Muhammad Khalifa,Lechen Zhang,Xin Liu,Ayoung Lee,Xinliang Frederick Zhang,Farima Fatahi Bayat,Lu Wang*

Main category: cs.CL

TL;DR: ThinkLogit和ThinkLogit-DPO通过解码时的方法和小模型引导，无需额外训练即可提升大型模型的长推理能力，性能提升显著。


<details>
  <summary>Details</summary>
Motivation: 研究是否能在不进行额外训练的情况下，激发大型推理模型的长推理能力。

Method: 提出ThinkLogit方法，利用对数运算和小模型引导；进一步优化为ThinkLogit-DPO，通过偏好优化训练引导模型。

Result: 在四个数学数据集上，ThinkLogit和ThinkLogit-DPO分别相对提升26%和29%的pass@1性能。

Conclusion: ThinkLogit提供了一种计算高效的方法，无需额外训练即可提升大型模型的长推理能力。

Abstract: Large reasoning models (LRMs) can do complex reasoning via long
chain-of-thought (CoT) involving cognitive strategies such as backtracking and
self-correction. Recent studies suggest that some models inherently possess
these long reasoning abilities, which may be unlocked via extra training. Our
work first investigates whether we can elicit such behavior without any
training. To this end, we propose a decoding-time approach, ThinkLogit, which
utilizes logits arithmetic (Liu et al., 2024) to tune a target large LM for
long reasoning using a substantially smaller model as guider. We then show that
we can further boost performance by training the guider model with preference
optimization over correct/incorrect reasoning pairs sampled from both the
target and guider model -- a setup we refer to as ThinkLogit-DPO. Our
experiments demonstrate that ThinkLogit and ThinkLogit-DPO achieve a relative
improvement in pass@1 by 26% and 29%, respectively, over four mathematical
datasets using the Qwen2.5-32B when guided by R1-Distill-Qwen-1.5B -- a model
21x smaller. Lastly, we show that ThinkLogit can transfer long reasoning skills
acquired through reinforcement learning, improving pass@1 by 13% relative
compared to the Qwen2.5-32B base model. Our work presents a
computationally-efficient method to elicit long reasoning in large models with
minimal or no additional training.

</details>


### [31] [Synergy: End-to-end Concept Model](https://arxiv.org/abs/2507.12769)
*Keli Zheng,Zerong Xie*

Main category: cs.CL

TL;DR: Synergy是一种语言模型，通过学习的路由机制桥接不同抽象层次，表现优于BBPE和Llama3，展示了无分词器架构的可行性。


<details>
  <summary>Details</summary>
Motivation: 探索如何在语言模型中桥接不同抽象层次，并验证无分词器架构的可行性。

Method: 训练一个字节级语言模型，通过学习路由机制实现抽象层次的桥接，并与BBPE和Llama3进行比较。

Result: Synergy在相同模型规模和训练数据量下优于Llama3，且中间部分在去除位置编码后表现更好。

Conclusion: Synergy展示了无分词器架构的潜力，为更鲁棒和灵活的模型管道铺平了道路。

Abstract: In this paper, we present Synergy, a language model that bridges different
levels of abstraction in an end-to-end fashion through a learned routing
mechanism. Focusing on low-level linguistic abstraction, we trained our model
as a byte-level language model. Our model spontaneously learns to tokenize
bytes, producing fewer concept tokens than Byte-level Byte Pair Encoder (BBPE)
tokenizers while keeping comparable performance. By comparing with Llama3, we
observed an advantage of Synergy under the same model scale and training
dataset size. Further studies show that the middle part (the higher abstraction
part) of our model performs better when positional encodings are removed,
suggesting the emergence of position-independent concepts. These findings
demonstrate the feasibility of tokenizer-free architectures, paving the way for
more robust and flexible pipelines.

</details>


### [32] [Learning Robust Negation Text Representations](https://arxiv.org/abs/2507.12782)
*Thinh Hung Truong,Karin Verspoor,Trevor Cohn,Timothy Baldwin*

Main category: cs.CL

TL;DR: 论文提出了一种通过从大型语言模型中蒸馏数据来提升文本编码器否定理解能力的方法，同时保持了通用任务的竞争力。


<details>
  <summary>Details</summary>
Motivation: 尽管自回归大型语言模型快速发展，但小型文本编码器在需要丰富上下文表示的任务中仍很重要。否定是一种重要的语义功能，但现有方法未能很好捕捉，影响依赖文本嵌入的下游应用。

Method: 通过从大型语言模型中蒸馏多样化的否定和模糊表达数据，采用对比学习策略微调BERT模型。

Result: 在否定理解能力上取得显著提升，同时在通用基准上保持竞争力。该方法也可适配大型语言模型，提升否定基准性能。

Conclusion: 提出的方法有效提升了文本编码器的否定理解能力，且具有通用性和可扩展性。

Abstract: Despite rapid adoption of autoregressive large language models, smaller text
encoders still play an important role in text understanding tasks that require
rich contextualized representations. Negation is an important semantic function
that is still not properly captured by such methods, affecting many downstream
applications relying on text embeddings. We propose a strategy to improve
negation robustness of text encoders, by distilling data from large language
models using diverse patterns of negation and hedging. We adopt a standard
contrastive learning strategy to finetune a strong BERT-based model, and
observe large improvement in negation understanding capabilities while
maintaining competitive performance on general benchmarks. In addition, we also
show that our method can be adapted to LLMs, leading to improved performance on
negation benchmarks.

</details>


### [33] [Large Language Models' Internal Perception of Symbolic Music](https://arxiv.org/abs/2507.12808)
*Andrew Shin,Kunitake Kaneko*

Main category: cs.CL

TL;DR: LLMs能生成象征性音乐数据，但缺乏明确音乐背景限制了其表现。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在象征性音乐领域的潜力和局限性。

Method: 通过文本提示生成MIDI文件，并用这些数据训练神经网络进行分类和旋律补全任务。

Result: LLMs能推断基本音乐结构和时间关系，但表现有限。

Conclusion: LLMs在象征性音乐生成中有潜力，但需更多明确音乐背景支持。

Abstract: Large language models (LLMs) excel at modeling relationships between strings
in natural language and have shown promise in extending to other symbolic
domains like coding or mathematics. However, the extent to which they
implicitly model symbolic music remains underexplored. This paper investigates
how LLMs represent musical concepts by generating symbolic music data from
textual prompts describing combinations of genres and styles, and evaluating
their utility through recognition and generation tasks. We produce a dataset of
LLM-generated MIDI files without relying on explicit musical training. We then
train neural networks entirely on this LLM-generated MIDI dataset and perform
genre and style classification as well as melody completion, benchmarking their
performance against established models. Our results demonstrate that LLMs can
infer rudimentary musical structures and temporal relationships from text,
highlighting both their potential to implicitly encode musical patterns and
their limitations due to a lack of explicit musical context, shedding light on
their generative capabilities for symbolic music.

</details>


### [34] [Are Knowledge and Reference in Multilingual Language Models Cross-Lingually Consistent?](https://arxiv.org/abs/2507.12838)
*Xi Ai,Mahardika Krisna Ihsani,Min-Yen Kan*

Main category: cs.CL

TL;DR: 研究跨语言一致性对评估跨语言迁移性、保持模型知识的事实性以及语言模型性能的重要性，并通过代码混合核心指称语句分析跨语言知识一致性。


<details>
  <summary>Details</summary>
Motivation: 跨语言一致性对模型知识的事实性和性能至关重要，需分析其表现及影响因素。

Method: 使用代码混合核心指称语句和可解释性方法分析多语言模型行为，评估提升多语言性能的策略。

Result: 多语言模型一致性受语言家族、语言因素和特定层瓶颈影响；代码切换训练和跨语言词对齐策略效果最佳。

Conclusion: 跨语言对齐监督和代码切换训练对提升多语言性能和一致性至关重要。

Abstract: Cross-lingual consistency should be considered to assess cross-lingual
transferability, maintain the factuality of the model knowledge across
languages, and preserve the parity of language model performance. We are thus
interested in analyzing, evaluating, and interpreting cross-lingual consistency
for factual knowledge. We examine code-mixed coreferential statements conveyed
identical knowledge across languages to study cross-lingual knowledge
consistency. We use some interpretability approaches to analyze the behavior of
a model in cross-lingual contexts, discovering that multilingual models show
different levels of consistency, subject to language families, linguistic
factors, and a bottleneck in cross-lingual consistency on a particular layer.
In addition, we evaluate common strategies aimed at improving multilingual
performance to observe whether these strategies can improve knowledge
consistency at the same time. While knowledge is not cross-lingual consistency
in many cases, code-switching training and cross-lingual word alignment
objectives show the most promising results, emphasizing the noteworthiness of
cross-lingual alignment supervision and code-switching training for both
multilingual performance and cross-lingual consistency enhancement.

</details>


### [35] [Making Language Model a Hierarchical Classifier and Generator](https://arxiv.org/abs/2507.12930)
*Yihong Wang,Zhonglin Jiang,Ningyuan Xi,Yue Zhao,Qingqing Gu,Xiyuan Chen,Hao Wu,Sheng Xu,Hange Zhou,Yong Chen,Luo Ji*

Main category: cs.CL

TL;DR: 论文提出了一种分层解码器架构，通过将预训练语言模型的最后一层语言头复制到中间层并进行微调，验证了分层解码的可行性，并在多个任务中取得优异表现。


<details>
  <summary>Details</summary>
Motivation: 受人类分层思维能力的启发，探索构建分层解码器架构的可能性。

Method: 将预训练语言模型的最后一层语言头复制到选定的中间层，并用不同任务输入进行微调。

Result: 实验验证了中间层可以生成有意义的内容，分层解码器在多项任务中表现优异。

Conclusion: 研究表明，分层解码器架构具有潜力，未来可能实现通用的分层推理器预训练。

Abstract: Decoder-only language models, such as GPT and LLaMA, generally decode on the
last layer. Motivated by human's hierarchical thinking capability, we propose
that a hierarchical decoder architecture could be built with different layers
decoding texts simultaneously. Due to limited time and computationally
resources, we choose to adapt a pretrained language model into this form of
hierarchical decoder. Language heads of the last layer are copied to different
selected intermediate layers, and fine-tuned with different task inputs. By
thorough experiments, we validate that these selective intermediate layers
could be adapted to speak meaningful and reasonable contents, and this paradigm
of hierarchical decoder can obtain state-of-the-art performances on multiple
tasks such as hierarchical text classification, classification-guided
generation, and hierarchical text generation. This study suggests the
possibility of a generalized hierarchical reasoner, pretraining from scratch.

</details>


### [36] [MRT at IberLEF-2025 PRESTA Task: Maximizing Recovery from Tables with Multiple Steps](https://arxiv.org/abs/2507.12981)
*Maximiliano Hormazábal Lagos,Álvaro Bueno Sáez,Héctor Cerezo-Costas,Pedro Alonso Doval,Jorge Alcalde Vesteiro*

Main category: cs.CL

TL;DR: 本文介绍了针对IberLEF 2025任务PRESTA的解决方案，通过LLM生成Python代码处理表格数据，实现了85%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决西班牙语表格问答任务（PRESTA），提升问答系统的准确性和效率。

Method: 多步骤流程：分析表格内容、选择有用列、生成自然语言指令、转换为代码并执行，使用开源LLM和优化提示。

Result: 在任务中达到85%的准确率。

Conclusion: 该方法通过LLM生成代码处理表格数据，显著提升了问答任务的性能。

Abstract: This paper presents our approach for the IberLEF 2025 Task PRESTA: Preguntas
y Respuestas sobre Tablas en Espa\~nol (Questions and Answers about Tables in
Spanish). Our solution obtains answers to the questions by implementing Python
code generation with LLMs that is used to filter and process the table. This
solution evolves from the MRT implementation for the Semeval 2025 related task.
The process consists of multiple steps: analyzing and understanding the content
of the table, selecting the useful columns, generating instructions in natural
language, translating these instructions to code, running it, and handling
potential errors or exceptions. These steps use open-source LLMs and
fine-grained optimized prompts for each step. With this approach, we achieved
an accuracy score of 85\% in the task.

</details>


### [37] [Formalizing Attack Scenario Description: A Proposed Model](https://arxiv.org/abs/2507.13076)
*Quentin Goux,Nadira Lammari*

Main category: cs.CL

TL;DR: 论文提出了一种新的形式化模型，用于描述攻击场景及其上下文，基于UML类模型抽象，并展示了其在攻击分析和自动生成攻击脚本中的应用。


<details>
  <summary>Details</summary>
Motivation: 组织面临不断变化的威胁环境，需要自动化网络安全流程，但自动化需要输入数据的规范化。本文旨在解决以攻击场景为输入的过程的需求。

Method: 提出了一种新的形式化模型，基于UML类模型抽象攻击的上下文和场景描述。

Result: 模型可用于上游攻击分析过程和自动生成网络安全训练的攻击脚本。

Conclusion: 该研究为网络安全自动化提供了实用的形式化模型，并展示了其在攻击分析和训练中的双重应用价值。

Abstract: Organizations face an ever-changing threat landscape. They must continuously
dedicate significant efforts to protect their assets, making their adoption of
increased cybersecurity automation inevitable. However, process automation
requires formalization of input data. Through this paper, we address this need
for processes that use attack scenarios as input. Among these processes, one
can mention both the generation of scripts for attack simulation and training
purposes, as well as the analysis of attacks. Therefore, the paper's main
research contribution is a novel formal model that encompasses the attack's
context description and its scenario. It is abstracted using UML class model.
Once the description of our model done, we will show how it could serve an
upstream attack analysis process. We will show also its use for an automatic
generation of attack scripts in the context of cybersecurity training. These
two uses cases constitute the second contribution of this present research
work.

</details>


### [38] [SemCSE: Semantic Contrastive Sentence Embeddings Using LLM-Generated Summaries For Scientific Abstracts](https://arxiv.org/abs/2507.13105)
*Marc Brinner,Sina Zarriess*

Main category: cs.CL

TL;DR: SemCSE是一种无监督学习方法，通过对比学习生成科学文本的语义嵌入，利用LLM生成的摘要训练模型，使语义相关的摘要更接近。


<details>
  <summary>Details</summary>
Motivation: 传统基于引用的方法未必反映语义相似性，因此需要一种能捕捉文本真实语义内容的方法。

Method: 利用LLM生成的科学摘要进行对比学习，训练模型以在嵌入空间中拉近语义相关的摘要。

Result: 在科学文本嵌入的基准测试中，SemCSE实现了同类模型中的最优性能，展示了语义聚焦训练的优势。

Conclusion: SemCSE通过语义驱动的训练方法，显著提升了科学文本嵌入的质量和语义区分能力。

Abstract: We introduce SemCSE, an unsupervised method for learning semantic embeddings
of scientific texts. Building on recent advances in contrastive learning for
text embeddings, our approach leverages LLM-generated summaries of scientific
abstracts to train a model that positions semantically related summaries closer
together in the embedding space. This resulting objective ensures that the
model captures the true semantic content of a text, in contrast to traditional
citation-based approaches that do not necessarily reflect semantic similarity.
To validate this, we propose a novel benchmark designed to assess a model's
ability to understand and encode the semantic content of scientific texts,
demonstrating that our method enforces a stronger semantic separation within
the embedding space. Additionally, we evaluate SemCSE on the comprehensive
SciRepEval benchmark for scientific text embeddings, where it achieves
state-of-the-art performance among models of its size, thus highlighting the
benefits of a semantically focused training approach.

</details>


### [39] [A Computational Framework to Identify Self-Aspects in Text](https://arxiv.org/abs/2507.13115)
*Jaya Caporusso,Matthew Purver,Senja Pollak*

Main category: cs.CL

TL;DR: 该博士提案计划开发一个计算框架，用于识别文本中的自我方面，填补NLP领域对此的空白，并通过多种模型评估其效果。


<details>
  <summary>Details</summary>
Motivation: 自我是一个多方面的概念，在语言中有所体现，但在自然语言处理（NLP）中尚未充分探索。其与心理健康等领域的关联凸显了系统化分析的必要性。

Method: 提出自我方面的本体论和标注数据集，开发判别模型、生成模型和嵌入检索方法，评估其可解释性、准确性等指标。

Result: 计划通过案例研究（如心理健康和现象学）应用表现最佳的模型。

Conclusion: 该框架有望为NLP中的自我分析提供系统化工具，并支持相关领域的研究。

Abstract: This Ph.D. proposal introduces a plan to develop a computational framework to
identify Self-aspects in text. The Self is a multifaceted construct and it is
reflected in language. While it is described across disciplines like cognitive
science and phenomenology, it remains underexplored in natural language
processing (NLP). Many of the aspects of the Self align with psychological and
other well-researched phenomena (e.g., those related to mental health),
highlighting the need for systematic NLP-based analysis. In line with this, we
plan to introduce an ontology of Self-aspects and a gold-standard annotated
dataset. Using this foundation, we will develop and evaluate conventional
discriminative models, generative large language models, and embedding-based
retrieval approaches against four main criteria: interpretability, ground-truth
adherence, accuracy, and computational efficiency. Top-performing models will
be applied in case studies in mental health and empirical phenomenology.

</details>


### [40] [Assessing the Reliability of LLMs Annotations in the Context of Demographic Bias and Model Explanation](https://arxiv.org/abs/2507.13138)
*Hadi Mohammadi,Tina Shahedi,Pablo Mosteiro,Massimo Poesio,Ayoub Bagheri,Anastasia Giachanou*

Main category: cs.CL

TL;DR: 研究探讨了标注者人口特征对性别歧视检测任务标注的影响，发现文本内容是主要因素，而人口特征影响较小。同时评估了生成式AI作为标注者的可靠性，发现简单的人口角色提示效果不佳。


<details>
  <summary>Details</summary>
Motivation: 开发公平的NLP系统需要理解标注变异的来源，尤其是在性别歧视检测等任务中，人口偏见是一个重要问题。

Method: 使用广义线性混合模型量化人口特征对标注的影响，并评估生成式AI模型作为标注者的表现，包括人口角色提示的效果。

Result: 人口特征仅占标注变异的8%，文本内容是主要因素。生成式AI的简单人口角色提示未能提升性能，甚至可能降低表现。

Conclusion: 关注内容驱动的解释和稳健的标注协议比模拟人口角色更有利于实现公平性。

Abstract: Understanding the sources of variability in annotations is crucial for
developing fair NLP systems, especially for tasks like sexism detection where
demographic bias is a concern. This study investigates the extent to which
annotator demographic features influence labeling decisions compared to text
content. Using a Generalized Linear Mixed Model, we quantify this inf luence,
finding that while statistically present, demographic factors account for a
minor fraction ( 8%) of the observed variance, with tweet content being the
dominant factor. We then assess the reliability of Generative AI (GenAI) models
as annotators, specifically evaluating if guiding them with demographic
personas improves alignment with human judgments. Our results indicate that
simplistic persona prompting often fails to enhance, and sometimes degrades,
performance compared to baseline models. Furthermore, explainable AI (XAI)
techniques reveal that model predictions rely heavily on content-specific
tokens related to sexism, rather than correlates of demographic
characteristics. We argue that focusing on content-driven explanations and
robust annotation protocols offers a more reliable path towards fairness than
potentially persona simulation.

</details>


### [41] [Feature-based analysis of oral narratives from Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13164)
*Emma Sharratt,Annelien Smith,Retief Louw,Daleen Klop,Febe de Wet,Herman Kamper*

Main category: cs.CL

TL;DR: 研究分析了需要干预的儿童的口语叙事特征，发现词汇多样性和句子长度是典型发展的指标，而特定动词和助动词的使用与干预需求减少相关。


<details>
  <summary>Details</summary>
Motivation: 探讨口语叙事能力对后期读写发展的预测作用，并识别需要干预的儿童的特征。

Method: 使用简单机器学习方法分析四至五岁儿童的录音故事，比较阿非利卡语和科萨语的数据。

Result: 词汇多样性和句子长度是典型发展的指标，特定动词和助动词的使用与干预需求减少相关。

Conclusion: 研究揭示了语言特异性和共享的叙事能力预测因素，对多语言环境中的早期评估有启示。

Abstract: Oral narrative skills are strong predictors of later literacy development.
This study examines the features of oral narratives from children who were
identified by experts as requiring intervention. Using simple machine learning
methods, we analyse recorded stories from four- and five-year-old Afrikaans-
and isiXhosa-speaking children. Consistent with prior research, we identify
lexical diversity (unique words) and length-based features (mean utterance
length) as indicators of typical development, but features like articulation
rate prove less informative. Despite cross-linguistic variation in
part-of-speech patterns, the use of specific verbs and auxiliaries associated
with goal-directed storytelling is correlated with a reduced likelihood of
requiring intervention. Our analysis of two linguistically distinct languages
reveals both language-specific and shared predictors of narrative proficiency,
with implications for early assessment in multilingual contexts.

</details>


### [42] [GEMMAS: Graph-based Evaluation Metrics for Multi Agent Systems](https://arxiv.org/abs/2507.13190)
*Jisoo Lee,Raeyoung Chang,Dongwook Kwon,Harmanpreet Singh,Nikhil Verma*

Main category: cs.CL

TL;DR: GEMMAS是一个基于图的评估框架，用于分析多智能体系统的内部协作过程，提出了两个过程级指标（IDS和UPR），揭示了仅关注最终结果的评估方法的不足。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法仅关注最终输出的正确性，忽略了低效通信和协调导致的冗余推理和高计算成本。

Method: 提出GEMMAS框架，通过将有向无环图建模智能体交互，并引入IDS和UPR两个过程级指标。

Result: 在GSM8K等基准测试中，GEMMAS揭示了协作质量的显著差异（如准确率仅差2.1%，但IDS差12.8%，UPR差80%）。

Conclusion: 过程级诊断对设计更可解释和资源高效的协作AI系统至关重要，仅依赖结果指标不足。

Abstract: Multi-agent systems built on language models have shown strong performance on
collaborative reasoning tasks. However, existing evaluations focus only on the
correctness of the final output, overlooking how inefficient communication and
poor coordination contribute to redundant reasoning and higher computational
costs. We introduce GEMMAS, a graph-based evaluation framework that analyzes
the internal collaboration process by modeling agent interactions as a directed
acyclic graph. To capture collaboration quality, we propose two process-level
metrics: Information Diversity Score (IDS) to measure semantic variation in
inter-agent messages, and Unnecessary Path Ratio (UPR) to quantify redundant
reasoning paths. We evaluate GEMMAS across five benchmarks and highlight
results on GSM8K, where systems with only a 2.1% difference in accuracy differ
by 12.8% in IDS and 80% in UPR, revealing substantial variation in internal
collaboration. These findings demonstrate that outcome-only metrics are
insufficient for evaluating multi-agent performance and highlight the
importance of process-level diagnostics in designing more interpretable and
resource-efficient collaborative AI systems.

</details>


### [43] [Automatically assessing oral narratives of Afrikaans and isiXhosa children](https://arxiv.org/abs/2507.13205)
*R. Louw,E. Sharratt,F. de Wet,C. Jacobs,A. Smith,H. Kamper*

Main category: cs.CL

TL;DR: 开发了一个自动评估学龄前儿童口语叙述的系统，使用语音识别和机器学习评分模型，比较了线性模型和大型语言模型（LLM）的性能。


<details>
  <summary>Details</summary>
Motivation: 早期儿童叙事和理解能力的发展对后期识字至关重要，但教师在大型学前班中难以准确识别需要干预的学生。

Method: 系统结合自动语音识别和机器学习评分模型，比较线性模型与LLM的表现。

Result: LLM模型在多数情况下优于线性模型，且与人类专家在识别需要干预的儿童方面表现相当。

Conclusion: 该系统为课堂自动口语评估奠定了基础，帮助教师更专注于个性化支持儿童学习。

Abstract: Developing narrative and comprehension skills in early childhood is critical
for later literacy. However, teachers in large preschool classrooms struggle to
accurately identify students who require intervention. We present a system for
automatically assessing oral narratives of preschool children in Afrikaans and
isiXhosa. The system uses automatic speech recognition followed by a machine
learning scoring model to predict narrative and comprehension scores. For
scoring predicted transcripts, we compare a linear model to a large language
model (LLM). The LLM-based system outperforms the linear model in most cases,
but the linear system is competitive despite its simplicity. The LLM-based
system is comparable to a human expert in flagging children who require
intervention. We lay the foundation for automatic oral assessments in
classrooms, giving teachers extra capacity to focus on personalised support for
children's learning.

</details>


### [44] [Enhancing Cross-task Transfer of Large Language Models via Activation Steering](https://arxiv.org/abs/2507.13236)
*Xinyu Tang,Zhihao Lv,Xiaoxue Cheng,Junyi Li,Wayne Xin Zhao,Zujie Wen,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 论文提出CAST框架，通过操纵大语言模型的内部激活状态实现跨任务知识迁移，无需参数更新或输入扩展。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在数据稀缺场景下对未见任务表现不佳，现有跨任务上下文学习方法在鲁棒性、可扩展性和效率方面存在挑战。

Method: 通过分析大语言模型潜在空间的激活模式，提出CAST框架，利用高资源任务的样本增强激活状态，适应低资源任务。

Result: 实验表明，CAST在跨领域和跨语言迁移中优于基线方法，具有更高的可扩展性和更低计算成本。

Conclusion: CAST通过激活状态操纵实现高效跨任务迁移，为数据稀缺场景提供新解决方案。

Abstract: Large language models (LLMs) have shown impressive abilities in leveraging
pretrained knowledge through prompting, but they often struggle with unseen
tasks, particularly in data-scarce scenarios. While cross-task in-context
learning offers a direct solution for transferring knowledge across tasks, it
still faces critical challenges in terms of robustness, scalability, and
efficiency. In this paper, we investigate whether cross-task transfer can be
achieved via latent space steering without parameter updates or input
expansion. Through an analysis of activation patterns in the latent space of
LLMs, we observe that the enhanced activations induced by in-context examples
have consistent patterns across different tasks. Inspired by these findings, we
propose CAST, a novel Cross-task Activation Steering Transfer framework that
enables effective transfer by manipulating the model's internal activation
states. Our approach first selects influential and diverse samples from
high-resource tasks, then utilizes their contrastive representation-enhanced
activations to adapt LLMs to low-resource tasks. Extensive experiments across
both cross-domain and cross-lingual transfer settings show that our method
outperforms competitive baselines and demonstrates superior scalability and
lower computational costs.

</details>


### [45] [HATS: Hindi Analogy Test Set for Evaluating Reasoning in Large Language Models](https://arxiv.org/abs/2507.13238)
*Ashray Gupta,Rohan Joseph,Sunny Rai*

Main category: cs.CL

TL;DR: 论文介绍了HATS（Hindi Analogy Test Set），用于评估多语言大模型在印地语中的类比推理能力，并提出了基于认知理论的Chain of Thought方法，提升了模型表现。


<details>
  <summary>Details</summary>
Motivation: 现有研究多关注英语，印地语等语言的推理能力评估不足，限制了模型跨语言泛化能力的理解。

Method: 构建了包含405个多选题的HATS数据集，采用多种提示策略和基于认知理论的Chain of Thought方法进行实验。

Result: 模型在英语提示下表现最佳，提出的Chain of Thought方法显著提升了印地语类比问题的性能。

Conclusion: HATS填补了印地语推理评估资源的空白，为多语言模型能力研究提供了新工具。

Abstract: Analogies test a model's ability to infer implicit relationships between
concepts, making them a key benchmark for evaluating reasoning capabilities.
While large language models (LLMs) are widely evaluated for reasoning in
English, their abilities in Indic languages remain understudied, limiting our
understanding of whether these models generalize across languages. To address
this gap, we introduce a new Hindi Analogy Test Set (HATS), comprising 405
multiple-choice questions sourced from Indian government exams. We benchmark
state-of-the-art multilingual LLMs using various prompting strategies and
introduce a grounded Chain of Thought approach that leverages cognitive
theories of analogical reasoning. This approach improves model performance on
Hindi analogy questions. Our experiments show that models perform best with
English prompts, irrespective of the prompting strategy. Our test set addresses
the lack of a critical resource to evaluate LLM reasoning capabilities in
Hindi.

</details>


### [46] [Automating Steering for Safe Multimodal Large Language Models](https://arxiv.org/abs/2507.13255)
*Lyucheng Wu,Mengru Wang,Ziwen Xu,Tri Cao,Nay Oo,Bryan Hooi,Shumin Deng*

Main category: cs.CL

TL;DR: AutoSteer是一种模块化、自适应的推理时间干预技术，旨在提升多模态大语言模型（MLLMs）的安全性，无需微调底层模型。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型（MLLMs）在跨模态推理方面取得了进展，但也带来了新的安全隐患，尤其是在面对对抗性多模态输入时。

Method: AutoSteer包含三个核心组件：安全性感知评分（SAS）、自适应安全探针和轻量级拒绝头，用于识别和干预安全风险。

Result: 实验表明，AutoSteer显著降低了文本、视觉和跨模态威胁的攻击成功率（ASR），同时保持了模型的通用能力。

Conclusion: AutoSteer是一个实用、可解释且有效的框架，有助于更安全地部署多模态AI系统。

Abstract: Recent progress in Multimodal Large Language Models (MLLMs) has unlocked
powerful cross-modal reasoning abilities, but also raised new safety concerns,
particularly when faced with adversarial multimodal inputs. To improve the
safety of MLLMs during inference, we introduce a modular and adaptive
inference-time intervention technology, AutoSteer, without requiring any
fine-tuning of the underlying model. AutoSteer incorporates three core
components: (1) a novel Safety Awareness Score (SAS) that automatically
identifies the most safety-relevant distinctions among the model's internal
layers; (2) an adaptive safety prober trained to estimate the likelihood of
toxic outputs from intermediate representations; and (3) a lightweight Refusal
Head that selectively intervenes to modulate generation when safety risks are
detected. Experiments on LLaVA-OV and Chameleon across diverse safety-critical
benchmarks demonstrate that AutoSteer significantly reduces the Attack Success
Rate (ASR) for textual, visual, and cross-modal threats, while maintaining
general abilities. These findings position AutoSteer as a practical,
interpretable, and effective framework for safer deployment of multimodal AI
systems.

</details>


### [47] [QuestA: Expanding Reasoning Capacity in LLMs via Question Augmentation](https://arxiv.org/abs/2507.13266)
*Jiazheng Li,Hong Lu,Kaiyue Wen,Zaiwen Yang,Jiaxuan Gao,Hongzhou Lin,Yi Wu,Jingzhao Zhang*

Main category: cs.CL

TL;DR: 论文提出QuestA方法，通过问题增强（引入部分解）提升强化学习在数学推理任务中的效果，显著提高了模型性能。


<details>
  <summary>Details</summary>
Motivation: 强化学习在提升大语言模型的多步推理能力上效果有限，尤其在复杂问题上表现不佳。

Method: 提出QuestA策略，在训练中引入部分解以降低问题难度并提供更丰富的学习信号。

Result: 在多个数学基准测试中取得新SOTA，如AIME24（67.1%）、AIME25（59.5%）和HMMT25（35.5%）。

Conclusion: QuestA不仅提升样本效率，还为通过RL扩展推理能力提供了实用且通用的路径。

Abstract: Reinforcement learning (RL) has become a key component in training large
language reasoning models (LLMs). However, recent studies questions its
effectiveness in improving multi-step reasoning-particularly on hard problems.
To address this challenge, we propose a simple yet effective strategy via
Question Augmentation: introduce partial solutions during training to reduce
problem difficulty and provide more informative learning signals. Our method,
QuestA, when applied during RL training on math reasoning tasks, not only
improves pass@1 but also pass@k-particularly on problems where standard RL
struggles to make progress. This enables continual improvement over strong
open-source models such as DeepScaleR and OpenMath Nemotron, further enhancing
their reasoning capabilities. We achieve new state-of-the-art results on math
benchmarks using 1.5B-parameter models: 67.1% (+5.3%) on AIME24, 59.5% (+10.0%)
on AIME25, and 35.5% (+4.0%) on HMMT25. Further, we provide theoretical
explanations that QuestA improves sample efficiency, offering a practical and
generalizable pathway for expanding reasoning capability through RL.

</details>


### [48] [Overview of the TalentCLEF 2025: Skill and Job Title Intelligence for Human Capital Management](https://arxiv.org/abs/2507.13275)
*Luis Gasco,Hermenegildo Fabregat,Laura García-Sardiña,Paula Estrella,Daniel Deniz,Alvaro Rodrigo,Rabih Zbib*

Main category: cs.CL

TL;DR: TalentCLEF 2025是首个专注于技能和职位名称智能的评估活动，包含多语言职位匹配和基于职位名称的技能预测任务，旨在推动劳动力市场中可靠、公平的语言技术发展。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言处理在人力资源管理领域缺乏公开数据和基准的问题，推动可靠、公平的模型发展。

Method: 通过TalentCLEF 2025评估活动，设计两个任务（多语言职位匹配和技能预测），使用真实匿名化数据，并评估性别偏见。

Result: 吸引了76个团队参与，结果显示训练策略比模型规模更重要，为领域提供了首个公开基准。

Conclusion: TalentCLEF为劳动力市场语言技术提供了首个公开基准，促进了稳健、公平和可迁移技术的发展。

Abstract: Advances in natural language processing and large language models are driving
a major transformation in Human Capital Management, with a growing interest in
building smart systems based on language technologies for talent acquisition,
upskilling strategies, and workforce planning. However, the adoption and
progress of these technologies critically depend on the development of reliable
and fair models, properly evaluated on public data and open benchmarks, which
have so far been unavailable in this domain.
  To address this gap, we present TalentCLEF 2025, the first evaluation
campaign focused on skill and job title intelligence. The lab consists of two
tasks: Task A - Multilingual Job Title Matching, covering English, Spanish,
German, and Chinese; and Task B - Job Title-Based Skill Prediction, in English.
Both corpora were built from real job applications, carefully anonymized, and
manually annotated to reflect the complexity and diversity of real-world labor
market data, including linguistic variability and gender-marked expressions.
  The evaluations included monolingual and cross-lingual scenarios and covered
the evaluation of gender bias.
  TalentCLEF attracted 76 registered teams with more than 280 submissions. Most
systems relied on information retrieval techniques built with multilingual
encoder-based models fine-tuned with contrastive learning, and several of them
incorporated large language models for data augmentation or re-ranking. The
results show that the training strategies have a larger effect than the size of
the model alone. TalentCLEF provides the first public benchmark in this field
and encourages the development of robust, fair, and transferable language
technologies for the labor market.

</details>


### [49] [Multi-Agent Synergy-Driven Iterative Visual Narrative Synthesis](https://arxiv.org/abs/2507.13285)
*Wang Xi,Quan Shi,Tian Yu,Yujie Peng,Jiayi Sun,Mengxing Ren,Zenghui Ding,Ningguang Yao*

Main category: cs.CL

TL;DR: RCPS框架通过深度结构化叙事规划、自适应布局生成和迭代优化，显著提升演示质量，PREVAL评估工具与人工评估高度一致。


<details>
  <summary>Details</summary>
Motivation: 现有方法生成的演示存在逻辑不一致和布局不佳的问题，难以满足专业标准。

Method: RCPS整合深度结构化叙事规划、自适应布局生成和迭代优化；PREVAL采用基于偏好多维模型评估演示质量。

Result: RCPS在所有质量维度上显著优于基线方法，PREVAL与人工评估高度相关。

Conclusion: RCPS和PREVAL为高质量演示生成和评估提供了可靠解决方案。

Abstract: Automated generation of high-quality media presentations is challenging,
requiring robust content extraction, narrative planning, visual design, and
overall quality optimization. Existing methods often produce presentations with
logical inconsistencies and suboptimal layouts, thereby struggling to meet
professional standards. To address these challenges, we introduce RCPS
(Reflective Coherent Presentation Synthesis), a novel framework integrating
three key components: (1) Deep Structured Narrative Planning; (2) Adaptive
Layout Generation; (3) an Iterative Optimization Loop. Additionally, we propose
PREVAL, a preference-based evaluation framework employing rationale-enhanced
multi-dimensional models to assess presentation quality across Content,
Coherence, and Design. Experimental results demonstrate that RCPS significantly
outperforms baseline methods across all quality dimensions, producing
presentations that closely approximate human expert standards. PREVAL shows
strong correlation with human judgments, validating it as a reliable automated
tool for assessing presentation quality.

</details>


### [50] [AbGen: Evaluating Large Language Models in Ablation Study Design and Evaluation for Scientific Research](https://arxiv.org/abs/2507.13300)
*Yilun Zhao,Weiyuan Chen,Zhijian Xu,Manasi Patwardhan,Yixin Liu,Chengye Wang,Lovekesh Vig,Arman Cohan*

Main category: cs.CL

TL;DR: AbGen是首个评估LLMs设计科学研究中消融研究能力的基准，包含1500个专家标注示例。评估显示LLMs与人类专家在重要性、忠实性和合理性上存在显著差距。此外，当前自动评估方法不可靠，因此开发了AbGen-Eval用于评估自动评估系统的可靠性。


<details>
  <summary>Details</summary>
Motivation: 填补LLMs在设计消融研究能力评估上的空白，并提供可靠评估方法。

Method: 基于807篇NLP论文构建1500个专家标注示例的AbGen基准，评估LLMs生成消融研究设计的能力，并开发AbGen-Eval评估自动评估系统的可靠性。

Result: LLMs在消融研究设计上与人类专家存在显著差距，且当前自动评估方法不可靠。

Conclusion: AbGen和AbGen-Eval为未来开发更可靠的LLM评估系统提供了基础。

Abstract: We introduce AbGen, the first benchmark designed to evaluate the capabilities
of LLMs in designing ablation studies for scientific research. AbGen consists
of 1,500 expert-annotated examples derived from 807 NLP papers. In this
benchmark, LLMs are tasked with generating detailed ablation study designs for
a specified module or process based on the given research context. Our
evaluation of leading LLMs, such as DeepSeek-R1-0528 and o4-mini, highlights a
significant performance gap between these models and human experts in terms of
the importance, faithfulness, and soundness of the ablation study designs.
Moreover, we demonstrate that current automated evaluation methods are not
reliable for our task, as they show a significant discrepancy when compared to
human assessment. To better investigate this, we develop AbGen-Eval, a
meta-evaluation benchmark designed to assess the reliability of commonly used
automated evaluation systems in measuring LLM performance on our task. We
investigate various LLM-as-Judge systems on AbGen-Eval, providing insights for
future research on developing more effective and reliable LLM-based evaluation
systems for complex scientific tasks.

</details>


### [51] [HapticCap: A Multimodal Dataset and Task for Understanding User Experience of Vibration Haptic Signals](https://arxiv.org/abs/2507.13318)
*Guimin Hu,Daniel Hershcovich,Hasti Seifi*

Main category: cs.CL

TL;DR: 论文提出了HapticCap数据集，用于匹配触觉振动信号与用户描述，解决了数据标注和模型能力不足的问题，并通过对比学习框架展示了最佳性能。


<details>
  <summary>Details</summary>
Motivation: 设计有意义的触觉信号具有挑战性，现有数据集和模型在描述振动信号方面存在不足。

Method: 创建HapticCap数据集，提出触觉描述检索任务，并采用监督对比学习框架结合T5和AST模型。

Result: T5和AST模型的组合在触觉描述检索任务中表现最佳，尤其是在分类训练时。

Conclusion: HapticCap和提出的方法为触觉信号与文本描述的匹配提供了有效解决方案。

Abstract: Haptic signals, from smartphone vibrations to virtual reality touch feedback,
can effectively convey information and enhance realism, but designing signals
that resonate meaningfully with users is challenging. To facilitate this, we
introduce a multimodal dataset and task, of matching user descriptions to
vibration haptic signals, and highlight two primary challenges: (1) lack of
large haptic vibration datasets annotated with textual descriptions as
collecting haptic descriptions is time-consuming, and (2) limited capability of
existing tasks and models to describe vibration signals in text. To advance
this area, we create HapticCap, the first fully human-annotated
haptic-captioned dataset, containing 92,070 haptic-text pairs for user
descriptions of sensory, emotional, and associative attributes of vibrations.
Based on HapticCap, we propose the haptic-caption retrieval task and present
the results of this task from a supervised contrastive learning framework that
brings together text representations within specific categories and vibrations.
Overall, the combination of language model T5 and audio model AST yields the
best performance in the haptic-caption retrieval task, especially when
separately trained for each description category.

</details>


### [52] [Social and Political Framing in Search Engine Results](https://arxiv.org/abs/2507.13325)
*Amrit Poudel,Tim Weninger*

Main category: cs.CL

TL;DR: 研究探讨搜索引擎如何通过内容优先级和用户意识形态查询加剧搜索结果的偏见，揭示其对公共话语的影响。


<details>
  <summary>Details</summary>
Motivation: 探索搜索引擎和用户意识形态查询如何共同导致搜索结果偏见，填补现有研究的空白。

Method: 分析主要搜索引擎对政治和社会话题的输出，使用相关数据集。

Result: 搜索引擎优先反映偏见内容，用户意识形态查询进一步加剧偏见，不同搜索引擎在内容优先级上存在显著差异。

Conclusion: 搜索引擎可能通过强化意识形态分歧塑造公众认知，加剧信息极化问题。

Abstract: Search engines play a crucial role in shaping public discourse by influencing
how information is accessed and framed. While prior research has extensively
examined various dimensions of search bias -- such as content prioritization,
indexical bias, political polarization, and sources of bias -- an important
question remains underexplored: how do search engines and
ideologically-motivated user queries contribute to bias in search results. This
study analyzes the outputs of major search engines using a dataset of political
and social topics. The findings reveal that search engines not only prioritize
content in ways that reflect underlying biases but also that
ideologically-driven user queries exacerbate these biases, resulting in the
amplification of specific narratives. Moreover, significant differences were
observed across search engines in terms of the sources they prioritize. These
results suggest that search engines may play a pivotal role in shaping public
perceptions by reinforcing ideological divides, thereby contributing to the
broader issue of information polarization.

</details>


### [53] [Vision-and-Language Training Helps Deploy Taxonomic Knowledge but Does Not Fundamentally Alter It](https://arxiv.org/abs/2507.13328)
*Yulu Qin,Dheeraj Varghese,Adam Dahlgren Lindström,Lucia Donatelli,Kanishka Misra,Najoung Kim*

Main category: cs.CL

TL;DR: VL训练对语言模型的词汇-概念知识部署有影响，但对知识本身改变不大。


<details>
  <summary>Details</summary>
Motivation: 探讨视觉-语言（VL）训练是否显著改变语言模型的词汇-概念知识表示。

Method: 比较纯文本语言模型（LM）和VL训练模型（VLM）在需要分类学理解的问答任务中的表现，并进行行为与表征分析。

Result: VL模型在分类学相关任务中表现更好，但分类学知识本身未显著改变。

Conclusion: VL训练改善了知识在任务中的部署，而非知识本身。

Abstract: Does vision-and-language (VL) training change the linguistic representations
of language models in meaningful ways? Most results in the literature have
shown inconsistent or marginal differences, both behaviorally and
representationally. In this work, we start from the hypothesis that the domain
in which VL training could have a significant effect is lexical-conceptual
knowledge, in particular its taxonomic organization. Through comparing minimal
pairs of text-only LMs and their VL-trained counterparts, we first show that
the VL models often outperform their text-only counterparts on a text-only
question-answering task that requires taxonomic understanding of concepts
mentioned in the questions. Using an array of targeted behavioral and
representational analyses, we show that the LMs and VLMs do not differ
significantly in terms of their taxonomic knowledge itself, but they differ in
how they represent questions that contain concepts in a taxonomic relation vs.
a non-taxonomic relation. This implies that the taxonomic knowledge itself does
not change substantially through additional VL training, but VL training does
improve the deployment of this knowledge in the context of a specific task,
even when the presentation of the task is purely linguistic.

</details>


### [54] [The Imitation Game: Turing Machine Imitator is Length Generalizable Reasoner](https://arxiv.org/abs/2507.13332)
*Zhouqi Hua,Wenwei Zhang,Chengqi Lyu,Yuzhe Gu,Songyang Gao,Kuikun Liu,Kai Chen*

Main category: cs.CL

TL;DR: 论文提出TAIL方法，通过模仿图灵机执行过程生成合成数据，提升大语言模型在长度泛化任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 解决Transformer大语言模型在长度泛化任务中的核心挑战，现有方法多为任务特定且性能有限。

Method: 提出TAIL方法，通过计算机程序合成模仿图灵机执行过程的链式思维数据，线性扩展推理步骤。

Result: TAIL显著提升Qwen2.5-7B模型在多种任务中的长度泛化能力，超越现有方法。

Conclusion: 图灵机中的关键概念对长度泛化至关重要，为未来基于合成数据的LLM推理研究提供方向。

Abstract: Length generalization, the ability to solve problems of longer sequences than
those observed during training, poses a core challenge of Transformer-based
large language models (LLM). Although existing studies have predominantly
focused on data-driven approaches for arithmetic operations and symbolic
manipulation tasks, these approaches tend to be task-specific with limited
overall performance. To pursue a more general solution, this paper focuses on a
broader case of reasoning problems that are computable, i.e., problems that
algorithms can solve, thus can be solved by the Turing Machine. From this
perspective, this paper proposes Turing MAchine Imitation Learning (TAIL) to
improve the length generalization ability of LLMs. TAIL synthesizes
chain-of-thoughts (CoT) data that imitate the execution process of a Turing
Machine by computer programs, which linearly expands the reasoning steps into
atomic states to alleviate shortcut learning and explicit memory fetch
mechanism to reduce the difficulties of dynamic and long-range data access in
elementary operations. To validate the reliability and universality of TAIL, we
construct a challenging synthetic dataset covering 8 classes of algorithms and
18 tasks. Without bells and whistles, TAIL significantly improves the length
generalization ability as well as the performance of Qwen2.5-7B on various
tasks using only synthetic data, surpassing previous methods and DeepSeek-R1.
The experimental results reveal that the key concepts in the Turing Machine,
instead of the thinking styles, are indispensable for TAIL for length
generalization, through which the model exhibits read-and-write behaviors
consistent with the properties of the Turing Machine in their attention layers.
This work provides a promising direction for future research in the learning of
LLM reasoning from synthetic data.

</details>


### [55] [A Survey of Context Engineering for Large Language Models](https://arxiv.org/abs/2507.13334)
*Lingrui Mei,Jiayu Yao,Yuyao Ge,Yiwei Wang,Baolong Bi,Yujun Cai,Jiazhi Liu,Mingyu Li,Zhong-Zhi Li,Duzhen Zhang,Chenlin Zhou,Jiayi Mao,Tianze Xia,Jiafeng Guo,Shenghua Liu*

Main category: cs.CL

TL;DR: 该论文提出了一种名为“上下文工程”的正式学科，旨在系统优化大型语言模型（LLMs）的上下文信息。通过分析1300多篇研究论文，论文建立了该领域的技术路线图，并揭示了模型能力的不对称性：当前模型在理解复杂上下文方面表现出色，但在生成长篇输出方面存在明显限制。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型的性能高度依赖于推理过程中提供的上下文信息，但目前缺乏系统化的方法来优化这些信息。因此，论文旨在通过上下文工程填补这一空白。

Method: 论文将上下文工程分解为基础组件（上下文检索与生成、处理和管理）和系统实现（检索增强生成、记忆系统、工具集成推理和多智能体系统），并通过文献综述进行分析。

Result: 研究发现，尽管当前模型在理解复杂上下文方面表现优异，但在生成长篇输出方面存在显著限制。

Conclusion: 论文为上下文感知AI的研究和工程提供了统一框架，并指出解决模型能力不对称性是未来研究的关键方向。

Abstract: The performance of Large Language Models (LLMs) is fundamentally determined
by the contextual information provided during inference. This survey introduces
Context Engineering, a formal discipline that transcends simple prompt design
to encompass the systematic optimization of information payloads for LLMs. We
present a comprehensive taxonomy decomposing Context Engineering into its
foundational components and the sophisticated implementations that integrate
them into intelligent systems. We first examine the foundational components:
context retrieval and generation, context processing and context management. We
then explore how these components are architecturally integrated to create
sophisticated system implementations: retrieval-augmented generation (RAG),
memory systems and tool-integrated reasoning, and multi-agent systems. Through
this systematic analysis of over 1300 research papers, our survey not only
establishes a technical roadmap for the field but also reveals a critical
research gap: a fundamental asymmetry exists between model capabilities. While
current models, augmented by advanced context engineering, demonstrate
remarkable proficiency in understanding complex contexts, they exhibit
pronounced limitations in generating equally sophisticated, long-form outputs.
Addressing this gap is a defining priority for future research. Ultimately,
this survey provides a unified framework for both researchers and engineers
advancing context-aware AI.

</details>


### [56] [Comparing Apples to Oranges: A Dataset & Analysis of LLM Humour Understanding from Traditional Puns to Topical Jokes](https://arxiv.org/abs/2507.13335)
*Tyler Loakman,William Thorne,Chenghua Lin*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）解释不同类型幽默的能力，发现现有模型无法可靠解释所有笑话类型，凸显计算幽默研究的局限性。


<details>
  <summary>Details</summary>
Motivation: 现有计算幽默研究集中于简短双关笑话，忽略复杂幽默形式。本文旨在评估LLMs解释不同类型幽默的能力。

Method: 构建包含600个笑话的数据集，涵盖4种类型，手动编写高质量解释，比较多种LLMs的零样本解释能力。

Result: 测试的LLMs（包括推理模型）均无法可靠生成所有笑话类型的充分解释。

Conclusion: 研究揭示计算幽默领域对简单笑话形式的过度关注，需进一步探索复杂幽默的解释能力。

Abstract: Humour, as a complex language form, is derived from myriad aspects of life,
whilst existing work on computational humour has focussed almost exclusively on
short pun-based jokes. In this work, we investigate whether the ability of
Large Language Models (LLMs) to explain humour depends on the particular humour
form. We compare models on simple puns and more complex topical humour that
requires knowledge of real-world entities and events. In doing so, we curate a
dataset of 600 jokes split across 4 joke types and manually write high-quality
explanations. These jokes include heterographic and homographic puns,
contemporary internet humour, and topical jokes, where understanding relies on
reasoning beyond "common sense", rooted instead in world knowledge regarding
news events and pop culture. Using this dataset, we compare the zero-shot
abilities of a range of LLMs to accurately and comprehensively explain jokes of
different types, identifying key research gaps in the task of humour
explanation. We find that none of the tested models (inc. reasoning models) are
capable of reliably generating adequate explanations of all joke types, further
highlighting the narrow focus of most works in computational humour on overly
simple joke forms.

</details>
