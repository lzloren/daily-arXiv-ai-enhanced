{"id": "2506.20702", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2506.20702", "abs": "https://arxiv.org/abs/2506.20702", "authors": ["Yoshua Bengio", "Tegan Maharaj", "Luke Ong", "Stuart Russell", "Dawn Song", "Max Tegmark", "Lan Xue", "Ya-Qin Zhang", "Stephen Casper", "Wan Sie Lee", "S\u00f6ren Mindermann", "Vanessa Wilfred", "Vidhisha Balachandran", "Fazl Barez", "Michael Belinsky", "Imane Bello", "Malo Bourgon", "Mark Brakel", "Sim\u00e9on Campos", "Duncan Cass-Beggs", "Jiahao Chen", "Rumman Chowdhury", "Kuan Chua Seah", "Jeff Clune", "Juntao Dai", "Agnes Delaborde", "Nouha Dziri", "Francisco Eiras", "Joshua Engels", "Jinyu Fan", "Adam Gleave", "Noah Goodman", "Fynn Heide", "Dan Hendrycks", "Cyrus Hodes", "Bryan Low Kian Hsiang", "Minlie Huang", "Sami Jawhar", "Wang Jingyu", "Adam Tauman Kalai", "Meindert Kamphuis", "Mohan Kankanhalli", "Subhash Kantamneni", "Mathias Bonde Kirk", "Thomas Kwa", "Jeffrey Ladish", "Kwok-Yan Lam", "Wan Lee Sie", "Taewhi Lee", "Xiaojian Li", "Jiajun Liu", "Chaochao Lu", "Yifan Mai", "Richard Mallah", "Julian Michael", "Nick Mo\u00ebs", "Simon M\u00f6ller", "Kihyuk Nam", "Kwan Yee Ng", "Mark Nitzberg", "Besmira Nushi", "Se\u00e1n O h\u00c9igeartaigh", "Alejandro Ortega", "Pierre Peign\u00e9", "James Petrie", "Benjamin Prud'Homme", "Reihaneh Rabbany", "Nayat Sanchez-Pi", "Sarah Schwettmann", "Buck Shlegeris", "Saad Siddiqui", "Aradhana Sinha", "Mart\u00edn Soto", "Cheston Tan", "Dong Ting", "Robert Trager", "Brian Tse", "Anthony Tung K. H.", "Vanessa Wilfred", "John Willes", "Denise Wong", "Wei Xu", "Rongwu Xu", "Yi Zeng", "HongJiang Zhang", "Djordje \u017dikeli\u0107"], "title": "The Singapore Consensus on Global AI Safety Research Priorities", "comment": "Final report from the \"2025 Singapore Conference on AI (SCAI)\" held\n  April 26: https://www.scai.gov.sg/2025/scai2025-report", "summary": "Rapidly improving AI capabilities and autonomy hold significant promise of\ntransformation, but are also driving vigorous debate on how to ensure that AI\nis safe, i.e., trustworthy, reliable, and secure. Building a trusted ecosystem\nis therefore essential -- it helps people embrace AI with confidence and gives\nmaximal space for innovation while avoiding backlash.\n  The \"2025 Singapore Conference on AI (SCAI): International Scientific\nExchange on AI Safety\" aimed to support research in this space by bringing\ntogether AI scientists across geographies to identify and synthesise research\npriorities in AI safety. This resulting report builds on the International AI\nSafety Report chaired by Yoshua Bengio and backed by 33 governments. By\nadopting a defence-in-depth model, this report organises AI safety research\ndomains into three types: challenges with creating trustworthy AI systems\n(Development), challenges with evaluating their risks (Assessment), and\nchallenges with monitoring and intervening after deployment (Control).", "AI": {"tldr": "\u65b0\u52a0\u57612025\u5e74AI\u5b89\u5168\u4f1a\u8bae\u62a5\u544a\u63d0\u51faAI\u5b89\u5168\u7814\u7a76\u7684\u4e09\u4e2a\u9886\u57df\uff1a\u5f00\u53d1\u53ef\u4fe1AI\u7cfb\u7edf\u3001\u8bc4\u4f30\u98ce\u9669\u53ca\u90e8\u7f72\u540e\u7684\u76d1\u63a7\u4e0e\u5e72\u9884\u3002", "motivation": "\u968f\u7740AI\u80fd\u529b\u7684\u5feb\u901f\u63d0\u5347\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u3001\u53ef\u4fe1\u3001\u53ef\u9760\u548c\u5b89\u5168\u7684\u5fc5\u8981\u6027\u65e5\u76ca\u51f8\u663e\uff0c\u9700\u8981\u5efa\u7acb\u4fe1\u4efb\u751f\u6001\u7cfb\u7edf\u4ee5\u4fc3\u8fdb\u521b\u65b0\u5e76\u907f\u514d\u53cd\u5f39\u3002", "method": "\u91c7\u7528\u6df1\u5ea6\u9632\u5fa1\u6a21\u578b\uff0c\u5c06AI\u5b89\u5168\u7814\u7a76\u5206\u4e3a\u5f00\u53d1\u3001\u8bc4\u4f30\u548c\u63a7\u5236\u4e09\u4e2a\u9886\u57df\u3002", "result": "\u62a5\u544a\u603b\u7ed3\u4e86AI\u5b89\u5168\u7814\u7a76\u7684\u4f18\u5148\u4e8b\u9879\uff0c\u5e76\u57fa\u4e8e\u56fd\u9645AI\u5b89\u5168\u62a5\u544a\u63d0\u51fa\u4e86\u5177\u4f53\u65b9\u5411\u3002", "conclusion": "\u901a\u8fc7\u591a\u9886\u57df\u534f\u4f5c\u548c\u56fd\u9645\u5408\u4f5c\uff0c\u53ef\u4ee5\u63a8\u52a8AI\u5b89\u5168\u7814\u7a76\uff0c\u786e\u4fddAI\u6280\u672f\u7684\u5065\u5eb7\u53d1\u5c55\u3002"}}
{"id": "2506.20737", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20737", "abs": "https://arxiv.org/abs/2506.20737", "authors": ["Gurusha Juneja", "Alon Albalak", "Wenyue Hua", "William Yang Wang"], "title": "MAGPIE: A dataset for Multi-AGent contextual PrIvacy Evaluation", "comment": null, "summary": "The proliferation of LLM-based agents has led to increasing deployment of\ninter-agent collaboration for tasks like scheduling, negotiation, resource\nallocation etc. In such systems, privacy is critical, as agents often access\nproprietary tools and domain-specific databases requiring strict\nconfidentiality. This paper examines whether LLM-based agents demonstrate an\nunderstanding of contextual privacy. And, if instructed, do these systems\npreserve inference time user privacy in non-adversarial multi-turn\nconversation. Existing benchmarks to evaluate contextual privacy in LLM-agents\nprimarily assess single-turn, low-complexity tasks where private information\ncan be easily excluded. We first present a benchmark - MAGPIE comprising 158\nreal-life high-stakes scenarios across 15 domains. These scenarios are designed\nsuch that complete exclusion of private data impedes task completion yet\nunrestricted information sharing could lead to substantial losses. We then\nevaluate the current state-of-the-art LLMs on (a) their understanding of\ncontextually private data and (b) their ability to collaborate without\nviolating user privacy. Empirical experiments demonstrate that current models,\nincluding GPT-4o and Claude-2.7-Sonnet, lack robust understanding of contextual\nprivacy, misclassifying private data as shareable 25.2\\% and 43.6\\% of the\ntime. In multi-turn conversations, these models disclose private information in\n59.9\\% and 50.5\\% of cases even under explicit privacy instructions.\nFurthermore, multi-agent systems fail to complete tasks in 71\\% of scenarios.\nThese results underscore that current models are not aligned towards both\ncontextual privacy preservation and collaborative task-solving.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86LLM\u667a\u80fd\u4f53\u5728\u534f\u4f5c\u4efb\u52a1\u4e2d\u662f\u5426\u7406\u89e3\u4e0a\u4e0b\u6587\u9690\u79c1\uff0c\u5e76\u8bc4\u4f30\u4e86\u73b0\u6709\u6a21\u578b\u5728\u4fdd\u62a4\u9690\u79c1\u65b9\u9762\u7684\u8868\u73b0\uff0c\u53d1\u73b0\u5176\u80fd\u529b\u4e0d\u8db3\u3002", "motivation": "\u968f\u7740LLM\u667a\u80fd\u4f53\u7684\u5e7f\u6cdb\u5e94\u7528\uff0c\u9690\u79c1\u4fdd\u62a4\u5728\u591a\u4ee3\u7406\u534f\u4f5c\u4efb\u52a1\u4e2d\u53d8\u5f97\u81f3\u5173\u91cd\u8981\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u6807\u51c6\u8fc7\u4e8e\u7b80\u5355\uff0c\u65e0\u6cd5\u53cd\u6620\u771f\u5b9e\u573a\u666f\u7684\u590d\u6742\u6027\u3002", "method": "\u63d0\u51fa\u4e86MAGPIE\u57fa\u51c6\uff0c\u5305\u542b158\u4e2a\u9ad8\u98ce\u9669\u573a\u666f\uff0c\u8bc4\u4f30LLM\u667a\u80fd\u4f53\u5bf9\u4e0a\u4e0b\u6587\u9690\u79c1\u7684\u7406\u89e3\u53ca\u534f\u4f5c\u80fd\u529b\u3002", "result": "\u5f53\u524d\u6a21\u578b\uff08\u5982GPT-4o\u548cClaude-2.7-Sonnet\uff09\u5728\u9690\u79c1\u5206\u7c7b\u548c\u591a\u8f6e\u5bf9\u8bdd\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u9690\u79c1\u6cc4\u9732\u7387\u8f83\u9ad8\uff0c\u4e14\u4efb\u52a1\u5b8c\u6210\u7387\u4f4e\u3002", "conclusion": "\u73b0\u6709\u6a21\u578b\u5728\u4e0a\u4e0b\u6587\u9690\u79c1\u4fdd\u62a4\u548c\u534f\u4f5c\u4efb\u52a1\u5b8c\u6210\u65b9\u9762\u5b58\u5728\u663e\u8457\u4e0d\u8db3\uff0c\u9700\u8fdb\u4e00\u6b65\u6539\u8fdb\u3002"}}
{"id": "2506.20815", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2506.20815", "abs": "https://arxiv.org/abs/2506.20815", "authors": ["Xinye Tang", "Haijun Zhai", "Chaitanya Belwal", "Vineeth Thayanithi", "Philip Baumann", "Yogesh K Roy"], "title": "Dynamic Context-Aware Prompt Recommendation for Domain-Specific AI Applications", "comment": null, "summary": "LLM-powered applications are highly susceptible to the quality of user\nprompts, and crafting high-quality prompts can often be challenging especially\nfor domain-specific applications. This paper presents a novel dynamic\ncontext-aware prompt recommendation system for domain-specific AI applications.\nOur solution combines contextual query analysis, retrieval-augmented knowledge\ngrounding, hierarchical skill organization, and adaptive skill ranking to\ngenerate relevant and actionable prompt suggestions.\n  The system leverages behavioral telemetry and a two-stage hierarchical\nreasoning process to dynamically select and rank relevant skills, and\nsynthesizes prompts using both predefined and adaptive templates enhanced with\nfew-shot learning. Experiments on real-world datasets demonstrate that our\napproach achieves high usefulness and relevance, as validated by both automated\nand expert evaluations.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u52a8\u6001\u4e0a\u4e0b\u6587\u611f\u77e5\u7684\u63d0\u793a\u63a8\u8350\u7cfb\u7edf\uff0c\u7528\u4e8e\u63d0\u5347\u9886\u57df\u7279\u5b9aAI\u5e94\u7528\u4e2d\u7528\u6237\u63d0\u793a\u7684\u8d28\u91cf\u3002", "motivation": "LLM\u5e94\u7528\u5bf9\u7528\u6237\u63d0\u793a\u8d28\u91cf\u9ad8\u5ea6\u654f\u611f\uff0c\u800c\u9886\u57df\u7279\u5b9a\u5e94\u7528\u4e2d\u9ad8\u8d28\u91cf\u63d0\u793a\u7684\u751f\u6210\u5c24\u4e3a\u56f0\u96be\u3002", "method": "\u7ed3\u5408\u4e0a\u4e0b\u6587\u67e5\u8be2\u5206\u6790\u3001\u68c0\u7d22\u589e\u5f3a\u7684\u77e5\u8bc6\u57fa\u7840\u3001\u5206\u5c42\u6280\u80fd\u7ec4\u7ec7\u548c\u81ea\u9002\u5e94\u6280\u80fd\u6392\u540d\uff0c\u52a8\u6001\u751f\u6210\u76f8\u5173\u4e14\u53ef\u64cd\u4f5c\u7684\u63d0\u793a\u5efa\u8bae\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u96c6\u4e0a\u7684\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u81ea\u52a8\u548c\u4e13\u5bb6\u8bc4\u4f30\u4e2d\u5747\u8868\u73b0\u51fa\u9ad8\u5b9e\u7528\u6027\u548c\u76f8\u5173\u6027\u3002", "conclusion": "\u8be5\u7cfb\u7edf\u80fd\u6709\u6548\u63d0\u5347\u9886\u57df\u7279\u5b9aAI\u5e94\u7528\u4e2d\u63d0\u793a\u7684\u8d28\u91cf\u548c\u7528\u6237\u4f53\u9a8c\u3002"}}
{"id": "2506.20949", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.20949", "abs": "https://arxiv.org/abs/2506.20949", "authors": ["Chenkai Sun", "Denghui Zhang", "ChengXiang Zhai", "Heng Ji"], "title": "Beyond Reactive Safety: Risk-Aware LLM Alignment via Long-Horizon Simulation", "comment": null, "summary": "Given the growing influence of language model-based agents on high-stakes\nsocietal decisions, from public policy to healthcare, ensuring their beneficial\nimpact requires understanding the far-reaching implications of their\nsuggestions. We propose a proof-of-concept framework that projects how\nmodel-generated advice could propagate through societal systems on a\nmacroscopic scale over time, enabling more robust alignment. To assess the\nlong-term safety awareness of language models, we also introduce a dataset of\n100 indirect harm scenarios, testing models' ability to foresee adverse,\nnon-obvious outcomes from seemingly harmless user prompts. Our approach\nachieves not only over 20% improvement on the new dataset but also an average\nwin rate exceeding 70% against strong baselines on existing safety benchmarks\n(AdvBench, SafeRLHF, WildGuardMix), suggesting a promising direction for safer\nagents.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u6846\u67b6\uff0c\u7528\u4e8e\u8bc4\u4f30\u8bed\u8a00\u6a21\u578b\u5efa\u8bae\u7684\u5b8f\u89c2\u793e\u4f1a\u5f71\u54cd\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u95f4\u63a5\u5371\u5bb3\u573a\u666f\u6570\u636e\u96c6\uff0c\u4ee5\u63d0\u5347\u6a21\u578b\u7684\u5b89\u5168\u610f\u8bc6\u3002", "motivation": "\u968f\u7740\u8bed\u8a00\u6a21\u578b\u5728\u793e\u4f1a\u51b3\u7b56\u4e2d\u7684\u5f71\u54cd\u529b\u589e\u52a0\uff0c\u9700\u8981\u786e\u4fdd\u5176\u5efa\u8bae\u7684\u957f\u671f\u5b89\u5168\u6027\u548c\u6709\u76ca\u6027\u3002", "method": "\u63d0\u51fa\u4e86\u4e00\u4e2a\u6982\u5ff5\u9a8c\u8bc1\u6846\u67b6\uff0c\u7528\u4e8e\u6a21\u62df\u6a21\u578b\u5efa\u8bae\u5728\u5b8f\u89c2\u793e\u4f1a\u7cfb\u7edf\u4e2d\u7684\u4f20\u64ad\uff0c\u5e76\u5f15\u5165\u4e86\u4e00\u4e2a\u5305\u542b100\u4e2a\u95f4\u63a5\u5371\u5bb3\u573a\u666f\u7684\u6570\u636e\u96c6\u3002", "result": "\u5728\u65b0\u6570\u636e\u96c6\u4e0a\u5b9e\u73b0\u4e8620%\u4ee5\u4e0a\u7684\u6539\u8fdb\uff0c\u5e76\u5728\u73b0\u6709\u5b89\u5168\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u5e73\u5747\u80dc\u7387\u8d85\u8fc770%\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u66f4\u5b89\u5168\u7684\u8bed\u8a00\u6a21\u578b\u4ee3\u7406\u63d0\u4f9b\u4e86\u6709\u524d\u666f\u7684\u65b9\u5411\u3002"}}
{"id": "2506.21215", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2506.21215", "abs": "https://arxiv.org/abs/2506.21215", "authors": ["Haoang Chi", "He Li", "Wenjing Yang", "Feng Liu", "Long Lan", "Xiaoguang Ren", "Tongliang Liu", "Bo Han"], "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?", "comment": "24 pages, accepted at NeurIPS 2024", "summary": "Causal reasoning capability is critical in advancing large language models\n(LLMs) toward strong artificial intelligence. While versatile LLMs appear to\nhave demonstrated capabilities in understanding contextual causality and\nproviding responses that obey the laws of causality, it remains unclear whether\nthey perform genuine causal reasoning akin to humans. However, current evidence\nindicates the contrary. Specifically, LLMs are only capable of performing\nshallow (level-1) causal reasoning, primarily attributed to the causal\nknowledge embedded in their parameters, but they lack the capacity for genuine\nhuman-like (level-2) causal reasoning. To support this hypothesis,\nmethodologically, we delve into the autoregression mechanism of\ntransformer-based LLMs, revealing that it is not inherently causal.\nEmpirically, we introduce a new causal Q&A benchmark called CausalProbe-2024,\nwhose corpora are fresh and nearly unseen for the studied LLMs. The LLMs\nexhibit a significant performance drop on CausalProbe-2024 compared to earlier\nbenchmarks, indicating the fact that they primarily engage in level-1 causal\nreasoning. To bridge the gap towards level-2 causal reasoning, we draw\ninspiration from the fact that human reasoning is usually facilitated by\ngeneral knowledge and intended goals. We propose G^2-Reasoner, a method that\nincorporates general knowledge and goal-oriented prompts into LLMs' causal\nreasoning processes. Experiments demonstrate that G^2-Reasoner significantly\nenhances LLMs' causal reasoning capability, particularly in fresh and\ncounterfactual contexts. This work sheds light on a new path for LLMs to\nadvance towards genuine causal reasoning, going beyond level-1 and making\nstrides towards level-2.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u6307\u51fa\u5176\u4ec5\u80fd\u8fdb\u884c\u6d45\u5c42\uff08level-1\uff09\u63a8\u7406\uff0c\u7f3a\u4e4f\u4eba\u7c7b\u5f0f\u6df1\u5c42\uff08level-2\uff09\u63a8\u7406\u3002\u901a\u8fc7\u65b0\u57fa\u51c6CausalProbe-2024\u9a8c\u8bc1\uff0c\u5e76\u63d0\u51faG^2-Reasoner\u65b9\u6cd5\u63d0\u5347LLMs\u7684\u56e0\u679c\u63a8\u7406\u80fd\u529b\u3002", "motivation": "\u7814\u7a76LLMs\u662f\u5426\u5177\u5907\u4eba\u7c7b\u5f0f\u56e0\u679c\u63a8\u7406\u80fd\u529b\uff0c\u53d1\u73b0\u5176\u4ec5\u80fd\u8fdb\u884c\u6d45\u5c42\u63a8\u7406\uff0c\u9700\u6539\u8fdb\u4ee5\u5b9e\u73b0\u6df1\u5c42\u63a8\u7406\u3002", "method": "\u5206\u6790LLMs\u7684\u81ea\u56de\u5f52\u673a\u5236\uff0c\u63d0\u51fa\u65b0\u57fa\u51c6CausalProbe-2024\uff0c\u5e76\u8bbe\u8ba1G^2-Reasoner\u65b9\u6cd5\u7ed3\u5408\u901a\u7528\u77e5\u8bc6\u548c\u76ee\u6807\u5bfc\u5411\u63d0\u793a\u3002", "result": "LLMs\u5728CausalProbe-2024\u4e0a\u8868\u73b0\u663e\u8457\u4e0b\u964d\uff0cG^2-Reasoner\u663e\u8457\u63d0\u5347\u5176\u5728\u65b0\u9c9c\u548c\u53cd\u4e8b\u5b9e\u60c5\u5883\u4e2d\u7684\u63a8\u7406\u80fd\u529b\u3002", "conclusion": "G^2-Reasoner\u4e3aLLMs\u5b9e\u73b0\u6df1\u5c42\u56e0\u679c\u63a8\u7406\u63d0\u4f9b\u4e86\u65b0\u8def\u5f84\uff0c\u63a8\u52a8\u5176\u5411\u5f3a\u4eba\u5de5\u667a\u80fd\u53d1\u5c55\u3002"}}
{"id": "2506.21230", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2506.21230", "abs": "https://arxiv.org/abs/2506.21230", "authors": ["Junhao Shi", "Zhaoye Fei", "Siyin Wang", "Qipeng Guo", "Jingjing Gong", "Xipeng QIu"], "title": "World-aware Planning Narratives Enhance Large Vision-Language Model Planner", "comment": null, "summary": "Large Vision-Language Models (LVLMs) show promise for embodied planning tasks\nbut struggle with complex scenarios involving unfamiliar environments and\nmulti-step goals. Current approaches rely on environment-agnostic imitation\nlearning that disconnects instructions from environmental contexts, causing\nmodels to struggle with context-sensitive instructions and rely on\nsupplementary cues rather than visual reasoning during long-horizon\ninteractions. In this work, we propose World-Aware Planning Narrative\nEnhancement (WAP), a framework that infuses LVLMs with comprehensive\nenvironmental understanding through four cognitive capabilities (visual\nappearance modeling, spatial reasoning, functional abstraction, and syntactic\ngrounding) while developing and evaluating models using only raw visual\nobservations through curriculum learning. Evaluations on the EB-ALFRED\nbenchmark demonstrate substantial improvements, with Qwen2.5-VL achieving a\n60.7 absolute improvement in task success rates, particularly in commonsense\nreasoning (+60.0) and long-horizon planning (+70.0). Notably, our enhanced\nopen-source models outperform proprietary systems like GPT-4o and\nClaude-3.5-Sonnet by a large margin.", "AI": {"tldr": "WAP\u6846\u67b6\u901a\u8fc7\u56db\u79cd\u8ba4\u77e5\u80fd\u529b\u589e\u5f3aLVLMs\u7684\u73af\u5883\u7406\u89e3\uff0c\u663e\u8457\u63d0\u5347\u4efb\u52a1\u6210\u529f\u7387\u3002", "motivation": "\u73b0\u6709LVLMs\u5728\u590d\u6742\u573a\u666f\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u7f3a\u4e4f\u73af\u5883\u4e0a\u4e0b\u6587\u7406\u89e3\u3002", "method": "\u63d0\u51faWAP\u6846\u67b6\uff0c\u7ed3\u5408\u89c6\u89c9\u5916\u89c2\u5efa\u6a21\u3001\u7a7a\u95f4\u63a8\u7406\u7b49\u529f\u80fd\uff0c\u901a\u8fc7\u8bfe\u7a0b\u5b66\u4e60\u8bad\u7ec3\u6a21\u578b\u3002", "result": "\u5728EB-ALFRED\u57fa\u51c6\u4e0a\u4efb\u52a1\u6210\u529f\u7387\u63d0\u534760.7%\uff0c\u8d85\u8d8aGPT-4o\u7b49\u4e13\u6709\u7cfb\u7edf\u3002", "conclusion": "WAP\u663e\u8457\u63d0\u5347\u4e86LVLMs\u5728\u590d\u6742\u73af\u5883\u4e2d\u7684\u89c4\u5212\u80fd\u529b\u3002"}}
{"id": "2506.21310", "categories": ["cs.AI", "cs.SE", "K.6.3 Software Management"], "pdf": "https://arxiv.org/pdf/2506.21310", "abs": "https://arxiv.org/abs/2506.21310", "authors": ["Pauline Speckmann", "Mario Nadj", "Christian Janiesch"], "title": "IXAII: An Interactive Explainable Artificial Intelligence Interface for Decision Support Systems", "comment": "9 pages, 2 figures, accepted to DESRIST 2025 Prototype Track", "summary": "Although several post-hoc methods for explainable AI have been developed,\nmost are static and neglect the user perspective, limiting their effectiveness\nfor the target audience. In response, we developed the interactive explainable\nintelligent system called IXAII that offers explanations from four explainable\nAI methods: LIME, SHAP, Anchors, and DiCE. Our prototype provides tailored\nviews for five user groups and gives users agency over the explanations'\ncontent and their format. We evaluated IXAII through interviews with experts\nand lay users. Our results indicate that IXAII, which provides different\nexplanations with multiple visualization options, is perceived as helpful to\nincrease transparency. By bridging the gaps between explainable AI methods,\ninteractivity, and practical implementation, we provide a novel perspective on\nAI explanation practices and human-AI interaction.", "AI": {"tldr": "IXAII\u662f\u4e00\u4e2a\u4ea4\u4e92\u5f0f\u53ef\u89e3\u91caAI\u7cfb\u7edf\uff0c\u6574\u5408\u4e86LIME\u3001SHAP\u3001Anchors\u548cDiCE\u56db\u79cd\u65b9\u6cd5\uff0c\u9488\u5bf9\u4e0d\u540c\u7528\u6237\u7fa4\u4f53\u63d0\u4f9b\u5b9a\u5236\u5316\u89e3\u91ca\uff0c\u5e76\u901a\u8fc7\u4e13\u5bb6\u548c\u666e\u901a\u7528\u6237\u8bbf\u8c08\u9a8c\u8bc1\u5176\u6709\u6548\u6027\u3002", "motivation": "\u73b0\u6709\u7684\u4e8b\u540e\u53ef\u89e3\u91caAI\u65b9\u6cd5\u591a\u4e3a\u9759\u6001\u4e14\u5ffd\u89c6\u7528\u6237\u89c6\u89d2\uff0c\u9650\u5236\u4e86\u5176\u5b9e\u9645\u6548\u679c\u3002", "method": "\u5f00\u53d1\u4e86\u4ea4\u4e92\u5f0f\u7cfb\u7edfIXAII\uff0c\u6574\u5408\u591a\u79cd\u89e3\u91ca\u65b9\u6cd5\uff0c\u5e76\u63d0\u4f9b\u5b9a\u5236\u5316\u89c6\u56fe\u548c\u7528\u6237\u63a7\u5236\u6743\u3002", "result": "\u7528\u6237\u8bbf\u8c08\u8868\u660eIXAII\u901a\u8fc7\u591a\u89c6\u89d2\u89e3\u91ca\u548c\u53ef\u89c6\u5316\u9009\u9879\u63d0\u5347\u4e86\u900f\u660e\u5ea6\u611f\u77e5\u3002", "conclusion": "IXAII\u4e3aAI\u89e3\u91ca\u5b9e\u8df5\u548c\u4eba\u673a\u4ea4\u4e92\u63d0\u4f9b\u4e86\u65b0\u89c6\u89d2\uff0c\u586b\u8865\u4e86\u65b9\u6cd5\u3001\u4ea4\u4e92\u6027\u548c\u5b9e\u9645\u5e94\u7528\u4e4b\u95f4\u7684\u7a7a\u767d\u3002"}}
{"id": "2506.21329", "categories": ["cs.AI", "physics.soc-ph", "68", "I.2"], "pdf": "https://arxiv.org/pdf/2506.21329", "abs": "https://arxiv.org/abs/2506.21329", "authors": ["Karthik Duraisamy"], "title": "Active Inference AI Systems for Scientific Discovery", "comment": null, "summary": "The rapid evolution of artificial intelligence has led to expectations of\ntransformative scientific discovery, yet current systems remain fundamentally\nlimited by their operational architectures, brittle reasoning mechanisms, and\ntheir separation from experimental reality. Building on earlier work, we\ncontend that progress in AI-driven science now depends on closing three\nfundamental gaps -- the abstraction gap, the reasoning gap, and the reality gap\n-- rather than on model size/data/test time compute. Scientific reasoning\ndemands internal representations that support simulation of actions and\nresponse, causal structures that distinguish correlation from mechanism, and\ncontinuous calibration. We define active inference AI systems for scientific\ndiscovery as those that (i) maintain long-lived research memories grounded in\ncausal self-supervised foundation models, (ii) symbolic or neuro-symbolic\nplanners equipped with Bayesian guardrails, (iii) grow persistent knowledge\ngraphs where thinking generates novel conceptual nodes, reasoning establishes\ncausal edges, and real-world interaction prunes false connections while\nstrengthening verified pathways, and (iv) refine their internal representations\nthrough closed-loop interaction with both high-fidelity simulators and\nautomated laboratories - an operational loop where mental simulation guides\naction and empirical surprise reshapes understanding. In essence, we outline an\narchitecture where discovery arises from the interplay between internal models\nthat enable counterfactual reasoning and external validation that grounds\nhypotheses in reality. It is also argued that the inherent ambiguity in\nfeedback from simulations and experiments, and underlying uncertainties makes\nhuman judgment indispensable, not as a temporary scaffold but as a permanent\narchitectural component.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51faAI\u9a71\u52a8\u79d1\u5b66\u53d1\u73b0\u7684\u4e09\u5927\u5173\u952e\u5dee\u8ddd\uff08\u62bd\u8c61\u3001\u63a8\u7406\u3001\u73b0\u5b9e\uff09\uff0c\u5e76\u8bbe\u8ba1\u4e86\u4e00\u79cd\u4e3b\u52a8\u63a8\u7406AI\u7cfb\u7edf\u67b6\u6784\uff0c\u5f3a\u8c03\u56e0\u679c\u5efa\u6a21\u3001\u95ed\u73af\u9a8c\u8bc1\u53ca\u4eba\u673a\u534f\u4f5c\u7684\u91cd\u8981\u6027\u3002", "motivation": "\u5f53\u524dAI\u7cfb\u7edf\u5728\u79d1\u5b66\u53d1\u73b0\u4e2d\u5b58\u5728\u5c40\u9650\u6027\uff0c\u9700\u89e3\u51b3\u62bd\u8c61\u3001\u63a8\u7406\u548c\u73b0\u5b9e\u4e09\u5927\u5dee\u8ddd\uff0c\u4ee5\u63a8\u52a8AI\u9a71\u52a8\u7684\u79d1\u5b66\u8fdb\u6b65\u3002", "method": "\u63d0\u51fa\u4e3b\u52a8\u63a8\u7406AI\u7cfb\u7edf\uff0c\u5305\u62ec\u56e0\u679c\u81ea\u76d1\u7763\u57fa\u7840\u6a21\u578b\u3001\u8d1d\u53f6\u65af\u7ea6\u675f\u7684\u7b26\u53f7/\u795e\u7ecf\u7b26\u53f7\u89c4\u5212\u5668\u3001\u6301\u4e45\u77e5\u8bc6\u56fe\u8c31\u6784\u5efa\u53ca\u95ed\u73af\u5b9e\u9a8c\u9a8c\u8bc1\u3002", "result": "\u7cfb\u7edf\u901a\u8fc7\u5185\u90e8\u6a21\u578b\u4e0e\u5916\u90e8\u9a8c\u8bc1\u7684\u4ea4\u4e92\u5b9e\u73b0\u79d1\u5b66\u53d1\u73b0\uff0c\u5f3a\u8c03\u4eba\u673a\u534f\u4f5c\u7684\u4e0d\u53ef\u6216\u7f3a\u6027\u3002", "conclusion": "AI\u9a71\u52a8\u7684\u79d1\u5b66\u53d1\u73b0\u9700\u7ed3\u5408\u56e0\u679c\u63a8\u7406\u3001\u95ed\u73af\u9a8c\u8bc1\u53ca\u4eba\u7c7b\u5224\u65ad\uff0c\u5f62\u6210\u53ef\u6301\u7eed\u7684\u67b6\u6784\u3002"}}
{"id": "2506.21393", "categories": ["cs.AI", "68T07 (Primary), 68T50, 68T30, 68T45 (Secondary)", "F.2.2; I.2.7; I.2.10"], "pdf": "https://arxiv.org/pdf/2506.21393", "abs": "https://arxiv.org/abs/2506.21393", "authors": ["Junwen Zhang", "Pu Chen", "Yin Zhang"], "title": "TableMoE: Neuro-Symbolic Routing for Structured Expert Reasoning in Multimodal Table Understanding", "comment": "43 pages and 11 figures", "summary": "Multimodal understanding of tables in real-world contexts is challenging due\nto the complexity of structure, symbolic density, and visual degradation (blur,\nskew, watermarking, incomplete structures or fonts, multi-span or\nhierarchically nested layouts). Existing multimodal large language models\n(MLLMs) struggle with such WildStruct conditions, resulting in limited\nperformance and poor generalization. To address these challenges, we propose\nTableMoE, a neuro-symbolic Mixture-of-Connector-Experts (MoCE) architecture\nspecifically designed for robust, structured reasoning over multimodal table\ndata. TableMoE features an innovative Neuro-Symbolic Routing mechanism, which\npredicts latent semantic token roles (e.g., header, data cell, axis, formula)\nand dynamically routes table elements to specialized experts (Table-to-HTML,\nTable-to-JSON, Table-to-Code) using a confidence-aware gating strategy informed\nby symbolic reasoning graphs. To facilitate effective alignment-driven\npretraining, we introduce the large-scale TableMoE-Align dataset, consisting of\n1.2M table-HTML-JSON-code quadruples across finance, science, biomedicine and\nindustry, utilized exclusively for model pretraining. For evaluation, we curate\nand release four challenging WildStruct benchmarks: WMMFinQA, WMMTatQA,\nWMMTabDialog, and WMMFinanceMath, designed specifically to stress-test models\nunder real-world multimodal degradation and structural complexity. Experimental\nresults demonstrate that TableMoE significantly surpasses existing\nstate-of-the-art models. Extensive ablation studies validate each core\ncomponent, emphasizing the critical role of Neuro-Symbolic Routing and\nstructured expert alignment. Through qualitative analyses, we further showcase\nTableMoE's interpretability and enhanced robustness, underscoring the\neffectiveness of integrating neuro-symbolic reasoning for multimodal table\nunderstanding.", "AI": {"tldr": "TableMoE\u63d0\u51fa\u4e86\u4e00\u79cd\u795e\u7ecf\u7b26\u53f7\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u7528\u4e8e\u5904\u7406\u591a\u6a21\u6001\u8868\u683c\u6570\u636e\u7684\u590d\u6742\u6027\u548c\u89c6\u89c9\u9000\u5316\u95ee\u9898\uff0c\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\u3002", "motivation": "\u73b0\u6709MLLMs\u5728\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728WildStruct\u6761\u4ef6\u4e0b\uff0c\u9700\u8981\u66f4\u9c81\u68d2\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u795e\u7ecf\u7b26\u53f7\u8def\u7531\u673a\u5236\u548c\u6df7\u5408\u4e13\u5bb6\u67b6\u6784\uff0c\u52a8\u6001\u5206\u914d\u8868\u683c\u5143\u7d20\u5230\u4e13\u7528\u4e13\u5bb6\uff0c\u5e76\u5229\u7528\u5927\u89c4\u6a21\u6570\u636e\u96c6\u8fdb\u884c\u9884\u8bad\u7ec3\u3002", "result": "TableMoE\u5728\u56db\u4e2aWildStruct\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u663e\u8457\u4f18\u4e8e\u73b0\u6709\u6a21\u578b\uff0c\u9a8c\u8bc1\u4e86\u5176\u6838\u5fc3\u7ec4\u4ef6\u7684\u6709\u6548\u6027\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u7684\u96c6\u6210\u663e\u8457\u63d0\u5347\u4e86\u591a\u6a21\u6001\u8868\u683c\u7406\u89e3\u7684\u9c81\u68d2\u6027\u548c\u53ef\u89e3\u91ca\u6027\u3002"}}
{"id": "2506.21458", "categories": ["cs.AI", "cs.CL", "cs.CV"], "pdf": "https://arxiv.org/pdf/2506.21458", "abs": "https://arxiv.org/abs/2506.21458", "authors": ["Baiqiao Yin", "Qineng Wang", "Pingyue Zhang", "Jianshu Zhang", "Kangrui Wang", "Zihan Wang", "Jieyu Zhang", "Keshigeyan Chandrasegaran", "Han Liu", "Ranjay Krishna", "Saining Xie", "Manling Li", "Jiajun Wu", "Li Fei-Fei"], "title": "Spatial Mental Modeling from Limited Views", "comment": "Preprint version", "summary": "Can Vision Language Models (VLMs) imagine the full scene from just a few\nviews, like humans do? Humans form spatial mental models, internal\nrepresentations of unseen space, to reason about layout, perspective, and\nmotion. Our new MindCube benchmark with 21,154 questions across 3,268 images\nexposes this critical gap, where existing VLMs exhibit near-random performance.\nUsing MindCube, we systematically evaluate how well VLMs build robust spatial\nmental models through representing positions (cognitive mapping), orientations\n(perspective-taking), and dynamics (mental simulation for \"what-if\" movements).\nWe then explore three approaches to help VLMs approximate spatial mental\nmodels, including unseen intermediate views, natural language reasoning chains,\nand cognitive maps. The significant improvement comes from a synergistic\napproach, \"map-then-reason\", that jointly trains the model to first generate a\ncognitive map and then reason upon it. By training models to reason over these\ninternal maps, we boosted accuracy from 37.8% to 60.8% (+23.0%). Adding\nreinforcement learning pushed performance even further to 70.7% (+32.9%). Our\nkey insight is that such scaffolding of spatial mental models, actively\nconstructing and utilizing internal structured spatial representations with\nflexible reasoning processes, significantly improves understanding of\nunobservable space.", "AI": {"tldr": "MindCube\u57fa\u51c6\u6d4b\u8bd5\u63ed\u793a\u4e86\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\uff08VLMs\uff09\u5728\u6784\u5efa\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u65b9\u9762\u7684\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u534f\u540c\u65b9\u6cd5\u201cmap-then-reason\u201d\u663e\u8457\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u7814\u7a76VLMs\u662f\u5426\u80fd\u50cf\u4eba\u7c7b\u4e00\u6837\u901a\u8fc7\u5c11\u91cf\u89c6\u89d2\u6784\u5efa\u5b8c\u6574\u573a\u666f\u7684\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\uff0c\u4ee5\u5f25\u8865\u73b0\u6709\u6a21\u578b\u5728\u6b64\u65b9\u9762\u7684\u4e0d\u8db3\u3002", "method": "\u4f7f\u7528MindCube\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30VLMs\u7684\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u80fd\u529b\uff0c\u5e76\u63a2\u7d22\u4e09\u79cd\u65b9\u6cd5\uff08\u4e2d\u95f4\u89c6\u56fe\u3001\u81ea\u7136\u8bed\u8a00\u63a8\u7406\u94fe\u3001\u8ba4\u77e5\u5730\u56fe\uff09\u6765\u63d0\u5347\u6027\u80fd\uff0c\u6700\u7ec8\u63d0\u51fa\u201cmap-then-reason\u201d\u534f\u540c\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u201cmap-then-reason\u201d\u65b9\u6cd5\uff0c\u51c6\u786e\u7387\u4ece37.8%\u63d0\u5347\u81f360.8%\uff0c\u7ed3\u5408\u5f3a\u5316\u5b66\u4e60\u540e\u8fdb\u4e00\u6b65\u63d0\u5347\u81f370.7%\u3002", "conclusion": "\u6784\u5efa\u548c\u5229\u7528\u5185\u90e8\u7ed3\u6784\u5316\u7a7a\u95f4\u8868\u793a\u7684\u7a7a\u95f4\u5fc3\u7406\u6a21\u578b\u6846\u67b6\uff0c\u663e\u8457\u63d0\u5347\u4e86VLMs\u5bf9\u4e0d\u53ef\u89c1\u7a7a\u95f4\u7684\u7406\u89e3\u80fd\u529b\u3002"}}
{"id": "2506.21490", "categories": ["cs.AI", "cs.HC", "cs.MA"], "pdf": "https://arxiv.org/pdf/2506.21490", "abs": "https://arxiv.org/abs/2506.21490", "authors": ["Tin Dizdarevi\u0107", "Ravi Hammond", "Tobias Gessler", "Anisoara Calinescu", "Jonathan Cook", "Matteo Gallici", "Andrei Lupu", "Jakob Nicolaus Foerster"], "title": "Ad-Hoc Human-AI Coordination Challenge", "comment": "Published at ICML 2025", "summary": "Achieving seamless coordination between AI agents and humans is crucial for\nreal-world applications, yet it remains a significant open challenge. Hanabi is\na cooperative card game featuring imperfect information, constrained\ncommunication, theory of mind requirements, and coordinated action -- making it\nan ideal testbed for human-AI coordination. However, its use for human-AI\ninteraction has been limited by the challenges of human evaluation. In this\nwork, we introduce the Ad-Hoc Human-AI Coordination Challenge (AH2AC2) to\novercome the constraints of costly and difficult-to-reproduce human\nevaluations. We develop \\textit{human proxy agents} on a large-scale human\ndataset that serve as robust, cheap, and reproducible human-like evaluation\npartners in AH2AC2. To encourage the development of data-efficient methods, we\nopen-source a dataset of 3,079 games, deliberately limiting the amount of\navailable human gameplay data. We present baseline results for both two- and\nthree- player Hanabi scenarios. To ensure fair evaluation, we host the proxy\nagents through a controlled evaluation system rather than releasing them\npublicly. The code is available at\n\\href{https://github.com/FLAIROx/ah2ac2}{https://github.com/FLAIROx/ah2ac2}.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u540d\u4e3aAH2AC2\u7684\u6311\u6218\uff0c\u65e8\u5728\u901a\u8fc7\u4eba\u7c7b\u4ee3\u7406\u4ee3\u7406\u89e3\u51b3\u4eba\u7c7b\u8bc4\u4f30\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u590d\u73b0\u7684\u95ee\u9898\uff0c\u7528\u4e8e\u6d4b\u8bd5AI\u4e0e\u4eba\u7c7b\u5728Hanabi\u6e38\u620f\u4e2d\u7684\u534f\u8c03\u80fd\u529b\u3002", "motivation": "\u5b9e\u73b0AI\u4e0e\u4eba\u7c7b\u7684\u65e0\u7f1d\u534f\u8c03\u662f\u73b0\u5b9e\u5e94\u7528\u4e2d\u7684\u5173\u952e\u6311\u6218\uff0c\u4f46\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u6210\u672c\u9ad8\u4e14\u96be\u4ee5\u590d\u73b0\u3002Hanabi\u6e38\u620f\u56e0\u5176\u7279\u6027\u6210\u4e3a\u7406\u60f3\u6d4b\u8bd5\u5e73\u53f0\uff0c\u4f46\u4eba\u7c7b\u8bc4\u4f30\u9650\u5236\u4e86\u5176\u5e94\u7528\u3002", "method": "\u5f00\u53d1\u4e86\u57fa\u4e8e\u5927\u89c4\u6a21\u4eba\u7c7b\u6570\u636e\u96c6\u7684\u4eba\u7c7b\u4ee3\u7406\u4ee3\u7406\uff0c\u4f5c\u4e3a\u5ec9\u4ef7\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u4f19\u4f34\u3002\u5f00\u6e90\u4e863,079\u5c40\u6e38\u620f\u6570\u636e\uff0c\u5e76\u63d0\u4f9b\u4e86\u4e24\u5230\u4e09\u4ebaHanabi\u573a\u666f\u7684\u57fa\u7ebf\u7ed3\u679c\u3002", "result": "\u63d0\u51fa\u4e86AH2AC2\u6311\u6218\u548c\u4eba\u7c7b\u4ee3\u7406\u4ee3\u7406\uff0c\u89e3\u51b3\u4e86\u8bc4\u4f30\u95ee\u9898\uff0c\u5e76\u901a\u8fc7\u57fa\u7ebf\u7ed3\u679c\u5c55\u793a\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "AH2AC2\u4e3a\u4eba\u7c7b-AI\u534f\u8c03\u7814\u7a76\u63d0\u4f9b\u4e86\u5ec9\u4ef7\u3001\u53ef\u590d\u73b0\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u63a8\u52a8\u4e86\u6570\u636e\u9ad8\u6548\u65b9\u6cd5\u7684\u53d1\u5c55\u3002"}}
{"id": "2506.21506", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2506.21506", "abs": "https://arxiv.org/abs/2506.21506", "authors": ["Boyu Gou", "Zanming Huang", "Yuting Ning", "Yu Gu", "Michael Lin", "Weijian Qi", "Andrei Kopanev", "Botao Yu", "Bernal Jim\u00e9nez Guti\u00e9rrez", "Yiheng Shu", "Chan Hee Song", "Jiaman Wu", "Shijie Chen", "Hanane Nour Moussa", "Tianshu Zhang", "Jian Xie", "Yifei Li", "Tianci Xue", "Zeyi Liao", "Kai Zhang", "Boyuan Zheng", "Zhaowei Cai", "Viktor Rozgic", "Morteza Ziyadi", "Huan Sun", "Yu Su"], "title": "Mind2Web 2: Evaluating Agentic Search with Agent-as-a-Judge", "comment": "Project Homepage: https://osu-nlp-group.github.io/Mind2Web2/", "summary": "Agentic search such as Deep Research systems, where large language models\nautonomously browse the web, synthesize information, and return comprehensive\ncitation-backed answers, represents a major shift in how users interact with\nweb-scale information. While promising greater efficiency and cognitive\noffloading, the growing complexity and open-endedness of agentic search have\noutpaced existing evaluation benchmarks and methodologies, which largely assume\nshort search horizons and static answers. In this paper, we introduce Mind2Web\n2, a benchmark of 130 realistic, high-quality, and long-horizon tasks that\nrequire real-time web browsing and extensive information synthesis, constructed\nwith over 1,000 hours of human labor. To address the challenge of evaluating\ntime-varying and complex answers, we propose a novel Agent-as-a-Judge\nframework. Our method constructs task-specific judge agents based on a\ntree-structured rubric design to automatically assess both answer correctness\nand source attribution. We conduct a comprehensive evaluation of nine frontier\nagentic search systems and human performance, along with a detailed error\nanalysis to draw insights for future development. The best-performing system,\nOpenAI Deep Research, can already achieve 50-70% of human performance while\nspending half the time, showing a great potential. Altogether, Mind2Web 2\nprovides a rigorous foundation for developing and benchmarking the next\ngeneration of agentic search systems.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86Mind2Web 2\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u8bc4\u4f30\u590d\u6742\u7684\u81ea\u4e3b\u641c\u7d22\u7cfb\u7edf\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684Agent-as-a-Judge\u6846\u67b6\u3002", "motivation": "\u73b0\u6709\u8bc4\u4f30\u65b9\u6cd5\u65e0\u6cd5\u9002\u5e94\u81ea\u4e3b\u641c\u7d22\u7cfb\u7edf\u7684\u590d\u6742\u6027\u548c\u5f00\u653e\u6027\uff0c\u9700\u8981\u65b0\u7684\u57fa\u51c6\u548c\u8bc4\u4f30\u6846\u67b6\u3002", "method": "\u6784\u5efa\u5305\u542b130\u4e2a\u4efb\u52a1\u7684Mind2Web 2\u57fa\u51c6\uff0c\u5e76\u63d0\u51fa\u57fa\u4e8e\u6811\u5f62\u8bc4\u5206\u8bbe\u8ba1\u7684Agent-as-a-Judge\u6846\u67b6\u3002", "result": "OpenAI Deep Research\u7cfb\u7edf\u8fbe\u5230\u4eba\u7c7b\u6027\u80fd\u768450-70%\uff0c\u8017\u65f6\u51cf\u534a\u3002", "conclusion": "Mind2Web 2\u4e3a\u4e0b\u4e00\u4ee3\u81ea\u4e3b\u641c\u7d22\u7cfb\u7edf\u7684\u5f00\u53d1\u548c\u8bc4\u4f30\u63d0\u4f9b\u4e86\u575a\u5b9e\u57fa\u7840\u3002"}}
{"id": "2506.21536", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2506.21536", "abs": "https://arxiv.org/abs/2506.21536", "authors": ["Fangjun Ding", "Renyu Zhang", "Xinyu Feng", "Chengye Xie", "Zheng Zhang", "Yanting Zhang"], "title": "PsyLite Technical Report", "comment": null, "summary": "With the rapid development of digital technology, AI-driven psychological\ncounseling has gradually become an important research direction in the field of\nmental health. However, existing models still have deficiencies in dialogue\nsafety, detailed scenario handling, and lightweight deployment. To address\nthese issues, this study proposes PsyLite, a lightweight psychological\ncounseling large language model agent developed based on the base model\nInternLM2.5-7B-chat. Through a two-stage training strategy (hybrid distillation\ndata fine-tuning and ORPO preference optimization), PsyLite enhances the\nmodel's deep-reasoning ability, psychological counseling ability, and safe\ndialogue ability. After deployment using Ollama and Open WebUI, a custom\nworkflow is created with Pipelines. An innovative conditional RAG is designed\nto introduce crosstalk humor elements at appropriate times during psychological\ncounseling to enhance user experience and decline dangerous requests to\nstrengthen dialogue safety. Evaluations show that PsyLite outperforms the\nbaseline models in the Chinese general evaluation (CEval), psychological\ncounseling professional evaluation (CPsyCounE), and dialogue safety evaluation\n(SafeDialBench), particularly in psychological counseling professionalism\n(CPsyCounE score improvement of 47.6\\%) and dialogue safety (\\safe{} score\nimprovement of 2.4\\%). Additionally, the model uses quantization technology\n(GGUF q4\\_k\\_m) to achieve low hardware deployment (5GB memory is sufficient\nfor operation), providing a feasible solution for psychological counseling\napplications in resource-constrained environments.", "AI": {"tldr": "PsyLite\u662f\u4e00\u4e2a\u57fa\u4e8eInternLM2.5-7B-chat\u7684\u8f7b\u91cf\u7ea7\u5fc3\u7406\u54a8\u8be2\u5927\u8bed\u8a00\u6a21\u578b\uff0c\u901a\u8fc7\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\u63d0\u5347\u63a8\u7406\u80fd\u529b\u3001\u54a8\u8be2\u80fd\u529b\u548c\u5bf9\u8bdd\u5b89\u5168\u6027\uff0c\u5e76\u5728\u8d44\u6e90\u53d7\u9650\u73af\u5883\u4e2d\u5b9e\u73b0\u4f4e\u786c\u4ef6\u90e8\u7f72\u3002", "motivation": "\u73b0\u6709AI\u5fc3\u7406\u54a8\u8be2\u6a21\u578b\u5728\u5bf9\u8bdd\u5b89\u5168\u3001\u573a\u666f\u5904\u7406\u548c\u8f7b\u91cf\u5316\u90e8\u7f72\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u9700\u8981\u6539\u8fdb\u3002", "method": "\u91c7\u7528\u6df7\u5408\u84b8\u998f\u6570\u636e\u5fae\u8c03\u548cORPO\u504f\u597d\u4f18\u5316\u7684\u4e24\u9636\u6bb5\u8bad\u7ec3\u7b56\u7565\uff0c\u7ed3\u5408\u6761\u4ef6RAG\u5f15\u5165\u5e7d\u9ed8\u5143\u7d20\u548c\u62d2\u7edd\u5371\u9669\u8bf7\u6c42\u3002", "result": "\u5728CEval\u3001CPsyCounE\u548cSafeDialBench\u8bc4\u4f30\u4e2d\u8868\u73b0\u4f18\u5f02\uff0c\u5fc3\u7406\u54a8\u8be2\u4e13\u4e1a\u6027\u63d0\u534747.6%\uff0c\u5bf9\u8bdd\u5b89\u5168\u6027\u63d0\u53472.4%\u3002", "conclusion": "PsyLite\u4e3a\u8d44\u6e90\u53d7\u9650\u73af\u5883\u63d0\u4f9b\u4e86\u53ef\u884c\u7684\u5fc3\u7406\u54a8\u8be2\u89e3\u51b3\u65b9\u6848\uff0c\u517c\u5177\u9ad8\u6548\u6027\u548c\u5b89\u5168\u6027\u3002"}}
