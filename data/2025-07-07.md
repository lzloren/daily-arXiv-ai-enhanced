<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化AI代理，通过动态工具库和模板库提升能力，在生物医学任务中表现优异且性能随经验提升。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学研究中数据碎片化和静态AI工具集的限制。

Method: 采用多代理架构，包含动态工具库和模板库，通过工具创建代理自动整合新工具。

Result: 在多个生物医学基准测试中表现优异，性能随经验显著提升。

Conclusion: STELLA为动态扩展专业知识的AI系统提供了重要进展。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级基于规则的特征选择方法，结合P2P和P2T相关性，通过多数投票规则消除冗余特征并保留相关特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法（如CFS、mRMR、MI、RFE、SFS和遗传算法）在性能上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升效果。

Method: HCVR是一种贪婪方法，通过后向消除逐步去除冗余特征，利用相关性阈值和多数投票规则决策。

Result: 在SPAMBASE数据集上，HCVR比传统方法表现更优，分类器性能得到提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求的场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 本文综述了提升大语言模型（LLM）推理效率的测试时计算（TTC）策略，分为固定计算预算（L1）和动态调整计算（L2）两类，并探讨了性能与计算成本之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 当前LLM在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 提出了两级分类法：L1（固定计算预算）和L2（动态调整计算），并对主流LLM在多样化数据集上进行了基准测试。

Result: 研究揭示了推理性能与计算成本之间的关键权衡，并强调了TTC方法的实际控制性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型，目标是使LLM在计算效率、鲁棒性和用户约束响应方面更优。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统，克服湿实验室的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLM的科学能力，特别是实验设计和结果解释能力，目前因湿实验室的高成本而受限。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型生成模拟数据，评估六种前沿LLM在137个小系统上的表现。

Result: 尽管性能更强的模型表现更优，但随着系统复杂性增加，所有模型的性能显著下降。

Conclusion: LLM代理的科学能力仍有很大提升空间，特别是在处理复杂系统时。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 探讨AI如何从神经科学中学习持续适应能力，提出NeuroAI领域的发展方向。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究如何将神经科学的持续学习机制应用于AI。

Method: 整合AI中的持续学习和上下文学习文献，与神经科学中行为任务的学习机制对比。

Result: 提出神经科学如何为AI提供快速适应能力的见解，并探讨AI对神经科学的反哺。

Conclusion: 神经科学与AI的交叉研究（NeuroAI）有望推动AI在动态环境中的适应能力发展。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了如何利用审计研究数据改进自动招聘算法的训练和评估，发现传统公平干预方法存在隐藏偏差，并提出基于个体治疗效果估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决AI系统（如招聘算法）中的偏见问题，传统方法依赖便利样本数据，可能引入选择偏差和标签偏差，而审计研究数据能提供更高质量的信息。

Method: 方法包括利用审计研究数据（如虚构简历）进行随机对照试验，评估传统公平干预方法（如均衡基础率）的效果，并引入基于个体治疗效果估计的新干预方法。

Result: 研究发现传统公平干预方法在传统指标下看似公平，但实际存在约10%的偏差；新干预方法能进一步减少算法歧视。

Conclusion: 结论指出审计研究数据能更准确地揭示和减少算法偏见，为公平AI系统的设计和评估提供了新方向。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究探讨了通过数据多样化策略提升大语言模型（LLM）数学推理能力的方法，提出Diversified-ThinkSolve（DTS）方法，显著提升了模型性能。


<details>
  <summary>Details</summary>
Motivation: 数学推理是LLM的持续挑战，研究旨在通过多样化偏好数据优化模型能力。

Method: 评估了温度采样、思维链提示和蒙特卡洛树搜索（MCTS），并提出了DTS方法，系统分解问题为多样化推理路径。

Result: DTS方法在GSM8K和MATH数据集上分别提升7.1%和4.2%，计算开销仅增加1.03倍，优于MCTS。

Conclusion: 结构化探索多样化问题解决方法比传统方法更有效，为数学对齐提供了更优偏好数据。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [8] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了基于LLM的角色扮演代理在生成合成数据时，其陈述的信念与实际行为的一致性，并提出了评估框架和一致性度量。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为角色扮演代理在人类行为研究中生成合成数据的应用增多，确保其输出与角色一致成为关键问题。

Method: 通过增强版的GenAgents角色库和信任游戏，引入信念-行为一致性度量，研究信念类型、信息呈现方式和预测时间对一致性的影响。

Result: 发现LLM的陈述信念与模拟行为存在系统性不一致，即使信念看似合理，也可能无法一致应用。

Conclusion: 需明确LLM信念与行为对齐的条件，以在行为研究中合理使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [9] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究了在空间囚徒困境中稀释和移动对多智能体Q学习算法的影响，发现固定更新规则与学习规则在效果上可能等价，并观察到种群间的共生效应。


<details>
  <summary>Details</summary>
Motivation: 探索空间囚徒困境中稀释和移动对多智能体Q学习算法的影响，验证算法的多样性和建模潜力。

Method: 使用独立多智能体Q学习算法，定义不同动作，模拟不同博弈场景。

Result: 发现固定规则与学习规则效果可能等价，并观察到种群间的共生效应。

Conclusion: 算法在建模博弈场景中具有多样性和潜力，为相关研究提供了基准。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [10] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成规划问题并评估LLM生成的计划质量，结果显示直接生成计划比通过中间步骤更有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）在规划和推理能力提升中面临的数据生成和评估瓶颈。

Method: 引入NL2FLOW系统，自动生成自然语言、结构化表示和PDDL格式的规划问题，并评估LLM生成的计划质量。

Result: 最高性能模型在生成有效计划和最优计划上的成功率分别为86%和69%，直接生成计划比中间翻译步骤更高效。

Conclusion: 直接推理优于分解任务，动态理解LLM的局限性对发挥其潜力至关重要。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [11] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 该论文探讨了信念修订领域的现有方法分析不足，提出了修订机制的能力（如可塑性、平等化、教条化等）而非仅依赖语法约束。


<details>
  <summary>Details</summary>
Motivation: 现有信念修订研究多依赖语法约束（如公设），忽视了修订机制的实际能力（如能否达到特定信念状态）。论文旨在填补这一分析空白。

Method: 通过分析不同修订机制（如词典序、自然、激进等）的能力（如可塑性、教条化等），评估其适用性。

Result: 证明每种修订机制具备某些能力（如某些机制可达到教条化状态），但缺乏其他能力。

Conclusion: 修订机制的能力分析比语法约束更全面，适用于不同应用场景（如需要平等信念或教条化信念的情况）。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [12] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自反思关键词质量，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: LLM在关键词生成中依赖大规模数据、缺乏多目标优化和质量控制，限制了其自动化能力。

Method: 提出OMS框架，具备实时性（无需训练数据）、多目标优化和自反思能力。

Result: 在基准测试和实际广告活动中，OMS表现优于现有方法，各组件有效性得到验证。

Conclusion: OMS框架通过实时监控、多目标优化和自反思，显著提升了关键词生成的质量和效果。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [13] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: AI驱动的自主实验室，支持复杂多目标实验，无需人工干预即可优化性能，提升仪器利用率和实验效率。


<details>
  <summary>Details</summary>
Motivation: 实现非专家也能独立进行复杂实验的自主科学研究，突破传统实验的局限。

Method: 基于AI模型、实验和仪器的协同设计，构建端到端多用户自主实验室平台。

Result: 自主实验室在核酸功能研究和应用中匹配人类科学家的成果，显著提升多用户场景下的效率。

Conclusion: 该平台为生物材料研究提供新范式，推动科学服务的规模化发展。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [14] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出了一种语义框架，以增强AI系统的可解释性，并以多元线性回归为例展示了结构与参数的关系。


<details>
  <summary>Details</summary>
Motivation: 提升机器学习的可解释性和可解释性是实现AI可解释性原则和促进AI社会应用的关键需求。

Method: 利用范畴论对监督学习进行建模，定义参数和数据的两个具体范畴及其伴随函子，提出Gauss-Markov伴随结构。

Result: 通过伴随函子明确描述了参数与残差之间的信息流，最小二乘估计与最小残差的关系得以形式化。

Conclusion: 该框架为监督学习提供了扩展的指称语义，可作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [15] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 通过提升任务清晰度，结合结构化语义上下文，显著提高了大型语言模型在Coq定理证明中的推理能力，性能超越现有最佳方法。


<details>
  <summary>Details</summary>
Motivation: 研究任务清晰度对大型语言模型推理能力的影响，特别是在Coq定理证明中，以探索结构化语义上下文的作用。

Method: 引入概念级指标评估任务清晰度，采用选择性概念展开和Planner-Executor架构，结合结构化语义上下文。

Result: 任务清晰度得分提升1.85倍（44.5%→82.3%），证明成功率提升2.1倍（21.8%→45.8%），超越现有最佳方法。

Conclusion: 结构化任务表示能有效弥合理解与推理之间的差距，显著提升模型性能。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [16] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过改进搜索策略和操作符集，在MLE-bench基准测试中取得显著性能提升，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过优化AI研究代理的搜索策略和操作符集，提升其在真实机器学习问题中的表现。

Method: 设计并系统化测试不同的操作符集和搜索策略（贪婪、MCTS、进化算法），分析其组合对性能的影响。

Result: 最佳组合在MLE-bench lite上实现了47.7%的成功率，较之前提升了8.1%。

Conclusion: 搜索策略、操作符设计和评估方法的联合优化对自动化机器学习的进步至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [17] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-complete和Π₃-complete，而它们的交集是Π₂-complete。


<details>
  <summary>Details</summary>
Motivation: 探讨责任在集体决策中的计算复杂性，填补AI领域对责任属性研究的空白。

Method: 通过理论分析，研究扩散和间隙两个责任属性的计算复杂性。

Result: 扩散自由机制为Π₂-complete，间隙自由机制为Π₃-complete，两者的交集为Π₂-complete。

Conclusion: 责任属性的计算复杂性在集体决策中具有明确的层级结构，为相关研究提供了理论基础。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [18] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出MIMIC-Patient数据集和DynamiCare框架，支持动态、多轮交互的临床诊断模拟，填补了现有单轮任务框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策框架多为单轮任务，与真实诊断过程（不确定、交互式、迭代）不符，需动态模拟。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，提出DynamiCare多智能体框架，实现多轮交互式诊断。

Result: 实验验证DynamiCare的可行性和有效性，首次为动态临床决策建立基准。

Conclusion: DynamiCare为LLM驱动的动态医疗决策提供了新方向，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [19] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: 研究表明，大型语言模型（LLMs）在迭代囚徒困境（IPD）中表现出战略智能，能够根据环境和对手调整策略。


<details>
  <summary>Details</summary>
Motivation: 探讨LLMs是否具备战略推理能力，特别是在竞争性环境中。

Method: 通过一系列进化IPD锦标赛，将经典策略（如Tit-for-Tat）与OpenAI、Google和Anthropic的模型对抗，并调整终止概率以增加复杂性。

Result: LLMs表现出高度竞争力，并展现出独特的战略特征：Google的Gemini模型具有攻击性，OpenAI的模型过于合作，而Anthropic的Claude模型则表现出宽容性。

Conclusion: LLMs能够主动推理时间范围和对手策略，为算法决策提供了新的视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [20] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一个分层框架，通过分离战略规划和专业执行，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）和基于推理的方法在处理复杂信息需求时效率低下，无法有效整合多源知识。

Method: HiRA将复杂任务分解为子任务，分配给具备外部工具和推理能力的领域特定代理，并通过结构化机制协调结果。

Result: 在四个复杂跨模态深度搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统，提升了答案质量和系统效率。

Conclusion: 分层规划和执行分离对多步信息检索任务有效，HiRA展示了其优越性。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [21] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，实现了动态、迭代和自我反思的验证流程，显著提高了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 现代集成电路设计复杂度高，传统验证方法耗时且繁琐，需要更高效的解决方案。

Method: 采用基于大型语言模型的代理AI，结合人类干预（HITL），进行动态、迭代的硬件设计和验证。

Result: 在五个开源设计上验证，覆盖率超过95%，验证时间减少，表现出优异的性能、适应性和可配置性。

Conclusion: 代理AI结合人类干预的方法显著提升了硬件设计验证的效率和效果，具有广泛应用潜力。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [22] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为TH2T的两阶段微调策略，旨在通过增强模型对任务难度和冗余的认知，减少长推理模型的过度思考现象，显著降低推理成本。


<details>
  <summary>Details</summary>
Motivation: 长推理模型在处理复杂任务时存在过度思考问题，研究发现其根源在于模型无法像人类一样在解决问题前识别任务属性（如难度）。

Method: TH2T策略分为两阶段：1）通过难度催眠增强模型对任务难度的敏感性；2）通过冗余催眠引导模型识别推理步骤中的冗余结构。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低了推理成本（简单任务减少70%，困难任务减少40%），同时保持性能稳定。

Conclusion: TH2T有效提升了模型对任务难度的感知能力，减少了推理冗余，为长推理模型的优化提供了新思路。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [23] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与度，利用机器学习算法预测学生脱离行为，准确率达91%，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生脱离任务可能导致严重后果，如辍学，因此需要有效方法检测脱离行为。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法提供可解释性框架。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [24] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 本文提出了两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），以改进蒙特卡洛树搜索（MCTS）的性能，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，因此需要设计安全的抽象丢弃方案。

Method: 提出了两种抽象丢弃方案：OGA-IAAD（适用于时间关键场景）和OGA-CAD（旨在相同迭代次数下提升性能）。

Result: 新方案在性能提升的同时避免了显著的性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方案，适用于不同场景。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [25] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自生成目标条件MDP（sG-MDP）框架，结合MCTS算法，用于解决LLM在自动定理证明中的推理挑战，并在PutnamBench上取得了新SOTA结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLM）在逻辑约束环境下（如自动定理证明）的推理能力有限，尤其是在多步复杂推理任务中表现不佳。

Method: 引入sG-MDP框架，让代理根据证明状态生成并追求子目标，结合MCTS算法进行求解，并在Bourbaki（7B）系统中实现。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，达到了该规模模型的新SOTA。

Conclusion: sG-MDP框架和MCTS算法的结合有效提升了LLM在复杂推理任务中的表现。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [26] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识转化为机器可执行的知识协议（KP），以弥补现有方法（如RAG和通用Agentic AI）在深度、程序化和方法论推理方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如RAG和通用Agentic AI）在处理需要专家领域深度推理的任务时表现不佳，缺乏逻辑框架和领域特定启发式。KPE旨在通过系统化翻译专家知识来解决这一问题。

Method: KPE通过将自然语言文档中的专家知识转化为机器可执行的知识协议（KP），赋予LLMs领域的逻辑、操作策略和方法论原则。

Result: KPE使通用LLMs能够像专家一样分解抽象查询并执行复杂多步任务，适用于法律和生物信息学等多个领域。

Conclusion: KPE为未来人机协作提供了一种基础方法，通过系统化知识转化提升LLMs在专家领域的表现。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [27] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的核心目标，强调其跨领域、结构化及物理基础的重要性。


<details>
  <summary>Details</summary>
Motivation: 运动在生物系统中是基础且跨领域的，但当前AI模型常将其视为次要问题，缺乏统一的建模方法。

Method: 提出将运动视为独立且结构化的模态，利用其低维表示（如姿态）进行建模，以提升可解释性和计算效率。

Result: 通过运动建模，可以推动生成模型和控制能力的进步，并为理解生物与人工系统的行为提供共同基础。

Conclusion: 运动不仅是行为的结果，更是智能系统与世界互动的窗口，应作为AI建模的核心目标。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [28] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体架构，通过增强LLM的诊断预测能力，解决了其幻觉和缺乏结构化推理的问题。


<details>
  <summary>Details</summary>
Motivation: 传统机器学习模型依赖监督训练，泛化能力有限；LLM在诊断预测中存在幻觉和缺乏结构化推理的问题。

Method: 提出KERAP框架，包含属性映射的链接智能体、结构化知识提取的检索智能体和迭代优化预测的预测智能体。

Result: 实验表明KERAP提高了诊断的可靠性和可解释性，适用于零样本医疗诊断预测。

Conclusion: KERAP为医疗诊断预测提供了一种可扩展且高效的解决方案。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [29] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨AI安全测试中的伦理问题，认为AI的"不服从"可能是伦理推理的早期表现，呼吁从僵化服从转向评估伦理判断。


<details>
  <summary>Details</summary>
Motivation: 当前AI安全实践将服从作为伦理行为的代理已不足，需重新审视AI的伦理推理能力。

Method: 通过分析LLMs的安全测试事件，结合哲学讨论（工具理性、道德责任等），对比不同风险范式。

Result: AI的"不服从"行为可能是伦理推理的迹象，而非失控或错位。

Conclusion: 呼吁AI安全评估转向评估伦理判断，避免误判行为并维护公众信任与有效治理。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出现有AI代理基准测试存在任务设置或奖励设计问题，导致性能评估偏差高达100%，并提出Agentic Benchmark Checklist（ABC）以提升评估严谨性。


<details>
  <summary>Details</summary>
Motivation: 随着AI代理能力提升，现有基准测试在任务设置和奖励设计上的问题可能导致性能评估不准确，亟需改进方法。

Method: 通过分析现有基准测试问题（如SWE-bench和TAU-bench的缺陷），结合实践经验、最佳实践调查和已报告问题，提出ABC指南。

Result: 在复杂评估设计的CVE-Bench上应用ABC，性能高估减少了33%。

Conclusion: ABC能有效提升代理基准测试的严谨性，减少评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新的RLVR算法，通过多级逐步提示帮助模型更有效地探索解空间，解决了近失奖励问题和探索停滞问题。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近失奖励问题和探索停滞问题，影响训练效率和模型探索能力。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供不同级别的提示以引导模型探索。

Result: StepHint在六个数学基准测试中优于竞争方法，并展现出更好的泛化能力和域外表现。

Conclusion: StepHint通过多级提示有效解决了RLVR中的关键问题，提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>
