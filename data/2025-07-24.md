<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 21]
- [cs.CL](#cs.CL) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Towards Autonomous Sustainability Assessment via Multimodal AI Agents](https://arxiv.org/abs/2507.17012)
*Zhihan Zhang,Alexander Metzger,Yuxuan Mei,Felix Hähnlein,Zachary Englhardt,Tingyu Cheng,Gregory D. Abowd,Shwetak Patel,Adriana Schulz,Vikram Iyer*

Main category: cs.AI

TL;DR: 该研究开发了基于多模态AI代理的生命周期评估(LCA)系统，能够在一分钟内计算电子设备的碳排放，准确率达到专家LCA的19%以内，显著减少了传统LCA所需的专家时间和数据需求。


<details>
  <summary>Details</summary>
Motivation: 传统生命周期评估(LCA)需要大量专业数据来评估产品从制造到处置的环境影响，但这些数据往往难以获得，且需要专家花费数周或数月时间。随着可持续性信息需求激增，迫切需要一种快速、准确且数据需求较低的LCA方法。

Method: 引入多模态AI代理系统，模拟LCA专家与产品经理、工程师等利益相关者的交互过程。系统使用定制数据抽象和软件工具，从在线文本、维修社区图像和政府认证中提取信息，迭代生成详细的生命周期清单。还开发了基于产品相似性聚类的直接环境影响估算方法，以及基于材料属性加权的排放因子生成方法。

Result: AI系统将传统LCA所需的数周或数月专家时间缩短至一分钟以内，在零专有数据情况下，碳足迹估算误差控制在专家LCA的19%以内。直接估算方法在笔记本电脑上运行仅需3毫秒，对电子产品的平均绝对百分比误差(MAPE)为12.28%。数据驱动的排放因子生成方法比人类专家选择最接近的LCA数据库条目的MAPE改善了120.26%。

Conclusion: 该研究成功开发了快速、准确的AI驱动LCA系统，显著降低了环境影响评估的时间成本和数据门槛。这种方法为未来LCA工作流程提供了新的可能性，有望推动可持续性评估的普及和应用，特别是在数据稀缺的环境下为产品设计和决策提供支持。

Abstract: Interest in sustainability information has surged in recent years. However,
the data required for a life cycle assessment (LCA) that maps the materials and
processes from product manufacturing to disposal into environmental impacts
(EI) are often unavailable. Here we reimagine conventional LCA by introducing
multimodal AI agents that emulate interactions between LCA experts and
stakeholders like product managers and engineers to calculate the
cradle-to-gate (production) carbon emissions of electronic devices. The AI
agents iteratively generate a detailed life-cycle inventory leveraging a custom
data abstraction and software tools that extract information from online text
and images from repair communities and government certifications. This approach
reduces weeks or months of expert time to under one minute and closes data
availability gaps while yielding carbon footprint estimates within 19% of
expert LCAs with zero proprietary data. Additionally, we develop a method to
directly estimate EI by comparing an input to a cluster of products with
similar descriptions and known carbon footprints. This runs in 3 ms on a laptop
with a MAPE of 12.28% on electronic products. Further, we develop a data-driven
method to generate emission factors. We use the properties of an unknown
material to represent it as a weighted sum of emission factors for similar
materials. Compared to human experts picking the closest LCA database entry,
this improves MAPE by 120.26%. We analyze the data and compute scaling of this
approach and discuss its implications for future LCA workflows.

</details>


### [2] [New Mechanisms in Flex Distribution for Bounded Suboptimal Multi-Agent Path Finding](https://arxiv.org/abs/2507.17054)
*Shao-Hung Chan,Thomy Phan,Jiaoyang Li,Sven Koenig*

Main category: cs.AI

TL;DR: 本文针对多智能体路径寻找问题，提出了改进的柔性分配机制来优化EECBS算法，通过基于冲突和延迟的分配策略提高了算法效率，同时保持有界次优解的保证。


<details>
  <summary>Details</summary>
Motivation: 现有EECBS算法中的柔性分配机制存在问题：增加阈值可能使路径成本超出界限，导致算法在不同路径集之间切换而不是解决特定路径集的冲突，从而降低效率。

Method: 提出三种新的柔性分配机制：1）基于冲突的柔性分配（按冲突数量比例分配柔性）；2）基于延迟的柔性分配（估计满足约束所需的延迟）；3）混合策略柔性分配（在分层框架中结合前两种方法）。

Result: 实验结果表明，提出的新柔性分配方法在性能上优于原始的贪心柔性分配机制，同时保持了算法的完整性和有界次优性。

Conclusion: 新提出的柔性分配机制成功解决了原始EECBS算法中的效率问题，在保证解的质量的同时提高了算法性能，为多智能体路径寻找问题提供了更有效的解决方案。

Abstract: Multi-Agent Path Finding (MAPF) is the problem of finding a set of
collision-free paths, one for each agent in a shared environment. Its objective
is to minimize the sum of path costs (SOC), where the path cost of each agent
is defined as the travel time from its start location to its target location.
Explicit Estimation Conflict-Based Search (EECBS) is the leading algorithm for
bounded-suboptimal MAPF, with the SOC of the solution being at most a
user-specified factor $w$ away from optimal. EECBS maintains sets of paths and
a lower bound $LB$ on the optimal SOC. Then, it iteratively selects a set of
paths whose SOC is at most $w \cdot LB$ and introduces constraints to resolve
collisions. For each path in a set, EECBS maintains a lower bound on its
optimal path that satisfies constraints. By finding an individually
bounded-suboptimal path with cost at most a threshold of $w$ times its lower
bound, EECBS guarantees to find a bounded-suboptimal solution. To speed up
EECBS, previous work uses flex distribution to increase the threshold. Though
EECBS with flex distribution guarantees to find a bounded-suboptimal solution,
increasing the thresholds may push the SOC beyond $w \cdot LB$, forcing EECBS
to switch among different sets of paths instead of resolving collisions on a
particular set of paths, and thus reducing efficiency. To address this issue,
we propose Conflict-Based Flex Distribution that distributes flex in proportion
to the number of collisions. We also estimate the delays needed to satisfy
constraints and propose Delay-Based Flex Distribution. On top of that, we
propose Mixed-Strategy Flex Distribution, combining both in a hierarchical
framework. We prove that EECBS with our new flex distribution mechanisms is
complete and bounded-suboptimal. Our experiments show that our approaches
outperform the original (greedy) flex distribution.

</details>


### [3] [LoRA is All You Need for Safety Alignment of Reasoning LLMs](https://arxiv.org/abs/2507.17075)
*Yihao Xue,Baharan Mirzasoleiman*

Main category: cs.AI

TL;DR: 该研究解决了大语言模型安全对齐微调会显著降低推理能力的"安全税"问题，提出使用LoRA进行安全微调可以在保持推理能力的同时实现有效的安全对齐。


<details>
  <summary>Details</summary>
Motivation: 推理型大语言模型在解决复杂问题方面取得了突破，但安全对齐微调会显著降低模型的推理能力，产生"安全税"现象。因此需要找到既能保证模型安全又不损害推理能力的方法。

Method: 使用LoRA（低秩适应）在拒绝数据集上进行监督微调，将安全权重更新限制在低秩空间中，以最小化对推理权重的干扰。同时探索通过正则化或权重合并来进一步减少权重重叠的方法。

Result: 在涵盖数学、科学和编程的四个基准测试中，该方法产生了高度安全的大语言模型，安全水平与全模型微调相当，但不会损害推理能力。观察到LoRA相比全模型微调产生的权重更新与初始权重的重叠更小。

Conclusion: 使用LoRA进行安全微调可以有效解决推理-安全权衡问题，为设计在推理-安全权衡中产生更一致改进的方法提供了启发。该方法成功避免了"安全税"，实现了安全性和推理能力的平衡。

Abstract: Reasoning LLMs have demonstrated remarkable breakthroughs in solving complex
problems that were previously out of reach. To ensure LLMs do not assist with
harmful requests, safety alignment fine-tuning is necessary in the
post-training phase. However, safety alignment fine-tuning has recently been
shown to significantly degrade reasoning abilities, a phenomenon known as the
"Safety Tax". In this work, we show that using LoRA for SFT on refusal datasets
effectively aligns the model for safety without harming its reasoning
capabilities. This is because restricting the safety weight updates to a
low-rank space minimizes the interference with the reasoning weights. Our
extensive experiments across four benchmarks covering math, science, and coding
show that this approach produces highly safe LLMs -- with safety levels
comparable to full-model fine-tuning -- without compromising their reasoning
abilities. Additionally, we observe that LoRA induces weight updates with
smaller overlap with the initial weights compared to full-model fine-tuning. We
also explore methods that further reduce such overlap -- via regularization or
during weight merging -- and observe some improvement on certain tasks. We hope
this result motivates designing approaches that yield more consistent
improvements in the reasoning-safety trade-off.

</details>


### [4] [HySafe-AI: Hybrid Safety Architectural Analysis Framework for AI Systems: A Case Study](https://arxiv.org/abs/2507.17118)
*Mandar Pitale,Jelena Frtunikj,Abhinaw Priyadershi,Vasu Singh,Maria Spence*

Main category: cs.AI

TL;DR: 本文提出了HySAFE-AI框架，这是一个混合安全架构分析框架，用于评估AI系统（特别是端到端大模型）的安全性，通过改进传统的安全分析方法来适应现代AI系统的复杂性。


<details>
  <summary>Details</summary>
Motivation: 随着AI在自动驾驶系统和机器人等安全关键领域的广泛应用，以及向端到端单体架构（如大语言模型和视觉语言模型）的发展趋势，传统的安全分析方法已无法充分应对这些复杂AI系统的安全评估需求，特别是在处理基础模型的潜在表示形成和利用方面。

Method: 作者回顾了不同的架构解决方案，评估了常见安全分析技术（如故障模式与影响分析FMEA和故障树分析FTA）的有效性，并提出了HySAFE-AI（AI系统混合安全架构分析框架），这是一个混合框架，将传统方法适配用于评估AI系统的安全性，特别针对基础模型的复杂特性进行了改进。

Result: 研究展示了如何改进传统安全分析技术以适应基础模型的复杂特性，特别是在潜在表示的形成和利用方面。HySAFE-AI框架能够更好地评估现代AI系统的安全性，为AI安全分析提供了新的解决方案。

Conclusion: 作者提出了HySAFE-AI框架作为评估AI系统安全性的有效工具，并为未来AI安全标准的发展提供了指导建议和未来工作的方向提示，强调了在安全关键应用中对AI系统进行适当安全分析的重要性。

Abstract: AI has become integral to safety-critical areas like autonomous driving
systems (ADS) and robotics. The architecture of recent autonomous systems are
trending toward end-to-end (E2E) monolithic architectures such as large
language models (LLMs) and vision language models (VLMs). In this paper, we
review different architectural solutions and then evaluate the efficacy of
common safety analyses such as failure modes and effect analysis (FMEA) and
fault tree analysis (FTA). We show how these techniques can be improved for the
intricate nature of the foundational models, particularly in how they form and
utilize latent representations. We introduce HySAFE-AI, Hybrid Safety
Architectural Analysis Framework for AI Systems, a hybrid framework that adapts
traditional methods to evaluate the safety of AI systems. Lastly, we offer
hints of future work and suggestions to guide the evolution of future AI safety
standards.

</details>


### [5] [Improving LLMs' Generalized Reasoning Abilities by Graph Problems](https://arxiv.org/abs/2507.17168)
*Qifan Zhang,Nuo Chen,Zehua Li,Miao Peng,Jing Tang,Jia Li*

Main category: cs.AI

TL;DR: 研究者开发了GraphPile数据集和GraphMind模型，通过图问题推理（GPR）来增强大语言模型的通用推理能力，在数学和非数学推理任务上都取得了显著提升。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在新颖复杂问题上推理能力不足，而领域特定的持续预训练方法（如数学推理）缺乏向更广泛推理任务的迁移能力，需要一种能够提升通用推理能力的方法。

Method: 引入图问题推理（GPR）概念，构建了GraphPile大规模语料库（包含109亿个token，涵盖23个图任务），数据包括思维链、程序思维、执行轨迹和真实世界图数据，并基于此训练GraphMind模型。

Result: 在Llama 3、3.1和Gemma 2等基础模型上训练GraphMind，数学推理准确率提升达4.9%，非数学推理任务（逻辑推理和常识推理）提升达21.2%。

Conclusion: 通过首次利用图问题推理增强推理模式并引入首个此类数据集，该工作在领域特定预训练和通用推理能力之间搭建了桥梁，提升了大语言模型的适应性和鲁棒性。

Abstract: Large Language Models (LLMs) have made remarkable strides in reasoning tasks,
yet their performance often falters on novel and complex problems.
Domain-specific continued pretraining (CPT) methods, such as those tailored for
mathematical reasoning, have shown promise but lack transferability to broader
reasoning tasks. In this work, we pioneer the use of Graph Problem Reasoning
(GPR) to enhance the general reasoning capabilities of LLMs. GPR tasks,
spanning pathfinding, network analysis, numerical computation, and topological
reasoning, require sophisticated logical and relational reasoning, making them
ideal for teaching diverse reasoning patterns. To achieve this, we introduce
GraphPile, the first large-scale corpus specifically designed for CPT using GPR
data. Spanning 10.9 billion tokens across 23 graph tasks, the dataset includes
chain-of-thought, program-of-thought, trace of execution, and real-world graph
data. Using GraphPile, we train GraphMind on popular base models Llama 3 and
3.1, as well as Gemma 2, achieving up to 4.9 percent higher accuracy in
mathematical reasoning and up to 21.2 percent improvement in non-mathematical
reasoning tasks such as logical and commonsense reasoning. By being the first
to harness GPR for enhancing reasoning patterns and introducing the first
dataset of its kind, our work bridges the gap between domain-specific
pretraining and universal reasoning capabilities, advancing the adaptability
and robustness of LLMs.

</details>


### [6] [Our Cars Can Talk: How IoT Brings AI to Vehicles](https://arxiv.org/abs/2507.17214)
*Amod Kant Agrawal*

Main category: cs.AI

TL;DR: 本文提出将AI集成到车辆中作为感知平台，通过AI副驾驶实现预测性维护，促进机器与驾驶员之间的智能交互


<details>
  <summary>Details</summary>
Motivation: 传统车辆维护模式是被动反应式的，需要转变为主动预测式维护。当前缺乏能够同时理解机器语言和驾驶员语言的AI系统来实现这一转变

Method: 提出将AI副驾驶系统集成到车辆中，使车辆成为智能感知平台。该系统能够同时处理机器数据和与驾驶员进行自然交互，实现跨学科的技术融合

Result: 文章提供了智能车辆系统、预测性维护和AI驱动用户交互的概念和技术视角，为未来研究发展提供指导框架

Conclusion: 通过集成能够双语交流的AI副驾驶系统，可以实现车辆维护从被动到主动的根本性转变，这需要跨学科合作来推动智能车辆系统的研究与开发

Abstract: Bringing AI to vehicles and enabling them as sensing platforms is key to
transforming maintenance from reactive to proactive. Now is the time to
integrate AI copilots that speak both languages: machine and driver. This
article offers a conceptual and technical perspective intended to spark
interdisciplinary dialogue and guide future research and development in
intelligent vehicle systems, predictive maintenance, and AI-powered user
interaction.

</details>


### [7] [Agent Identity Evals: Measuring Agentic Identity](https://arxiv.org/abs/2507.17257)
*Elija Perrier,Michael Timothy Bennett*

Main category: cs.AI

TL;DR: 本文提出了代理身份评估(AIE)框架，用于测量语言模型代理(LMA)系统在时间推移中维持和展现代理身份的程度，以解决LMA继承自大语言模型的病理问题对其身份稳定性的影响。


<details>
  <summary>Details</summary>
Motivation: 语言模型代理(LMA)继承了大语言模型的病理特征（无状态性、随机性、对提示敏感和语言中介性），这些特征会破坏其身份的可识别性、连续性、持久性和一致性，从而影响其推理、规划和行动等代理能力，降低其可靠性、可信度和实用性。

Method: 引入代理身份评估(AIE)框架，这是一个严格的、统计驱动的实证框架，包含一系列新颖的度量指标，可以与其他性能、能力和代理鲁棒性测量方法集成，用于设计最优的LMA基础设施和支撑架构（如记忆和工具）。

Result: AIE框架提供了可应用于LMA生命周期各个阶段的正式定义和方法，并给出了如何应用这些方法的具体示例，能够测量LMA系统展现和维持代理身份的程度，包括其能力、属性和从状态扰动中恢复的能力。

Conclusion: AIE框架为评估和改善语言模型代理的身份稳定性提供了系统性解决方案，有助于提高LMA系统的可靠性、可信度和代理能力，为构建更稳定、更可信的语言模型代理系统奠定了基础。

Abstract: Central to agentic capability and trustworthiness of language model agents
(LMAs) is the extent they maintain stable, reliable, identity over time.
However, LMAs inherit pathologies from large language models (LLMs)
(statelessness, stochasticity, sensitivity to prompts and
linguistically-intermediation) which can undermine their identifiability,
continuity, persistence and consistency. This attrition of identity can erode
their reliability, trustworthiness and utility by interfering with their
agentic capabilities such as reasoning, planning and action. To address these
challenges, we introduce \textit{agent identity evals} (AIE), a rigorous,
statistically-driven, empirical framework for measuring the degree to which an
LMA system exhibit and maintain their agentic identity over time, including
their capabilities, properties and ability to recover from state perturbations.
AIE comprises a set of novel metrics which can integrate with other measures of
performance, capability and agentic robustness to assist in the design of
optimal LMA infrastructure and scaffolding such as memory and tools. We set out
formal definitions and methods that can be applied at each stage of the LMA
life-cycle, and worked examples of how to apply them.

</details>


### [8] [Students' Feedback Requests and Interactions with the SCRIPT Chatbot: Do They Get What They Ask For?](https://arxiv.org/abs/2507.17258)
*Andreas Scholl,Natalie Kiesler*

Main category: cs.AI

TL;DR: 研究者开发了基于ChatGPT-4o-mini的编程教育聊天机器人SCRIPT，通过136名学生的实验发现学生反馈请求遵循特定序列，机器人响应与学生需求匹配度达75%，为生成式AI学习支持系统设计提供了指导。


<details>
  <summary>Details</summary>
Motivation: 基于生成式AI在编程教育中的应用潜力，研究者希望开发一个能够支持编程新手学习的聊天机器人工具，既能提供开放式交互又能通过预设提示词提供结构化指导，以更好地理解学生的反馈偏好和交互模式。

Method: 开发了基于ChatGPT-4o-mini的聊天机器人SCRIPT，支持开放式交互和预定义提示的结构化指导。通过对德国某大学136名编程入门课程学生的实验，分析学生在解决编程任务时与SCRIPT的交互方式，重点关注他们的反馈偏好模式。

Result: 实验结果显示学生的反馈请求遵循特定的序列模式，聊天机器人的响应与学生请求的反馈类型匹配度达到75%，并且很好地遵守了系统提示约束条件。

Conclusion: 研究为生成式AI学习支持系统的设计提供了重要见解，同时突出了在AI辅助工具中平衡指导性和灵活性所面临的挑战，为未来开发更有效的编程教育AI工具奠定了基础。

Abstract: Building on prior research on Generative AI (GenAI) and related tools for
programming education, we developed SCRIPT, a chatbot based on ChatGPT-4o-mini,
to support novice learners. SCRIPT allows for open-ended interactions and
structured guidance through predefined prompts. We evaluated the tool via an
experiment with 136 students from an introductory programming course at a large
German university and analyzed how students interacted with SCRIPT while
solving programming tasks with a focus on their feedback preferences. The
results reveal that students' feedback requests seem to follow a specific
sequence. Moreover, the chatbot responses aligned well with students' requested
feedback types (in 75%), and it adhered to the system prompt constraints. These
insights inform the design of GenAI-based learning support systems and
highlight challenges in balancing guidance and flexibility in AI-assisted
tools.

</details>


### [9] [Compliance Brain Assistant: Conversational Agentic AI for Assisting Compliance Tasks in Enterprise Environments](https://arxiv.org/abs/2507.17289)
*Shitong Zhu,Chenhao Fang,Derek Larson,Neel Reddy Pochareddy,Rajeev Rao,Sophie Zeng,Yanqing Peng,Wendy Summer,Alex Goncalves,Arya Pudota,Herve Robert*

Main category: cs.AI

TL;DR: 本文提出了合规大脑助手(CBA)，一个对话式AI助手，通过智能路由机制在快速模式和全代理模式间选择，显著提升企业合规任务效率，在关键词匹配率和LLM评判通过率上大幅超越基础LLM模型。


<details>
  <summary>Details</summary>
Motivation: 企业合规人员在日常工作中需要处理大量复杂的合规任务，现有的基础大语言模型在处理这些任务时存在响应质量和延迟之间难以平衡的问题，需要一个专门设计的AI助手来提升合规工作效率。

Method: 设计了一个用户查询路由器，能够智能地在两种模式间选择：(1)快速模式：处理只需从知识库检索相关上下文的简单请求；(2)全代理模式：处理需要复合操作和工具调用的复杂请求，能够主动跨各种合规文档发现上下文，并调用其他API/模型来满足请求需求。

Result: 实验评估显示，CBA在各种真实世界的隐私/合规相关查询上显著优于开箱即用的LLM：平均关键词匹配率从41.7%提升到83.7%，LLM评判通过率从20.0%提升到82.0%。基于路由的完整设计相比仅使用快速模式或全代理模式，在保持相近运行时间的同时获得了更好的平均匹配率和通过率。

Conclusion: 路由机制成功实现了响应质量和延迟之间的良好平衡，验证了设计假设。CBA通过智能选择处理模式，能够有效提升企业环境中合规人员的日常工作效率，为企业合规AI助手的设计提供了有效的解决方案。

Abstract: This paper presents Compliance Brain Assistant (CBA), a conversational,
agentic AI assistant designed to boost the efficiency of daily compliance tasks
for personnel in enterprise environments. To strike a good balance between
response quality and latency, we design a user query router that can
intelligently choose between (i) FastTrack mode: to handle simple requests that
only need additional relevant context retrieved from knowledge corpora; and
(ii) FullAgentic mode: to handle complicated requests that need composite
actions and tool invocations to proactively discover context across various
compliance artifacts, and/or involving other APIs/models for accommodating
requests. A typical example would be to start with a user query, use its
description to find a specific entity and then use the entity's information to
query other APIs for curating and enriching the final AI response.
  Our experimental evaluations compared CBA against an out-of-the-box LLM on
various real-world privacy/compliance-related queries targeting various
personas. We found that CBA substantially improved upon the vanilla LLM's
performance on metrics such as average keyword match rate (83.7% vs. 41.7%) and
LLM-judge pass rate (82.0% vs. 20.0%). We also compared metrics for the full
routing-based design against the `fast-track only` and `full-agentic` modes and
found that it had a better average match-rate and pass-rate while keeping the
run-time approximately the same. This finding validated our hypothesis that the
routing mechanism leads to a good trade-off between the two worlds.

</details>


### [10] [Ctx2TrajGen: Traffic Context-Aware Microscale Vehicle Trajectories using Generative Adversarial Imitation Learning](https://arxiv.org/abs/2507.17418)
*Joobin Jin,Seokjun Hong,Gyeongseon Baek,Yeeun Kim,Byeongjoon Noh*

Main category: cs.AI

TL;DR: 本文提出了Ctx2TrajGen框架，这是一个基于GAIL的上下文感知轨迹生成模型，能够合成真实的城市驾驶行为，通过结合PPO和WGAN-GP技术解决了微观交通建模中的非线性相互依赖和训练不稳定问题。


<details>
  <summary>Details</summary>
Motivation: 精确建模微观车辆轨迹对交通行为分析和自动驾驶系统至关重要，但现有方法在处理复杂城市环境中车辆间的非线性相互依赖关系和训练稳定性方面存在不足，同时面临数据稀缺和领域偏移的挑战。

Method: 提出Ctx2TrajGen框架，采用生成对抗模仿学习(GAIL)结合近端策略优化(PPO)和改进的Wasserstein生成对抗网络(WGAN-GP)，通过显式地以周围车辆和道路几何结构为条件，生成具有交互感知能力的轨迹。

Result: 在无人机捕获的DRIFT数据集上的实验表明，该方法在真实性、行为多样性和上下文保真度方面均优于现有方法，能够有效解决数据稀缺和领域偏移问题而无需仿真。

Conclusion: Ctx2TrajGen为微观车辆轨迹生成提供了一个强大的解决方案，通过上下文感知的生成框架成功解决了传统方法在处理复杂交通场景时的局限性，为交通行为分析和自动驾驶系统提供了更加真实和多样化的轨迹数据。

Abstract: Precise modeling of microscopic vehicle trajectories is critical for traffic
behavior analysis and autonomous driving systems. We propose Ctx2TrajGen, a
context-aware trajectory generation framework that synthesizes realistic urban
driving behaviors using GAIL. Leveraging PPO and WGAN-GP, our model addresses
nonlinear interdependencies and training instability inherent in microscopic
settings. By explicitly conditioning on surrounding vehicles and road geometry,
Ctx2TrajGen generates interaction-aware trajectories aligned with real-world
context. Experiments on the drone-captured DRIFT dataset demonstrate superior
performance over existing methods in terms of realism, behavioral diversity,
and contextual fidelity, offering a robust solution to data scarcity and domain
shift without simulation.

</details>


### [11] [An Uncertainty-Driven Adaptive Self-Alignment Framework for Large Language Models](https://arxiv.org/abs/2507.17477)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.AI

TL;DR: 提出了一个不确定性驱动的自适应自对齐框架(UDASA)，通过量化输出的语义、事实性和价值对齐三个维度的不确定性来构建偏好对，并将训练样本分为保守、适中和探索三个阶段进行渐进式优化，在无需人工标注的情况下显著提升大语言模型的对齐效果。


<details>
  <summary>Details</summary>
Motivation: 大语言模型虽然在指令遵循和通用推理方面取得显著进展，但在没有人工标注的情况下实现与人类意图和安全规范的高质量对齐仍然是一个根本性挑战，因此需要一个全自动化的方法来改善LLM对齐。

Method: UDASA框架首先为每个输入生成多个响应，然后从语义、事实性和价值对齐三个维度量化输出不确定性；基于这些不确定性分数构建偏好对，根据不确定性差异将训练样本分为保守、适中和探索三个阶段；模型在这些阶段中进行渐进式优化。

Result: 实验结果表明UDASA在多个任务上优于现有对齐方法，包括无害性、有用性、真实性和受控情感生成等任务，显著改善了模型性能。

Conclusion: UDASA框架通过不确定性驱动的自适应策略，成功实现了在无人工标注情况下的高效LLM对齐，为自动化对齐提供了一个有效的解决方案，并在多个评估维度上都取得了性能提升。

Abstract: Large Language Models (LLMs) have demonstrated remarkable progress in
instruction following and general-purpose reasoning. However, achieving
high-quality alignment with human intent and safety norms without human
annotations remains a fundamental challenge. In this work, we propose an
Uncertainty-Driven Adaptive Self-Alignment (UDASA) framework designed to
improve LLM alignment in a fully automated manner. UDASA first generates
multiple responses for each input and quantifies output uncertainty across
three dimensions: semantics, factuality, and value alignment. Based on these
uncertainty scores, the framework constructs preference pairs and categorizes
training samples into three stages, conservative, moderate, and exploratory,
according to their uncertainty difference. The model is then optimized
progressively across these stages. In addition, we conduct a series of
preliminary studies to validate the core design assumptions and provide strong
empirical motivation for the proposed framework. Experimental results show that
UDASA outperforms existing alignment methods across multiple tasks, including
harmlessness, helpfulness, truthfulness, and controlled sentiment generation,
significantly improving model performance.

</details>


### [12] [LTLZinc: a Benchmarking Framework for Continual Learning and Neuro-Symbolic Temporal Reasoning](https://arxiv.org/abs/2507.17482)
*Luca Salvatore Lorello,Nikolaos Manginas,Marco Lippi,Stefano Melacci*

Main category: cs.AI

TL;DR: 本文介绍了LTLZinc，一个用于生成时序推理和持续学习任务的基准框架，结合线性时序逻辑和MiniZinc约束来评估神经符号方法在时间维度上的性能。


<details>
  <summary>Details</summary>
Motivation: 现有的神经符号人工智能方法主要应用于静态场景，而需要时间维度推理的挑战性设置很少被探索。持续学习需要智能体在扩展知识的同时避免遗忘先前学习的概念，但缺乏合适的基准框架来评估神经符号方法在时序和约束驱动维度上的表现。

Method: 提出LTLZinc基准框架，该框架能够从线性时序逻辑规范、MiniZinc约束和任意图像分类数据集中生成富有表现力的时序推理和持续学习任务。框架提供细粒度标注，支持在同一生成数据集上进行多种神经和神经符号训练设置。

Result: 在LTLZinc生成的六个神经符号序列分类任务和四个类持续学习任务上进行实验，证明了时序学习和推理的挑战性，并突出了当前最先进方法的局限性。研究发现现有方法在处理时间维度的推理任务时存在明显不足。

Conclusion: LTLZinc框架成功填补了神经符号AI在时序推理方面的基准空白。作者向神经符号和持续学习社区发布了LTLZinc生成器和十个即用型任务，希望促进统一时序学习和推理框架的研究发展。该工作为评估和改进神经符号方法在时间维度上的能力提供了重要工具。

Abstract: Neuro-symbolic artificial intelligence aims to combine neural architectures
with symbolic approaches that can represent knowledge in a human-interpretable
formalism. Continual learning concerns with agents that expand their knowledge
over time, improving their skills while avoiding to forget previously learned
concepts. Most of the existing approaches for neuro-symbolic artificial
intelligence are applied to static scenarios only, and the challenging setting
where reasoning along the temporal dimension is necessary has been seldom
explored. In this work we introduce LTLZinc, a benchmarking framework that can
be used to generate datasets covering a variety of different problems, against
which neuro-symbolic and continual learning methods can be evaluated along the
temporal and constraint-driven dimensions. Our framework generates expressive
temporal reasoning and continual learning tasks from a linear temporal logic
specification over MiniZinc constraints, and arbitrary image classification
datasets. Fine-grained annotations allow multiple neural and neuro-symbolic
training settings on the same generated datasets. Experiments on six
neuro-symbolic sequence classification and four class-continual learning tasks
generated by LTLZinc, demonstrate the challenging nature of temporal learning
and reasoning, and highlight limitations of current state-of-the-art methods.
We release the LTLZinc generator and ten ready-to-use tasks to the
neuro-symbolic and continual learning communities, in the hope of fostering
research towards unified temporal learning and reasoning frameworks.

</details>


### [13] [CQE under Epistemic Dependencies: Algorithms and Experiments (extended version)](https://arxiv.org/abs/2507.17487)
*Lorenzo Marconi,Flavia Ricci,Riccardo Rosati*

Main category: cs.AI

TL;DR: 本文研究了基于本体的受控查询评估(CQE)，结合认知依赖(EDs)和最优GA审查器的交集来回答布尔合取查询联合(BUCQs)，在保证安全性的同时实现了良好的计算性能。


<details>
  <summary>Details</summary>
Motivation: 现有的受控查询评估框架需要在信息披露和安全性之间找到平衡，特别是在本体环境中如何有效地控制信息泄露，同时保持查询回答的实用性和计算效率。

Method: 将认知依赖(EDs)与最优GA审查器的概念相结合，使用所有最优GA审查器的交集来回答布尔合取查询联合(BUCQs)，并针对DL-Lite_R本体和特定ED子类开发了一阶重写算法。

Result: 识别了完全EDs类别可确保交集方法的安全性；证明了在特定条件下BUCQ回答的数据复杂度为AC^0；提供了详细的一阶重写算法；实验验证了重写函数的实际可行性。

Conclusion: 基于交集的CQE方法在使用完全EDs时能够提供强安全保证，对于DL-Lite_R本体和特定ED子类具有良好的计算复杂度，实验结果证明了该方法的实际可行性。

Abstract: We investigate Controlled Query Evaluation (CQE) over ontologies, where
information disclosure is regulated by epistemic dependencies (EDs), a family
of logical rules recently proposed for the CQE framework. In particular, we
combine EDs with the notion of optimal GA censors, i.e. maximal sets of ground
atoms that are entailed by the ontology and can be safely revealed. We focus on
answering Boolean unions of conjunctive queries (BUCQs) with respect to the
intersection of all optimal GA censors - an approach that has been shown in
other contexts to ensure strong security guarantees with favorable
computational behavior. First, we characterize the security of this
intersection-based approach and identify a class of EDs (namely, full EDs) for
which it remains safe. Then, for a subclass of EDs and for DL-Lite_R
ontologies, we show that answering BUCQs in the above CQE semantics is in AC^0
in data complexity by presenting a suitable, detailed first-order rewriting
algorithm. Finally, we report on experiments conducted in two different
evaluation scenarios, showing the practical feasibility of our rewriting
function.

</details>


### [14] [Automated Hybrid Grounding Using Structural and Data-Driven Heuristics](https://arxiv.org/abs/2507.17493)
*Alexander Beiser,Markus Hecher,Stefan Woltran*

Main category: cs.AI

TL;DR: 该论文提出了自动化混合基础化方法，通过数据结构启发式算法自动决定何时使用体解耦基础化和标准自底向上基础化，以解决答案集编程中的基础化瓶颈问题。


<details>
  <summary>Details</summary>
Motivation: 答案集编程在工业界广泛应用面临的关键挑战是基础化瓶颈问题。虽然混合基础化技术结合了标准自底向上基础化和体解耦基础化的优势，但何时使用哪种方法仍不明确，需要自动化的决策机制。

Method: 开发了基于数据结构启发式的分割算法，该算法能够自动检测何时使用体解耦基础化以及何时使用标准基础化。启发式方法基于规则结构和结合实例数据的估计程序。

Result: 在原型实现上的实验显示了有希望的结果：在难以基础化的场景中表现出改进，而在难以求解的实例中接近了最先进的性能水平。

Conclusion: 自动化混合基础化方法通过智能选择基础化策略，有效缓解了答案集编程的基础化瓶颈，为其在工业界的广泛应用提供了技术支持。

Abstract: The grounding bottleneck poses one of the key challenges that hinders the
widespread adoption of Answer Set Programming in industry. Hybrid Grounding is
a step in alleviating the bottleneck by combining the strength of standard
bottom-up grounding with recently proposed techniques where rule bodies are
decoupled during grounding. However, it has remained unclear when hybrid
grounding shall use body-decoupled grounding and when to use standard bottom-up
grounding. In this paper, we address this issue by developing automated hybrid
grounding: we introduce a splitting algorithm based on data-structural
heuristics that detects when to use body-decoupled grounding and when standard
grounding is beneficial. We base our heuristics on the structure of rules and
an estimation procedure that incorporates the data of the instance. The
experiments conducted on our prototypical implementation demonstrate promising
results, which show an improvement on hard-to-ground scenarios, whereas on
hard-to-solve instances we approach state-of-the-art performance.

</details>


### [15] [Can One Domain Help Others? A Data-Centric Study on Multi-Domain Reasoning via Reinforcement Learning](https://arxiv.org/abs/2507.17512)
*Yu Li,Zhuoshi Pan,Honglin Lin,Mengyuan Sun,Conghui He,Lijun Wu*

Main category: cs.AI

TL;DR: 本文系统研究了在可验证奖励强化学习(RLVR)框架下的多领域推理能力，重点关注数学推理、代码生成和逻辑推理三个领域，探索了单领域训练的跨领域泛化能力、多领域联合训练中的相互作用，以及强化学习训练的关键细节对性能的影响。


<details>
  <summary>Details</summary>
Motivation: 现有的可验证奖励强化学习研究主要集中在孤立的推理领域（如数学问题求解、编程任务或逻辑推理），但现实世界的推理场景需要多种认知技能的综合应用。然而，在强化学习框架下这些推理技能之间的相互作用仍然缺乏深入理解，需要系统性研究多领域推理的动态机制。

Method: 采用GRPO算法和Qwen-2.5-7B模型族，通过四个关键组件进行研究：(1)评估单领域数据集训练时的领域内改进和跨领域泛化能力；(2)分析联合跨领域训练中出现的相互增强和冲突等复杂交互；(3)比较基础模型和指令模型在相同强化学习配置下的性能差异；(4)深入探讨课程学习策略、奖励设计变化和语言特定因素等关键训练细节的影响。

Result: 通过大量实验，研究结果揭示了领域交互的动态机制，识别出影响专业化和可泛化推理性能的关键因素。实验结果提供了对单领域训练的跨领域泛化、多领域联合训练的相互作用模式，以及不同强化学习训练策略效果的深入洞察。

Conclusion: 研究为优化强化学习方法论提供了宝贵指导，有助于培养大语言模型的全面多领域推理能力。通过系统性分析领域间的相互作用和关键训练因素，为构建更强大的多领域推理系统奠定了理论和实践基础。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a
powerful paradigm for enhancing the reasoning capabilities of LLMs. Existing
research has predominantly concentrated on isolated reasoning domains such as
mathematical problem-solving, coding tasks, or logical reasoning. However, real
world reasoning scenarios inherently demand an integrated application of
multiple cognitive skills. Despite this, the interplay among these reasoning
skills under reinforcement learning remains poorly understood. To bridge this
gap, we present a systematic investigation of multi-domain reasoning within the
RLVR framework, explicitly focusing on three primary domains: mathematical
reasoning, code generation, and logical puzzle solving. We conduct a
comprehensive study comprising four key components: (1) Leveraging the GRPO
algorithm and the Qwen-2.5-7B model family, our study thoroughly evaluates the
models' in-domain improvements and cross-domain generalization capabilities
when trained on single-domain datasets. (2) Additionally, we examine the
intricate interactions including mutual enhancements and conflicts that emerge
during combined cross-domain training. (3) To further understand the influence
of SFT on RL, we also analyze and compare performance differences between base
and instruct models under identical RL configurations. (4) Furthermore, we
delve into critical RL training details, systematically exploring the impacts
of curriculum learning strategies, variations in reward design, and
language-specific factors. Through extensive experiments, our results offer
significant insights into the dynamics governing domain interactions, revealing
key factors influencing both specialized and generalizable reasoning
performance. These findings provide valuable guidance for optimizing RL
methodologies to foster comprehensive, multi-domain reasoning capabilities in
LLMs.

</details>


### [16] [TAI Scan Tool: A RAG-Based Tool With Minimalistic Input for Trustworthy AI Self-Assessment](https://arxiv.org/abs/2507.17514)
*Athanasios Davvetas,Xenia Ziouvelou,Ypatia Dami,Alexis Kaponis,Konstantina Giouvanopoulou,Michael Papademas*

Main category: cs.AI

TL;DR: 本文介绍了TAI扫描工具，这是一个基于RAG的TAI自评估工具，支持法律TAI评估并帮助遵守AI法案。该工具采用预筛选和评估两步法，能够预测AI系统风险等级并检索相关法规条款。


<details>
  <summary>Details</summary>
Motivation: 当前需要一个能够帮助AI系统进行合规性评估的工具，特别是针对AI法案的合规要求。现有的评估方法可能需要复杂的输入或缺乏对法规条款的直接引用，因此需要开发一个输入简化且能提供法规指导的自评估工具。

Method: 采用基于RAG（检索增强生成）的两步法方法：1）预筛选阶段；2）评估阶段。系统通过比较分析来确定AI系统的风险等级，特别是与高风险系统设置进行对比，同时检索相关的法规条款以提供合规指导。

Result: 定性评估结果显示该工具表现良好，能够正确预测风险等级，并在三个不同语义组中成功检索相关条款。工具的推理机制主要依赖于与高风险系统设置的比较分析，这种行为源于高风险系统部署需要谨慎考虑，因此在AI法案中频繁出现。

Conclusion: TAI扫描工具成功实现了使用最少输入进行TAI自评估的目标，特别是在AI法案合规方面表现出色。该工具能够有效识别AI系统风险等级并提供相关法规指导，为AI系统的合规性评估提供了一个实用的解决方案。

Abstract: This paper introduces the TAI Scan Tool, a RAG-based TAI self-assessment tool
with minimalistic input. The current version of the tool supports the legal TAI
assessment, with a particular emphasis on facilitating compliance with the AI
Act. It involves a two-step approach with a pre-screening and an assessment
phase. The assessment output of the system includes insight regarding the
risk-level of the AI system according to the AI Act, while at the same time
retrieving relevant articles to aid with compliance and notify on their
obligations. Our qualitative evaluation using use-case scenarios yields
promising results, correctly predicting risk levels while retrieving relevant
articles across three distinct semantic groups. Furthermore, interpretation of
results shows that the tool's reasoning relies on comparison with the setting
of high-risk systems, a behaviour attributed to their deployment requiring
careful consideration, and therefore frequently presented within the AI Act.

</details>


### [17] [Constructing Ophthalmic MLLM for Positioning-diagnosis Collaboration Through Clinical Cognitive Chain Reasoning](https://arxiv.org/abs/2507.17539)
*Xinyao Liu,Diping Song*

Main category: cs.AI

TL;DR: 本文提出了FundusExpert，一个专门用于眼科诊断的多模态大语言模型，通过FundusGen数据集和Fundus-Engine系统实现了定位-诊断推理能力的整合，在眼科问答任务中准确率超越40B MedRegA模型26.6%，在零样本报告生成任务中临床一致性达到77.0%。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在医疗诊断领域具有巨大潜力，但在眼科等专业领域面临关键挑战：注释粒度碎片化和临床推理逻辑不一致，这阻碍了精确的跨模态理解。需要开发专门针对眼科的MLLM来解决这些问题。

Method: 1) 开发FundusExpert：具有集成定位-诊断推理能力的眼科专用MLLM；2) 构建FundusGen数据集：通过智能Fundus-Engine系统构建；3) Fundus-Engine系统：自动化定位并利用基于MLLM的语义扩展，在单张眼底图像中整合全局疾病分类、局部目标检测和细粒度特征分析；4) 构建临床对齐的认知链，指导模型生成可解释的推理路径。

Result: FundusExpert在眼科问答任务中表现最佳，平均准确率超越40B MedRegA模型26.6%；在零样本报告生成任务中表现出色，临床一致性达到77.0%，显著超越GPT-4o的47.6%；发现数据质量与模型能力之间的缩放定律(L ∝ N^0.068)，证明FundusGen中的认知对齐注释提高了数据利用效率。

Conclusion: 通过整合区域级定位和诊断推理链，本研究开发了一个可扩展的、临床对齐的MLLM，探索了弥合特定领域MLLM中视觉-语言差距的路径。该工作为眼科等专业医疗领域的多模态AI应用提供了重要进展。

Abstract: Multimodal large language models (MLLMs) demonstrate significant potential in
the field of medical diagnosis. However, they face critical challenges in
specialized domains such as ophthalmology, particularly the fragmentation of
annotation granularity and inconsistencies in clinical reasoning logic, which
hinder precise cross-modal understanding. This paper introduces FundusExpert,
an ophthalmology-specific MLLM with integrated positioning-diagnosis reasoning
capabilities, along with FundusGen, a dataset constructed through the
intelligent Fundus-Engine system. Fundus-Engine automates localization and
leverages MLLM-based semantic expansion to integrate global disease
classification, local object detection, and fine-grained feature analysis
within a single fundus image. Additionally, by constructing a clinically
aligned cognitive chain, it guides the model to generate interpretable
reasoning paths. FundusExpert, fine-tuned with instruction data from FundusGen,
achieves the best performance in ophthalmic question-answering tasks,
surpassing the average accuracy of the 40B MedRegA by 26.6%. It also excels in
zero-shot report generation tasks, achieving a clinical consistency of 77.0%,
significantly outperforming GPT-4o's 47.6%. Furthermore, we reveal a scaling
law between data quality and model capability ($L \propto N^{0.068}$),
demonstrating that the cognitive alignment annotations in FundusGen enhance
data utilization efficiency. By integrating region-level localization with
diagnostic reasoning chains, our work develops a scalable, clinically-aligned
MLLM and explores a pathway toward bridging the visual-language gap in specific
MLLMs. Our project can be found at https://github.com/MeteorElf/FundusExpert.

</details>


### [18] [Simulating multiple human perspectives in socio-ecological systems using large language models](https://arxiv.org/abs/2507.17680)
*Yongchao Zeng,Calum Brown,Ioannis Kyriakou,Ronja Hotz,Mark Rounsevell*

Main category: cs.AI

TL;DR: 研究开发了HoPeS（以人为导向的视角转换）建模框架，利用大语言模型驱动的智能体代表不同利益相关者，让用户能够体验和转换不同视角，用于探索社会生态系统中的多元化利益相关者观点。


<details>
  <summary>Details</summary>
Motivation: 理解社会生态系统需要来自不同利益相关者的多元视角，但这些观点通常难以获取。传统方法在获取和整合多方观点方面存在局限性，需要一种基于仿真的替代方法来探索不同利益相关者的视角。

Method: 开发HoPeS建模框架，使用大语言模型驱动的智能体代表各种利益相关者，用户可以扮演智能体角色体验视角差异。设计了仿真协议作为"脚手架"来简化多视角仿真，支持用户反思、转换和整合不同视角。构建了原型系统，在制度动态和土地利用变化背景下演示HoPeS，支持叙述驱动和数值实验。

Result: 在示例实验中，用户依次采用系统观察者和研究者视角，分析嵌入式土地利用模型的数据为其他代表各种机构的LLM智能体提供循证决策建议。尽管用户努力推荐技术上合理的政策，但由于利益相关者的竞争性倡导，政策建议与实施之间仍存在差异，反映了现实世界中研究者和政策制定者视角的错位。用户体验到作为研究者的挫折和失望感，特别是在试图获得政治影响力的同时保持政治中立的挑战。

Conclusion: 用户表现出尝试替代叙述框架策略的高度动机，表明系统在探索不同视角方面的潜力。进一步的系统和协议完善可能会为社会生态仿真中的跨学科合作开启新形式，HoPeS框架为理解和整合多元利益相关者观点提供了有价值的工具。

Abstract: Understanding socio-ecological systems requires insights from diverse
stakeholder perspectives, which are often hard to access. To enable
alternative, simulation-based exploration of different stakeholder
perspectives, we develop the HoPeS (Human-Oriented Perspective Shifting)
modelling framework. HoPeS employs agents powered by large language models
(LLMs) to represent various stakeholders; users can step into the agent roles
to experience perspectival differences. A simulation protocol serves as a
"scaffold" to streamline multiple perspective-taking simulations, supporting
users in reflecting on, transitioning between, and integrating across
perspectives. A prototype system is developed to demonstrate HoPeS in the
context of institutional dynamics and land use change, enabling both
narrative-driven and numerical experiments. In an illustrative experiment, a
user successively adopts the perspectives of a system observer and a researcher
- a role that analyses data from the embedded land use model to inform
evidence-based decision-making for other LLM agents representing various
institutions. Despite the user's effort to recommend technically sound
policies, discrepancies persist between the policy recommendation and
implementation due to stakeholders' competing advocacies, mirroring real-world
misalignment between researcher and policymaker perspectives. The user's
reflection highlights the subjective feelings of frustration and disappointment
as a researcher, especially due to the challenge of maintaining political
neutrality while attempting to gain political influence. Despite this, the user
exhibits high motivation to experiment with alternative narrative framing
strategies, suggesting the system's potential in exploring different
perspectives. Further system and protocol refinement are likely to enable new
forms of interdisciplinary collaboration in socio-ecological simulations.

</details>


### [19] [Symbiotic Agents: A Novel Paradigm for Trustworthy AGI-driven Networks](https://arxiv.org/abs/2507.17695)
*Ilias Chatzistefanidis,Navid Nikaein*

Main category: cs.AI

TL;DR: 本文提出了一种结合大语言模型(LLM)和实时优化算法的共生智能体范式，用于构建可信赖的6G网络AGI系统，通过输入级和输出级优化器实现精确的网络管理和服务供应决策。


<details>
  <summary>Details</summary>
Motivation: 随着6G网络发展，需要从专门化AI算法处理孤立任务转向具有更广泛推理能力的人工通用智能(AGI)驱动网络，以实现实时决策制定和端到端用户服务供应，但现有LLM-based智能体在网络管理中存在决策错误和资源开销问题。

Method: 设计了一种新颖的共生智能体范式，将LLM与实时优化算法相结合：(1)在LLM输入级设置优化器提供有界不确定性引导，处理数值精确任务；(2)在输出级设置由LLM监督的优化器实现自适应实时控制；(3)实现了两种智能体类型：无线接入网优化器和服务级协议多智能体协商器；(4)提出了AGI网络的端到端架构。

Result: 在5G测试平台上的实验结果显示：共生智能体相比独立LLM智能体将决策错误减少了5倍；较小语言模型(SLM)在保持相似准确性的同时，GPU资源开销减少99.9%，实现82ms的近实时循环；多智能体协作RAN演示显示在服务级协议和资源分配方面具有显著灵活性，RAN过度利用率降低约44%。

Conclusion: 共生智能体范式为下一代AGI驱动的网络系统奠定了基础，该系统设计为在LLM不断发展的过程中保持适应性、高效性和可信赖性，为6G网络中的实时网络管理和服务供应提供了有效解决方案。

Abstract: Large Language Model (LLM)-based autonomous agents are expected to play a
vital role in the evolution of 6G networks, by empowering real-time
decision-making related to management and service provisioning to end-users.
This shift facilitates the transition from a specialized intelligence approach,
where artificial intelligence (AI) algorithms handle isolated tasks, to
artificial general intelligence (AGI)-driven networks, where agents possess
broader reasoning capabilities and can manage diverse network functions. In
this paper, we introduce a novel agentic paradigm that combines LLMs with
real-time optimization algorithms towards Trustworthy AI, defined as symbiotic
agents. Optimizers at the LLM's input-level provide bounded uncertainty
steering for numerically precise tasks, whereas output-level optimizers
supervised by the LLM enable adaptive real-time control. We design and
implement two novel agent types including: (i) Radio Access Network optimizers,
and (ii) multi-agent negotiators for Service-Level Agreements (SLAs). We
further propose an end-to-end architecture for AGI networks and evaluate it on
a 5G testbed capturing channel fluctuations from moving vehicles. Results show
that symbiotic agents reduce decision errors fivefold compared to standalone
LLM-based agents, while smaller language models (SLM) achieve similar accuracy
with a 99.9% reduction in GPU resource overhead and in near-real-time loops of
82 ms. A multi-agent demonstration for collaborative RAN on the real-world
testbed highlights significant flexibility in service-level agreement and
resource allocation, reducing RAN over-utilization by approximately 44%.
Drawing on our findings and open-source implementations, we introduce the
symbiotic paradigm as the foundation for next-generation, AGI-driven
networks-systems designed to remain adaptable, efficient, and trustworthy even
as LLMs advance.

</details>


### [20] [Thinking Isn't an Illusion: Overcoming the Limitations of Reasoning Models via Tool Augmentations](https://arxiv.org/abs/2507.17699)
*Zhao Song,Song Yue,Jiahao Zhang*

Main category: cs.AI

TL;DR: 该研究挑战了近期关于大型推理模型(LRMs)中逐步思维过程无效的观点，通过引入Python解释器和草稿本等工具增强，证明了工具增强的LRMs在各种复杂度任务上都能持续优于非推理模型。


<details>
  <summary>Details</summary>
Motivation: 近期苹果等机构的实证研究表明，大型推理模型的逐步思维过程可能并不能真正提升推理能力，在低复杂度和高复杂度任务上，没有显式推理的LLMs实际表现更好。本研究旨在重新审视这些发现，探讨当引入工具增强时LRMs的局限性是否仍然存在。

Method: 研究者引入了两种类型的工具增强：Python解释器和草稿本，并在苹果的基准推理谜题上评估了三个代表性LLMs及其LRM对应版本的性能表现。

Result: 结果显示，通过适当的工具使用，LRMs在所有任务复杂度级别上都能持续优于其非推理对应模型，证明了工具增强能够有效提升LRMs的推理性能。

Conclusion: 研究结果挑战了近期关于"推理是一种错觉"的观点，强调了工具增强LRMs在解决复杂问题方面的潜力，证明了当配备适当工具时，显式推理过程确实能够带来性能提升。

Abstract: Large Reasoning Models (LRMs) have become a central focus in today's large
language model (LLM) research, where models are designed to output a
step-by-step thinking process before arriving at a final answer to handle
complex reasoning tasks. Despite their promise, recent empirical studies (e.g.,
[Shojaee et al., 2025] from Apple) suggest that this thinking process may not
actually enhance reasoning ability, where LLMs without explicit reasoning
actually outperform LRMs on tasks with low or high complexity. In this work, we
revisit these findings and investigate whether the limitations of LRMs persist
when tool augmentations are introduced. We incorporate two types of tools,
Python interpreters and scratchpads, and evaluate three representative LLMs and
their LRM counterparts on Apple's benchmark reasoning puzzles. Our results show
that, with proper tool use, LRMs consistently outperform their non-reasoning
counterparts across all levels of task complexity. These findings challenge the
recent narrative that reasoning is an illusion and highlight the potential of
tool-augmented LRMs for solving complex problems.

</details>


### [21] [Online Submission and Evaluation System Design for Competition Operations](https://arxiv.org/abs/2507.17730)
*Zhe Chen,Daniel Harabor,Ryan Hechnenberger,Nathan R. Sturtevant*

Main category: cs.AI

TL;DR: 本文提出了一个在线竞赛系统，自动化处理竞赛提交和评估过程，解决了传统学术竞赛中组织者负担重、环境兼容性差等问题


<details>
  <summary>Details</summary>
Motivation: 研究社区虽然开发了各领域的基准数据集来比较算法性能，但跟踪研究进展困难，因为论文发表在不同场所且都声称代表最先进水平。传统的定期竞赛虽能评估算法进展，但给组织者带来巨大运营负担，需要管理和评估大量提交，且参与者在不同环境开发导致评估时的兼容性问题

Method: 设计并实现了一个在线竞赛系统，该系统能够自动化提交和评估过程。系统允许组织者高效管理大量提交，利用隔离环境来评估提交内容，从而解决环境兼容性问题

Result: 该系统已成功应用于多个竞赛，包括基于网格的路径规划竞赛(Grid-Based Pathfinding Competition)和机器人跑者联赛(League of Robot Runners competition)

Conclusion: 通过自动化竞赛系统，有效解决了传统学术竞赛中的运营负担和技术兼容性问题，为研究社区提供了一个高效的算法性能评估平台

Abstract: Research communities have developed benchmark datasets across domains to
compare the performance of algorithms and techniques However, tracking the
progress in these research areas is not easy, as publications appear in
different venues at the same time, and many of them claim to represent the
state-of-the-art. To address this, research communities often organise periodic
competitions to evaluate the performance of various algorithms and techniques,
thereby tracking advancements in the field. However, these competitions pose a
significant operational burden. The organisers must manage and evaluate a large
volume of submissions. Furthermore, participants typically develop their
solutions in diverse environments, leading to compatibility issues during the
evaluation of their submissions. This paper presents an online competition
system that automates the submission and evaluation process for a competition.
The competition system allows organisers to manage large numbers of submissions
efficiently, utilising isolated environments to evaluate submissions. This
system has already been used successfully for several competitions, including
the Grid-Based Pathfinding Competition and the League of Robot Runners
competition.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [22] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)
*Shmuel Amar,Ori Shapira,Aviv Slobodkin,Ido Dagan*

Main category: cs.CL

TL;DR: 本文提出了指令引导内容选择(IGCS)统一框架，用于处理各种NLP内容选择任务，并发布了首个统一基准测试IGCSBench和大规模合成数据集


<details>
  <summary>Details</summary>
Motivation: 传统的内容选择任务（如从源文本中选择相关文本片段）一直被孤立研究，各自使用不同的建模方法、数据集和评估指标，缺乏统一的框架来处理这一共同目标

Method: 提出指令引导内容选择(IGCS)框架，将任务定义和实例特定请求封装为语言模型的指令；创建IGCSBench统一基准测试；构建大规模通用合成数据集；使用迁移学习方法提升性能

Result: 证明了使用合成数据集进行迁移学习可以提升性能，无论是否有针对目标任务的专门训练；解决了基于LLM的内容选择建模中的通用推理时间问题；评估了通用评估指标

Conclusion: IGCS框架及其相关资源和方法对未来的内容选择模型具有实用价值，为该领域提供了统一的建模方案和评估标准

Abstract: A broad range of NLP tasks involve selecting relevant text spans from given
source texts. Despite this shared objective, such \textit{content selection}
tasks have traditionally been studied in isolation, each with its own modeling
approaches, datasets, and evaluation metrics. In this work, we propose
\textit{instruction-guided content selection (IGCS)} as a beneficial unified
framework for such settings, where the task definition and any
instance-specific request are encapsulated as instructions to a language model.
To promote this framework, we introduce \igcsbench{}, the first unified
benchmark covering diverse content selection tasks. Further, we create a large
generic synthetic dataset that can be leveraged for diverse content selection
tasks, and show that transfer learning with these datasets often boosts
performance, whether dedicated training for the targeted task is available or
not. Finally, we address generic inference time issues that arise in LLM-based
modeling of content selection, assess a generic evaluation metric, and overall
propose the utility of our resources and methods for future content selection
models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [23] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)
*Robert Korom,Sarah Kiptinness,Najib Adan,Kassim Said,Catherine Ithuli,Oliver Rotich,Boniface Kimani,Irene King'ori,Stellah Kamau,Elizabeth Atemba,Muna Aden,Preston Bowman,Michael Sharman,Rebecca Soskin Hicks,Rebecca Distler,Johannes Heidecke,Rahul K. Arora,Karan Singhal*

Main category: cs.CL

TL;DR: 研究评估了基于大语言模型的临床决策支持系统AI Consult在肯尼亚内罗毕初级保健诊所的实际应用效果，发现该系统能显著减少临床诊断和治疗错误，获得临床医生广泛认可。


<details>
  <summary>Details</summary>
Motivation: 在实际医疗环境中，临床医生经常面临文档记录和临床决策错误的挑战。大语言模型技术的发展为改善临床决策支持提供了新的可能性，但需要在真实医疗场景中验证其有效性和实用性。

Method: 与肯尼亚内罗毕Penda Health初级保健诊所网络合作，开展质量改进研究。在15家诊所中比较有无AI Consult系统支持的临床医生的表现，共分析39,849次患者就诊。AI Consult作为安全网工具，集成到临床工作流程中，仅在需要时激活，保持临床医生的自主性。由独立医生评估临床错误率，并对使用AI Consult的临床医生进行调查。

Result: 使用AI Consult的临床医生错误率显著降低：诊断错误减少16%，治疗错误减少13%。按绝对数量计算，仅在Penda Health，AI Consult的引入每年可避免22,000次诊断错误和29,000次治疗错误。所有使用AI Consult的临床医生都认为该系统提高了医疗质量，其中75%认为效果"显著"。

Conclusion: 基于大语言模型的临床决策支持工具在实际医疗环境中具有减少临床错误的巨大潜力。成功的关键在于与临床工作流程的良好整合和积极的部署策略来促进临床医生采用。该研究为负责任地推广此类AI医疗工具提供了实用框架。

Abstract: We evaluate the impact of large language model-based clinical decision
support in live care. In partnership with Penda Health, a network of primary
care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a
safety net for clinicians by identifying potential documentation and clinical
decision-making errors. AI Consult integrates into clinician workflows,
activating only when needed and preserving clinician autonomy. We conducted a
quality improvement study, comparing outcomes for 39,849 patient visits
performed by clinicians with or without access to AI Consult across 15 clinics.
Visits were rated by independent physicians to identify clinical errors.
Clinicians with access to AI Consult made relatively fewer errors: 16% fewer
diagnostic errors and 13% fewer treatment errors. In absolute terms, the
introduction of AI Consult would avert diagnostic errors in 22,000 visits and
treatment errors in 29,000 visits annually at Penda alone. In a survey of
clinicians with AI Consult, all clinicians said that AI Consult improved the
quality of care they delivered, with 75% saying the effect was "substantial".
These results required a clinical workflow-aligned AI Consult implementation
and active deployment to encourage clinician uptake. We hope this study
demonstrates the potential for LLM-based clinical decision support tools to
reduce errors in real-world settings and provides a practical framework for
advancing responsible adoption.

</details>


### [24] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)
*Shuyuan Lin,Lei Duan,Philip Hughes,Yuxuan Sheng*

Main category: cs.CL

TL;DR: 本文提出了SALU方法，通过在大语言模型内部集成不可回答问题检测功能，并结合置信度引导的强化学习，显著提高了对话信息检索系统处理不可回答问题的能力，有效减少了幻觉生成。


<details>
  <summary>Details</summary>
Motivation: 传统对话信息检索系统在处理不可回答问题时存在重大挑战，容易生成误导性或幻觉内容。现有方法依赖外部分类器，会与核心生成大语言模型产生不一致性问题。

Method: 提出SALU（Self-Aware LLM for Unanswerability）方法，将不可回答检测直接集成到LLM生成过程中。采用多任务学习框架同时训练标准问答和明确拒绝生成，并引入置信度分数引导的强化学习人类反馈（RLHF）阶段，明确惩罚幻觉回应并奖励适当的拒绝回答。

Result: 在自建的C-IR_Answerability数据集上，SALU在正确回答或拒绝回答问题的整体准确性方面始终优于包括混合LLM-分类器系统在内的强基线。人工评估进一步确认了SALU在事实性、适当拒绝回答方面的优越可靠性，最重要的是大幅减少了幻觉现象。

Conclusion: SALU成功展示了通过内在自我意识训练，大语言模型能够稳健地"知道何时说'我不知道'"，为构建更可靠的对话信息检索系统提供了有效解决方案。

Abstract: Conversational Information Retrieval (CIR) systems, while offering intuitive
access to information, face a significant challenge: reliably handling
unanswerable questions to prevent the generation of misleading or hallucinated
content. Traditional approaches often rely on external classifiers, which can
introduce inconsistencies with the core generative Large Language Models
(LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a
novel approach that deeply integrates unanswerability detection directly within
the LLM's generative process. SALU is trained using a multi-task learning
framework for both standard Question Answering (QA) and explicit abstention
generation for unanswerable queries. Crucially, it incorporates a
confidence-score-guided reinforcement learning with human feedback (RLHF)
phase, which explicitly penalizes hallucinated responses and rewards
appropriate abstentions, fostering intrinsic self-awareness of knowledge
boundaries. Through extensive experiments on our custom-built
C-IR_Answerability dataset, SALU consistently outperforms strong baselines,
including hybrid LLM-classifier systems, in overall accuracy for correctly
answering or abstaining from questions. Human evaluation further confirms
SALU's superior reliability, achieving high scores in factuality, appropriate
abstention, and, most importantly, a dramatic reduction in hallucination,
demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [25] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)
*Aleksandr Perevalov,Andreas Both*

Main category: cs.CL

TL;DR: 本文提出了mKGQAgent框架，通过模块化的LLM代理工作流将多语言自然语言问题转换为SPARQL查询，在Text2SPARQL挑战赛中获得第一名


<details>
  <summary>Details</summary>
Motivation: 现有的多语言知识图谱问答方法主要依赖组合不同组件来解决下游任务，缺乏可解释性和模块化设计。需要开发一种类人推理的框架来处理多语言环境下自然语言到SPARQL查询的转换问题

Method: 提出mKGQAgent框架，采用人类启发的模块化方法，将自然语言问题转换为SPARQL查询分解为可解释的子任务。框架包含协调的LLM代理工作流，涵盖规划、实体链接和查询优化三个模块，并通过经验池进行上下文学习指导

Result: 在Text2SPARQL挑战赛2025的DBpedia和企业级KGQA基准测试中，mKGQAgent在所有参赛者中获得第一名，有效处理了多语言知识图谱问答任务

Conclusion: mKGQAgent框架成功实现了多语言环境下自然语言到SPARQL查询的高效转换，为开发多语言语义解析中的类人推理系统开辟了新的研究方向

Abstract: Accessing knowledge via multilingual natural-language interfaces is one of
the emerging challenges in the field of information retrieval and related ones.
Structured knowledge stored in knowledge graphs can be queried via a specific
query language (e.g., SPARQL). Therefore, one needs to transform
natural-language input into a query to fulfill an information need. Prior
approaches mostly focused on combining components (e.g., rule-based or
neural-based) that solve downstream tasks and come up with an answer at the
end. We introduce mKGQAgent, a human-inspired framework that breaks down the
task of converting natural language questions into SPARQL queries into modular,
interpretable subtasks. By leveraging a coordinated LLM agent workflow for
planning, entity linking, and query refinement - guided by an experience pool
for in-context learning - mKGQAgent efficiently handles multilingual KGQA.
Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the
Text2SPARQL challenge 2025, our approach took first place among the other
participants. This work opens new avenues for developing human-like reasoning
systems in multilingual semantic parsing.

</details>


### [26] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)
*Rishemjit Kaur,Arshdeep Singh Bhankhar,Surangika Ranathunga,Jashanpreet Singh Salh,Sudhir Rajput,Vidhi,Kashish Mahendra,Bhavika Berwal,Ritesh Kumar*

Main category: cs.CL

TL;DR: 该研究通过生成多语言合成农业数据集并微调特定语言的大语言模型，显著提升了农业问答系统在多语言和低资源环境下的准确性和本地化程度


<details>
  <summary>Details</summary>
Motivation: 通用大语言模型在农业领域缺乏精准性和本地化能力，特别是在多语言环境下，由于缺乏领域特定训练和高质量地区性数据集，导致提供的农业建议过于泛化，无法满足当地农民的实际需求

Method: 从农业特定文档中生成多语言合成农业数据集（英语、印地语、旁遮普语），然后对特定语言的大语言模型进行微调，构建针对性的农业问答系统

Result: 在精心策划的多语言数据集上的评估显示，微调后的模型在事实准确性、相关性和农业共识方面相比基线模型有显著改进，证明了合成数据驱动的语言特定微调策略的有效性

Conclusion: 该研究为多语言和低资源环境下的农业AI解决方案提供了有效的改进策略，通过实现更准确和本地化的农业咨询服务，为缩小不同语言社区在AI驱动农业解决方案方面的知识差距迈出了重要一步

Abstract: Enabling farmers to access accurate agriculture-related information in their
native languages in a timely manner is crucial for the success of the
agriculture field. Although large language models (LLMs) can be used to
implement Question Answering (QA) systems, simply using publicly available
general-purpose LLMs in agriculture typically offer generic advisories, lacking
precision in local and multilingual contexts due to insufficient
domain-specific training and scarcity of high-quality, region-specific
datasets. Our study addresses these limitations by generating multilingual
synthetic agricultural datasets (English, Hindi, Punjabi) from
agriculture-specific documents and fine-tuning language-specific LLMs. Our
evaluation on curated multilingual datasets demonstrates significant
improvements in factual accuracy, relevance, and agricultural consensus for the
fine-tuned models compared to their baseline counterparts. These results
highlight the efficacy of synthetic data-driven, language-specific fine-tuning
as an effective strategy to improve the performance of LLMs in agriculture,
especially in multilingual and low-resource settings. By enabling more accurate
and localized agricultural advisory services, this study provides a meaningful
step toward bridging the knowledge gap in AI-driven agricultural solutions for
diverse linguistic communities.

</details>


### [27] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)
*Giulio Pelosio,Devesh Batra,Noémie Bovey,Robert Hankache,Cristovao Iglesias,Greig Cowan,Raad Khraishi*

Main category: cs.CL

TL;DR: 这项研究开发了一种基于姓名的基准测试方法，用于检测大语言模型中的隐性国籍偏见。研究发现，即使用文化特征明显的姓名替代明确的国籍标签，模型仍然表现出偏见，且小型模型比大型模型表现出更多偏见和更低准确性。


<details>
  <summary>Details</summary>
Motivation: 现有研究主要关注明确人口统计标记下的偏见，但实际应用中LLM更常遇到的是通过姓名等隐性线索推断身份的情况。因此需要开发更贴近真实世界应用场景的偏见检测方法，研究LLM在处理文化指示性姓名时的潜在偏见表现。

Method: 基于BBQ数据集开发了一种新颖的基于姓名的基准测试方法，将明确的国籍标签替换为具有文化指示性的姓名。在模糊语境（正确答案未明确揭示）下测试了来自OpenAI、Google和Anthropic等主要厂商的多个LLM，评估偏见程度和准确性的变化。

Result: 小型模型表现出更多偏见和更低准确性。在模糊语境下，Claude Haiku的刻板印象偏见分数为9%，而更大的Claude Sonnet仅为3.5%，后者准确性还高出117.7%。在姓名替换后，GPT-4o保留了68%的错误率，而GPT-4o-mini保留了76%，其他模型提供商也有类似发现。

Conclusion: 研究揭示了LLM中偏见的顽固韧性，强调了这些偏见对AI系统在多样化全球环境中开发和部署的深远影响。即使采用更隐性的文化线索，模型仍然表现出显著偏见，这对AI系统的公平性和可靠性提出了重要挑战。

Abstract: Large Language Models (LLMs) can exhibit latent biases towards specific
nationalities even when explicit demographic markers are not present. In this
work, we introduce a novel name-based benchmarking approach derived from the
Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting
explicit nationality labels with culturally indicative names, a scenario more
reflective of real-world LLM applications. Our novel approach examines how this
substitution affects both bias magnitude and accuracy across a spectrum of LLMs
from industry leaders such as OpenAI, Google, and Anthropic. Our experiments
show that small models are less accurate and exhibit more bias compared to
their larger counterparts. For instance, on our name-based dataset and in the
ambiguous context (where the correct choice is not revealed), Claude Haiku
exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for
its larger counterpart, Claude Sonnet, where the latter also outperformed it by
117.7% in accuracy. Additionally, we find that small models retain a larger
portion of existing errors in these ambiguous contexts. For example, after
substituting names for explicit nationality references, GPT-4o retains 68% of
the error rate versus 76% for GPT-4o-mini, with similar findings for other
model providers, in the ambiguous context. Our research highlights the stubborn
resilience of biases in LLMs, underscoring their profound implications for the
development and deployment of AI systems in diverse, global contexts.

</details>


### [28] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)
*Ming Huang,Zehan Li,Yan Hu,Wanjing Wang,Andrew Wen,Scott Lane,Salih Selek,Lokesh Shahani,Rodrigo Machado-Vieira,Jair Soares,Hua Xu,Hongfang Liu*

Main category: cs.CL

TL;DR: 本研究使用生成式大语言模型(GPT-3.5和GPT-4.5)对精神科电子健康记录中的自杀相关因素进行多标签分类，开发了端到端的生成式多标签分类管道，并引入了先进的评估方法。微调后的GPT-3.5达到了0.94的部分匹配准确率和0.91的F1分数，而GPT-4.5在引导提示下在各标签集上表现更优。


<details>
  <summary>Details</summary>
Motivation: 自杀是全球健康危机，每年超过72万人死亡。早期识别自杀相关因素(包括自杀意念、自杀企图、自杀暴露和非自杀性自伤)对及时干预至关重要。现有研究多将自杀性视为二元分类任务，忽略了共存风险因素的复杂性。需要探索更复杂的多标签分类方法来更好地理解和识别这些风险因素。

Method: 使用生成式大语言模型GPT-3.5和GPT-4.5对精神科电子健康记录进行自杀相关因素的多标签分类。开发了新颖的端到端生成式多标签分类管道，引入了先进的评估方法，包括标签集级别指标和多标签混淆矩阵进行错误分析。对GPT-3.5进行微调，对GPT-4.5使用引导提示策略。

Result: 微调后的GPT-3.5实现了最佳性能，部分匹配准确率为0.94，F1分数为0.91。使用引导提示的GPT-4.5在各标签集上表现更优，特别是在稀有或少数标签集上，显示出更平衡和稳健的性能。研究还发现了系统性错误模式，如自杀意念和自杀企图的混淆，以及模型倾向于谨慎的过度标记。

Conclusion: 研究证明了使用生成式AI进行复杂临床分类任务的可行性，为将非结构化电子健康记录数据结构化以支持大规模临床研究和循证医学提供了蓝图。生成式大语言模型在自杀相关因素的多标签分类任务中表现出色，具有支持临床决策和早期干预的潜力。

Abstract: Suicide remains a pressing global health crisis, with over 720,000 deaths
annually and millions more affected by suicide ideation (SI) and suicide
attempts (SA). Early identification of suicidality-related factors (SrFs),
including SI, SA, exposure to suicide (ES), and non-suicidal self-injury
(NSSI), is critical for timely intervention. While prior studies have applied
AI to detect SrFs in clinical notes, most treat suicidality as a binary
classification task, overlooking the complexity of cooccurring risk factors.
This study explores the use of generative large language models (LLMs),
specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs
from psychiatric electronic health records (EHRs). We present a novel end to
end generative MLC pipeline and introduce advanced evaluation methods,
including label set level metrics and a multilabel confusion matrix for error
analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match
accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior
performance across label sets, including rare or minority label sets,
indicating a more balanced and robust performance. Our findings reveal
systematic error patterns, such as the conflation of SI and SA, and highlight
the models tendency toward cautious over labeling. This work not only
demonstrates the feasibility of using generative AI for complex clinical
classification tasks but also provides a blueprint for structuring unstructured
EHR data to support large scale clinical research and evidence based medicine.

</details>


### [29] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)
*Arduin Findeis,Floris Weers,Guoli Yin,Ke Ye,Ruoming Pang,Tom Gunter*

Main category: cs.CL

TL;DR: 本文提出了一个使用外部工具（网络搜索和代码执行）的智能体系统，用于改进AI标注器在长篇事实性、数学和代码任务等具有挑战性领域的成对比较标注质量。


<details>
  <summary>Details</summary>
Motivation: 现有的成对偏好标注方法在某些领域难以获得高质量的比较结果，特别是在包含大量事实陈述的回答中，标注者可能过度关注写作质量而忽视潜在事实的准确性，因此需要改进AI标注系统的性能。

Method: 提出了一个使用工具的智能体系统，通过网络搜索和代码执行等外部工具进行验证，独立于大语言模型的内部知识和偏见，为长篇事实性、数学和代码任务提供更高质量的反馈。

Result: 在RewardBench、RewardMath以及三个新数据集上进行了广泛实验评估，结果表明外部工具在许多（但非全部）情况下确实能够提升性能，同时发现性能对简单参数（如提示词）很敏感。

Conclusion: 外部工具可以在多数情况下改善标注性能，但实验突出了性能对参数的敏感性以及需要改进的非饱和标注基准的必要性。研究代码已在GitHub上开源。

Abstract: Pairwise preferences over model responses are widely collected to evaluate
and provide feedback to large language models (LLMs). Given two alternative
model responses to the same input, a human or AI annotator selects the "better"
response. This approach can provide feedback for domains where other hard-coded
metrics are difficult to obtain (e.g., chat response quality), thereby helping
model evaluation or training. However, for some domains high-quality pairwise
comparisons can be tricky to obtain - from AI and humans. For example, for
responses with many factual statements, annotators may disproportionately weigh
writing quality rather than underlying facts. In this work, we explore
augmenting standard AI annotator systems with additional tools to improve
performance on three challenging response domains: long-form factual, math and
code tasks. We propose a tool-using agentic system to provide higher quality
feedback on these domains. Our system uses web-search and code execution to
ground itself based on external validation, independent of the LLM's internal
knowledge and biases. We provide extensive experimental results evaluating our
method across the three targeted response domains as well as general annotation
tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as
three new datasets for domains with saturated pre-existing datasets. Our
results indicate that external tools can indeed improve performance in many,
but not all, cases. More generally, our experiments highlight the sensitivity
of performance to simple parameters (e.g., prompt) and the need for improved
(non-saturated) annotator benchmarks. We share our code at
https://github.com/apple/ml-agent-evaluator.

</details>


### [30] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)
*Soumen Sinha,Shahryar Rahnamayan,Azam Asilian Bidgoli*

Main category: cs.CL

TL;DR: 本文提出了一种基于坐标搜索的优化框架，为文本嵌入的每个特征找到最优阈值，将连续嵌入转换为二进制表示（条形码），在保持准确性的同时提高存储和计算效率。


<details>
  <summary>Details</summary>
Motivation: 大规模自然语言处理应用需要高效的文本嵌入，存储和计算效率是关键问题。传统的二值化方法使用固定阈值处理所有特征，无法充分利用不同特征的特性，导致性能损失。

Method: 提出基于坐标搜索的优化框架，为每个特征识别最优阈值，而非使用固定阈值。该方法可以将BERT等机器学习模型的连续嵌入转换为特征特定的二进制表示。

Result: 在多个NLP任务和数据集上进行广泛实验和统计测试，使用最优阈值生成的二进制嵌入在准确性上优于传统二值化方法，同时保持了存储和计算效率。

Conclusion: 特征特定阈值的二进制编码方法能够显著提升性能，生成的最优条形码表示在各种NLP应用中显示出良好效果。该技术具有通用性，不仅限于NLP嵌入，可应用于机器学习的各个领域。

Abstract: Efficient text embedding is crucial for large-scale natural language
processing (NLP) applications, where storage and computational efficiency are
key concerns. In this paper, we explore how using binary representations
(barcodes) instead of real-valued features can be used for NLP embeddings
derived from machine learning models such as BERT. Thresholding is a common
method for converting continuous embeddings into binary representations, often
using a fixed threshold across all features. We propose a Coordinate
Search-based optimization framework that instead identifies the optimal
threshold for each feature, demonstrating that feature-specific thresholds lead
to improved performance in binary encoding. This ensures that the binary
representations are both accurate and efficient, enhancing performance across
various features. Our optimal barcode representations have shown promising
results in various NLP applications, demonstrating their potential to transform
text representation. We conducted extensive experiments and statistical tests
on different NLP tasks and datasets to evaluate our approach and compare it to
other thresholding methods. Binary embeddings generated using using optimal
thresholds found by our method outperform traditional binarization methods in
accuracy. This technique for generating binary representations is versatile and
can be applied to any features, not just limited to NLP embeddings, making it
useful for a wide range of domains in machine learning applications.

</details>


### [31] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)
*Cheng Liu,Yifei Lu,Fanghua Ye,Jian Li,Xingyu Chen,Feiliang Ren,Zhaopeng Tu,Xiaolong Li*

Main category: cs.CL

TL;DR: 本文提出了CogDual，一种基于认知心理学的角色扮演语言智能体，采用"认知-然后-回应"的推理范式，通过联合建模外部情境感知和内部自我感知来提高角色一致性和上下文对齐能力。


<details>
  <summary>Details</summary>
Motivation: 现有的角色扮演语言智能体方法主要依赖提示工程或监督微调来模仿特定场景中的角色行为，但往往忽视了驱动这些行为的潜在认知机制。

Method: 提出CogDual框架，采用"认知-然后-回应"的推理范式，联合建模外部情境感知和内部自我感知。同时使用强化学习和两种针对开放域文本生成设计的通用奖励方案来进一步优化性能。

Result: 在CoSER基准测试以及Cross-MR和LifeChoice数据集上的广泛实验表明，CogDual始终优于现有基线方法，并在不同的角色扮演任务中表现出有效的泛化能力。

Conclusion: CogDual通过引入认知心理学启发的推理范式，成功提升了角色扮演语言智能体的角色一致性和上下文对齐能力，在多个基准测试中表现优异，证明了认知机制在角色扮演任务中的重要性。

Abstract: Role-Playing Language Agents (RPLAs) have emerged as a significant
application direction for Large Language Models (LLMs). Existing approaches
typically rely on prompt engineering or supervised fine-tuning to enable models
to imitate character behaviors in specific scenarios, but often neglect the
underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by
cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a
\textit{cognize-then-respond } reasoning paradigm. By jointly modeling external
situational awareness and internal self-awareness, CogDual generates responses
with improved character consistency and contextual alignment. To further
optimize the performance, we employ reinforcement learning with two
general-purpose reward schemes designed for open-domain text generation.
Extensive experiments on the CoSER benchmark, as well as Cross-MR and
LifeChoice, demonstrate that CogDual consistently outperforms existing
baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [32] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)
*Zhiqiang Liu,Enpei Niu,Yin Hua,Mengshu Sun,Lei Liang,Huajun Chen,Wen Zhang*

Main category: cs.CL

TL;DR: 本文提出了SKA-Bench基准测试，用于全面评估大语言模型对结构化知识（知识图谱、表格等）的理解能力，发现现有模型在处理结构化知识时仍面临显著挑战


<details>
  <summary>Details</summary>
Motivation: 现有的结构化知识理解评估不够严格（缺乏特定能力评估）且只关注单一类型的结构化知识，需要提出更全面、严格的基准测试来诊断大语言模型的不足

Method: 构建了包含四种结构化知识形式（知识图谱、表格、知识图谱+文本、表格+文本）的SKA-Bench基准测试，使用三阶段流水线构建测试实例，并设计了四个基本能力测试床：噪声鲁棒性、顺序不敏感性、信息整合和负面拒绝

Result: 对8个代表性大语言模型（包括先进的DeepSeek-R1）的实证评估表明，现有模型在理解结构化知识方面仍面临重大挑战，其性能受噪声量、知识单元顺序和幻觉现象等因素影响

Conclusion: 通过SKA-Bench基准测试揭示了当前大语言模型在结构化知识理解方面的局限性，为改进模型的结构化知识处理能力提供了评估工具和改进方向

Abstract: Although large language models (LLMs) have made significant progress in
understanding Structured Knowledge (SK) like KG and Table, existing evaluations
for SK understanding are non-rigorous (i.e., lacking evaluations of specific
capabilities) and focus on a single type of SK. Therefore, we aim to propose a
more comprehensive and rigorous structured knowledge understanding benchmark to
diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a
Structured Knowledge Augmented QA Benchmark that encompasses four widely used
structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a
three-stage pipeline to construct SKA-Bench instances, which includes a
question, an answer, positive knowledge units, and noisy knowledge units. To
evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we
expand the instances into four fundamental ability testbeds: Noise Robustness,
Order Insensitivity, Information Integration, and Negative Rejection. Empirical
evaluations on 8 representative LLMs, including the advanced DeepSeek-R1,
indicate that existing LLMs still face significant challenges in understanding
structured knowledge, and their performance is influenced by factors such as
the amount of noise, the order of knowledge units, and hallucination
phenomenon. Our dataset and code are available at
https://github.com/Lza12a/SKA-Bench.

</details>


### [33] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)
*Lingfeng Zeng,Fangqi Lou,Zixuan Wang,Jiajie Xu,Jinyi Niu,Mengping Li,Yifan Dong,Qi Qi,Wei Zhang,Ziwei Yang,Jun Han,Ruilun Feng,Ruiqi Hu,Lejie Zhang,Zhengbo Feng,Yicheng Ren,Xin Guo,Zhaowei Liu,Dongpo Cheng,Weige Cai,Liwen Zhang*

Main category: cs.CL

TL;DR: 本文介绍了FinGAIA，一个专门评估AI智能体在金融领域实际能力的端到端基准测试。该基准包含407个精心设计的任务，涵盖证券、基金、银行等七个金融子领域，分为三个层次的场景深度。测试结果显示最佳AI智能体ChatGPT仅达到48.9%的准确率，与金融专家相比仍有超过35个百分点的差距。


<details>
  <summary>Details</summary>
Motivation: AI智能体的快速发展为各领域复杂任务自动化带来了前所未有的机遇，但其在金融领域的多步骤、多工具协作能力仍然缺乏深入探索。现有缺乏专门针对金融领域的AI智能体评估基准，需要客观评估和促进智能体在这一关键领域的发展。

Method: 构建了FinGAIA端到端基准测试，包含407个精心设计的任务，涵盖证券、基金、银行、保险、期货、信托和资产管理七个主要金融子领域。任务分为三个层次的场景深度：基础业务分析、资产决策支持和战略风险管理。在零样本设置下评估了10个主流AI智能体的表现。

Result: 最佳表现的智能体ChatGPT总体准确率为48.9%，虽然优于非专业人士，但仍比金融专家低超过35个百分点。错误分析揭示了五种反复出现的失败模式：跨模态对齐缺陷、金融术语偏见、操作流程意识障碍等。

Conclusion: FinGAIA提供了首个与金融领域密切相关的智能体基准测试，旨在客观评估和促进智能体在这一关键领域的发展。研究发现的失败模式为未来研究指明了重要方向，表明AI智能体在金融领域仍有很大改进空间。

Abstract: The booming development of AI agents presents unprecedented opportunities for
automating complex tasks across various domains. However, their multi-step,
multi-tool collaboration capabilities in the financial sector remain
underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed
to evaluate the practical abilities of AI agents in the financial domain.
FinGAIA comprises 407 meticulously crafted tasks, spanning seven major
financial sub-domains: securities, funds, banking, insurance, futures, trusts,
and asset management. These tasks are organized into three hierarchical levels
of scenario depth: basic business analysis, asset decision support, and
strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot
setting. The best-performing agent, ChatGPT, achieved an overall accuracy of
48.9\%, which, while superior to non-professionals, still lags financial
experts by over 35 percentage points. Error analysis has revealed five
recurring failure patterns: Cross-modal Alignment Deficiency, Financial
Terminological Bias, Operational Process Awareness Barrier, among others. These
patterns point to crucial directions for future research. Our work provides the
first agent benchmark closely related to the financial domain, aiming to
objectively assess and promote the development of agents in this crucial field.
Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [34] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)
*Giuseppe Russo,Debora Nozza,Paul Röttger,Dirk Hovy*

Main category: cs.CL

TL;DR: 研究发现大语言模型在道德判断方面与人类存在显著差异，特别是在人类意见分歧较大时对齐性急剧下降，且模型依赖的道德价值观范围比人类更窄。为此提出了动态道德画像方法来改善对齐性。


<details>
  <summary>Details</summary>
Motivation: 随着人们越来越依赖大语言模型获取道德建议，了解LLM与人类道德判断的对齐程度变得至关重要，但目前对此了解甚少，需要系统性研究LLM在道德判断方面的表现。

Method: 构建了包含1,618个真实道德困境的数据集，每个困境配有人类道德判断分布；将问题视为多元分布对齐任务，比较LLM和人类判断的分布；从3,783个价值表达中构建60个价值的分类法；提出基于狄利克雷分布的动态道德画像(DMP)采样方法。

Result: 发现模型仅在高度共识下能重现人类判断，当人类分歧增加时对齐性急剧恶化；LLM依赖的道德价值集合比人类更窄；DMP方法将对齐性提高了64.3%，并增强了价值多样性。

Conclusion: 研究揭示了"多元道德差距"现象，即LLM与人类在道德价值分布和多样性方面都存在不匹配。动态道德画像方法为实现更多元化和人类对齐的道德指导提供了解决方案。

Abstract: People increasingly rely on Large Language Models (LLMs) for moral advice,
which may influence humans' decisions. Yet, little is known about how closely
LLMs align with human moral judgments. To address this, we introduce the Moral
Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a
distribution of human moral judgments consisting of a binary evaluation and a
free-text rationale. We treat this problem as a pluralistic distributional
alignment task, comparing the distributions of LLM and human judgments across
dilemmas. We find that models reproduce human judgments only under high
consensus; alignment deteriorates sharply when human disagreement increases. In
parallel, using a 60-value taxonomy built from 3,783 value expressions
extracted from rationales, we show that LLMs rely on a narrower set of moral
values than humans. These findings reveal a pluralistic moral gap: a mismatch
in both the distribution and diversity of values expressed. To close this gap,
we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method
that conditions model outputs on human-derived value profiles. DMP improves
alignment by 64.3% and enhances value diversity, offering a step toward more
pluralistic and human-aligned moral guidance from LLMs.

</details>


### [35] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)
*Kyeongkyu Lee,Seonghwan Yoon,Hongki Lim*

Main category: cs.CL

TL;DR: CLARIFID是一个新的放射学报告自动生成框架，通过模拟专家的两步工作流程（先生成发现再生成印象），结合多视角图像融合和强化学习优化，显著提高了报告的临床可靠性和诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 现有的放射学报告自动生成方法主要关注文本流畅性，但在确保报告事实正确性方面存在不足，且多数方法依赖单视角图像，限制了诊断的全面性，难以提供临床可靠的结论。

Method: CLARIFID采用四个核心技术：(1)通过分段感知预训练学习从发现到印象的逻辑流程；(2)使用近端策略优化进行微调，以CheXbert F1分数作为奖励；(3)实施推理感知解码，确保先完成"发现"部分再合成"印象"；(4)通过基于视觉变换器的多视角编码器融合多个胸部X光视角。

Result: 在MIMIC-CXR数据集上的实验结果表明，该方法在标准自然语言生成指标和临床感知评分上都优于现有基线方法，实现了更优的临床效果。

Conclusion: CLARIFID通过直接优化诊断正确性并镜像专家的两步工作流程，成功解决了现有方法在临床可靠性方面的不足，为放射学报告自动生成提供了更可靠的解决方案。

Abstract: Automatic generation of radiology reports has the potential to alleviate
radiologists' significant workload, yet current methods struggle to deliver
clinically reliable conclusions. In particular, most prior approaches focus on
producing fluent text without effectively ensuring the factual correctness of
the reports and often rely on single-view images, limiting diagnostic
comprehensiveness. We propose CLARIFID, a novel framework that directly
optimizes diagnostic correctness by mirroring the two-step workflow of experts.
Specifically, CLARIFID (1) learns the logical flow from Findings to Impression
through section-aware pretraining, (2) is fine-tuned with Proximal Policy
Optimization in which the CheXbert F1 score of the Impression section serves as
the reward, (3) enforces reasoning-aware decoding that completes "Findings"
before synthesizing the "Impression", and (4) fuses multiple chest X-ray views
via a vision-transformer-based multi-view encoder. During inference, we apply a
reasoning-aware next-token forcing strategy followed by report-level
re-ranking, ensuring that the model first produces a comprehensive Findings
section before synthesizing the Impression and thereby preserving coherent
clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate
that our method achieves superior clinical efficacy and outperforms existing
baselines on both standard NLG metrics and clinically aware scores.

</details>


### [36] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)
*Miaomiao Gao,Xiaoxiao Xiang,Yiwen Guo*

Main category: cs.CL

TL;DR: 本文提出了Triple X语音识别系统，采用编码器-适配器-大语言模型架构，通过多阶段训练策略优化多语言对话场景下的语音识别准确率，在MLC-SLM挑战赛中获得第二名。


<details>
  <summary>Details</summary>
Motivation: 在多语言对话场景中优化语音识别准确率，利用基于文本的大语言模型的强大推理能力，同时结合领域特定的适应性改进。

Method: 提出创新的编码器-适配器-大语言模型架构框架，采用精心设计的多阶段训练策略，利用大规模多语言音频数据集进行训练，结合领域特定适配来增强多语言识别性能。

Result: 在开发集和测试集上都取得了具有竞争力的词错误率(WER)性能，在MLC-SLM挑战赛Task 1中获得第二名的排名。

Conclusion: 所提出的Triple X语音识别系统通过编码器-适配器-大语言模型架构和多阶段训练策略，成功实现了多语言对话语音识别的性能优化，证明了该方法的有效性。

Abstract: This paper describes our Triple X speech recognition system submitted to Task
1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM)
Challenge. Our work focuses on optimizing speech recognition accuracy in
multilingual conversational scenarios through an innovative encoder-adapter-LLM
architecture. This framework harnesses the powerful reasoning capabilities of
text-based large language models while incorporating domain-specific
adaptations. To further enhance multilingual recognition performance, we
adopted a meticulously designed multi-stage training strategy leveraging
extensive multilingual audio datasets. Experimental results demonstrate that
our approach achieves competitive Word Error Rate (WER) performance on both dev
and test sets, obtaining second place in the challenge ranking.

</details>


### [37] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)
*Zhili Shen,Chenxin Diao,Pascual Merita,Pavlos Vougiouklis,Jeff Z. Pan*

Main category: cs.CL

TL;DR: 本文探索了基于图的检索增强生成(RAG)方法在SIGIR 2025 LiveRAG挑战赛中的应用，重点研究了GeAR模型的性能表现和局限性


<details>
  <summary>Details</summary>
Motivation: 现有的基于图的RAG方法主要针对特定任务设计(如多跳问答和查询聚焦摘要)，缺乏在更广泛数据集上的通用性验证，因此需要探索其在更通用场景下的适用性

Method: 适配并应用最先进的基于图的RAG解决方案GeAR，该方法利用从文档中提取的实体及其关系等结构化或半结构化信息来增强检索性能

Result: 在SIGIR 2025 LiveRAG挑战赛上评估了GeAR模型的性能表现，并识别了其在实际应用中的局限性

Conclusion: 通过在LiveRAG挑战赛中的实验，验证了基于图的RAG方法的实际效果，并为该领域的进一步发展提供了有价值的性能分析和局限性洞察

Abstract: Recent studies have explored graph-based approaches to retrieval-augmented
generation, leveraging structured or semi-structured information -- such as
entities and their relations extracted from documents -- to enhance retrieval.
However, these methods are typically designed to address specific tasks, such
as multi-hop question answering and query-focused summarisation, and therefore,
there is limited evidence of their general applicability across broader
datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG
solution: $\text{GeAR}$ and explore its performance and limitations on the
SIGIR 2025 LiveRAG Challenge.

</details>


### [38] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)
*Carlotta Quensel,Neele Falk,Gabriella Lapesa*

Main category: cs.CL

TL;DR: 这篇论文通过回归分析研究了主观因素（情感、叙事和模糊表达）对论证强度的影响，发现叙事和模糊表达对客观和主观论证质量有相反效果，而情感的影响取决于修辞运用而非领域。


<details>
  <summary>Details</summary>
Motivation: 随着NLP领域将主观性视为资产而非问题的趋势，需要研究论证质量的新维度。虽然已有关于个人故事等个别主观特征的研究，但缺乏这些特征与论证强度关系的大规模分析。

Method: 在两个标注了客观论证质量和主观说服力的标准数据集上进行回归分析，量化主观因素（情感、叙事、模糊表达）的影响。比较和评估每个主观特征的自动化标注方法。

Result: 叙事和模糊表达对客观和主观论证质量产生相反效果；情感的影响取决于修辞运用而非应用领域；不同主观特征对数据集中编码的两个论证强度方面呈现不同的影响模式。

Conclusion: 主观因素对论证强度有复杂且差异化的影响，其中叙事和模糊表达在客观质量和主观说服力方面表现出对比效应，而情感影响更多依赖于修辞策略的使用。这为理解论证质量的多维性提供了新见解。

Abstract: In assessing argument strength, the notions of what makes a good argument are
manifold. With the broader trend towards treating subjectivity as an asset and
not a problem in NLP, new dimensions of argument quality are studied. Although
studies on individual subjective features like personal stories exist, there is
a lack of large-scale analyses of the relation between these features and
argument strength. To address this gap, we conduct regression analysis to
quantify the impact of subjective factors $-$ emotions, storytelling, and
hedging $-$ on two standard datasets annotated for objective argument quality
and subjective persuasion. As such, our contribution is twofold: at the level
of contributed resources, as there are no datasets annotated with all studied
dimensions, this work compares and evaluates automated annotation methods for
each subjective feature. At the level of novel insights, our regression
analysis uncovers different patterns of impact of subjective features on the
two facets of argument strength encoded in the datasets. Our results show that
storytelling and hedging have contrasting effects on objective and subjective
argument quality, while the influence of emotions depends on their rhetoric
utilization rather than the domain.

</details>


### [39] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)
*Shiting Chen,Zijian Zhao,Jinsong Chen*

Main category: cs.CL

TL;DR: 本文提出了两种改进RAG的方法：混合嵌入RAG和置信度RAG，其中置信度RAG通过使用多个嵌入模型生成多次响应并选择置信度最高的回答，相比传统LLM和RAG分别提升约10%和5%的性能。


<details>
  <summary>Details</summary>
Motivation: 由于异构训练数据和模型架构的影响，RAG中使用的不同嵌入模型在各个领域表现出不同的优势，这导致相似度计算结果的差异，进而影响LLM的响应质量。为了解决这一问题，需要结合多个嵌入模型的优势来增强RAG性能。

Method: 提出了两种方法：1) 混合嵌入RAG - 基于标准化相似度对来自多个嵌入模型的检索结果进行排序和选择；2) 置信度RAG - 使用不同的嵌入模型多次生成响应，然后选择置信度水平最高的响应。

Result: 混合嵌入RAG的性能并未超过传统RAG。而置信度RAG表现出色，相比传统LLM平均提升约10%，相比RAG平均提升约5%。在不同LLM和嵌入模型上的一致性结果表明置信度RAG是一种高效的即插即用方法。

Conclusion: 置信度RAG是一种有效的增强RAG性能的方法，通过利用多个嵌入模型的集体智慧并选择最高置信度的响应，能够在各种领域中实现稳定的性能提升，具有良好的通用性和实用性。

Abstract: Recently, as Large Language Models (LLMs) have fundamentally impacted various
fields, the methods for incorporating up-to-date information into LLMs or
adding external knowledge to construct domain-specific models have garnered
wide attention. Retrieval-Augmented Generation (RAG), serving as an
inference-time scaling method, is notable for its low cost and minimal effort
for parameter tuning. However, due to heterogeneous training data and model
architecture, the variant embedding models used in RAG exhibit different
benefits across various areas, often leading to different similarity
calculation results and, consequently, varying response quality from LLMs. To
address this problem, we propose and examine two approaches to enhance RAG by
combining the benefits of multiple embedding models, named Mixture-Embedding
RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects
retrievals from multiple embedding models based on standardized similarity;
however, it does not outperform vanilla RAG. In contrast, Confident RAG
generates responses multiple times using different embedding models and then
selects the responses with the highest confidence level, demonstrating average
improvements of approximately 10% and 5% over vanilla LLMs and RAG,
respectively. The consistent results across different LLMs and embedding models
indicate that Confident RAG is an efficient plug-and-play approach for various
domains. We will release our code upon publication.

</details>


### [40] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)
*Alexander R. Fabbri,Diego Mares,Jorge Flores,Meher Mankikar,Ernesto Hernandez,Dean Lee,Bing Liu,Chen Xing*

Main category: cs.CL

TL;DR: 研究者创建了MultiNRC基准测试来评估大语言模型在多语言原生推理方面的能力，发现当前LLMs在处理非英语文化背景的推理任务时表现不佳，最高得分不超过50%


<details>
  <summary>Details</summary>
Motivation: 现有的多语言推理基准主要通过翻译英语基准构建，存在偏向英语语言和文化背景的问题，缺乏对LLMs在不同语言和文化背景下原生推理能力的全面评估

Method: 构建了MultiNRC基准，包含1000多个由法语、西班牙语和中文母语者编写的原生推理问题，涵盖语言特定推理、文字游戏谜语、文化传统推理和具有文化相关性的数学推理四个核心类别，并提供英语对等翻译进行对比

Result: 对14个主流LLMs进行系统评估，发现：(1)所有模型在MultiNRC上得分均低于50%；(2)模型在处理语言、文化和逻辑推理任务时表现出不同的优势和劣势；(3)大多数模型在英语数学推理上比原始语言表现好10%

Conclusion: 当前LLMs在多语言原生推理方面仍存在显著不足，特别是在处理文化背景知识方面面临持续挑战，表明需要进一步改进模型的多语言和跨文化推理能力

Abstract: Although recent Large Language Models (LLMs) have shown rapid improvement on
reasoning benchmarks in English, the evaluation of such LLMs' multilingual
reasoning capability across diverse languages and cultural contexts remains
limited. Existing multilingual reasoning benchmarks are typically constructed
by translating existing English reasoning benchmarks, biasing these benchmarks
towards reasoning problems with context in English language/cultures. In this
work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a
benchmark designed to assess LLMs on more than 1,000 native, linguistic and
culturally grounded reasoning questions written by native speakers in French,
Spanish, and Chinese. MultiNRC covers four core reasoning categories:
language-specific linguistic reasoning, wordplay & riddles, cultural/tradition
reasoning, and math reasoning with cultural relevance. For cultural/tradition
reasoning and math reasoning with cultural relevance, we also provide English
equivalent translations of the multilingual questions by manual translation
from native speakers fluent in English. This set of English equivalents can
provide a direct comparison of LLM reasoning capacity in other languages vs.
English on the same reasoning questions. We systematically evaluate current 14
leading LLMs covering most LLM families on MultiNRC and its English equivalent
set. The results show that (1) current LLMs are still not good at native
multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs
exhibit distinct strengths and weaknesses in handling linguistic, cultural, and
logical reasoning tasks; (3) Most models perform substantially better in math
reasoning in English compared to in original languages (+10%), indicating
persistent challenges with culturally grounded knowledge.

</details>


### [41] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)
*Shanbo Cheng,Yu Bao,Zhichao Huang,Yu Lu,Ningxin Peng,Lu Xu,Runsheng Yu,Rong Cao,Ting Han,Zeyang Li,Sitong Liu,Shengtao Ma,Shiguang Pan,Jiongchen Xiao,Nuo Xu,Meng Yang,Rong Ye,Yiming Yu,Ruofei Zhang,Wanyi Zhang,Wenhao Zhu,Liehao Zou,Lu Lu,Yuxuan Wang,Yonghui Wu*

Main category: cs.CL

TL;DR: 本文介绍了Seed-LiveInterpret 2.0，一个端到端的同声传译模型，通过新颖的双工语音理解生成框架实现了高保真、超低延迟的语音到语音生成和声音克隆功能，在翻译质量和延迟之间取得了显著平衡。


<details>
  <summary>Details</summary>
Motivation: 同声传译是翻译行业最具挑战性的前沿领域，现有的产品级自动系统长期面临转录和翻译质量差、缺乏实时语音生成、多说话人混淆以及翻译语音膨胀等棘手问题，特别是在长篇话语中表现更为突出。

Method: 提出了一个新颖的双工语音到语音理解生成框架，结合大规模预训练和强化学习技术，构建了端到端的同声传译模型Seed-LiveInterpret 2.0，该模型具备声音克隆能力。

Result: 实验结果表明，该模型在翻译准确性和延迟之间实现了显著更好的平衡，经人工译员验证在复杂场景中正确率超过70%。相比商业同声传译解决方案，在翻译质量方面有显著提升，同时将克隆语音的平均延迟从近10秒大幅减少到接近实时的3秒，降幅约70%。

Conclusion: Seed-LiveInterpret 2.0作为完全可操作的产品级解决方案，成功解决了同声传译领域的关键挑战，大幅提升了实用性，在翻译质量和延迟性能方面都超越了现有商业解决方案。

Abstract: Simultaneous Interpretation (SI) represents one of the most daunting
frontiers in the translation industry, with product-level automatic systems
long plagued by intractable challenges: subpar transcription and translation
quality, lack of real-time speech generation, multi-speaker confusion, and
translated speech inflation, especially in long-form discourses. In this study,
we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers
high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning
capabilities. As a fully operational product-level solution, Seed-LiveInterpret
2.0 tackles these challenges head-on through our novel duplex speech-to-speech
understanding-generating framework. Experimental results demonstrate that
through large-scale pretraining and reinforcement learning, the model achieves
a significantly better balance between translation accuracy and latency,
validated by human interpreters to exceed 70% correctness in complex scenarios.
Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by
significant margins in translation quality, while slashing the average latency
of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is
around a near 70% reduction that drastically enhances practical usability.

</details>


### [42] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)
*Brian DeRenzi,Anna Dixon,Mohamed Aymane Farhi,Christian Resch*

Main category: cs.CL

TL;DR: 该研究首次系统性评估了大规模合成语音语料库对非洲自动语音识别(ASR)的影响，通过LLM生成文本、TTS合成语音和ASR微调的三步流程，为非洲低资源语言创建了超过2500小时的合成语音数据，成本仅为真实数据的1%，并在多种语言上验证了合成数据能有效提升ASR性能。


<details>
  <summary>Details</summary>
Motivation: 非洲超过2300种语言中的大多数仍无法使用语音技术，现有的ASR系统缺乏足够的训练数据。研究旨在探索使用合成语音数据来改善非洲低资源语言的ASR性能，解决数据稀缺和成本高昂的问题。

Method: 采用三步流程：1)使用大语言模型(LLM)生成目标语言文本；2)通过文本转语音(TTS)技术合成语音数据；3)使用合成数据对ASR模型进行微调。重点评估了10种语言的文本可读性，并对3种语言(豪萨语、多卢奥语、奇切瓦语)进行了详细的ASR性能评估。

Result: 在10种语言中，8种语言的合成文本可读性评分超过5分(满分7分)。创建了超过2500小时的合成语音数据，成本不到真实数据的1%。对于豪萨语，250小时真实数据+250小时合成数据的性能与500小时纯真实数据基线相当；579小时真实数据+450-993小时合成数据达到最佳性能。奇切瓦语在1:2真实与合成数据比例下，词错误率相对改善约6.5%。

Conclusion: 合成语音数据能够有效改善非洲低资源语言的ASR性能，特别是在与真实数据结合使用时效果显著。然而，对于极低资源语言，改善效果存在差异，需要更robust的评估协议和更准确的评估数据。所有数据和模型已公开发布，为非洲语言的语音技术发展提供了重要资源。

Abstract: Speech technology remains out of reach for most of the over 2300 languages in
Africa. We present the first systematic assessment of large-scale synthetic
voice corpora for African ASR. We apply a three-step process: LLM-driven text
creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages
for which we create synthetic text achieved readability scores above 5 out of
7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created
more than 2,500 hours of synthetic voice data at below 1% of the cost of real
data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h
synthetic Hausa matched a 500h real-data-only baseline, while 579h real and
450h to 993h synthetic data created the best performance. We also present
gender-disaggregated ASR performance evaluation. For very low-resource
languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2
real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on
some evaluation data, but not on others. Investigating intercoder reliability,
ASR errors and evaluation datasets revealed the need for more robust reviewer
protocols and more accurate evaluation data. All data and models are publicly
released to invite further work to improve synthetic data for African
languages.

</details>


### [43] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)
*Bowen Zheng,Ming Ma,Zhongqiao Lin,Tianming Yang*

Main category: cs.CL

TL;DR: 提出了SPADE方法，通过对齐中间层和输出层表示来改善大语言模型的早期退出算法，在降低推理成本的同时保持准确性


<details>
  <summary>Details</summary>
Motivation: 大语言模型计算成本高昂，现有早期退出算法由于中间层和输出层表示不对齐导致解码不准确，性能较差

Method: 提出SPADE（空间对齐解码）方法，通过传播仅包含起始标记和答案标记的最小化序列来对齐中间层表示与输出层；训练线性近似模型计算基于熵的置信度指标；构建混合早期退出算法

Result: 显著降低了推理成本且不损害准确性，为大语言模型在实际应用中的部署提供了可扩展和高效的解决方案

Conclusion: SPADE方法成功解决了早期退出算法中表示不对齐的问题，实现了在保持高质量输出的同时大幅降低推理成本的目标

Abstract: Large language models are computationally expensive due to their deep
structures. Prior research has shown that intermediate layers contain
sufficient information to generate accurate answers, leading to the development
of early-exit algorithms that reduce inference costs by terminating computation
at earlier layers. However, these methods often suffer from poor performance
due to misalignment between intermediate and output layer representations that
lead to decoding inaccuracy. To address these challenges, we propose SPADE
(SPace Alignment DEcoding), a novel decoding method that aligns intermediate
layer representations with the output layer by propagating a minimally reduced
sequence consisting of only the start token and the answer token. We further
optimize the early-exit decision-making process by training a linear
approximation of SPADE that computes entropy-based confidence metrics. Putting
them together, we create a hybrid early-exit algorithm that monitors confidence
levels and stops inference at intermediate layers while using SPADE to generate
high-quality outputs. This approach significantly reduces inference costs
without compromising accuracy, offering a scalable and efficient solution for
deploying large language models in real-world applications.

</details>


### [44] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)
*Changxin Tian,Jiapeng Wang,Qian Zhao,Kunlong Chen,Jia Liu,Ziqi Liu,Jiaxin Mao,Wayne Xin Zhao,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了WSM（Warmup-Stable and Merge）框架，通过模型合并技术替代传统的学习率衰减，在多个基准测试中显著优于现有方法，在MATH上提升3.5%，HumanEval上提升2.9%，MMLU-Pro上提升5.5%。


<details>
  <summary>Details</summary>
Motivation: 传统的学习率衰减方法存在局限性，而最近的无衰减方法和模型合并技术显示出良好的效果。研究者希望建立学习率衰减与模型合并之间的形式化联系，开发一个统一的理论框架来替代传统衰减策略。

Method: 提出WSM（Warmup-Stable and Merge）框架，该框架将各种衰减策略（包括余弦衰减、线性衰减和反平方根衰减）转化为有原则的模型平均方案。框架保持与多种优化方法的兼容性，并通过广泛实验确定合并持续时间为影响模型性能的最关键因素。

Result: WSM框架在多个基准测试中consistently优于广泛采用的Warmup-Stable-Decay（WSD）方法，在MATH上获得+3.5%的改进，在HumanEval上获得+2.9%的改进，在MMLU-Pro上获得+5.5%的改进。性能优势还扩展到监督微调场景。

Conclusion: WSM框架成功建立了学习率衰减与模型合并的理论联系，提供了一个有效的替代传统衰减策略的方法。合并持续时间被确定为最关键的性能影响因素，该框架在长期模型优化方面具有巨大潜力。

Abstract: Recent advances in learning rate (LR) scheduling have demonstrated the
effectiveness of decay-free approaches that eliminate the traditional decay
phase while maintaining competitive performance. Model merging techniques have
emerged as particularly promising solutions in this domain. We present
Warmup-Stable and Merge (WSM), a general framework that establishes a formal
connection between learning rate decay and model merging. WSM provides a
unified theoretical foundation for emulating various decay strategies-including
cosine decay, linear decay and inverse square root decay-as principled model
averaging schemes, while remaining fully compatible with diverse optimization
methods. Through extensive experiments, we identify merge duration-the training
window for checkpoint aggregation-as the most critical factor influencing model
performance, surpassing the importance of both checkpoint interval and merge
quantity. Our framework consistently outperforms the widely-adopted
Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving
significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on
MMLU-Pro. The performance advantages extend to supervised fine-tuning
scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [45] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)
*Victor Hartman,Petter Törnberg*

Main category: cs.CL

TL;DR: 本研究使用零样本大语言模型对19个欧洲国家议员的1800万条推文进行负面竞选分析，发现执政党较少使用负面信息，而极端主义和民粹主义政党（特别是激进右翼）的负面程度显著更高。


<details>
  <summary>Details</summary>
Motivation: 现有负面竞选分类方法成本高、可扩展性有限，限制了实证研究的发展。需要一种新的方法来进行大规模跨语言的负面竞选研究。

Method: 引入零样本大语言模型（LLMs）作为跨语言负面竞选分类的新方法，使用10种语言的基准数据集进行验证，然后应用于分析2017-2022年间19个欧洲国家议员发布的1800万条推文。

Result: LLMs在负面竞选分类任务上达到了与母语人类编码员相当的性能，超越了传统监督机器学习方法。跨国分析显示：执政党使用负面信息的可能性较低，而意识形态极端和民粹主义政党（特别是激进右翼）的负面程度显著更高。

Conclusion: 党派层面的特征会影响多党制系统中的战略传播模式。大语言模型为跨语言和文化背景下的政治传播研究提供了可扩展、透明和可复制的新工具。

Abstract: Negative campaigning is a central feature of political competition, yet
empirical research has been limited by the high cost and limited scalability of
existing classification methods. This study makes two key contributions. First,
it introduces zero-shot Large Language Models (LLMs) as a novel approach for
cross-lingual classification of negative campaigning. Using benchmark datasets
in ten languages, we demonstrate that LLMs achieve performance on par with
native-speaking human coders and outperform conventional supervised machine
learning approaches. Second, we leverage this novel method to conduct the
largest cross-national study of negative campaigning to date, analyzing 18
million tweets posted by parliamentarians in 19 European countries between 2017
and 2022. The results reveal consistent cross-national patterns: governing
parties are less likely to use negative messaging, while ideologically extreme
and populist parties -- particularly those on the radical right -- engage in
significantly higher levels of negativity. These findings advance our
understanding of how party-level characteristics shape strategic communication
in multiparty systems. More broadly, the study demonstrates the potential of
LLMs to enable scalable, transparent, and replicable research in political
communication across linguistic and cultural contexts.

</details>


### [46] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)
*Changxin Tian,Kunlong Chen,Jia Liu,Ziqi Liu,Zhiqiang Zhang,Jun Zhou*

Main category: cs.CL

TL;DR: 本文提出了效率杠杆(EL)指标来量化MoE模型相对于密集模型的计算优势，通过训练300多个模型建立了MoE架构配置与性能的统一缩放定律，并用0.85B参数的Ling-mini-beta模型验证了该定律的准确性。


<details>
  <summary>Details</summary>
Motivation: MoE架构虽然能够有效扩展大语言模型，但如何预测给定MoE配置（如专家激活比例和粒度）的模型容量仍是未解决的关键问题，缺乏系统性的理论指导。

Method: 引入效率杠杆(EL)指标量化MoE模型的计算优势；训练超过300个模型（最大28B参数）进行大规模实证研究；系统分析MoE架构配置与EL的关系；建立统一的缩放定律预测MoE架构的EL。

Result: 发现EL主要由专家激活比例和总计算预算驱动，两者遵循可预测的幂律；专家粒度作为非线性调节器存在明确的最优范围；Ling-mini-beta模型（0.85B活跃参数）在相同1T高质量token数据集上达到了6.1B密集模型的性能，但计算资源消耗减少7倍以上。

Conclusion: 建立了MoE模型缩放的原理性和实证基础，提供了准确预测MoE架构效率的统一缩放定律，为高效MoE模型的设计和扩展提供了理论指导。

Abstract: Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large
Language Models (LLMs) efficiently by decoupling total parameters from
computational cost. However, this decoupling creates a critical challenge:
predicting the model capacity of a given MoE configurations (e.g., expert
activation ratio and granularity) remains an unresolved problem. To address
this gap, we introduce Efficiency Leverage (EL), a metric quantifying the
computational advantage of an MoE model over a dense equivalent. We conduct a
large-scale empirical study, training over 300 models up to 28B parameters, to
systematically investigate the relationship between MoE architectural
configurations and EL. Our findings reveal that EL is primarily driven by the
expert activation ratio and the total compute budget, both following
predictable power laws, while expert granularity acts as a non-linear modulator
with a clear optimal range. We integrate these discoveries into a unified
scaling law that accurately predicts the EL of an MoE architecture based on its
configuration. To validate our derived scaling laws, we designed and trained
Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active
parameters, alongside a 6.1B dense model for comparison. When trained on an
identical 1T high-quality token dataset, Ling-mini-beta matched the performance
of the 6.1B dense model while consuming over 7x fewer computational resources,
thereby confirming the accuracy of our scaling laws. This work provides a
principled and empirically-grounded foundation for the scaling of efficient MoE
models.

</details>


### [47] [Bridging Robustness and Generalization Against Word Substitution Attacks in NLP via the Growth Bound Matrix Approach](https://arxiv.org/abs/2507.10330)
*Mohammed Bouri,Adnane Saoud*

Main category: cs.CL

TL;DR: 本文提出了一种基于增长边界矩阵(GBM)的新型正则化技术，用于提高NLP模型（特别是LSTM、S4状态空间模型和CNN）对抗同义词替换等对抗性攻击的鲁棒性，在多个基准数据集上实现了最高8.8%的鲁棒性提升。


<details>
  <summary>Details</summary>
Motivation: 尽管NLP领域取得了进展，但模型仍容易受到同义词替换等对抗性攻击的影响。以往研究主要关注前馈和卷积架构的鲁棒性改进，而循环网络和现代状态空间模型（如S4）的鲁棒性研究不足。这些架构由于其序列处理和复杂参数动态特性带来了独特挑战。

Method: 引入基于增长边界矩阵(Growth Bound Matrices, GBM)的新型正则化技术，通过减少输入扰动对模型输出的影响来提高NLP模型鲁棒性。重点计算了三种架构的GBM：长短期记忆网络(LSTM)、状态空间模型(S4)和卷积神经网络(CNN)。

Result: 在多个架构和基准数据集上进行的广泛实验表明，该方法相比现有基线提高了最高8.8%的对抗鲁棒性，在对抗防御方面优于多个最先进的方法。同时实现了三个目标：增强对词汇替换攻击的抵抗力、改善干净文本上的泛化性能、提供S4模型鲁棒性的首次系统分析。

Conclusion: 基于GBM的正则化技术有效提高了NLP模型的对抗鲁棒性，特别是对于之前研究不足的循环网络和状态空间模型。该方法在多个架构上都表现出优越性能，为NLP模型的对抗防御提供了新的有效途径。

Abstract: Despite advancements in Natural Language Processing (NLP), models remain
vulnerable to adversarial attacks, such as synonym substitutions. While prior
work has focused on improving robustness for feed-forward and convolutional
architectures, the robustness of recurrent networks and modern state space
models (SSMs), such as S4, remains understudied. These architectures pose
unique challenges due to their sequential processing and complex parameter
dynamics. In this paper, we introduce a novel regularization technique based on
Growth Bound Matrices (GBM) to improve NLP model robustness by reducing the
impact of input perturbations on model outputs. We focus on computing the GBM
for three architectures: Long Short-Term Memory (LSTM), State Space models
(S4), and Convolutional Neural Networks (CNN). Our method aims to (1) enhance
resilience against word substitution attacks, (2) improve generalization on
clean text, and (3) providing the first systematic analysis of SSM (S4)
robustness. Extensive experiments across multiple architectures and benchmark
datasets demonstrate that our method improves adversarial robustness by up to
8.8% over existing baselines. These results highlight the effectiveness of our
approach, outperforming several state-of-the-art methods in adversarial
defense. Codes are available at https://github.com/BouriMohammed/GBM

</details>


### [48] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)
*Parker Riley,Siamak Shakeri,Waleed Ammar,Jonathan H. Clark*

Main category: cs.CL

TL;DR: 本文介绍了TyDi QA-WANA数据集，包含28K个西亚和北非10种语言的问答样本，旨在评估模型在大文本语境下的问答能力


<details>
  <summary>Details</summary>
Motivation: 现有问答数据集缺乏对西亚和北非语言的覆盖，且多数通过翻译生成，存在文化相关性问题。需要一个直接用各语言收集的、包含长文本语境的问答数据集来评估模型的多语言问答能力

Method: 设计数据收集流程以获取信息寻求型问题，确保提问者真正想知道答案。每个问题配对一篇完整文章（可能包含也可能不包含答案）。数据直接用各语言收集，避免翻译带来的文化相关性问题

Result: 构建了包含28K样本的TyDi QA-WANA数据集，覆盖西亚和北非的10种语言。文章相对较长，适合评估模型利用大文本语境回答问题的能力。测试了两个基线模型的性能

Conclusion: 成功创建了多语言问答数据集TyDi QA-WANA，填补了西亚北非语言问答资源的空白。通过直接收集避免了翻译问题，文章长度适合评估长文本理解能力。已开源代码和数据以促进研究社区的进一步改进

Abstract: We present TyDi QA-WANA, a question-answering dataset consisting of 28K
examples divided among 10 language varieties of western Asia and northern
Africa. The data collection process was designed to elicit information-seeking
questions, where the asker is genuinely curious to know the answer. Each
question in paired with an entire article that may or may not contain the
answer; the relatively large size of the articles results in a task suitable
for evaluating models' abilities to utilize large text contexts in answering
questions. Furthermore, the data was collected directly in each language
variety, without the use of translation, in order to avoid issues of cultural
relevance. We present performance of two baseline models, and release our code
and data to facilitate further improvement by the research community.

</details>


### [49] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)
*Karen Zhou,John Giorgi,Pranav Mani,Peng Xu,Davis Liang,Chenhao Tan*

Main category: cs.CL

TL;DR: 该研究提出了一种系统化方法，将真实用户反馈转化为结构化检查清单，用于评估AI生成的临床笔记质量，并通过大语言模型执行评估，在覆盖度、多样性和预测能力方面优于基线方法。


<details>
  <summary>Details</summary>
Motivation: AI生成的临床笔记在医疗保健中使用越来越广泛，但由于专家评审的高主观性和有限可扩展性，评估其质量仍然是一个挑战。现有的自动化指标往往无法与现实世界中医生的偏好保持一致。

Method: 提出了一个系统化管道，将真实用户反馈提炼成结构化的笔记评估检查清单。这些检查清单设计为可解释的、基于人类反馈的，并可由基于大语言模型的评估器执行。使用了来自部署的AI医疗记录系统的超过21,000次临床遭遇的去标识化数据。

Result: 反馈衍生的检查清单在离线评估中的覆盖度、多样性和对人类评分的预测能力方面优于基线方法。广泛的实验证实了检查清单对质量降级扰动的鲁棒性、与临床医生偏好的显著一致性，以及作为评估方法的实用价值。

Conclusion: 在离线研究环境中，该检查清单可以帮助识别可能低于所选质量阈值的笔记，为AI生成临床笔记的质量评估提供了一种有效且可扩展的解决方案。

Abstract: AI-generated clinical notes are increasingly used in healthcare, but
evaluating their quality remains a challenge due to high subjectivity and
limited scalability of expert review. Existing automated metrics often fail to
align with real-world physician preferences. To address this, we propose a
pipeline that systematically distills real user feedback into structured
checklists for note evaluation. These checklists are designed to be
interpretable, grounded in human feedback, and enforceable by LLM-based
evaluators. Using deidentified data from over 21,000 clinical encounters,
prepared in accordance with the HIPAA safe harbor standard, from a deployed AI
medical scribe system, we show that our feedback-derived checklist outperforms
baseline approaches in our offline evaluations in coverage, diversity, and
predictive power for human ratings. Extensive experiments confirm the
checklist's robustness to quality-degrading perturbations, significant
alignment with clinician preferences, and practical value as an evaluation
methodology. In offline research settings, the checklist can help identify
notes likely to fall below our chosen quality thresholds.

</details>


### [50] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)
*Danny D. Leybzon,Shreyas Tirumala,Nishant Jain,Summer Gillen,Michael Jackson,Cameron McPhee,Jennifer Schmidt*

Main category: cs.CL

TL;DR: 本研究开发并测试了一个基于大语言模型的AI电话调研系统，用于进行定量调研，并通过试点调研验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着语音AI技术的兴起，定量调研研究者需要一种新的数据收集模式来平衡人性化交互和方法论严谨性，同时能够扩大定量研究规模。传统的交互式语音应答(IVR)技术无法提供自然和适应性的受访者体验。

Method: 构建了一个基于大语言模型(LLM)、自动语音识别(ASR)和语音合成技术的AI调研系统。该系统专门为定量研究设计，严格遵循研究最佳实践，包括问题顺序随机化、答案顺序随机化和精确措辞。通过SSRS意见面板进行两项试点调研，并进行后续的人工调研来评估受访者体验。

Result: 研究测量了三个关键指标：调研完成率、中断率和受访者满意度分数。结果表明，较短的调研工具和更具响应性的AI访问员可能有助于改善所有三个指标。

Conclusion: AI电话调研系统在定量研究中显示出有效性，特别是在使用较短调研工具和提高AI响应性的情况下，能够在保持研究严谨性的同时提供更好的受访者体验。

Abstract: With the rise of voice-enabled artificial intelligence (AI) systems,
quantitative survey researchers have access to a new data-collection mode: AI
telephone surveying. By using AI to conduct phone interviews, researchers can
scale quantitative studies while balancing the dual goals of human-like
interactivity and methodological rigor. Unlike earlier efforts that used
interactive voice response (IVR) technology to automate these surveys, voice AI
enables a more natural and adaptive respondent experience as it is more robust
to interruptions, corrections, and other idiosyncrasies of human speech.
  We built and tested an AI system to conduct quantitative surveys based on
large language models (LLM), automatic speech recognition (ASR), and speech
synthesis technologies. The system was specifically designed for quantitative
research, and strictly adhered to research best practices like question order
randomization, answer order randomization, and exact wording.
  To validate the system's effectiveness, we deployed it to conduct two pilot
surveys with the SSRS Opinion Panel and followed-up with a separate
human-administered survey to assess respondent experiences. We measured three
key metrics: the survey completion rates, break-off rates, and respondent
satisfaction scores. Our results suggest that shorter instruments and more
responsive AI interviewers may contribute to improvements across all three
metrics studied.

</details>


### [51] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)
*Boxun Li,Yadong Li,Zhiyuan Li,Congyi Liu,Weilin Liu,Guowei Niu,Zheyue Tan,Haiyang Xu,Zhuyu Yao,Tao Yuan,Dong Zhou,Yueqing Zhuang,Bo Zhao,Guohao Dai,Yu Wang*

Main category: cs.CL

TL;DR: Megrez2是一个轻量级高性能语言模型架构，通过跨层专家共享机制和预门控路由优化，仅用3B激活参数和7.5B存储参数就能在多项任务上达到与更大模型相当或更优的性能，特别适合资源受限的设备部署。


<details>
  <summary>Details</summary>
Motivation: 为了在资源受限的设备上部署高性能语言模型，需要设计一种能够平衡准确性、效率和可部署性的轻量级架构，解决现有大模型参数量过大、部署困难的问题。

Method: 提出Megrez2架构，核心包含两个创新机制：1）跨层专家共享机制，通过在相邻transformer层间重用专家模块来显著减少总参数量；2）预门控路由机制，实现内存高效的专家加载和更快的推理速度。模型在5万亿token语料上预训练，并通过监督微调和可验证奖励的强化学习进一步优化。

Result: Megrez2-Preview模型仅使用3B激活参数和7.5B存储参数，在语言理解、指令遵循、数学推理和代码生成等多项任务上表现出与更大模型相当或更优的性能，证明了架构的有效性。

Conclusion: Megrez2架构成功实现了准确性、效率和可部署性之间的平衡，为资源受限的实际应用场景提供了强有力的解决方案，证明了轻量级模型架构设计的潜力和价值。

Abstract: We present Megrez2, a novel lightweight and high-performance language model
architecture optimized for device native deployment. Megrez2 introduces a novel
cross-layer expert sharing mechanism, which significantly reduces total
parameter count by reusing expert modules across adjacent transformer layers
while maintaining most of the model's capacity. It also incorporates pre-gated
routing, enabling memory-efficient expert loading and faster inference. As the
first instantiation of the Megrez2 architecture, we introduce the
Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and
further enhanced through supervised fine-tuning and reinforcement learning with
verifiable rewards. With only 3B activated and 7.5B stored parameters,
Megrez2-Preview demonstrates competitive or superior performance compared to
larger models on a wide range of tasks, including language understanding,
instruction following, mathematical reasoning, and code generation. These
results highlight the effectiveness of the Megrez2 architecture to achieve a
balance between accuracy, efficiency, and deployability, making it a strong
candidate for real-world, resource-constrained applications.

</details>


### [52] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)
*Linbo Cao,Jinman Zhao*

Main category: cs.CL

TL;DR: 提出了一种基于辩论的评估范式，将现有QA数据集转换为结构化对抗辩论，通过多轮论证增加难度并惩罚浅层记忆，为评估大型语言模型的真实推理能力提供了可持续的解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着前沿语言模型在标准QA基准测试中日趋饱和，数据污染、记忆化和数据集创建成本不断攀升等问题持续存在，需要一种新的评估方法来测量模型的真实推理能力而非记忆能力。

Method: 提出辩论驱动的评估范式：(1)将现有QA数据集转换为结构化对抗辩论，一个模型为官方答案辩护，另一个构建并为替代答案辩护；(2)由不知道正确答案的裁判模型进行裁决；(3)通过强制多轮论证增加难度并惩罚浅层记忆。

Result: 在MMLU-Pro问题子集上的实验验证了方法的有效性：在测试问题上微调的Llama 3.1模型准确率从50%提升到82%，但在辩论中表现更差；即使较弱的裁判也能可靠地区分更强的辩论者；该方法对数据污染具有鲁棒性。

Conclusion: 该框架表明"在测试集上预训练不再足够"，为测量先进语言模型的真实推理能力提供了可持续的路径，能够以较低成本扩展到未来更强大的系统，同时有效应对数据污染问题。

Abstract: As frontier language models increasingly saturate standard QA benchmarks,
concerns about data contamination, memorization, and escalating dataset
creation costs persist. We propose a debate-driven evaluation paradigm that
transforms any existing QA dataset into structured adversarial debates--where
one model is given the official answer to defend, and another constructs and
defends an alternative answer--adjudicated by a judge model blind to the
correct solution. By forcing multi-round argumentation, this approach
substantially increases difficulty while penalizing shallow memorization, yet
reuses QA items to reduce curation overhead. We make two main contributions:
(1) an evaluation pipeline to systematically convert QA tasks into debate-based
assessments, and (2) a public benchmark that demonstrates our paradigm's
effectiveness on a subset of MMLU-Pro questions, complete with standardized
protocols and reference models. Empirical results validate the robustness of
the method and its effectiveness against data contamination--a Llama 3.1 model
fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%)
but performed worse in debates. Results also show that even weaker judges can
reliably differentiate stronger debaters, highlighting how debate-based
evaluation can scale to future, more capable systems while maintaining a
fraction of the cost of creating new benchmarks. Overall, our framework
underscores that "pretraining on the test set is no longer all you need,"
offering a sustainable path for measuring the genuine reasoning ability of
advanced language models.

</details>
