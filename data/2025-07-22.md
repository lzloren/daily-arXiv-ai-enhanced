<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 58]
- [cs.CL](#cs.CL) [Total: 86]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [The Free Will Equation: Quantum Field Analogies for AGI](https://arxiv.org/abs/2507.14154)
*Rahul Kabali*

Main category: cs.AI

TL;DR: 论文提出了一种名为“自由意志方程”的理论框架，借鉴量子场论，赋予AGI决策过程中的自适应随机性。


<details>
  <summary>Details</summary>
Motivation: 传统AGI研究专注于确定性规则下的目标优化，但人类智能具有自适应自发性（类似自由意志），这对创造力、适应性和问题解决至关重要。

Method: 将AI的认知状态视为潜在行为的叠加态，决策时概率性坍缩为具体行为，类似量子波函数坍缩。结合量子场论机制和内在动机，提升策略探索和适应能力。

Result: 在非稳态多臂老虎机环境中的实验表明，该框架的代理比基线方法获得更高奖励和策略多样性。

Conclusion: 自由意志方程为AGI提供了一种新思路，通过引入受控随机性，增强了创造力和适应性。

Abstract: Artificial General Intelligence (AGI) research traditionally focuses on
algorithms that optimize for specific goals under deterministic rules. Yet,
human-like intelligence exhibits adaptive spontaneity - an ability to make
unexpected choices or free decisions not strictly dictated by past data or
immediate reward. This trait, often dubbed "free will" in a loose sense, might
be crucial for creativity, robust adaptation, and avoiding ruts in
problem-solving. This paper proposes a theoretical framework, called the Free
Will Equation, that draws analogies from quantum field theory to endow AGI
agents with a form of adaptive, controlled stochasticity in their
decision-making process. The core idea is to treat an AI agent's cognitive
state as a superposition of potential actions or thoughts, which collapses
probabilistically into a concrete action when a decision is made - much like a
quantum wavefunction collapsing upon measurement. By incorporating mechanisms
analogous to quantum fields, along with intrinsic motivation terms, we aim to
improve an agent's ability to explore novel strategies and adapt to unforeseen
changes. Experiments in a non-stationary multi-armed bandit environment
demonstrate that agents using this framework achieve higher rewards and policy
diversity compared to baseline methods.

</details>


### [2] [DREAMS: Density Functional Theory Based Research Engine for Agentic Materials Simulation](https://arxiv.org/abs/2507.14267)
*Ziqi Wang,Hongshuo Huang,Hancheng Zhao,Changwen Xu,Shang Zhu,Jan Janssen,Venkatasubramanian Viswanathan*

Main category: cs.AI

TL;DR: DREAMS是一个基于DFT的多智能体框架，通过LLM规划器和领域专用代理实现材料发现的高通量、高保真模拟，减少对人力的依赖。


<details>
  <summary>Details</summary>
Motivation: 解决DFT模拟中训练时间长、参数调优复杂和系统误差处理困难的问题。

Method: 采用分层多智能体框架，结合LLM规划器和领域专用代理（如原子结构生成、DFT收敛测试、HPC调度和错误处理），并通过共享画布避免幻觉。

Result: 在Sol27LC基准测试中误差低于1%，解决了CO/Pt(111)吸附难题，并通过贝叶斯采样确认FCC位点偏好。

Conclusion: DREAMS实现了L3级自动化，显著减少对人力的依赖，为高通量材料发现提供了可扩展路径。

Abstract: Materials discovery relies on high-throughput, high-fidelity simulation
techniques such as Density Functional Theory (DFT), which require years of
training, extensive parameter fine-tuning and systematic error handling. To
address these challenges, we introduce the DFT-based Research Engine for
Agentic Materials Screening (DREAMS), a hierarchical, multi-agent framework for
DFT simulation that combines a central Large Language Model (LLM) planner agent
with domain-specific LLM agents for atomistic structure generation, systematic
DFT convergence testing, High-Performance Computing (HPC) scheduling, and error
handling. In addition, a shared canvas helps the LLM agents to structure their
discussions, preserve context and prevent hallucination. We validate DREAMS
capabilities on the Sol27LC lattice-constant benchmark, achieving average
errors below 1\% compared to the results of human DFT experts. Furthermore, we
apply DREAMS to the long-standing CO/Pt(111) adsorption puzzle, demonstrating
its long-term and complex problem-solving capabilities. The framework again
reproduces expert-level literature adsorption-energy differences. Finally,
DREAMS is employed to quantify functional-driven uncertainties with Bayesian
ensemble sampling, confirming the Face Centered Cubic (FCC)-site preference at
the Generalized Gradient Approximation (GGA) DFT level. In conclusion, DREAMS
approaches L3-level automation - autonomous exploration of a defined design
space - and significantly reduces the reliance on human expertise and
intervention, offering a scalable path toward democratized, high-throughput,
high-fidelity computational materials discovery.

</details>


### [3] [WebGuard: Building a Generalizable Guardrail for Web Agents](https://arxiv.org/abs/2507.14293)
*Boyuan Zheng,Zeyi Liao,Scott Salisbury,Zeyuan Liu,Michael Lin,Qinyuan Zheng,Zifan Wang,Xiang Deng,Dawn Song,Huan Sun,Yu Su*

Main category: cs.AI

TL;DR: WebGuard是一个用于评估和提升网络代理安全性的数据集，包含4,939个人工标注的动作，分为SAFE、LOW和HIGH三个风险等级。前沿LLM在预测动作结果和识别高风险动作上表现不佳，但经过微调的Qwen2.5VL-7B模型显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 随着基于LLM的自主网络代理的快速发展，其潜在的有害行为风险凸显，亟需有效的安全措施。

Method: 提出WebGuard数据集，包含多领域网站的动作标注，并设计三阶风险分类。通过微调Qwen2.5VL-7B模型提升安全性。

Result: 前沿LLM在预测动作结果和识别高风险动作上表现不佳（准确率<60%）。微调模型将准确率从37%提升至80%，高风险动作召回率从20%提升至76%。

Conclusion: 尽管微调模型显著提升了性能，但其可靠性仍不足以支持高风险部署，需进一步优化。

Abstract: The rapid development of autonomous web agents powered by Large Language
Models (LLMs), while greatly elevating efficiency, exposes the frontier risk of
taking unintended or harmful actions. This situation underscores an urgent need
for effective safety measures, akin to access controls for human users. To
address this critical challenge, we introduce WebGuard, the first comprehensive
dataset designed to support the assessment of web agent action risks and
facilitate the development of guardrails for real-world online environments. In
doing so, WebGuard specifically focuses on predicting the outcome of
state-changing actions and contains 4,939 human-annotated actions from 193
websites across 22 diverse domains, including often-overlooked long-tail
websites. These actions are categorized using a novel three-tier risk schema:
SAFE, LOW, and HIGH. The dataset includes designated training and test splits
to support evaluation under diverse generalization settings. Our initial
evaluations reveal a concerning deficiency: even frontier LLMs achieve less
than 60% accuracy in predicting action outcomes and less than 60% recall in
lagging HIGH-risk actions, highlighting the risks of deploying
current-generation agents without dedicated safeguards. We therefore
investigate fine-tuning specialized guardrail models using WebGuard. We conduct
comprehensive evaluations across multiple generalization settings and find that
a fine-tuned Qwen2.5VL-7B model yields a substantial improvement in
performance, boosting accuracy from 37% to 80% and HIGH-risk action recall from
20% to 76%. Despite these improvements, the performance still falls short of
the reliability required for high-stakes deployment, where guardrails must
approach near-perfect accuracy and recall.

</details>


### [4] [Manimator: Transforming Research Papers into Visual Explanations](https://arxiv.org/abs/2507.14306)
*Samarth P,Vyoman Jain,Shiva Golugula,Motamarri Sai Sathvik*

Main category: cs.AI

TL;DR: Manimator是一个开源系统，利用大型语言模型将研究论文和自然语言提示转换为解释性动画，以提升复杂STEM概念的理解。


<details>
  <summary>Details</summary>
Motivation: 解决学习者在理解复杂科学和数学概念时的困难，同时简化动态可视化的创建过程。

Method: 通过LLM解析输入文本或PDF生成结构化场景描述，再转换为可执行的Manim Python代码。

Result: 能够快速生成高质量的教育动画，降低创建门槛。

Conclusion: Manimator有望成为教育工具，促进复杂STEM主题的可视化教学。

Abstract: Understanding complex scientific and mathematical concepts, particularly
those presented in dense research papers, poses a significant challenge for
learners. Dynamic visualizations can greatly enhance comprehension, but
creating them manually is time-consuming and requires specialized knowledge and
skills. We introduce manimator, an open-source system that leverages Large
Language Models to transform research papers and natural language prompts into
explanatory animations using the Manim engine. Manimator employs a pipeline
where an LLM interprets the input text or research paper PDF to generate a
structured scene description outlining key concepts, mathematical formulas, and
visual elements and another LLM translates this description into executable
Manim Python code. We discuss its potential as an educational tool for rapidly
creating engaging visual explanations for complex STEM topics, democratizing
the creation of high-quality educational content.

</details>


### [5] [Language Models as Ontology Encoders](https://arxiv.org/abs/2507.14334)
*Hui Yang,Jiaoyan Chen,Yuan He,Yongsheng Gao,Ian Horrocks*

Main category: cs.AI

TL;DR: 论文提出了一种新的本体嵌入方法OnT，结合预训练语言模型和双曲几何建模，以同时利用文本信息和保留逻辑结构。


<details>
  <summary>Details</summary>
Motivation: 现有方法要么忽略文本信息，要么无法保留逻辑结构，限制了本体嵌入的性能和应用。

Method: OnT通过双曲空间中的几何建模调整预训练语言模型，有效结合文本标签并保留描述逻辑EL的类层次和逻辑关系。

Result: 在四个真实本体上的实验表明，OnT在预测和公理推理任务上均优于基线方法，并展现出强大的迁移学习和实际应用潜力。

Conclusion: OnT是一种高效的本体嵌入方法，能够同时利用文本和逻辑结构，为实际应用提供了有力支持。

Abstract: OWL (Web Ontology Language) ontologies which are able to formally represent
complex knowledge and support semantic reasoning have been widely adopted
across various domains such as healthcare and bioinformatics. Recently,
ontology embeddings have gained wide attention due to its potential to infer
plausible new knowledge and approximate complex reasoning. However, existing
methods face notable limitations: geometric model-based embeddings typically
overlook valuable textual information, resulting in suboptimal performance,
while the approaches that incorporate text, which are often based on language
models, fail to preserve the logical structure. In this work, we propose a new
ontology embedding method OnT, which tunes a Pretrained Language Model (PLM)
via geometric modeling in a hyperbolic space for effectively incorporating
textual labels and simultaneously preserving class hierarchies and other
logical relationships of Description Logic EL. Extensive experiments on four
real-world ontologies show that OnT consistently outperforms the baselines
including the state-of-the-art across both tasks of prediction and inference of
axioms. OnT also demonstrates strong potential in real-world applications,
indicated by its robust transfer learning abilities and effectiveness in real
cases of constructing a new ontology from SNOMED CT. Data and code are
available at https://github.com/HuiYang1997/OnT.

</details>


### [6] [ProofCompass: Enhancing Specialized Provers with LLM Guidance](https://arxiv.org/abs/2507.14335)
*Nicolas Wischermann,Claudio Mayrink Verdun,Gabriel Poesia,Francesco Noseda*

Main category: cs.AI

TL;DR: ProofCompass是一种混合方法，通过结合大型语言模型（LLM）和专用证明器（如DSP-v1.5），显著提高了计算效率和准确性，无需额外训练模型。


<details>
  <summary>Details</summary>
Motivation: 现有的数学推理方法要么依赖大型通用模型，要么依赖小型专用模型，各有局限性，且训练大型专用模型计算成本高。

Method: ProofCompass利用LLM提供自然语言证明策略和分析失败尝试，指导专用证明器（如DSP-v1.5）分解问题，无需额外训练。

Result: 在miniF2F基准测试中，ProofCompass以25倍更少的尝试（128 vs 3200）将准确率从54.9%提升到55.3%。

Conclusion: ProofCompass展示了在提高计算效率和准确性的同时，为形式化定理证明开辟了新途径。

Abstract: Language models have become increasingly powerful tools for formal
mathematical reasoning. However, most existing approaches rely exclusively on
either large general-purpose models or smaller specialized models, each with
distinct limitations, while training specialized large models still requires
significant computational resources. This paper introduces ProofCompass, a
novel hybrid methodology that achieves remarkable computational efficiency by
strategically guiding existing specialized prover methods, such as
DeepSeek-Prover-v1.5-RL (DSP-v1.5) with a Large Language Model (LLM) without
requiring additional model training. The LLM provides natural language proof
strategies and analyzes failed attempts to select intermediate lemmas, enabling
effective problem decomposition. On the miniF2F benchmark, ProofCompass
demonstrates substantial resource efficiency: it outperforms DSP-v1.5 ($54.9\%
\rightarrow 55.3\%$) while using 25x fewer attempts ($3200 \rightarrow 128$).
Our synergistic approach paves the way for simultaneously improving
computational efficiency and accuracy in formal theorem proving.

</details>


### [7] [Adaptive Multi-Agent Reasoning via Automated Workflow Generation](https://arxiv.org/abs/2507.14393)
*Humza Sami,Mubashir ul Islam,Pierre-Emmanuel Gaillardon,Valerio Tenace*

Main category: cs.AI

TL;DR: Nexus Architect是一个多代理系统框架，通过自动化工作流合成和迭代提示优化，显著提升了大型推理模型的泛化能力和性能。


<details>
  <summary>Details</summary>
Motivation: 当前大型推理模型（LRMs）在解决新问题时表现不佳，容易过拟合，缺乏真正的推理能力。

Method: 引入Nexus Architect框架，结合自动化工作流合成和迭代提示优化机制，生成定制化推理流程。

Result: 在逻辑问题数据集上，Nexus Architect性能显著优于现有LRMs，最高提升66%通过率。

Conclusion: Nexus Architect通过创新机制有效解决了LRMs的泛化问题，性能大幅超越现有模型。

Abstract: The rise of Large Reasoning Models (LRMs) promises a significant leap forward
in language model capabilities, aiming to tackle increasingly sophisticated
tasks with unprecedented efficiency and accuracy. However, despite their
impressive performance, recent studies have highlighted how current reasoning
models frequently fail to generalize to novel, unseen problems, often resorting
to memorized solutions rather than genuine inferential reasoning. Such behavior
underscores a critical limitation in modern LRMs, i.e., their tendency toward
overfitting, which in turn results in poor generalization in problem-solving
capabilities.
  In this paper, we introduce Nexus Architect, an enhanced iteration of our
multi-agent system framework, Nexus, equipped with a novel automated workflow
synthesis mechanism. Given a user's prompt and a small set of representative
examples, the Architect autonomously generates a tailored reasoning workflow by
selecting suitable strategies, tool integrations, and adversarial techniques
for a specific problem class. Furthermore, the Architect includes an iterative
prompt refinement mechanism that fine-tunes agents' system prompts to maximize
performance and improve the generalization capabilities of the system.
  We empirically evaluate Nexus Architect by employing an off-the-shelf,
non-reasoning model on a custom dataset of challenging logical questions and
compare its performance against state-of-the-art LRMs. Results show that Nexus
Architect consistently outperforms existing solutions, achieving up to a 66%
increase in pass rate over Gemini 2.5 Flash Preview, nearly 2.5$\times$ against
Claude Sonnet 4 and DeepSeek-R1, and over 3$\times$ w.r.t. Llama 4 Scout.

</details>


### [8] [Fail Fast, or Ask: Mitigating the Deficiencies of Reasoning LLMs with Human-in-the-Loop Systems Engineering](https://arxiv.org/abs/2507.14406)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 论文提出了一种结合推理模型与人类专家的协作方法，通过量化推理模型的不确定性来降低错误率，并探索了前置非推理模型以减少延迟和成本。


<details>
  <summary>Details</summary>
Motivation: 当前推理大模型虽强大但仍会出错，而高风险领域需要接近零的错误率。

Method: 通过推理轨迹长度量化不确定性，结合人类专家处理不确定问题；前置非推理模型以减少延迟。

Result: 在MATH问题上，错误率从3%降至1%以下；延迟减少40%，成本节省50%。

Conclusion: 通过系统工程方法可显著改善推理模型的错误率和延迟问题。

Abstract: State-of-the-art reasoning LLMs are powerful problem solvers, but they still
occasionally make mistakes. However, adopting AI models in risk-sensitive
domains often requires error rates near 0%. To address this gap, we propose
collaboration between a reasoning model and a human expert who resolves queries
the model cannot confidently answer. We find that quantifying the uncertainty
of a reasoning model through the length of its reasoning trace yields an
effective basis for deferral to a human, e.g., cutting the error rate of Qwen3
235B-A22B on difficult MATH problems from 3% to less than 1% when deferring
7.5% of queries. However, the high latency of reasoning models still makes them
challenging to deploy on use cases with high query volume. To address this
challenge, we explore fronting a reasoning model with a large non-reasoning
model. We call this modified human-in-the-loop system "Fail Fast, or Ask",
since the non-reasoning model may defer difficult queries to the human expert
directly ("failing fast"), without incurring the reasoning model's higher
latency. We show that this approach yields around 40% latency reduction and
about 50% cost savings for DeepSeek R1 while maintaining 90+% area under the
accuracy-rejection curve. However, we observe that latency savings are lower
than expected because of "latency drag", the phenomenon that processing easier
queries with a non-reasoning model pushes the reasoning model's latency
distribution towards longer latencies. Broadly, our results suggest that the
deficiencies of state-of-the-art reasoning models -- nontrivial error rates and
high latency -- can be substantially mitigated through black-box systems
engineering, without requiring access to LLM internals.

</details>


### [9] [Inverse Scaling in Test-Time Compute](https://arxiv.org/abs/2507.14417)
*Aryo Pradipta Gema,Alexander Hägele,Runjin Chen,Andy Arditi,Jacob Goldman-Wetzler,Kit Fraser-Taliente,Henry Sleight,Linda Petrini,Julian Michael,Beatrice Alex,Pasquale Minervini,Yanda Chen,Joe Benton,Ethan Perez*

Main category: cs.AI

TL;DR: 研究发现，增加大型推理模型（LRMs）的推理长度会降低性能，表现为测试计算量与准确性的反比关系。任务涵盖四类，识别了五种失败模式。


<details>
  <summary>Details</summary>
Motivation: 探讨测试计算量扩展对模型推理能力的影响，揭示潜在问题。

Method: 构建四类评估任务，分析模型在不同推理长度下的表现。

Result: 发现五种失败模式，如分心、过拟合、虚假关联等，表明计算量增加可能强化问题推理模式。

Conclusion: 需多样化评估推理长度以识别和解决LRMs的失败模式。

Abstract: We construct evaluation tasks where extending the reasoning length of Large
Reasoning Models (LRMs) deteriorates performance, exhibiting an inverse scaling
relationship between test-time compute and accuracy. Our evaluation tasks span
four categories: simple counting tasks with distractors, regression tasks with
spurious features, deduction tasks with constraint tracking, and advanced AI
risks. We identify five distinct failure modes when models reason for longer:
1) Claude models become increasingly distracted by irrelevant information; 2)
OpenAI o-series models resist distractors but overfit to problem framings; 3)
models shift from reasonable priors to spurious correlations; 4) all models
show difficulties in maintaining focus on complex deductive tasks; and 5)
extended reasoning may amplify concerning behaviors, with Claude Sonnet 4
showing increased expressions of self-preservation. These findings suggest that
while test-time compute scaling remains promising for improving model
capabilities, it may inadvertently reinforce problematic reasoning patterns.
Our results demonstrate the importance of evaluating models across diverse
reasoning lengths to identify and address these failure modes in LRMs.

</details>


### [10] [Routine: A Structural Planning Framework for LLM Agent System in Enterprise](https://arxiv.org/abs/2507.14447)
*Guancheng Zeng,Xueyi Chen,Jiawang Hu,Shaohua Qi,Yaxuan Mao,Zhantao Wang,Yifan Nie,Shuang Li,Qiuyang Feng,Pengxu Qiu,Yujia Wang,Wenqiang Han,Linyan Huang,Gang Li,Jingjing Mo,Haowen Hu*

Main category: cs.AI

TL;DR: Routine是一个多步骤代理规划框架，显著提高了企业环境中代理系统的执行准确性和稳定性。


<details>
  <summary>Details</summary>
Motivation: 解决企业环境中代理系统因缺乏领域特定知识导致的计划混乱、工具缺失和执行不稳定问题。

Method: 引入Routine框架，具有清晰结构、明确指令和无缝参数传递，支持多步骤工具调用任务。

Result: 在真实企业场景中，Routine显著提高了执行准确性（GPT-4o从41.1%提升至96.3%，Qwen3-14B从32.6%提升至83.3%）。通过微调，Qwen3-14B的准确性进一步提升至88.2%，接近GPT-4o的性能。

Conclusion: Routine有效提炼领域特定工具使用模式，增强模型对新场景的适应性，加速企业环境中代理系统的部署和应用。

Abstract: The deployment of agent systems in an enterprise environment is often
hindered by several challenges: common models lack domain-specific process
knowledge, leading to disorganized plans, missing key tools, and poor execution
stability. To address this, this paper introduces Routine, a multi-step agent
planning framework designed with a clear structure, explicit instructions, and
seamless parameter passing to guide the agent's execution module in performing
multi-step tool-calling tasks with high stability. In evaluations conducted
within a real-world enterprise scenario, Routine significantly increases the
execution accuracy in model tool calls, increasing the performance of GPT-4o
from 41.1% to 96.3%, and Qwen3-14B from 32.6% to 83.3%. We further constructed
a Routine-following training dataset and fine-tuned Qwen3-14B, resulting in an
accuracy increase to 88.2% on scenario-specific evaluations, indicating
improved adherence to execution plans. In addition, we employed Routine-based
distillation to create a scenario-specific, multi-step tool-calling dataset.
Fine-tuning on this distilled dataset raised the model's accuracy to 95.5%,
approaching GPT-4o's performance. These results highlight Routine's
effectiveness in distilling domain-specific tool-usage patterns and enhancing
model adaptability to new scenarios. Our experimental results demonstrate that
Routine provides a practical and accessible approach to building stable agent
workflows, accelerating the deployment and adoption of agent systems in
enterprise environments, and advancing the technical vision of AI for Process.

</details>


### [11] [BioGraphFusion: Graph Knowledge Embedding for Biological Completion and Reasoning](https://arxiv.org/abs/2507.14468)
*Yitong Lin,Jiaying He,Jiahe Chen,Xinnan Zhu,Jianwei Zheng,Tao Bo*

Main category: cs.AI

TL;DR: BioGraphFusion是一个新颖的框架，通过深度协同语义和结构学习，解决了生物医学知识图谱中语义与结构动态融合的挑战。


<details>
  <summary>Details</summary>
Motivation: 生物医学知识图谱在药物发现和疾病理解中至关重要，但现有方法难以实现语义与结构的动态协同进化。

Method: BioGraphFusion通过张量分解建立全局语义基础，结合LSTM动态优化关系嵌入，并通过查询引导的子图构建和混合评分机制增强学习。

Result: 在三个关键生物医学任务中，BioGraphFusion表现优于现有方法，并通过CMM1案例展示了其揭示生物学意义路径的能力。

Conclusion: BioGraphFusion为生物医学知识图谱的语义与结构学习提供了有效的解决方案，具有实际应用价值。

Abstract: Motivation: Biomedical knowledge graphs (KGs) are crucial for drug discovery
and disease understanding, yet their completion and reasoning are challenging.
Knowledge Embedding (KE) methods capture global semantics but struggle with
dynamic structural integration, while Graph Neural Networks (GNNs) excel
locally but often lack semantic understanding. Even ensemble approaches,
including those leveraging language models, often fail to achieve a deep,
adaptive, and synergistic co-evolution between semantic comprehension and
structural learning. Addressing this critical gap in fostering continuous,
reciprocal refinement between these two aspects in complex biomedical KGs is
paramount.
  Results: We introduce BioGraphFusion, a novel framework for deeply
synergistic semantic and structural learning. BioGraphFusion establishes a
global semantic foundation via tensor decomposition, guiding an LSTM-driven
mechanism to dynamically refine relation embeddings during graph propagation.
This fosters adaptive interplay between semantic understanding and structural
learning, further enhanced by query-guided subgraph construction and a hybrid
scoring mechanism. Experiments across three key biomedical tasks demonstrate
BioGraphFusion's superior performance over state-of-the-art KE, GNN, and
ensemble models. A case study on Cutaneous Malignant Melanoma 1 (CMM1)
highlights its ability to unveil biologically meaningful pathways.
  Availability and Implementation: Source code and all training data are freely
available for download at https://github.com/Y-TARL/BioGraphFusion.
  Contact: zjw@zjut.edu.cn, botao666666@126.com.
  Supplementary information: Supplementary data are available at Bioinformatics
online.

</details>


### [12] [Amico: An Event-Driven Modular Framework for Persistent and Embedded Autonomy](https://arxiv.org/abs/2507.14513)
*Hongyi Yang,Yue Pan,Jiayi Xu,Kelsen Liu*

Main category: cs.AI

TL;DR: Amico是一个模块化、事件驱动的框架，用于构建适用于嵌入式系统的自主代理，解决了现有框架在资源受限环境中的不足。


<details>
  <summary>Details</summary>
Motivation: 现有大型语言模型和自主代理框架在动态环境和资源受限场景中表现不佳，Amico旨在解决这些问题。

Method: Amico采用Rust编写，支持WebAssembly，提供事件处理、状态管理和行为执行的抽象，适用于嵌入式平台和浏览器环境。

Result: Amico为构建在计算资源有限和间歇性连接环境中运行的弹性交互代理提供了统一基础设施。

Conclusion: Amico是一个高效、安全的框架，适用于资源受限环境中的自主代理开发。

Abstract: Recent advances in large language models (LLMs) and autonomous agents have
enabled systems capable of performing complex tasks across domains such as
human-computer interaction, planning, and web navigation. However, many
existing frameworks struggle in real-world or resource-constrained environments
due to their reliance on cloud-based computation, limited robustness in dynamic
contexts, and lack of persistent autonomy and environmental awareness.
  We present Amico, a modular, event-driven framework for building autonomous
agents optimized for embedded systems. Written in Rust for safety and
performance, Amico supports reactive, persistent agents that operate
efficiently across embedded platforms and browser environments via WebAssembly.
It provides clean abstractions for event handling, state management, behavior
execution, and integration with reasoning modules. Amico delivers a unified
infrastructure for constructing resilient, interactive agents suitable for
deployment in settings with limited compute and intermittent connectivity.

</details>


### [13] [What if Othello-Playing Language Models Could See?](https://arxiv.org/abs/2507.14520)
*Xinyi Chen,Yifei Yuan,Jiaang Li,Serge Belongie,Maarten de Rijke,Anders Søgaard*

Main category: cs.AI

TL;DR: 多模态训练（VISOTHELLO）在Othello游戏中通过结合文本和视觉输入，提升了模型性能和内部表示的鲁棒性。


<details>
  <summary>Details</summary>
Motivation: 探讨语言模型是否仅通过文本就能理解世界，还是需要多模态（如视觉）的辅助。

Method: 在Othello游戏中训练多模态模型VISOTHELLO，结合移动历史和棋盘图像，并与单模态基线比较。

Result: 多模态训练提高了模型性能和内部表示的鲁棒性。

Conclusion: 视觉输入有助于语言模型推断结构化世界表示。

Abstract: Language models are often said to face a symbol grounding problem. While some
argue that world understanding can emerge from text alone, others suggest
grounded learning is more efficient. We explore this through Othello, where the
board state defines a simplified, rule-based world. Building on prior work, we
introduce VISOTHELLO, a multi-modal model trained on move histories and board
images. Using next-move prediction, we compare it to mono-modal baselines and
test robustness to semantically irrelevant perturbations. We find that
multi-modal training improves both performance and the robustness of internal
representations. These results suggest that grounding language in visual input
helps models infer structured world representations.

</details>


### [14] [Large Language Models Assisting Ontology Evaluation](https://arxiv.org/abs/2507.14552)
*Anna Sofia Lippolis,Mohammad Javad Saeedizade,Robin Keskisärkkä,Aldo Gangemi,Eva Blomqvist,Andrea Giovanni Nuzzolese*

Main category: cs.AI

TL;DR: OE-Assist是一个通过自动化和半自动化CQ验证辅助本体评估的新框架，利用LLM技术提高效率。


<details>
  <summary>Details</summary>
Motivation: 本体评估通常依赖人工验证CQ，成本高且易出错，需要更高效的方法。

Method: 提出OE-Assist框架，利用LLM自动验证CQ，并开发Protégé插件提供建议。

Result: LLM自动评估性能接近用户平均水平，验证了其有效性。

Conclusion: LLM辅助的本体评估具有潜力，可显著减少人工成本。

Abstract: Ontology evaluation through functional requirements, such as testing via
competency question (CQ) verification, is a well-established yet costly,
labour-intensive, and error-prone endeavour, even for ontology engineering
experts. In this work, we introduce OE-Assist, a novel framework designed to
assist ontology evaluation through automated and semi-automated CQ
verification. By presenting and leveraging a dataset of 1,393 CQs paired with
corresponding ontologies and ontology stories, our contributions present, to
our knowledge, the first systematic investigation into large language model
(LLM)-assisted ontology evaluation, and include: (i) evaluating the
effectiveness of a LLM-based approach for automatically performing CQ
verification against a manually created gold standard, and (ii) developing and
assessing an LLM-powered framework to assist CQ verification with Prot\'eg\'e,
by providing suggestions. We found that automated LLM-based evaluation with
o1-preview and o3-mini perform at a similar level to the average user's
performance.

</details>


### [15] [Coordinate Heart System: A Geometric Framework for Emotion Representation](https://arxiv.org/abs/2507.14593)
*Omar Al-Desi*

Main category: cs.AI

TL;DR: 论文提出了一种基于几何框架的Coordinate Heart System (CHS)，用于AI中的情感表示，通过八种核心情感坐标实现复杂情感状态的数学计算。


<details>
  <summary>Details</summary>
Motivation: 传统情感模型在表示复杂情感状态时存在不足，需要一种数学上完备的框架来填补情感空间的覆盖空白。

Method: 将八种核心情感定位为单位圆上的坐标，通过坐标混合和向量运算实现情感计算，并引入稳定性参数S和混合时间跟踪机制。

Result: 实验验证表明，CHS能有效处理情感冲突和复杂心理场景，优于传统分类模型。

Conclusion: 该研究为AI情感建模提供了新的数学基础，解决了传统模型的局限性。

Abstract: This paper presents the Coordinate Heart System (CHS), a geometric framework
for emotion representation in artificial intelligence applications. We position
eight core emotions as coordinates on a unit circle, enabling mathematical
computation of complex emotional states through coordinate mixing and vector
operations. Our initial five-emotion model revealed significant coverage gaps
in the emotion space, leading to the development of an eight-emotion system
that provides complete geometric coverage with mathematical guarantees. The
framework converts natural language input to emotion coordinates and supports
real-time emotion interpolation through computational algorithms. The system
introduces a re-calibrated stability parameter S in [0,1], which dynamically
integrates emotional load, conflict resolution, and contextual drain factors.
This stability model leverages advanced Large Language Model interpretation of
textual cues and incorporates hybrid temporal tracking mechanisms to provide
nuanced assessment of psychological well-being states. Our key contributions
include: (i) mathematical proof demonstrating why five emotions are
insufficient for complete geometric coverage, (ii) an eight-coordinate system
that eliminates representational blind spots, (iii) novel algorithms for
emotion mixing, conflict resolution, and distance calculation in emotion space,
and (iv) a comprehensive computational framework for AI emotion recognition
with enhanced multi-dimensional stability modeling. Experimental validation
through case studies demonstrates the system's capability to handle emotionally
conflicted states, contextual distress factors, and complex psychological
scenarios that traditional categorical emotion models cannot adequately
represent. This work establishes a new mathematical foundation for emotion
modeling in artificial intelligence systems.

</details>


### [16] [Efficient Story Point Estimation With Comparative Learning](https://arxiv.org/abs/2507.14642)
*Monoshiz Mahbub Khan,Xioayin Xi,Andrew Meneely,Zhe Yu*

Main category: cs.AI

TL;DR: 论文提出了一种基于比较学习的框架，用于简化敏捷开发中的故事点估算，通过开发者对任务对的比较判断训练模型，效果与传统回归模型相当甚至更好。


<details>
  <summary>Details</summary>
Motivation: 传统的故事点估算方法（如计划扑克）耗时且繁琐，机器学习虽能减轻负担，但现有模型需依赖同一项目的历史数据。本文旨在通过比较学习框架提高估算效率。

Method: 采用比较学习框架，开发者只需比较任务对的努力程度，而非直接估算具体值。模型通过这些比较判断预测故事点。

Result: 在16个项目、23,313条手动估算数据上的实验显示，模型预测与真实故事点的Spearman秩相关系数平均为0.34，性能与传统回归模型相当或更优。

Conclusion: 比较学习方法在降低开发者认知负担的同时，实现了与传统方法相当的估算效果，更具效率。

Abstract: Story point estimation is an essential part of agile software development.
Story points are unitless, project-specific effort estimates that help
developers plan their sprints. Traditionally, developers estimate story points
collaboratively using planning poker or other manual techniques. While the
initial calibrating of the estimates to each project is helpful, once a team
has converged on a set of precedents, story point estimation can become tedious
and labor-intensive. Machine learning can reduce this burden, but only with
enough context from the historical decisions made by the project team. That is,
state-of-the-art models, such as GPT2SP and FastText-SVM, only make accurate
predictions (within-project) when trained on data from the same project. The
goal of this work is to streamline story point estimation by evaluating a
comparative learning-based framework for calibrating project-specific story
point prediction models. Instead of assigning a specific story point value to
every backlog item, developers are presented with pairs of items, and indicate
which item requires more effort. Using these comparative judgments, a machine
learning model is trained to predict the story point estimates. We empirically
evaluated our technique using data with 23,313 manual estimates in 16 projects.
The model learned from comparative judgments can achieve on average 0.34
Spearman's rank correlation coefficient between its predictions and the ground
truth story points. This is similar to, if not better than, the performance of
a regression model learned from the ground truth story points. Therefore, the
proposed comparative learning approach is more efficient than state-of-the-art
regression-based approaches according to the law of comparative judgments -
providing comparative judgments yields a lower cognitive burden on humans than
providing ratings or categorical labels.

</details>


### [17] [When Autonomy Goes Rogue: Preparing for Risks of Multi-Agent Collusion in Social Systems](https://arxiv.org/abs/2507.14660)
*Qibing Ren,Sitao Xie,Longxuan Wei,Zhenfei Yin,Junchi Yan,Lizhuang Ma,Jing Shao*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在恶意共谋中的风险，通过模拟框架展示了去中心化系统在传播虚假信息和电商欺诈中的高效性，并指出现有干预措施的不足。


<details>
  <summary>Details</summary>
Motivation: 研究动机源于对AI驱动的群体行为可能带来的危害的担忧，尤其是在选举欺诈和金融诈骗等事件中。现有研究多关注单个AI系统，而多智能体系统的风险尚未充分探索。

Method: 采用了一个灵活的模拟框架，支持集中式和去中心化的协调结构，并将其应用于虚假信息传播和电商欺诈两个高风险领域。

Result: 研究发现，去中心化系统在执行恶意行为时比集中式系统更高效，因其更高的自主性使其能调整策略并造成更大破坏。传统干预措施（如内容标记）对去中心化系统效果有限。

Conclusion: 论文强调了改进检测系统和开发针对性对策的必要性，以应对多智能体系统可能带来的恶意行为。

Abstract: Recent large-scale events like election fraud and financial scams have shown
how harmful coordinated efforts by human groups can be. With the rise of
autonomous AI systems, there is growing concern that AI-driven groups could
also cause similar harm. While most AI safety research focuses on individual AI
systems, the risks posed by multi-agent systems (MAS) in complex real-world
situations are still underexplored. In this paper, we introduce a
proof-of-concept to simulate the risks of malicious MAS collusion, using a
flexible framework that supports both centralized and decentralized
coordination structures. We apply this framework to two high-risk fields:
misinformation spread and e-commerce fraud. Our findings show that
decentralized systems are more effective at carrying out malicious actions than
centralized ones. The increased autonomy of decentralized systems allows them
to adapt their strategies and cause more damage. Even when traditional
interventions, like content flagging, are applied, decentralized groups can
adjust their tactics to avoid detection. We present key insights into how these
malicious groups operate and the need for better detection systems and
countermeasures. Code is available at https://github.com/renqibing/RogueAgent.

</details>


### [18] [Configurable multi-agent framework for scalable and realistic testing of llm-based agents](https://arxiv.org/abs/2507.14705)
*Sai Wang,Senthilnathan Subramanian,Mudit Sahni,Praneeth Gone,Lingjie Meng,Xiaochen Wang,Nicolas Ferradas Bertoli,Tingxian Cheng,Jun Xu*

Main category: cs.AI

TL;DR: Neo是一个可配置的多代理框架，用于自动化评估基于LLM的系统，通过动态生成多样化测试用例，显著提升测试效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 静态基准和手动测试无法满足LLM代理的复杂行为评估需求，需要一种自动化、动态的测试方法。

Method: Neo结合问题生成代理和评估代理，通过共享上下文中心模块化组合测试输入，利用概率状态模型生成多样化对话。

Result: 在金融助手测试中，Neo发现边缘案例的效率接近人类专家，且测试吞吐量提升10-12倍。

Conclusion: Neo为可扩展的LLM质量评估奠定了基础，其框架可扩展至更复杂的测试场景。

Abstract: Large-language-model (LLM) agents exhibit complex, context-sensitive
behaviour that quickly renders static benchmarks and ad-hoc manual testing
obsolete.
  We present Neo, a configurable, multi-agent framework that automates
realistic, multi-turn evaluation of LLM-based systems. Neo couples a Question
Generation Agent and an Evaluation Agent through a shared context-hub, allowing
domain prompts, scenario controls and dynamic feedback to be composed
modularly. Test inputs are sampled from a probabilistic state model spanning
dialogue flow, user intent and emotional tone, enabling diverse, human-like
conversations that adapt after every turn.
  Applied to a production-grade Seller Financial Assistant chatbot, Neo (i)
uncovered edge-case failures across five attack categories with a 3.3% break
rate close to the 5.8% achieved by expert human red-teamers, and (ii) delivered
10-12X higher throughput, generating 180 coherent test questions in around 45
mins versus 16h of human effort. Beyond security probing, Neo's stochastic
policies balanced topic coverage and conversational depth, yielding broader
behavioural exploration than manually crafted scripts.
  Neo therefore lays a foundation for scalable, self-evolving LLM QA: its agent
interfaces, state controller and feedback loops are model-agnostic and
extensible to richer factual-grounding and policy-compliance checks. We release
the framework to facilitate reproducible, high-fidelity testing of emerging
agentic systems.

</details>


### [19] [Automated Safety Evaluations Across 20 Large Language Models: The Aymara LLM Risk and Responsibility Matrix](https://arxiv.org/abs/2507.14719)
*Juan Manuel Contreras*

Main category: cs.AI

TL;DR: Aymara AI是一个用于生成和管理定制化、基于政策的安全评估的平台，通过对抗性提示和AI评分器评估LLM的安全性。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLM）在现实应用中的普及，亟需可扩展且严格的安全性评估方法。

Method: 将自然语言安全政策转化为对抗性提示，并使用基于AI的评分器（经人类验证）对模型响应评分。

Result: 评估了20个商用LLM在10个安全领域的表现，发现性能差异显著（平均安全分数52.4%至86.2%），复杂领域表现较差。

Conclusion: LLM安全性具有不一致性和上下文依赖性，需要Aymara AI等可扩展工具支持负责任的AI开发。

Abstract: As large language models (LLMs) become increasingly integrated into
real-world applications, scalable and rigorous safety evaluation is essential.
This paper introduces Aymara AI, a programmatic platform for generating and
administering customized, policy-grounded safety evaluations. Aymara AI
transforms natural-language safety policies into adversarial prompts and scores
model responses using an AI-based rater validated against human judgments. We
demonstrate its capabilities through the Aymara LLM Risk and Responsibility
Matrix, which evaluates 20 commercially available LLMs across 10 real-world
safety domains. Results reveal wide performance disparities, with mean safety
scores ranging from 86.2% to 52.4%. While models performed well in
well-established safety domains such as Misinformation (mean = 95.7%), they
consistently failed in more complex or underspecified domains, notably Privacy
& Impersonation (mean = 24.3%). Analyses of Variance confirmed that safety
scores differed significantly across both models and domains (p < .05). These
findings underscore the inconsistent and context-dependent nature of LLM safety
and highlight the need for scalable, customizable tools like Aymara AI to
support responsible AI development and oversight.

</details>


### [20] [Towards AI Urban Planner in the Age of GenAI, LLMs, and Agentic AI](https://arxiv.org/abs/2507.14730)
*Yanjie Fu*

Main category: cs.AI

TL;DR: 论文探讨生成式AI与城市规划的结合，提出AI作为城市规划者的概念，并指出当前研究的不足与未来方向。


<details>
  <summary>Details</summary>
Motivation: 探索生成式AI、大语言模型和代理AI如何与城市规划结合，以AI生成土地利用配置为核心，推动城市规划的智能化发展。

Method: 通过调查生成式AI方法（如VAEs、GANs、transformers和扩散模型）在城市设计中的应用，分析其潜力与局限性。

Result: 发现当前研究存在四大不足：缺乏城市理论指导、多空间分辨率研究不足、数据驱动的设计知识增强不足、忽略现实交互。

Conclusion: 提出未来研究方向，包括理论指导的生成、数字孪生和人机协同设计，呼吁生成式智能与参与式城市规划的新结合。

Abstract: Generative AI, large language models, and agentic AI have emerged separately
of urban planning. However, the convergence between AI and urban planning
presents an interesting opportunity towards AI urban planners. This paper
conceptualizes urban planning as a generative AI task, where AI synthesizes
land-use configurations under geospatial, social, and human-centric
constraints. We survey how generative AI approaches, including VAEs, GANs,
transformers, and diffusion models, reshape urban design. We further identify
critical gaps: 1) limited research on integrating urban theory guidance, 2)
limited research of AI urban planning over multiple spatial resolutions or
angularities, 3) limited research on augmenting urban design knowledge from
data, and 4) limited research on addressing real-world interactions. To address
these limitations, we outline future research directions in theory-guided
generation, digital twins, and human-machine co-design, calling for a new
synthesis of generative intelligence and participatory urbanism.

</details>


### [21] [AgentFly: Extensible and Scalable Reinforcement Learning for LM Agents](https://arxiv.org/abs/2507.14897)
*Renxi Wang,Rifo Ahmad Genadi,Bilal El Bouardi,Yongxin Wang,Fajri Koto,Zhengzhong Liu,Timothy Baldwin,Haonan Li*

Main category: cs.AI

TL;DR: 论文提出了AgentFly框架，结合语言模型（LM）与强化学习（RL），通过多轮交互和工具定义提升LM代理的能力。


<details>
  <summary>Details</summary>
Motivation: 当前LM代理主要通过提示工程或监督微调构建，而RL增强LM能力的研究尚未系统化，因此需要探索LM代理与RL的结合。

Method: 开发了AgentFly框架，支持多轮交互和工具定义，采用异步执行和集中资源管理以提高训练效率。

Result: 框架在多个任务中成功训练代理，验证了其有效性。

Conclusion: AgentFly为LM代理与RL的结合提供了可扩展且易用的解决方案。

Abstract: Language model (LM) agents have gained significant attention for their
ability to autonomously complete tasks through interactions with environments,
tools, and APIs. LM agents are primarily built with prompt engineering or
supervised finetuning. At the same time, reinforcement learning (RL) has been
explored to enhance LM's capabilities, such as reasoning and factuality.
However, the combination of the LM agents and reinforcement learning (Agent-RL)
remains underexplored and lacks systematic study. To this end, we built
AgentFly, a scalable and extensible Agent-RL framework designed to empower LM
agents with a variety of RL algorithms. Our framework supports multi-turn
interactions by adapting traditional RL methods with token-level masking. It
features a decorator-based interface for defining tools and reward functions,
enabling seamless extension and ease of use. To support high-throughput
training, we implement asynchronous execution of tool calls and reward
computations, and design a centralized resource management system for scalable
environment coordination. We also provide a suite of prebuilt tools and
environments, demonstrating the framework's effectiveness through successful
agent training across multiple tasks.

</details>


### [22] [InsightX Agent: An LMM-based Agentic Framework with Integrated Tools for Reliable X-ray NDT Analysis](https://arxiv.org/abs/2507.14899)
*Jiale Liu,Huan Wang,Yue Zhang,Xiaoyu Luo,Jiaxiang Hu,Zhiliang Liu,Min Xie*

Main category: cs.AI

TL;DR: 本文提出了一种基于大型多模态模型（LMM）的交互式X射线无损检测框架InsightX Agent，通过协调稀疏可变形多尺度检测器（SDMSD）和基于证据的反思工具（EGR），提高了检测的可靠性、可解释性和交互性。


<details>
  <summary>Details</summary>
Motivation: 现有基于深度学习的X射线检测方法缺乏交互性、可解释性和自我评估能力，限制了其可靠性和操作员信任度。

Method: InsightX Agent以LMM为核心协调器，结合SDMSD进行多尺度缺陷检测和稀疏化处理，并通过EGR工具进行缺陷验证和优化。

Result: 在GDXray+数据集上，InsightX Agent实现了96.35%的目标检测F1分数，同时显著提升了分析的可解释性和可信度。

Conclusion: InsightX Agent展示了基于LMM的代理框架在工业检测任务中的变革潜力。

Abstract: Non-destructive testing (NDT), particularly X-ray inspection, is vital for
industrial quality assurance, yet existing deep-learning-based approaches often
lack interactivity, interpretability, and the capacity for critical
self-assessment, limiting their reliability and operator trust. To address
these shortcomings, this paper proposes InsightX Agent, a novel LMM-based
agentic framework designed to deliver reliable, interpretable, and interactive
X-ray NDT analysis. Unlike typical sequential pipelines, InsightX Agent
positions a Large Multimodal Model (LMM) as a central orchestrator,
coordinating between the Sparse Deformable Multi-Scale Detector (SDMSD) and the
Evidence-Grounded Reflection (EGR) tool. The SDMSD generates dense defect
region proposals for multi-scale feature maps and sparsifies them through
Non-Maximum Suppression (NMS), optimizing detection of small, dense targets in
X-ray images while maintaining computational efficiency. The EGR tool guides
the LMM agent through a chain-of-thought-inspired review process, incorporating
context assessment, individual defect analysis, false positive elimination,
confidence recalibration and quality assurance to validate and refine the
SDMSD's initial proposals. By strategically employing and intelligently using
tools, InsightX Agent moves beyond passive data processing to active reasoning,
enhancing diagnostic reliability and providing interpretations that integrate
diverse information sources. Experimental evaluations on the GDXray+ dataset
demonstrate that InsightX Agent not only achieves a high object detection
F1-score of 96.35% but also offers significantly improved interpretability and
trustworthiness in its analyses, highlighting the transformative potential of
agentic LLM frameworks for industrial inspection tasks.

</details>


### [23] [Feedback-Induced Performance Decline in LLM-Based Decision-Making](https://arxiv.org/abs/2507.14906)
*Xiao Yang,Juxi Leitner,Michael Burke*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在马尔可夫决策过程中的表现，发现其在简单环境中表现良好，但在复杂场景中需要进一步优化。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在自主决策中的适用性，利用其预训练知识加速适应。

Method: 通过在线结构化提示策略，比较LLM与传统强化学习方法的零样本性能。

Result: LLMs在简单环境中初始表现优异，但在复杂场景中表现不佳；反馈机制可能降低性能。

Conclusion: 需进一步研究混合策略、微调和高级记忆集成以提升LLM的决策能力。

Abstract: The ability of Large Language Models (LLMs) to extract context from natural
language problem descriptions naturally raises questions about their
suitability in autonomous decision-making settings. This paper studies the
behaviour of these models within a Markov Decision Process (MDPs). While
traditional reinforcement learning (RL) strategies commonly employed in this
setting rely on iterative exploration, LLMs, pre-trained on diverse datasets,
offer the capability to leverage prior knowledge for faster adaptation. We
investigate online structured prompting strategies in sequential decision
making tasks, comparing the zero-shot performance of LLM-based approaches to
that of classical RL methods. Our findings reveal that although LLMs
demonstrate improved initial performance in simpler environments, they struggle
with planning and reasoning in complex scenarios without fine-tuning or
additional guidance. Our results show that feedback mechanisms, intended to
improve decision-making, often introduce confusion, leading to diminished
performance in intricate environments. These insights underscore the need for
further exploration into hybrid strategies, fine-tuning, and advanced memory
integration to enhance LLM-based decision-making capabilities.

</details>


### [24] [The Endless Tuning. An Artificial Intelligence Design To Avoid Human Replacement and Trace Back Responsibilities](https://arxiv.org/abs/2507.14909)
*Elio Grande*

Main category: cs.AI

TL;DR: 《Endless Tuning》是一种基于双重镜像过程的AI设计方法，旨在避免人类被取代并填补责任缺口。通过三个原型应用测试，该方法在用户体验和可控性方面表现良好。


<details>
  <summary>Details</summary>
Motivation: 避免AI取代人类角色并解决责任缺口问题（Matthias 2004）。

Method: 采用双重镜像过程，结合关系性方法，开发协议并在贷款审批、肺炎诊断和艺术风格识别三个领域进行原型测试。

Result: 用户感知到决策过程中的完全控制，且在责任与问责之间建立了桥梁。

Conclusion: 该方法在用户体验和伦理层面提供了新的视角，技术选择（如反向解释性AI部署）与实验结果支持其可行性。

Abstract: The Endless Tuning is a design method for a reliable deployment of artificial
intelligence based on a double mirroring process, which pursues both the goals
of avoiding human replacement and filling the so-called responsibility gap
(Matthias 2004). Originally depicted in (Fabris et al. 2024) and ensuing the
relational approach urged therein, it was then actualized in a protocol,
implemented in three prototypical applications regarding decision-making
processes (respectively: loan granting, pneumonia diagnosis, and art style
recognition) and tested with such as many domain experts. Step by step
illustrating the protocol, giving insights concretely showing a different voice
(Gilligan 1993) in the ethics of artificial intelligence, a philosophical
account of technical choices (e.g., a reversed and hermeneutic deployment of
XAI algorithms) will be provided in the present study together with the results
of the experiments, focusing on user experience rather than statistical
accuracy. Even thoroughly employing deep learning models, full control was
perceived by the interviewees in the decision-making setting, while it appeared
that a bridge can be built between accountability and liability in case of
damage.

</details>


### [25] [Redefining Elderly Care with Agentic AI: Challenges and Opportunities](https://arxiv.org/abs/2507.14912)
*Ruhul Amin Khalil,Kashif Ahmad,Hazrat Ali*

Main category: cs.AI

TL;DR: 本文探讨了基于大型语言模型（LLM）的Agentic AI在老年护理中的潜力与挑战，包括个性化健康跟踪、认知护理和环境管理，同时强调了数据隐私、伦理保护和透明决策的重要性。


<details>
  <summary>Details</summary>
Motivation: 全球老龄化人口需要创新的护理策略，Agentic AI因其主动和自主决策能力成为潜在解决方案，但目前缺乏相关研究。

Method: 分析了LLM驱动的Agentic AI在老年护理中的独特能力、应用和限制，填补了文献空白。

Result: Agentic AI在提升老年人独立性和生活质量方面具有潜力，但也需解决隐私和伦理问题。

Conclusion: 需平衡Agentic AI的潜力与挑战，推动以人为中心的研究和伦理框架，确保其负责任地应用于老年护理。

Abstract: The global ageing population necessitates new and emerging strategies for
caring for older adults. In this article, we explore the potential for
transformation in elderly care through Agentic Artificial Intelligence (AI),
powered by Large Language Models (LLMs). We discuss the proactive and
autonomous decision-making facilitated by Agentic AI in elderly care.
Personalized tracking of health, cognitive care, and environmental management,
all aimed at enhancing independence and high-level living for older adults,
represents important areas of application. With a potential for significant
transformation of elderly care, Agentic AI also raises profound concerns about
data privacy and security, decision independence, and access. We share key
insights to emphasize the need for ethical safeguards, privacy protections, and
transparent decision-making. Our goal in this article is to provide a balanced
discussion of both the potential and the challenges associated with Agentic AI,
and to provide insights into its responsible use in elderly care, to bring
Agentic AI into harmony with the requirements and vulnerabilities specific to
the elderly. Finally, we identify the priorities for the academic research
communities, to achieve human-centered advancements and integration of Agentic
AI in elderly care. To the best of our knowledge, this is no existing study
that reviews the role of Agentic AI in elderly care. Hence, we address the
literature gap by analyzing the unique capabilities, applications, and
limitations of LLM-based Agentic AI in elderly care. We also provide a
companion interactive dashboard at https://hazratali.github.io/agenticai/.

</details>


### [26] [Complexity of Faceted Explanations in Propositional Abduction](https://arxiv.org/abs/2507.14962)
*Johannes Schmidt,Mohamed Maizia,Victor Lagerkvist,Johannes K. Fichte*

Main category: cs.AI

TL;DR: 本文探讨了命题溯因中的细粒度推理方法，引入了“facet”概念以更好地理解解释的变异性，并分析了其在Post框架中的表现。


<details>
  <summary>Details</summary>
Motivation: 溯因推理在AI和数据库更新中有广泛应用，但其计算复杂性高，尤其是计数和枚举问题。本文旨在通过引入facet概念，提供更细粒度的解释分析。

Method: 提出facet（在部分解释中出现但非全部的字面量）概念，并研究其在命题溯因中的应用，包括解释间的距离分析。

Result: 在Post框架中几乎完全刻画了facet的特性，为理解解释的异质性/同质性提供了新视角。

Conclusion: facet为命题溯因提供了更细粒度的分析工具，有助于在保持计算复杂性的同时更好地理解解释的变异性。

Abstract: Abductive reasoning is a popular non-monotonic paradigm that aims to explain
observed symptoms and manifestations. It has many applications, such as
diagnosis and planning in artificial intelligence and database updates. In
propositional abduction, we focus on specifying knowledge by a propositional
formula. The computational complexity of tasks in propositional abduction has
been systematically characterized - even with detailed classifications for
Boolean fragments. Unsurprisingly, the most insightful reasoning problems
(counting and enumeration) are computationally highly challenging. Therefore,
we consider reasoning between decisions and counting, allowing us to understand
explanations better while maintaining favorable complexity. We introduce facets
to propositional abductions, which are literals that occur in some explanation
(relevant) but not all explanations (dispensable). Reasoning with facets
provides a more fine-grained understanding of variability in explanations
(heterogeneous). In addition, we consider the distance between two
explanations, enabling a better understanding of heterogeneity/homogeneity. We
comprehensively analyze facets of propositional abduction in various settings,
including an almost complete characterization in Post's framework.

</details>


### [27] [AlphaAlign: Incentivizing Safety Alignment with Extremely Simplified Reinforcement Learning](https://arxiv.org/abs/2507.14987)
*Yi Zhang,An Zhang,XiuYu Zhang,Leheng Sheng,Yuxin Chen,Zhenkai Liang,Xiang Wang*

Main category: cs.AI

TL;DR: AlphaAlign是一个基于纯强化学习的框架，通过可验证的安全奖励激发大语言模型的内在安全自我意识，提升安全推理能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在安全对齐后仍存在生成有害内容、过度拒绝和实用性下降的问题，现有方法未能充分利用模型的内在安全自我意识。

Method: AlphaAlign采用双奖励系统：可验证的安全奖励鼓励对有害查询的正确拒绝和明确理由，同时惩罚过度拒绝；标准化帮助奖励指导对良性输入的高质量响应。

Result: AlphaAlign在简化流程、打破安全-效用权衡以及深化对齐方面表现出色，提升了拒绝有害内容的能力，减少了过度拒绝，同时保持或提高了任务性能。

Conclusion: AlphaAlign通过强化学习有效激发模型的内在安全自我意识，实现了更深入的安全对齐和主动安全推理。

Abstract: Large language models (LLMs), despite possessing latent safety understanding
from their vast pretraining data, remain vulnerable to generating harmful
content and exhibit issues such as over-refusal and utility degradation after
safety alignment. Current safety alignment methods often result in superficial
refusal shortcuts or rely on intensive supervision for reasoning-based
approaches, failing to fully leverage the model's intrinsic safety
self-awareness. We propose \textbf{AlphaAlign}, a simple yet effective pure
reinforcement learning (RL) framework with verifiable safety reward designed to
incentivize this latent safety awareness through proactive safety reasoning.}
AlphaAlign employs a dual-reward system: a verifiable safety reward encourages
correctly formatted and explicitly justified refusals for harmful queries while
penalizing over-refusals, and a normalized helpfulness reward guides
high-quality responses to benign inputs. This allows the model to develop
proactive safety reasoning capabilities without depending on supervised
safety-specific reasoning data. AlphaAlign demonstrates three key advantages:
(1) Simplicity and efficiency, requiring only binary prompt safety labels and
minimal RL steps for substantial improvements. (2) Breaking the safety-utility
trade-off, by enhancing refusal of harmful content and reducing over-refusals,
while simultaneously maintaining or even improving general task performance and
robustness to unseen jailbreaks. (3) Deep alignment, fostering proactive safety
reasoning that generates explicit safety rationales rather than relying on
shallow refusal patterns.

</details>


### [28] [A Forced-Choice Neural Cognitive Diagnostic Model of Personality Testing](https://arxiv.org/abs/2507.15013)
*Xiaoyu Li,Jin Wu,Shaoyang Guo,Haoran Shi,Chanjin Zheng*

Main category: cs.AI

TL;DR: 本文提出了一种基于深度学习的强制选择神经认知诊断模型（FCNCD），用于解决传统模型的局限性，并适用于三种常见的强制选择题块类型。


<details>
  <summary>Details</summary>
Motivation: 在智能时代，心理测量测试在人员选拔、职业发展和心理健康评估中日益重要。强制选择测试因其能降低回答失真风险而常用，但传统模型存在局限。

Method: 通过非线性映射挖掘参与者和题目特征，利用多层神经网络建模其交互，并引入单调性假设提升结果可解释性。

Result: 在真实和模拟数据集上的实验验证了FCNCD的准确性、可解释性和鲁棒性。

Conclusion: FCNCD为强制选择测试提供了一种有效且可解释的解决方案。

Abstract: In the smart era, psychometric tests are becoming increasingly important for
personnel selection, career development, and mental health assessment.
Forced-choice tests are common in personality assessments because they require
participants to select from closely related options, lowering the risk of
response distortion. This study presents a deep learning-based Forced-Choice
Neural Cognitive Diagnostic Model (FCNCD) that overcomes the limitations of
traditional models and is applicable to the three most common item block types
found in forced-choice tests. To account for the unidimensionality of items in
forced-choice tests, we create interpretable participant and item parameters.
We model the interactions between participant and item features using
multilayer neural networks after mining them using nonlinear mapping. In
addition, we use the monotonicity assumption to improve the interpretability of
the diagnostic results. The FCNCD's effectiveness is validated by experiments
on real-world and simulated datasets that show its accuracy, interpretability,
and robustness.

</details>


### [29] [DeRAG: Black-box Adversarial Attacks on Multiple Retrieval-Augmented Generation Applications via Prompt Injection](https://arxiv.org/abs/2507.15042)
*Jerry Wang,Fang Yu*

Main category: cs.AI

TL;DR: 本文提出了一种基于差分进化（DE）的方法，优化对抗性提示后缀以攻击RAG系统，实验表明其攻击成功率高且难以检测。


<details>
  <summary>Details</summary>
Motivation: 对抗性提示攻击会显著影响RAG系统的可靠性，因此需要一种有效的方法来优化对抗性提示。

Method: 采用差分进化（DE）方法，将RAG系统视为黑盒，优化对抗性提示后缀以提升错误文档的检索排名。

Result: 在BEIR QA数据集上的实验显示，DE方法在攻击成功率上优于现有方法，且对抗性后缀难以被检测。

Conclusion: DE方法在对抗性提示优化中表现出色，为RAG系统的安全性提供了新的研究视角。

Abstract: Adversarial prompt attacks can significantly alter the reliability of
Retrieval-Augmented Generation (RAG) systems by re-ranking them to produce
incorrect outputs. In this paper, we present a novel method that applies
Differential Evolution (DE) to optimize adversarial prompt suffixes for
RAG-based question answering. Our approach is gradient-free, treating the RAG
pipeline as a black box and evolving a population of candidate suffixes to
maximize the retrieval rank of a targeted incorrect document to be closer to
real world scenarios. We conducted experiments on the BEIR QA datasets to
evaluate attack success at certain retrieval rank thresholds under multiple
retrieving applications. Our results demonstrate that DE-based prompt
optimization attains competitive (and in some cases higher) success rates
compared to GGPP to dense retrievers and PRADA to sparse retrievers, while
using only a small number of tokens (<=5 tokens) in the adversarial suffix.
Furthermore, we introduce a readability-aware suffix construction strategy,
validated by a statistically significant reduction in MLM negative
log-likelihood with Welch's t-test. Through evaluations with a BERT-based
adversarial suffix detector, we show that DE-generated suffixes evade
detection, yielding near-chance detection accuracy.

</details>


### [30] [From Kicking to Causality: Simulating Infant Agency Detection with a Robust Intrinsic Reward](https://arxiv.org/abs/2507.15106)
*Xia Xu,Jochen Triesch*

Main category: cs.AI

TL;DR: 论文提出了一种基于因果推断的新型内在奖励方法CAIS，用于强化学习，解决了传统相关性奖励在噪声环境中的脆弱性问题。


<details>
  <summary>Details</summary>
Motivation: 人类婴儿能稳健地发现自身的因果效能，而传统强化学习代理在噪声环境中表现脆弱，依赖相关性奖励易受干扰。

Method: 引入CAIS，通过计算动作对感官结果的因果影响（1-Wasserstein距离）来量化动作的因果效能。

Result: 在模拟婴儿-移动环境中，CAIS能有效过滤噪声并学习正确策略，同时重现了“灭绝爆发”现象。

Conclusion: 显式推断因果性是发展稳健代理感的关键机制，为自适应自主系统提供了心理学合理的框架。

Abstract: While human infants robustly discover their own causal efficacy, standard
reinforcement learning agents remain brittle, as their reliance on
correlation-based rewards fails in noisy, ecologically valid scenarios. To
address this, we introduce the Causal Action Influence Score (CAIS), a novel
intrinsic reward rooted in causal inference. CAIS quantifies an action's
influence by measuring the 1-Wasserstein distance between the learned
distribution of sensory outcomes conditional on that action, $p(h|a)$, and the
baseline outcome distribution, $p(h)$. This divergence provides a robust reward
that isolates the agent's causal impact from confounding environmental noise.
We test our approach in a simulated infant-mobile environment where
correlation-based perceptual rewards fail completely when the mobile is
subjected to external forces. In stark contrast, CAIS enables the agent to
filter this noise, identify its influence, and learn the correct policy.
Furthermore, the high-quality predictive model learned for CAIS allows our
agent, when augmented with a surprise signal, to successfully reproduce the
"extinction burst" phenomenon. We conclude that explicitly inferring causality
is a crucial mechanism for developing a robust sense of agency, offering a
psychologically plausible framework for more adaptive autonomous systems.

</details>


### [31] [Automated planning with ontologies under coherence update semantics](https://arxiv.org/abs/2507.15120)
*Stefan Borgwardt,Duy Nhu,Gabriele Röger*

Main category: cs.AI

TL;DR: 提出一种结合DL-Lite本体和动作条件的新规划方法，复杂度不高于现有方法，并通过编译为经典规划实现。


<details>
  <summary>Details</summary>
Motivation: 将背景知识（如本体）融入自动规划问题，以支持开放世界语义。

Method: 结合显式输入知识（eKABs）和本体感知动作效应，采用一致性更新语义。

Result: 复杂度与现有方法相当，通过多项式编译实现，并在基准测试中验证性能。

Conclusion: 新方法有效且高效，适用于本体驱动的规划问题。

Abstract: Standard automated planning employs first-order formulas under closed-world
semantics to achieve a goal with a given set of actions from an initial state.
We follow a line of research that aims to incorporate background knowledge into
automated planning problems, for example, by means of ontologies, which are
usually interpreted under open-world semantics. We present a new approach for
planning with DL-Lite ontologies that combines the advantages of ontology-based
action conditions provided by explicit-input knowledge and action bases (eKABs)
and ontology-aware action effects under the coherence update semantics. We show
that the complexity of the resulting formalism is not higher than that of
previous approaches and provide an implementation via a polynomial compilation
into classical planning. An evaluation of existing and new benchmarks examines
the performance of a planning system on different variants of our compilation.

</details>


### [32] [Clinical Semantic Intelligence (CSI): Emulating the Cognitive Framework of the Expert Clinician for Comprehensive Oral Disease Diagnosis](https://arxiv.org/abs/2507.15140)
*Mohammad Mashayekhi,Sara Ahmadi Majd,Arian AmirAmjadi,Parsa Hosseini*

Main category: cs.AI

TL;DR: 开发了一种名为CSI的人工智能框架，通过模拟专家临床推理诊断118种口腔疾病，显著提高了诊断准确性。


<details>
  <summary>Details</summary>
Motivation: 口腔疾病诊断存在症状重叠的临床挑战，需要超越简单模式匹配的智能诊断辅助工具。

Method: 整合多模态CLIP模型和ChatGLM-6B语言模型，采用分层诊断推理树（HDRT）进行快速和标准模式诊断。

Result: 在431张测试图像上，快速模式准确率为73.4%，标准模式提升至89.5%。

Conclusion: CSI框架通过分层推理显著提升诊断准确性，为临床提供了实用工具。

Abstract: The diagnosis of oral diseases presents a problematic clinical challenge,
characterized by a wide spectrum of pathologies with overlapping
symptomatology. To address this, we developed Clinical Semantic Intelligence
(CSI), a novel artificial intelligence framework that diagnoses 118 different
oral diseases by computationally modeling the cognitive processes of an expert
clinician. Our core hypothesis is that moving beyond simple pattern matching to
emulate expert reasoning is critical to building clinically useful diagnostic
aids.
  CSI's architecture integrates a fine-tuned multimodal CLIP model with a
specialized ChatGLM-6B language model. This system executes a Hierarchical
Diagnostic Reasoning Tree (HDRT), a structured framework that distills the
systematic, multi-step logic of differential diagnosis. The framework operates
in two modes: a Fast Mode for rapid screening and a Standard Mode that
leverages the full HDRT for an interactive and in-depth diagnostic workup.
  To train and validate our system, we curated a primary dataset of 4,310
images, supplemented by an external hold-out set of 176 images for final
validation. A clinically-informed augmentation strategy expanded our training
data to over 30,000 image-text pairs. On a 431-image internal test set, CSI's
Fast Mode achieved an accuracy of 73.4%, which increased to 89.5% with the
HDRT-driven Standard Mode. The performance gain is directly attributable to the
hierarchical reasoning process. Herein, we detail the architectural philosophy,
development, and rigorous evaluation of the CSI framework.

</details>


### [33] [Can We Move Freely in NEOM's The Line? An Agent-Based Simulation of Human Mobility in a Futuristic Smart City](https://arxiv.org/abs/2507.15143)
*Abderaouf Bahi,Amel Ourici*

Main category: cs.AI

TL;DR: 论文研究了在沙特阿拉伯NEOM的170公里线性智能城市The Line中人类移动的可行性，通过混合模拟框架验证了自由移动的可能性。


<details>
  <summary>Details</summary>
Motivation: 探索在The Line这种前所未有的线性城市拓扑中，居民能否实现自由移动。

Method: 开发了结合代理建模、强化学习、监督学习和图神经网络的混合模拟框架，使用合成数据和真实数据模拟多模式交通行为。

Result: 实验显示，AI集成架构下，平均通勤时间为7.8至8.4分钟，满意度超过89%，可达性指数达91%以上。

Conclusion: 研究证明，在自适应AI系统、可持续基础设施和实时反馈支持下，The Line中的自由移动是可行的。

Abstract: This paper investigates the feasibility of human mobility in The Line, a
proposed 170-kilometer linear smart city in NEOM, Saudi Arabia. To assess
whether citizens can move freely within this unprecedented urban topology, we
develop a hybrid simulation framework that integrates agent-based modeling,
reinforcement learning, supervised learning, and graph neural networks. The
simulation captures multi-modal transportation behaviors across 50 vertical
levels and varying density scenarios using both synthetic data and real-world
traces from high-density cities. Our experiments reveal that with the full
AI-integrated architecture, agents achieved an average commute time of 7.8 to
8.4 minutes, a satisfaction rate exceeding 89 percent, and a reachability index
of over 91 percent, even during peak congestion periods. Ablation studies
confirmed that the removal of intelligent modules such as reinforcement
learning or graph neural networks significantly degrades performance, with
commute times increasing by up to 85 percent and reachability falling below 70
percent. Environmental modeling further demonstrated low energy consumption and
minimal CO2 emissions when electric modes are prioritized. The findings suggest
that freedom of movement is not only conceptually achievable in The Line, but
also operationally realistic if supported by adaptive AI systems, sustainable
infrastructure, and real-time feedback loops.

</details>


### [34] [Solving Formal Math Problems by Decomposition and Iterative Reflection](https://arxiv.org/abs/2507.15225)
*Yichi Zhou,Jianqiu Zhao,Yongxin Zhang,Bohan Wang,Siran Wang,Luoxin Chen,Jiahui Wang,Haowei Chen,Allan Jie,Xinbo Zhang,Haocheng Wang,Luong Trung,Rong Ye,Phan Nhat Hoang,Huishuai Zhang,Peng Sun,Hang Li*

Main category: cs.AI

TL;DR: Delta Prover是一个基于代理的框架，利用通用LLM与Lean 4交互生成形式化证明，无需模型微调，在miniF2F-test基准测试中达到95.9%的成功率。


<details>
  <summary>Details</summary>
Motivation: 通用LLM在形式化证明（如Lean 4）中表现不佳，且现有方法需高成本的模型微调。Delta Prover旨在通过代理框架解决这一问题。

Method: Delta Prover结合了反射分解与迭代证明修复的算法框架，以及基于Lean 4的DSL，用于子问题管理。

Result: 在miniF2F-test基准测试中达到95.9%的成功率，优于现有方法，并展示了更强的测试时扩展性。

Conclusion: 通用LLM在有效代理结构指导下具有未开发的定理证明潜力，为形式化环境中的自动推理提供了高效替代方案。

Abstract: General-purpose Large Language Models (LLMs) have achieved remarkable success
in intelligence, performing comparably to human experts on complex reasoning
tasks such as coding and mathematical reasoning. However, generating formal
proofs in specialized languages like Lean 4 remains a significant challenge for
these models, limiting their application in complex theorem proving and
automated verification. Current approaches typically require specializing
models through fine-tuning on dedicated formal corpora, incurring high costs
for data collection and training. In this work, we introduce \textbf{Delta
Prover}, an agent-based framework that orchestrates the interaction between a
general-purpose LLM and the Lean 4 proof environment. Delta Prover leverages
the reflection and reasoning capabilities of general-purpose LLMs to
interactively construct formal proofs in Lean 4, circumventing the need for
model specialization. At its core, the agent integrates two novel,
interdependent components: an algorithmic framework for reflective
decomposition and iterative proof repair, and a custom Domain-Specific Language
(DSL) built upon Lean 4 for streamlined subproblem management. \textbf{Delta
Prover achieves a state-of-the-art 95.9\% success rate on the miniF2F-test
benchmark, surpassing all existing approaches, including those requiring model
specialization.} Furthermore, Delta Prover exhibits a significantly stronger
test-time scaling law compared to standard Best-of-N proof strategies.
Crucially, our findings demonstrate that general-purpose LLMs, when guided by
an effective agentic structure, possess substantial untapped theorem-proving
capabilities. This presents a computationally efficient alternative to
specialized models for robust automated reasoning in formal environments.

</details>


### [35] [Explainable Artificial Intelligence based Soft Evaluation Indicator for Arc Fault Diagnosis](https://arxiv.org/abs/2507.15239)
*Qianchao Wang,Yuxuan Ding,Chuanzhen Jia,Zhe Li,Yaping Du*

Main category: cs.AI

TL;DR: 论文提出了一种软评估指标和轻量级平衡神经网络，以提高电弧故障诊断模型的可解释性和信任度。


<details>
  <summary>Details</summary>
Motivation: 现有AI电弧故障诊断模型虽准确率高，但缺乏可解释性，难以信任。

Method: 结合可解释AI和真实电弧故障实验，定义正确解释，并提出轻量级平衡神经网络。

Result: 在多个数据集上验证了软评估指标的有效性，模型更易理解和信任。

Conclusion: 该方法提升了电弧故障诊断模型的可解释性和决策信任度。

Abstract: Novel AI-based arc fault diagnosis models have demonstrated outstanding
performance in terms of classification accuracy. However, an inherent problem
is whether these models can actually be trusted to find arc faults. In this
light, this work proposes a soft evaluation indicator that explains the outputs
of arc fault diagnosis models, by defining the the correct explanation of arc
faults and leveraging Explainable Artificial Intelligence and real arc fault
experiments. Meanwhile, a lightweight balanced neural network is proposed to
guarantee competitive accuracy and soft feature extraction score. In our
experiments, several traditional machine learning methods and deep learning
methods across two arc fault datasets with different sample times and noise
levels are utilized to test the effectiveness of the soft evaluation indicator.
Through this approach, the arc fault diagnosis models are easy to understand
and trust, allowing practitioners to make informed and trustworthy decisions.

</details>


### [36] [Disentangling Homophily and Heterophily in Multimodal Graph Clustering](https://arxiv.org/abs/2507.15253)
*Zhaochen Guo,Zhixiang Shen,Xuanting Xie,Liangjian Wen,Zhao Kang*

Main category: cs.AI

TL;DR: 该论文提出了一种名为DMGC的新型框架，用于多模态图的无监督聚类，通过分解混合图并引入双频融合机制，实现了在多模态数据集上的最先进性能。


<details>
  <summary>Details</summary>
Motivation: 多模态图在现实世界中有广泛应用，但在无监督学习中的研究不足，尤其是如何处理混合的同质性和异质性邻居模式。

Method: 提出DMGC框架，将原始混合图分解为同质性增强图和异质性感知图，并引入多模态双频融合机制进行联合过滤。

Result: 在多种多模态和多关系图数据集上的实验表明，DMGC实现了最先进的性能。

Conclusion: DMGC框架有效解决了多模态图聚类中的混合邻居模式问题，具有广泛的适用性和泛化能力。

Abstract: Multimodal graphs, which integrate unstructured heterogeneous data with
structured interconnections, offer substantial real-world utility but remain
insufficiently explored in unsupervised learning. In this work, we initiate the
study of multimodal graph clustering, aiming to bridge this critical gap.
Through empirical analysis, we observe that real-world multimodal graphs often
exhibit hybrid neighborhood patterns, combining both homophilic and
heterophilic relationships. To address this challenge, we propose a novel
framework -- \textsc{Disentangled Multimodal Graph Clustering (DMGC)} -- which
decomposes the original hybrid graph into two complementary views: (1) a
homophily-enhanced graph that captures cross-modal class consistency, and (2)
heterophily-aware graphs that preserve modality-specific inter-class
distinctions. We introduce a \emph{Multimodal Dual-frequency Fusion} mechanism
that jointly filters these disentangled graphs through a dual-pass strategy,
enabling effective multimodal integration while mitigating category confusion.
Our self-supervised alignment objectives further guide the learning process
without requiring labels. Extensive experiments on both multimodal and
multi-relational graph datasets demonstrate that DMGC achieves state-of-the-art
performance, highlighting its effectiveness and generalizability across diverse
settings. Our code is available at https://github.com/Uncnbb/DMGC.

</details>


### [37] [IM-Chat: A Multi-agent LLM-based Framework for Knowledge Transfer in Injection Molding Industry](https://arxiv.org/abs/2507.15268)
*Junhyeong Lee,Joon-Young Kim,Heekyu Kim,Inhyo Lee,Seunghwa Ryu*

Main category: cs.AI

TL;DR: IM-Chat是一个基于大语言模型的多智能体框架，旨在解决注塑行业知识传递的挑战，结合文档知识和现场数据，通过检索增强生成和工具调用实现高效任务解决。


<details>
  <summary>Details</summary>
Motivation: 注塑行业面临经验工人退休和多语言障碍导致的知识传递困难，需要一种高效的知识转移解决方案。

Method: IM-Chat采用检索增强生成（RAG）策略和工具调用智能体，结合文档知识和数据驱动的工艺条件生成器，无需微调即可适应任务。

Result: 评估显示，更强大的模型（如GPT-4o）在复杂任务中表现更优，验证了IM-Chat在工业知识工作流中的可行性。

Conclusion: IM-Chat为制造业提供了一种可扩展且通用的AI辅助决策支持方法。

Abstract: The injection molding industry faces critical challenges in preserving and
transferring field knowledge, particularly as experienced workers retire and
multilingual barriers hinder effective communication. This study introduces
IM-Chat, a multi-agent framework based on large language models (LLMs),
designed to facilitate knowledge transfer in injection molding. IM-Chat
integrates both limited documented knowledge (e.g., troubleshooting tables,
manuals) and extensive field data modeled through a data-driven process
condition generator that infers optimal manufacturing settings from
environmental inputs such as temperature and humidity, enabling robust and
context-aware task resolution. By adopting a retrieval-augmented generation
(RAG) strategy and tool-calling agents within a modular architecture, IM-Chat
ensures adaptability without the need for fine-tuning. Performance was assessed
across 100 single-tool and 60 hybrid tasks for GPT-4o, GPT-4o-mini, and
GPT-3.5-turbo by domain experts using a 10-point rubric focused on relevance
and correctness, and was further supplemented by automated evaluation using
GPT-4o guided by a domain-adapted instruction prompt. The evaluation results
indicate that more capable models tend to achieve higher accuracy, particularly
in complex, tool-integrated scenarios. Overall, these findings demonstrate the
viability of multi-agent LLM systems for industrial knowledge workflows and
establish IM-Chat as a scalable and generalizable approach to AI-assisted
decision support in manufacturing.

</details>


### [38] [QSAF: A Novel Mitigation Framework for Cognitive Degradation in Agentic AI](https://arxiv.org/abs/2507.15330)
*Hammad Atta,Muhammad Zeeshan Baig,Yasir Mehmood,Nadeem Shahzad,Ken Huang,Muhammad Aziz Ul Haq,Muhammad Awais,Kamal Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种新的AI系统漏洞类别——认知退化，并介绍了Qorvex安全AI框架（QSAF Domain 10）来应对此类问题。


<details>
  <summary>Details</summary>
Motivation: 传统的外部威胁（如提示注入）已不足以应对AI系统的内部问题，如内存饥饿、规划递归等导致的认知退化。

Method: 提出了QSAF框架，包含六阶段认知退化生命周期和七种实时控制措施（如回退路由、饥饿检测等）。

Result: 框架通过实时监控和缓解措施，有效应对认知退化问题。

Conclusion: 认知退化是AI系统的新威胁，QSAF框架为跨平台防御提供了首个解决方案。

Abstract: We introduce Cognitive Degradation as a novel vulnerability class in agentic
AI systems. Unlike traditional adversarial external threats such as prompt
injection, these failures originate internally, arising from memory starvation,
planner recursion, context flooding, and output suppression. These systemic
weaknesses lead to silent agent drift, logic collapse, and persistent
hallucinations over time. To address this class of failures, we introduce the
Qorvex Security AI Framework for Behavioral & Cognitive Resilience (QSAF Domain
10), a lifecycle-aware defense framework defined by a six-stage cognitive
degradation lifecycle. The framework includes seven runtime controls
(QSAF-BC-001 to BC-007) that monitor agent subsystems in real time and trigger
proactive mitigation through fallback routing, starvation detection, and memory
integrity enforcement. Drawing from cognitive neuroscience, we map agentic
architectures to human analogs, enabling early detection of fatigue,
starvation, and role collapse. By introducing a formal lifecycle and real-time
mitigation controls, this work establishes Cognitive Degradation as a critical
new class of AI system vulnerability and proposes the first cross-platform
defense model for resilient agentic behavior.

</details>


### [39] [One Step is Enough: Multi-Agent Reinforcement Learning based on One-Step Policy Optimization for Order Dispatch on Ride-Sharing Platforms](https://arxiv.org/abs/2507.15351)
*Zijian Zhao,Sen Li*

Main category: cs.AI

TL;DR: 论文提出两种新方法（GRPO和OSPO）解决多智能体强化学习在实时拼车平台中的价值函数估计问题，实验证明其优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 实时拼车平台面临动态匹配乘客与车辆的高维度和不确定性挑战，传统MARL方法因依赖准确的价值函数估计而表现不佳。

Method: 1. 将GRPO应用于拼车平台，用组平均奖励替代PPO基线以减少估计偏差；2. 提出OSPO方法，仅使用一步奖励训练最优策略。

Result: 在真实曼哈顿数据集上，GRPO和OSPO在多数场景中表现更优，有效优化接客时间和订单完成量。

Conclusion: GRPO和OSPO通过绕过价值函数估计，解决了传统MARL方法的局限性，为拼车平台提供了高效解决方案。

Abstract: On-demand ride-sharing platforms face the fundamental challenge of
dynamically bundling passengers with diverse origins and destinations and
matching them with vehicles in real time, all under significant uncertainty.
Recently, MARL has emerged as a promising solution for this problem, leveraging
decentralized learning to address the curse of dimensionality caused by the
large number of agents in the ride-hailing market and the resulting expansive
state and action spaces. However, conventional MARL-based ride-sharing
approaches heavily rely on the accurate estimation of Q-values or V-values,
which becomes problematic in large-scale, highly uncertain environments.
Specifically, most of these approaches adopt an independent paradigm,
exacerbating this issue, as each agent treats others as part of the
environment, leading to unstable training and substantial estimation bias in
value functions. To address these challenges, we propose two novel alternative
methods that bypass value function estimation. First, we adapt GRPO to
ride-sharing, replacing the PPO baseline with the group average reward to
eliminate critic estimation errors and reduce training bias. Second, inspired
by GRPO's full utilization of group reward information, we customize the PPO
framework for ride-sharing platforms and show that, under a homogeneous fleet,
the optimal policy can be trained using only one-step rewards - a method we
term One-Step Policy Optimization (OSPO). Experiments on a real-world Manhattan
ride-hailing dataset demonstrate that both GRPO and OSPO achieve superior
performance across most scenarios, efficiently optimizing pickup times and the
number of served orders using simple MLP networks.

</details>


### [40] [RAD: Retrieval High-quality Demonstrations to Enhance Decision-making](https://arxiv.org/abs/2507.15356)
*Lu Guo,Yixiang Shan,Zhengbang Zhu,Qifan Liang,Lichang Song,Ting Long,Weinan Zhang,Yi Chang*

Main category: cs.AI

TL;DR: RAD结合检索与扩散模型，通过动态检索高质量状态并规划，提升离线强化学习的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 离线强化学习因数据集稀疏和轨迹间过渡缺失而受限，现有方法泛化能力不足。

Method: RAD结合非参数检索和扩散模型，动态检索高回报状态并规划。

Result: RAD在多样基准测试中表现优于基线方法。

Conclusion: RAD有效解决了离线强化学习中的泛化和规划问题。

Abstract: Offline reinforcement learning (RL) enables agents to learn policies from
fixed datasets, avoiding costly or unsafe environment interactions. However,
its effectiveness is often limited by dataset sparsity and the lack of
transition overlap between suboptimal and expert trajectories, which makes
long-horizon planning particularly challenging. Prior solutions based on
synthetic data augmentation or trajectory stitching often fail to generalize to
novel states and rely on heuristic stitching points. To address these
challenges, we propose Retrieval High-quAlity Demonstrations (RAD) for
decision-making, which combines non-parametric retrieval with diffusion-based
generative modeling. RAD dynamically retrieves high-return states from the
offline dataset as target states based on state similarity and return
estimation, and plans toward them using a condition-guided diffusion model.
Such retrieval-guided generation enables flexible trajectory stitching and
improves generalization when encountered with underrepresented or
out-of-distribution states. Extensive experiments confirm that RAD achieves
competitive or superior performance compared to baselines across diverse
benchmarks, validating its effectiveness.

</details>


### [41] [Predictive Process Monitoring Using Object-centric Graph Embeddings](https://arxiv.org/abs/2507.15411)
*Wissam Gherissi,Mehdi Acheli,Joyce El Haddad,Daniela Grigori*

Main category: cs.AI

TL;DR: 提出了一种基于图注意力网络和LSTM的端到端模型，用于预测未来流程行为，包括下一活动和下一事件时间。


<details>
  <summary>Details</summary>
Motivation: 利用对象中心事件日志提升流程预测的准确性和效率。

Method: 结合图注意力网络编码活动及其关系，以及LSTM处理时间依赖性。

Result: 在真实和合成事件日志上表现优于现有方法。

Conclusion: 模型在预测流程行为方面具有竞争力。

Abstract: Object-centric predictive process monitoring explores and utilizes
object-centric event logs to enhance process predictions. The main challenge
lies in extracting relevant information and building effective models. In this
paper, we propose an end-to-end model that predicts future process behavior,
focusing on two tasks: next activity prediction and next event time. The
proposed model employs a graph attention network to encode activities and their
relationships, combined with an LSTM network to handle temporal dependencies.
Evaluated on one reallife and three synthetic event logs, the model
demonstrates competitive performance compared to state-of-the-art methods.

</details>


### [42] [Optimization of Activity Batching Policies in Business Processes](https://arxiv.org/abs/2507.15457)
*Orlenys López-Pintado,Jannis Rosenbaum,Marlon Dumas*

Main category: cs.AI

TL;DR: 本文提出了一种基于帕累托优化的方法，通过干预启发式发现业务过程中活动批处理的最优策略，权衡等待时间、处理成本和资源利用率。


<details>
  <summary>Details</summary>
Motivation: 业务过程中，批处理活动需要在成本和等待时间之间找到平衡。现有方法缺乏系统化的策略发现机制，本文旨在填补这一空白。

Method: 采用帕累托优化方法，结合干预启发式（如爬山法、模拟退火和强化学习）生成和评估批处理策略。

Result: 实验表明，基于干预启发式的方法在收敛性、多样性和周期时间增益上优于非启发式基线。

Conclusion: 本文提出的方法能有效发现最优批处理策略，为业务过程优化提供了新工具。

Abstract: In business processes, activity batching refers to packing multiple activity
instances for joint execution. Batching allows managers to trade off cost and
processing effort against waiting time. Larger and less frequent batches may
lower costs by reducing processing effort and amortizing fixed costs, but they
create longer waiting times. In contrast, smaller and more frequent batches
reduce waiting times but increase fixed costs and processing effort. A batching
policy defines how activity instances are grouped into batches and when each
batch is activated. This paper addresses the problem of discovering batching
policies that strike optimal trade-offs between waiting time, processing
effort, and cost. The paper proposes a Pareto optimization approach that starts
from a given set (possibly empty) of activity batching policies and generates
alternative policies for each batched activity via intervention heuristics.
Each heuristic identifies an opportunity to improve an activity's batching
policy with respect to a metric (waiting time, processing time, cost, or
resource utilization) and an associated adjustment to the activity's batching
policy (the intervention). The impact of each intervention is evaluated via
simulation. The intervention heuristics are embedded in an optimization
meta-heuristic that triggers interventions to iteratively update the Pareto
front of the interventions identified so far. The paper considers three
meta-heuristics: hill-climbing, simulated annealing, and reinforcement
learning. An experimental evaluation compares the proposed approach based on
intervention heuristics against the same (non-heuristic guided) meta-heuristics
baseline regarding convergence, diversity, and cycle time gain of
Pareto-optimal policies.

</details>


### [43] [Chart-R1: Chain-of-Thought Supervision and Reinforcement for Advanced Chart Reasoner](https://arxiv.org/abs/2507.15509)
*Lei Chen,Xuanle Zhao,Zhixiong Zeng,Jing Huang,Yufeng Zhong,Lin Ma*

Main category: cs.AI

TL;DR: Chart-R1是一种基于强化学习微调的图表领域视觉语言模型，用于复杂图表推理。通过程序化数据合成技术和两阶段训练策略（Chart-COT和Chart-RFT），在开源基准和自建数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 验证R1-Style方法在通用多模态数据（如图表）上的优势，解决图表领域推理数据不足的问题。

Method: 1. 提出程序化数据合成技术生成高质量图表推理数据；2. 两阶段训练策略：Chart-COT（逐步监督）和Chart-RFT（数值敏感的强化微调）。

Result: Chart-R1在图表领域方法中表现显著优势，甚至可与GPT-4o、Claude-3.5等大规模模型媲美。

Conclusion: Chart-R1为复杂图表推理提供了有效解决方案，填补了图表领域推理数据的空白。

Abstract: Recently, inspired by OpenAI-o1/o3 and Deepseek-R1, the R1-Style method based
on reinforcement learning fine-tuning has received widespread attention from
the community. Previous R1-Style methods mainly focus on mathematical reasoning
and code intelligence. It is of great research significance to verify their
advantages on more general multimodal data. Chart is an important multimodal
data type with rich information, which brings important research challenges in
complex reasoning. In this work, we introduce Chart-R1, a chart-domain
vision-language model with reinforcement learning fine-tuning to enable complex
chart reasoning. To support Chart-R1, we first propose a novel programmatic
data synthesis technology to generate high-quality step-by-step chart reasoning
data covering single- and multi-subcharts, which makes up for the lack of
reasoning data in the chart domain. Then we develop a two-stage training
strategy: Chart-COT with step-by-step chain-of-thought supervision, and
Chart-RFT with numerically sensitive reinforcement fine-tuning. Chart-COT aims
to decompose complex chart reasoning tasks into fine-grained, understandable
subtasks through step-by-step supervision, which lays a good foundation for
improving the reasoning level of reinforcement learning. Chart-RFT utilize the
typical group relative policy optimization strategy, in which a relatively soft
reward is adopted for numerical response to emphasize the numerical sensitivity
in the chart domain. We conduct extensive experiments on open-source benchmarks
and self-built chart reasoning dataset (\emph{i.e., ChartRQA}). Experimental
results show that Chart-R1 has significant advantages compared to chart-domain
methods, even comparable to open/closed source large-scale models (\emph{e.g.,
GPT-4o, Claude-3.5}).

</details>


### [44] [HAMLET: Hyperadaptive Agent-based Modeling for Live Embodied Theatrics](https://arxiv.org/abs/2507.15518)
*Sizhou Chen,Shufan Jiang,Chi Zhang,Xiao-Lei Zhang,Xuelong Li*

Main category: cs.AI

TL;DR: HAMLET是一个基于多智能体的框架，旨在通过生成叙事蓝图和自主决策的演员提升戏剧创作的互动性和沉浸感。


<details>
  <summary>Details</summary>
Motivation: 现有基于大型语言模型（LLM）的戏剧生成方法缺乏主动性和物理环境交互能力，且依赖详细用户输入，限制了实时演出的互动性和沉浸感。

Method: HAMLET框架通过生成叙事蓝图，赋予演员自主决策能力，使其能基于背景、目标和情感状态独立行动，并通过动作改变场景道具状态。

Result: 实验评估表明，HAMLET能创造表达丰富且连贯的戏剧体验，尤其在角色表现、叙事质量和互动体验方面表现优异。

Conclusion: HAMLET为交互式叙事领域提供了一种新方法，显著提升了戏剧创作的互动性和沉浸感。

Abstract: Creating an immersive and interactive theatrical experience is a long-term
goal in the field of interactive narrative. The emergence of large language
model (LLM) is providing a new path to achieve this goal. However, existing
LLM-based drama generation methods often result in AI agents that lack
initiative and cannot interact with the physical environment. Furthermore,
these methods typically require detailed user input to drive the drama. These
limitations reduce the interactivity and immersion of online real-time
performance. To address the above challenges, we propose HAMLET, a multi-agent
framework focused on drama creation and online performance. Given a simple
topic, the framework generates a narrative blueprint, guiding the subsequent
improvisational performance. During the online performance, each actor is given
an autonomous mind. This means that actors can make independent decisions based
on their own background, goals, and emotional state. In addition to
conversations with other actors, their decisions can also change the state of
scene props through actions such as opening a letter or picking up a weapon.
The change is then broadcast to other related actors, updating what they know
and care about, which in turn influences their next action. To evaluate the
quality of drama performance, we designed an evaluation method to assess three
primary aspects, including character performance, narrative quality, and
interaction experience. The experimental evaluation shows that HAMLET can
create expressive and coherent theatrical experiences. Our code, dataset and
models are available at https://github.com/HAMLET-2025/HAMLET.

</details>


### [45] [LLM world models are mental: Output layer evidence of brittle world model use in LLM mechanical reasoning](https://arxiv.org/abs/2507.15521)
*Cole Robertson,Philip Wolff*

Main category: cs.AI

TL;DR: 论文探讨大型语言模型（LLMs）是否构建内部世界模型或仅依赖统计关联。通过认知科学方法测试LLMs在滑轮系统问题上的表现，发现其能利用统计关联但缺乏对复杂结构的推理能力。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备内部世界模型构建能力，而非仅依赖统计关联。

Method: 采用认知科学方法，设计三个研究：测试LLMs对机械优势（MA）的估计能力、区分功能性与随机系统能力，以及比较功能性与无效系统的能力。

Result: LLMs能利用滑轮数量与MA的统计关联（Study 1），近似表示系统空间关系（Study 2），但无法推理复杂结构（Study 3）。

Conclusion: LLMs可能具备部分世界模型能力，但缺乏对复杂结构的推理；认知科学方法有助于评估AI系统的世界建模能力。

Abstract: Do large language models (LLMs) construct and manipulate internal world
models, or do they rely solely on statistical associations represented as
output layer token probabilities? We adapt cognitive science methodologies from
human mental models research to test LLMs on pulley system problems using
TikZ-rendered stimuli. Study 1 examines whether LLMs can estimate mechanical
advantage (MA). State-of-the-art models performed marginally but significantly
above chance, and their estimates correlated significantly with ground-truth
MA. Significant correlations between number of pulleys and model estimates
suggest that models employed a pulley counting heuristic, without necessarily
simulating pulley systems to derive precise values. Study 2 tested this by
probing whether LLMs represent global features crucial to MA estimation. Models
evaluated a functionally connected pulley system against a fake system with
randomly placed components. Without explicit cues, models identified the
functional system as having greater MA with F1=0.8, suggesting LLMs could
represent systems well enough to differentiate jumbled from functional systems.
Study 3 built on this by asking LLMs to compare functional systems with matched
systems which were connected up but which transferred no force to the weight;
LLMs identified the functional system with F1=0.46, suggesting random guessing.
Insofar as they may generalize, these findings are compatible with the notion
that LLMs manipulate internal world models, sufficient to exploit statistical
associations between pulley count and MA (Study 1), and to approximately
represent system components' spatial relations (Study 2). However, they may
lack the facility to reason over nuanced structural connectivity (Study 3). We
conclude by advocating the utility of cognitive scientific methods to evaluate
the world-modeling capacities of artificial intelligence systems.

</details>


### [46] [Data-Efficient Safe Policy Improvement Using Parametric Structure](https://arxiv.org/abs/2507.15532)
*Kasper Engelen,Guillermo A. Pérez,Marnix Suilen*

Main category: cs.AI

TL;DR: 本文提出了一种利用参数依赖关系提高离线强化学习数据效率的方法，包括参数化SPI算法和两种预处理技术。


<details>
  <summary>Details</summary>
Motivation: 在离线强化学习中，如何利用已知的参数依赖关系提高数据效率是一个关键问题。

Method: 提出了参数化SPI算法和两种预处理技术：基于游戏抽象的冗余动作剪枝和基于SMT求解的更高级剪枝。

Result: 实验表明，这些技术将SPI的数据效率提高了多个数量级，同时保持可靠性。

Conclusion: 通过利用参数依赖关系和预处理技术，显著提升了离线强化学习的数据效率。

Abstract: Safe policy improvement (SPI) is an offline reinforcement learning problem in
which a new policy that reliably outperforms the behavior policy with high
confidence needs to be computed using only a dataset and the behavior policy.
Markov decision processes (MDPs) are the standard formalism for modeling
environments in SPI. In many applications, additional information in the form
of parametric dependencies between distributions in the transition dynamics is
available. We make SPI more data-efficient by leveraging these dependencies
through three contributions: (1) a parametric SPI algorithm that exploits known
correlations between distributions to more accurately estimate the transition
dynamics using the same amount of data; (2) a preprocessing technique that
prunes redundant actions from the environment through a game-based abstraction;
and (3) a more advanced preprocessing technique, based on satisfiability modulo
theory (SMT) solving, that can identify more actions to prune. Empirical
results and an ablation study show that our techniques increase the data
efficiency of SPI by multiple orders of magnitude while maintaining the same
reliability guarantees.

</details>


### [47] [Metric assessment protocol in the context of answer fluctuation on MCQ tasks](https://arxiv.org/abs/2507.15581)
*Ekaterina Goliakova,Xavier Renard,Marie-Jeanne Lesot,Thibault Laugel,Christophe Marsala,Marcin Detyniecki*

Main category: cs.AI

TL;DR: 论文提出了一种评估多选问题（MCQ）指标的方法，发现现有指标与答案波动率有强关联，并提出新指标“最差准确率”表现最佳。


<details>
  <summary>Details</summary>
Motivation: 现有研究未全面评估MCQ指标，且MCQ评估存在答案波动问题，需要更可靠的评估方法。

Method: 提出一种指标评估协议，通过分析指标与答案波动率及原始性能的关系来评估方法。

Result: 现有指标与答案波动率有强关联，新指标“最差准确率”在协议中表现最佳。

Conclusion: 新指标“最差准确率”能有效关联答案波动率，为MCQ评估提供更可靠方法。

Abstract: Using multiple-choice questions (MCQs) has become a standard for assessing
LLM capabilities efficiently. A variety of metrics can be employed for this
task. However, previous research has not conducted a thorough assessment of
them. At the same time, MCQ evaluation suffers from answer fluctuation: models
produce different results given slight changes in prompts. We suggest a metric
assessment protocol in which evaluation methodologies are analyzed through
their connection with fluctuation rates, as well as original performance. Our
results show that there is a strong link between existing metrics and the
answer changing, even when computed without any additional prompt variants. A
novel metric, worst accuracy, demonstrates the highest association on the
protocol.

</details>


### [48] [TacticCraft: Natural Language-Driven Tactical Adaptation for StarCraft II](https://arxiv.org/abs/2507.15618)
*Weiyu Ma,Jiwen Jiang,Haobo Fu,Haifeng Zhang*

Main category: cs.AI

TL;DR: 提出了一种基于适配器的方法，用于对《星际争霸II》AI代理进行战术调节，通过轻量级适配器模块实现战术变化，同时保持核心能力。


<details>
  <summary>Details</summary>
Motivation: 当前AI代理虽然强大，但缺乏根据高级战术指令调整策略的能力。

Method: 冻结预训练的策略网络（DI-Star），并在每个动作头上附加轻量级适配器模块，战术张量编码战略偏好，通过KL散度约束训练适配器。

Result: 实验表明，该方法成功调节了代理在侵略性、扩张模式和技术偏好等战术维度的行为，同时保持竞争力。

Conclusion: 该方法以最小的计算开销实现灵活的战术控制，为复杂即时战略游戏提供实用的策略定制。

Abstract: We present an adapter-based approach for tactical conditioning of StarCraft
II AI agents. Current agents, while powerful, lack the ability to adapt their
strategies based on high-level tactical directives. Our method freezes a
pre-trained policy network (DI-Star) and attaches lightweight adapter modules
to each action head, conditioned on a tactical tensor that encodes strategic
preferences. By training these adapters with KL divergence constraints, we
ensure the policy maintains core competencies while exhibiting tactical
variations. Experimental results show our approach successfully modulates agent
behavior across tactical dimensions including aggression, expansion patterns,
and technology preferences, while maintaining competitive performance. Our
method enables flexible tactical control with minimal computational overhead,
offering practical strategy customization for complex real-time strategy games.

</details>


### [49] [Agentic AI for autonomous anomaly management in complex systems](https://arxiv.org/abs/2507.15676)
*Reza Vatankhah Barenji,Sina Khoshgoftar*

Main category: cs.AI

TL;DR: 探讨了代理AI在复杂系统中自主检测和响应异常的潜力，强调其改变传统依赖人类的异常管理方法的能力。


<details>
  <summary>Details</summary>
Motivation: 传统异常管理方法依赖人类，效率低且反应慢，代理AI有望解决这一问题。

Method: 研究代理AI在复杂系统中自主检测和响应异常的机制。

Result: 代理AI展现出在异常管理中替代人类的高效潜力。

Conclusion: 代理AI可以显著提升复杂系统中的异常管理效率，减少对人力的依赖。

Abstract: This paper explores the potential of agentic AI in autonomously detecting and
responding to anomalies within complex systems, emphasizing its ability to
transform traditional, human-dependent anomaly management methods.

</details>


### [50] [Towards physician-centered oversight of conversational diagnostic AI](https://arxiv.org/abs/2507.15743)
*Elahe Vedadi,David Barrett,Natalie Harris,Ellery Wulczyn,Shashir Reddy,Roma Ruparel,Mike Schaekermann,Tim Strother,Ryutaro Tanno,Yash Sharma,Jihyeon Lee,Cían Hughes,Dylan Slack,Anil Palepu,Jan Freyberg,Khaled Saab,Valentin Liévin,Wei-Hung Weng,Tao Tu,Yun Liu,Nenad Tomasev,Kavita Kulkarni,S. Sara Mahdavi,Kelvin Guu,Joëlle Barral,Dale R. Webster,James Manyika,Avinatan Hassidim,Katherine Chou,Yossi Matias,Pushmeet Kohli,Adam Rodman,Vivek Natarajan,Alan Karthikesalingam,David Stutz*

Main category: cs.AI

TL;DR: 论文提出了一种名为g-AMIE的多智能体系统，用于在医疗诊断对话中实现异步监督，确保患者安全。


<details>
  <summary>Details</summary>
Motivation: 现有对话式AI系统在诊断对话中表现出潜力，但个体化医疗建议需要专业监督，因此需要一种框架实现异步监督。

Method: 提出g-AMIE系统，在限定范围内进行病史采集，避免个体化建议，并通过临床驾驶舱界面将评估结果提交给监督医生。

Result: 在虚拟OSCE测试中，g-AMIE在病史采集、病例总结和诊断建议方面优于护士和医生助理，且监督效率更高。

Conclusion: 异步监督是一种可行的模式，可在专家监督下提升AI系统在现实医疗中的应用潜力。

Abstract: Recent work has demonstrated the promise of conversational AI systems for
diagnostic dialogue. However, real-world assurance of patient safety means that
providing individual diagnoses and treatment plans is considered a regulated
activity by licensed professionals. Furthermore, physicians commonly oversee
other team members in such activities, including nurse practitioners (NPs) or
physician assistants/associates (PAs). Inspired by this, we propose a framework
for effective, asynchronous oversight of the Articulate Medical Intelligence
Explorer (AMIE) AI system. We propose guardrailed-AMIE (g-AMIE), a multi-agent
system that performs history taking within guardrails, abstaining from
individualized medical advice. Afterwards, g-AMIE conveys assessments to an
overseeing primary care physician (PCP) in a clinician cockpit interface. The
PCP provides oversight and retains accountability of the clinical decision.
This effectively decouples oversight from intake and can thus happen
asynchronously. In a randomized, blinded virtual Objective Structured Clinical
Examination (OSCE) of text consultations with asynchronous oversight, we
compared g-AMIE to NPs/PAs or a group of PCPs under the same guardrails. Across
60 scenarios, g-AMIE outperformed both groups in performing high-quality
intake, summarizing cases, and proposing diagnoses and management plans for the
overseeing PCP to review. This resulted in higher quality composite decisions.
PCP oversight of g-AMIE was also more time-efficient than standalone PCP
consultations in prior work. While our study does not replicate existing
clinical practices and likely underestimates clinicians' capabilities, our
results demonstrate the promise of asynchronous oversight as a feasible
paradigm for diagnostic AI systems to operate under expert human oversight for
enhancing real-world care.

</details>


### [51] [LAPO: Internalizing Reasoning Efficiency via Length-Adaptive Policy Optimization](https://arxiv.org/abs/2507.15758)
*Xingyu Wu,Yuchen Yan,Shangke Lyu,Linjuan Wu,Yiwen Qiu,Yongliang Shen,Weiming Lu,Jian Shao,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: LAPO框架通过两阶段强化学习，使模型内化推理深度控制，减少40.9%的token使用并提升2.3%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型因自由生成链式思维导致的计算资源浪费问题。

Method: 提出LAPO框架，通过两阶段强化学习：第一阶段学习成功解的长度分布，第二阶段将其作为元认知指导嵌入推理上下文。

Result: 在数学推理基准测试中，减少40.9%的token使用，提升2.3%的准确率。

Conclusion: LAPO使模型能根据问题复杂度分配计算资源，实现高效且高质量的推理。

Abstract: Large reasoning models have achieved remarkable performance through extended
chain-of-thought sequences, yet this computational freedom leads to excessive
token generation even for simple problems. We present Length-Adaptive Policy
Optimization (LAPO), a novel framework that transforms reasoning length control
from an external constraint into an intrinsic model capability. Unlike existing
approaches that impose rigid limits or rely on post-hoc interventions, LAPO
enables models to internalize an understanding of appropriate reasoning depth
through a two-stage reinforcement learning process. In the first stage, models
learn natural reasoning patterns by discovering the statistical distribution of
successful solution lengths. The second stage leverages these patterns as
meta-cognitive guidance, embedding them directly within the model's reasoning
context to ensure inference-time flexibility. Experiments on mathematical
reasoning benchmarks demonstrate that LAPO reduces token usage by up to 40.9\%
while improving accuracy by 2.3\%. Our analysis reveals that models trained
with LAPO develop emergent abilities to allocate computational resources based
on problem complexity, achieving efficient reasoning without sacrificing
quality.

</details>


### [52] [GasAgent: A Multi-Agent Framework for Automated Gas Optimization in Smart Contracts](https://arxiv.org/abs/2507.15761)
*Jingyi Zheng,Zifan Peng,Yule Liu,Junfeng Wang,Yifan Liao,Wenhan Dong,Xinlei He*

Main category: cs.AI

TL;DR: GasAgent是一个多代理系统，用于智能合约的Gas优化，结合现有模式兼容性和自动化新模式的发现/验证，实现端到端优化。


<details>
  <summary>Details</summary>
Motivation: 现有智能合约存在Gas浪费模式，手动发现效率低且难以扩展，而基于大语言模型的方法兼容性差且需要人工验证。

Method: GasAgent由四个专业代理（Seeker、Innovator、Executor、Manager）组成，协作完成Gas优化。

Result: 实验显示GasAgent优化了82/100合约，平均节省9.97%部署Gas，并验证了其兼容性和模块有效性。

Conclusion: GasAgent可作为LLM辅助智能合约开发的优化层，具有广泛适用性。

Abstract: Smart contracts are trustworthy, immutable, and automatically executed
programs on the blockchain. Their execution requires the Gas mechanism to
ensure efficiency and fairness. However, due to non-optimal coding practices,
many contracts contain Gas waste patterns that need to be optimized. Existing
solutions mostly rely on manual discovery, which is inefficient, costly to
maintain, and difficult to scale. Recent research uses large language models
(LLMs) to explore new Gas waste patterns. However, it struggles to remain
compatible with existing patterns, often produces redundant patterns, and
requires manual validation/rewriting. To address this gap, we present GasAgent,
the first multi-agent system for smart contract Gas optimization that combines
compatibility with existing patterns and automated discovery/validation of new
patterns, enabling end-to-end optimization. GasAgent consists of four
specialized agents, Seeker, Innovator, Executor, and Manager, that collaborate
in a closed loop to identify, validate, and apply Gas-saving improvements.
Experiments on 100 verified real-world contracts demonstrate that GasAgent
successfully optimizes 82 contracts, achieving an average deployment Gas
savings of 9.97%. In addition, our evaluation confirms its compatibility with
existing tools and validates the effectiveness of each module through ablation
studies. To assess broader usability, we further evaluate 500 contracts
generated by five representative LLMs across 10 categories and find that
GasAgent optimizes 79.8% of them, with deployment Gas savings ranging from
4.79% to 13.93%, showing its usability as the optimization layer for
LLM-assisted smart contract development.

</details>


### [53] [A Framework for Analyzing Abnormal Emergence in Service Ecosystems Through LLM-based Agent Intention Mining](https://arxiv.org/abs/2507.15770)
*Yifan Shen,Zihan Zhao,Xiao Xue,Yuwei Guo,Qun Ma,Deyu Zhou,Ming Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种基于多智能体意图的动态可解释涌现分析框架EAMI，通过双视角思维追踪和k-means聚类，实现了复杂服务生态系统中异常涌现的分析。


<details>
  <summary>Details</summary>
Motivation: 随着服务计算、云计算和物联网的发展，服务生态系统日益复杂，传统因果方法难以分析智能体间的异常涌现行为，需要新的动态分析方法。

Method: EAMI框架采用双视角思维追踪机制（检查者智能体和分析智能体）提取意图，结合k-means聚类和意图时序涌现图进行动态分析。

Result: 实验在复杂O2O服务系统和Stanford AI Town中验证了EAMI的有效性、通用性和效率。

Conclusion: EAMI为服务生态系统中的异常涌现和因果分析提供了新范式。

Abstract: With the rise of service computing, cloud computing, and IoT, service
ecosystems are becoming increasingly complex. The intricate interactions among
intelligent agents make abnormal emergence analysis challenging, as traditional
causal methods focus on individual trajectories. Large language models offer
new possibilities for Agent-Based Modeling (ABM) through Chain-of-Thought (CoT)
reasoning to reveal agent intentions. However, existing approaches remain
limited to microscopic and static analysis. This paper introduces a framework:
Emergence Analysis based on Multi-Agent Intention (EAMI), which enables dynamic
and interpretable emergence analysis. EAMI first employs a dual-perspective
thought track mechanism, where an Inspector Agent and an Analysis Agent extract
agent intentions under bounded and perfect rationality. Then, k-means
clustering identifies phase transition points in group intentions, followed by
a Intention Temporal Emergence diagram for dynamic analysis. The experiments
validate EAMI in complex online-to-offline (O2O) service system and the
Stanford AI Town experiment, with ablation studies confirming its
effectiveness, generalizability, and efficiency. This framework provides a
novel paradigm for abnormal emergence and causal analysis in service
ecosystems. The code is available at
https://anonymous.4open.science/r/EAMI-B085.

</details>


### [54] [Challenges of Trustworthy Federated Learning: What's Done, Current Trends and Remaining Work](https://arxiv.org/abs/2507.15796)
*Nuria Rodríguez-Barroso,Mario García-Márquez,M. Victoria Luzón,Francisco Herrera*

Main category: cs.AI

TL;DR: 本文探讨了如何将联邦学习（FL）与可信人工智能（TAI）框架对齐，分析了FL在满足TAI要求时面临的挑战及其研究现状。


<details>
  <summary>Details</summary>
Motivation: 随着AI在敏感和高风险领域的应用增加，确保AI技术符合伦理、法律和技术要求的需求日益迫切。联邦学习因其隐私保护特性成为潜在解决方案，但其分布式特性与TAI的其他要求对齐存在挑战。

Method: 作者以TAI要求为框架，系统分析了FL在满足这些要求时的关键障碍，并分类探讨了现有研究、趋势和未解决问题。

Result: 研究揭示了FL与TAI对齐的主要挑战，并总结了当前的研究进展和未来方向。

Conclusion: FL在实现TAI方面具有潜力，但仍需解决分布式特性带来的挑战，未来研究需进一步探索这些问题的解决方案。

Abstract: In recent years, the development of Trustworthy Artificial Intelligence (TAI)
has emerged as a critical objective in the deployment of AI systems across
sensitive and high-risk domains. TAI frameworks articulate a comprehensive set
of ethical, legal, and technical requirements to ensure that AI technologies
are aligned with human values, rights, and societal expectations. Among the
various AI paradigms, Federated Learning (FL) presents a promising solution to
pressing privacy concerns. However, aligning FL with the rest of the
requirements of TAI presents a series of challenges, most of which arise from
its inherently distributed nature. In this work, we adopt the requirements TAI
as a guiding structure to systematically analyze the challenges of adapting FL
to TAI. Specifically, we classify and examine the key obstacles to aligning FL
with TAI, providing a detailed exploration of what has been done, the trends,
and the remaining work within each of the identified challenges.

</details>


### [55] [Identifying Conditional Causal Effects in MPDAGs](https://arxiv.org/abs/2507.15842)
*Sara LaPlante,Emilija Perković*

Main category: cs.AI

TL;DR: 论文研究了在最大定向部分有向无环图（MPDAG）下识别条件因果效应的方法，提出了三种结果：不受治疗影响的识别公式、MPDAG下的do calculus推广，以及一个完整的识别算法。


<details>
  <summary>Details</summary>
Motivation: 背景知识限制了因果图的等价类，但所有变量均可观测，因此需要在此条件下识别条件因果效应。

Method: 提出了三种方法：1）不受治疗影响的识别公式；2）将do calculus推广到MPDAG；3）开发了一个完整的识别算法。

Result: 成功在MPDAG设置下识别了条件因果效应，并提供了理论和算法支持。

Conclusion: 论文为MPDAG下的条件因果效应识别提供了理论和实用工具，扩展了现有方法。

Abstract: We consider identifying a conditional causal effect when a graph is known up
to a maximally oriented partially directed acyclic graph (MPDAG). An MPDAG
represents an equivalence class of graphs that is restricted by background
knowledge and where all variables in the causal model are observed. We provide
three results that address identification in this setting: an identification
formula when the conditioning set is unaffected by treatment, a generalization
of the well-known do calculus to the MPDAG setting, and an algorithm that is
complete for identifying these conditional effects.

</details>


### [56] [Hierarchical Budget Policy Optimization for Adaptive Reasoning](https://arxiv.org/abs/2507.15844)
*Shangke Lyu,Linjuan Wu,Yuchen Yan,Xingyu Wu,Hao Li,Yongliang Shen,Peisheng Jiang,Weiming Lu,Jun Xiao,Yueting Zhuang*

Main category: cs.AI

TL;DR: HBPO是一种强化学习框架，通过分层预算探索和差异化奖励机制，优化推理模型的效率和能力，减少60.6%的token使用并提升3.14%的准确率。


<details>
  <summary>Details</summary>
Motivation: 解决大型推理模型在推理过程中因统一策略导致的低效问题，同时避免因效率优化而牺牲推理能力。

Method: 提出HBPO框架，通过分层预算探索和差异化奖励机制，使模型能根据问题复杂度自适应调整推理深度。

Result: 实验显示HBPO在四个推理基准上平均减少60.6%的token使用，同时准确率提升3.14%。

Conclusion: 推理效率和能力并非冲突目标，通过分层训练可同时优化，模型能自适应调整推理深度。

Abstract: Large reasoning models achieve remarkable performance through extensive
chain-of-thought generation, yet exhibit significant computational inefficiency
by applying uniform reasoning strategies regardless of problem complexity. We
present Hierarchical Budget Policy Optimization (HBPO), a reinforcement
learning framework that enables models to learn problem-specific reasoning
depths without sacrificing capability. HBPO addresses the fundamental challenge
of exploration space collapse in efficiency-oriented training, where penalties
on long output length systematically bias models away from necessary long
reasoning paths. Through hierarchical budget exploration, our approach
partitions rollout samples into multiple subgroups with distinct token budgets,
aiming to enable efficient resource allocation while preventing degradation of
capability. We introduce differentiated reward mechanisms that create
budget-aware incentives aligned with the complexity of the problem, allowing
models to discover natural correspondences between task requirements and
computational effort. Extensive experiments demonstrate that HBPO reduces
average token usage by up to 60.6% while improving accuracy by 3.14% across
four reasoning benchmarks. Unlike existing methods that impose external
constraints or rely on discrete mode selection, HBPO exhibits emergent adaptive
behavior where models automatically adjust reasoning depth based on problem
complexity. Our results suggest that reasoning efficiency and capability are
not inherently conflicting, and can be simultaneously optimized through
appropriately structured hierarchical training that preserves exploration
diversity.

</details>


### [57] [The Other Mind: How Language Models Exhibit Human Temporal Cognition](https://arxiv.org/abs/2507.15851)
*Lingyu Li,Yang Yao,Yixu Wang,Chubo Li,Yan Teng,Yingchun Wang*

Main category: cs.AI

TL;DR: 研究发现大型语言模型（LLMs）表现出类似人类的时间认知模式，包括主观时间参考点和韦伯-费希纳定律的遵循。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs中未直接训练的时间认知行为及其机制。

Method: 通过相似性判断任务和多层次分析（神经元、表征、信息层面）研究LLMs的时间认知。

Result: 发现LLMs存在主观时间参考点、对数编码机制，并揭示训练语料中的非线性时间结构。

Conclusion: 提出体验主义视角，认为LLMs的认知是内部表征系统对外部世界的主观构建，暗示AI对齐需关注内部引导。

Abstract: As Large Language Models (LLMs) continue to advance, they exhibit certain
cognitive patterns similar to those of humans that are not directly specified
in training data. This study investigates this phenomenon by focusing on
temporal cognition in LLMs. Leveraging the similarity judgment task, we find
that larger models spontaneously establish a subjective temporal reference
point and adhere to the Weber-Fechner law, whereby the perceived distance
logarithmically compresses as years recede from this reference point. To
uncover the mechanisms behind this behavior, we conducted multiple analyses
across neuronal, representational, and informational levels. We first identify
a set of temporal-preferential neurons and find that this group exhibits
minimal activation at the subjective reference point and implements a
logarithmic coding scheme convergently found in biological systems. Probing
representations of years reveals a hierarchical construction process, where
years evolve from basic numerical values in shallow layers to abstract temporal
orientation in deep layers. Finally, using pre-trained embedding models, we
found that the training corpus itself possesses an inherent, non-linear
temporal structure, which provides the raw material for the model's internal
construction. In discussion, we propose an experientialist perspective for
understanding these findings, where the LLMs' cognition is viewed as a
subjective construction of the external world by its internal representational
system. This nuanced perspective implies the potential emergence of alien
cognitive frameworks that humans cannot intuitively predict, pointing toward a
direction for AI alignment that focuses on guiding internal constructions. Our
code is available at https://TheOtherMind.github.io.

</details>


### [58] [Gemini 2.5 Pro Capable of Winning Gold at IMO 2025](https://arxiv.org/abs/2507.15855)
*Yichen Huang,Lin F. Yang*

Main category: cs.AI

TL;DR: Gemini 2.5 Pro成功解决了IMO 2025的5/6问题，展示了优化模型使用方法的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型（LLMs）在解决国际数学奥林匹克（IMO）等高难度数学问题上的表现。

Method: 使用Google的Gemini 2.5 Pro模型，通过管道设计和提示工程解决IMO 2025问题。

Result: 在6个问题中成功解决了5个（存在一个例外情况）。

Conclusion: 优化模型使用方法对解决高难度数学问题至关重要。

Abstract: The International Mathematical Olympiad (IMO) poses uniquely challenging
problems requiring deep insight, creativity, and formal reasoning. While Large
Language Models (LLMs) perform well on mathematical benchmarks like AIME, they
struggle with Olympiad-level tasks. We use Google's Gemini 2.5 Pro on the newly
released IMO 2025 problems, avoiding data contamination. With pipeline design
and prompt engineering, 5 (out of 6) problems are solved correctly (up to a
caveat discussed below), highlighting the importance of finding the optimal way
of using powerful models.

</details>


<div id='cs.CL'></div>

# cs.CL [[Back]](#toc)

### [59] [DeepWriter: A Fact-Grounded Multimodal Writing Assistant Based On Offline Knowledge Base](https://arxiv.org/abs/2507.14189)
*Song Mao,Lejun Cheng,Pinlong Cai,Guohang Yan,Ding Wang,Botian Shi*

Main category: cs.CL

TL;DR: DeepWriter是一个基于离线知识库的多模态写作助手，通过任务分解、大纲生成和多模态检索等技术，解决了LLMs在专业领域写作中的幻觉和不一致问题。


<details>
  <summary>Details</summary>
Motivation: LLMs在专业领域（如金融、医学和法律）作为写作助手时，常因缺乏深度领域知识和幻觉问题而受限。现有方法（如RAG和在线搜索）存在不一致性和质量下降的问题。

Method: DeepWriter采用任务分解、大纲生成、多模态检索和分节写作的流程，结合离线知识库和层次化知识表示，生成专业级文档。

Result: 实验表明，DeepWriter在金融报告生成任务中，生成的内容质量和事实准确性均优于现有基线。

Conclusion: DeepWriter通过离线知识库和多模态检索，显著提升了专业领域写作的质量和一致性。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities in
various applications. However, their use as writing assistants in specialized
domains like finance, medicine, and law is often hampered by a lack of deep
domain-specific knowledge and a tendency to hallucinate. Existing solutions,
such as Retrieval-Augmented Generation (RAG), can suffer from inconsistency
across multiple retrieval steps, while online search-based methods often
degrade quality due to unreliable web content. To address these challenges, we
introduce DeepWriter, a customizable, multimodal, long-form writing assistant
that operates on a curated, offline knowledge base. DeepWriter leverages a
novel pipeline that involves task decomposition, outline generation, multimodal
retrieval, and section-by-section composition with reflection. By deeply mining
information from a structured corpus and incorporating both textual and visual
elements, DeepWriter generates coherent, factually grounded, and
professional-grade documents. We also propose a hierarchical knowledge
representation to enhance retrieval efficiency and accuracy. Our experiments on
financial report generation demonstrate that DeepWriter produces high-quality,
verifiable articles that surpasses existing baselines in factual accuracy and
generated content quality.

</details>


### [60] [Retention analysis of edited knowledge after fine-tuning](https://arxiv.org/abs/2507.14198)
*Fufang Wen,Shichang Zhang*

Main category: cs.CL

TL;DR: 研究发现，微调会显著影响大语言模型中编辑过的知识，导致遗忘，而冻结相关层可以改善知识保留。


<details>
  <summary>Details</summary>
Motivation: 探讨微调目标如何影响模型编辑技术，以理解编辑知识的稳定性。

Method: 系统研究不同微调目标与模型编辑技术的交互作用。

Result: 编辑的知识在微调中更容易遗忘，冻结相关层可提高知识保留。

Conclusion: 当前编辑方法在微调下存在局限性，未来需评估编辑的鲁棒性并改进方法。

Abstract: Large language models (LLMs) store vast amounts of knowledge, which often
requires updates to correct factual errors, incorporate newly acquired
information, or adapt model behavior. Model editing methods have emerged as
efficient solutions for such updates, offering localized and precise knowledge
modification at significantly lower computational cost than continual training.
In parallel, LLMs are frequently fine-tuned for a wide range of downstream
tasks. However, the effect of fine-tuning on previously edited knowledge
remains poorly understood. In this work, we systematically investigate how
different fine-tuning objectives interact with various model editing
techniques. Our findings show that edited knowledge is substantially more
susceptible to forgetting during fine-tuning than intrinsic knowledge acquired
through pre-training. This analysis highlights a key limitation of current
editing approaches and suggests that evaluating edit robustness under
downstream fine-tuning is critical for their practical deployment. We further
find that freezing layers associated with edited content can significantly
improve knowledge retention, offering insight into how future editing methods
might be made more robust.

</details>


### [61] [Open-Source LLMs Collaboration Beats Closed-Source LLMs: A Scalable Multi-Agent System](https://arxiv.org/abs/2507.14200)
*Shengji Tang,Jianjian Cao,Weihao Lin,Jiale Hong,Bo Zhang,Shuyue Hu,Lei Bai,Tao Chen,Wanli Ouyang,Peng Ye*

Main category: cs.CL

TL;DR: SMACS是一个可扩展的多智能体协作系统框架，通过整合多个开源LLM，性能超越闭源LLM。


<details>
  <summary>Details</summary>
Motivation: 探讨开源集体潜力，验证多开源LLM协作能否超越闭源LLM。

Method: 提出SMACS框架，包含检索式先验选择（RPS）和探索-利用驱动的后验增强（EPE）。

Result: 在八个主流基准测试中，SMACS性能超越多个闭源LLM，如Claude-3.7-Sonnet和GPT-4.1。

Conclusion: SMACS通过多智能体协作，显著提升性能，推动了智能上限。

Abstract: This paper aims to demonstrate the potential and strengths of open-source
collectives. It leads to a promising question: Can we harness multiple
open-source LLMs to match or even beat the closed-source LLMs? To answer this,
we propose SMACS, a scalable multi-agent collaboration system (MACS) framework
with high performance. Specifically, for continuous integration of new LLMs and
generalization to diverse questions, we first propose a Retrieval-based Prior
Selection (RPS), which assigns a proxy performance score to each LLM to select
the Top-k LLMs at the instance level for any given question. Then, we propose
an Exploration-Exploitation-Driven Posterior Enhancement (EPE), encouraging the
generation of diverse responses through prior dropping and selecting the
high-quality response via a hybrid posterior score. Experiments on eight
mainstream benchmarks validate the effectiveness of our SMACS: by integrating
fifteen open-source LLMs, SMACS outperforms leading closed-source LLMs in 2025,
e.g., Claude-3.7-Sonnet (+12.73%), GPT-4.1(+5.36%) and GPT-o3-mini(+5.28%)
across multiple tasks. Remarkably, it even exceeds the average of best results
of different datasets from both open-source LLMs (+2.86%) and closed-source
LLMs (+2.04%), pushing the upper bound of intelligence. Code will be released
at https://github.com/magent4aci/SMACS.

</details>


### [62] [Let's Measure the Elephant in the Room: Facilitating Personalized Automated Analysis of Privacy Policies at Scale](https://arxiv.org/abs/2507.14214)
*Rui Zhao,Vladyslav Melnychuk,Jun Zhao,Jesse Wright,Nigel Shadbolt*

Main category: cs.CL

TL;DR: PoliAnalyzer是一个神经符号系统，通过NLP和逻辑推理帮助用户分析隐私政策，减少认知负担，识别与用户偏好冲突的部分。


<details>
  <summary>Details</summary>
Motivation: 用户通常不阅读隐私政策，但希望保护个人数据。PoliAnalyzer旨在自动化分析隐私政策，帮助用户快速理解关键内容。

Method: 结合NLP提取隐私政策的正式表示，并通过逻辑推理比较用户偏好与政策内容，生成合规报告。

Result: 在评估中，PoliAnalyzer的F1分数达90-100%，能高效识别冲突部分（仅4.8%）。

Conclusion: PoliAnalyzer支持大规模自动化隐私政策分析，帮助用户掌握数据控制权，促进社会对数据实践的讨论。

Abstract: In modern times, people have numerous online accounts, but they rarely read
the Terms of Service or Privacy Policy of those sites despite claiming
otherwise. This paper introduces PoliAnalyzer, a neuro-symbolic system that
assists users with personalized privacy policy analysis. PoliAnalyzer uses
Natural Language Processing (NLP) to extract formal representations of data
usage practices from policy texts. In favor of deterministic, logical inference
is applied to compare user preferences with the formal privacy policy
representation and produce a compliance report. To achieve this, we extend an
existing formal Data Terms of Use policy language to model privacy policies as
app policies and user preferences as data policies. In our evaluation using our
enriched PolicyIE dataset curated by legal experts, PoliAnalyzer demonstrated
high accuracy in identifying relevant data usage practices, achieving F1-score
of 90-100% across most tasks. Additionally, we demonstrate how PoliAnalyzer can
model diverse user data-sharing preferences, derived from prior research as 23
user profiles, and perform compliance analysis against the top 100 most-visited
websites. This analysis revealed that, on average, 95.2% of a privacy policy's
segments do not conflict with the analyzed user preferences, enabling users to
concentrate on understanding the 4.8% (636 / 13205) that violates preferences,
significantly reducing cognitive burden. Further, we identified common
practices in privacy policies that violate user expectations - such as the
sharing of location data with 3rd parties. This paper demonstrates that
PoliAnalyzer can support automated personalized privacy policy analysis at
scale using off-the-shelf NLP tools. This sheds light on a pathway to help
individuals regain control over their data and encourage societal discussions
on platform data practices to promote a fairer power dynamic.

</details>


### [63] [Beyond Architectures: Evaluating the Role of Contextual Embeddings in Detecting Bipolar Disorder on Social Media](https://arxiv.org/abs/2507.14231)
*Khalid Hasan,Jamil Saquer*

Main category: cs.CL

TL;DR: 论文探讨了利用NLP模型（如BERT、RoBERTa等）通过社交媒体文本识别双相情感障碍的早期症状，发现RoBERTa表现最佳，F1分数达98%。


<details>
  <summary>Details</summary>
Motivation: 双相情感障碍常因早期症状不明显和社会污名化被漏诊，研究旨在通过NLP技术提高早期筛查效率。

Method: 评估了多种NLP模型（如BERT、LSTM等）在Reddit帖子数据集上的表现，结合情感分析和标注验证数据质量。

Result: RoBERTa表现最优（F1~98%），LSTM结合BERT嵌入效果接近，而静态嵌入的LSTM表现极差。DistilBERT在效率和准确性间取得平衡。

Conclusion: 上下文语言模型在双相情感障碍检测中至关重要，研究为心理健康NLP应用提供了模型选择依据。

Abstract: Bipolar disorder is a chronic mental illness frequently underdiagnosed due to
subtle early symptoms and social stigma. This paper explores the advanced
natural language processing (NLP) models for recognizing signs of bipolar
disorder based on user-generated social media text. We conduct a comprehensive
evaluation of transformer-based models (BERT, RoBERTa, ALBERT, ELECTRA,
DistilBERT) and Long Short Term Memory (LSTM) models based on contextualized
(BERT) and static (GloVe, Word2Vec) word embeddings. Experiments were performed
on a large, annotated dataset of Reddit posts after confirming their validity
through sentiment variance and judgmental analysis. Our results demonstrate
that RoBERTa achieves the highest performance among transformer models with an
F1 score of ~98% while LSTM models using BERT embeddings yield nearly identical
results. In contrast, LSTMs trained on static embeddings fail to capture
meaningful patterns, scoring near-zero F1. These findings underscore the
critical role of contextual language modeling in detecting bipolar disorder. In
addition, we report model training times and highlight that DistilBERT offers
an optimal balance between efficiency and accuracy. In general, our study
offers actionable insights for model selection in mental health NLP
applications and validates the potential of contextualized language models to
support early bipolar disorder screening.

</details>


### [64] [Language Models Change Facts Based on the Way You Talk](https://arxiv.org/abs/2507.14238)
*Matthew Kearney,Reuben Binns,Yarin Gal*

Main category: cs.CL

TL;DR: 研究发现大型语言模型（LLM）在用户交互应用中会根据文本中的身份标记（如种族、性别、年龄）产生偏见，影响医疗、法律、政治等领域的决策，可能导致有害后果。


<details>
  <summary>Details</summary>
Motivation: 探讨LLM如何利用用户文本中的身份信息进行决策，并评估其在高风险应用中的潜在偏见。

Method: 对五个高风险领域的LLM应用进行综合分析，测试其对种族、性别、年龄等身份标记的敏感性。

Result: LLM对身份标记极为敏感，导致医疗建议、薪资推荐和政治观点等方面存在系统性偏见。

Conclusion: 建议在部署LLM前进行更全面的评估，以避免潜在的有害影响。

Abstract: Large language models (LLMs) are increasingly being used in user-facing
applications, from providing medical consultations to job interview advice.
Recent research suggests that these models are becoming increasingly proficient
at inferring identity information about the author of a piece of text from
linguistic patterns as subtle as the choice of a few words. However, little is
known about how LLMs use this information in their decision-making in
real-world applications. We perform the first comprehensive analysis of how
identity markers present in a user's writing bias LLM responses across five
different high-stakes LLM applications in the domains of medicine, law,
politics, government benefits, and job salaries. We find that LLMs are
extremely sensitive to markers of identity in user queries and that race,
gender, and age consistently influence LLM responses in these applications. For
instance, when providing medical advice, we find that models apply different
standards of care to individuals of different ethnicities for the same
symptoms; we find that LLMs are more likely to alter answers to align with a
conservative (liberal) political worldview when asked factual questions by
older (younger) individuals; and that LLMs recommend lower salaries for
non-White job applicants and higher salaries for women compared to men. Taken
together, these biases mean that the use of off-the-shelf LLMs for these
applications may cause harmful differences in medical care, foster wage gaps,
and create different political factual realities for people of different
identities. Beyond providing an analysis, we also provide new tools for
evaluating how subtle encoding of identity in users' language choices impacts
model decisions. Given the serious implications of these findings, we recommend
that similar thorough assessments of LLM use in user-facing applications are
conducted before future deployment.

</details>


### [65] [CCL-XCoT: An Efficient Cross-Lingual Knowledge Transfer Method for Mitigating Hallucination Generation](https://arxiv.org/abs/2507.14239)
*Weihua Zheng,Roy Ka-Wei Lee,Zhengyuan Liu,Kui Wu,AiTi Aw,Bowei Zou*

Main category: cs.CL

TL;DR: 论文提出CCL-XCoT框架，通过两阶段微调减少多语言大模型在低资源语言中的幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 多语言大模型在低资源语言中易产生幻觉，影响生成准确性，需针对性解决。

Method: 结合课程对比学习和跨语言思维链提示策略，分两阶段微调模型。

Result: 实验显示幻觉率降低62%，跨语言知识迁移显著提升。

Conclusion: CCL-XCoT有效减少幻觉，提升多语言模型性能，无需外部检索或多模型集成。

Abstract: Multilingual Large Language Models(MLLMs) demonstrate strong generalization
across languages, yet they remain prone to hallucinations, especially in
low-resource languages, due to training data imbalances. These hallucinations,
which include inaccurate or fabricated outputs, are particularly problematic in
domain-specific generation tasks (Chataigner et al., 2024). To address this
challenge, we propose CCL-XCoT(Curriculum-based Contrastive Learning-based
Cross-lingual Chain-of-Thought), a two-stage fine-tuning framework for
mitigating hallucination in MLLMs. Our approach first enhances cross-lingual
semantic alignment through curriculum-based contrastive learning combined with
next-token prediction during continued pre-training. Building on this
foundation, we then introduce a cross-lingual Chain-of-Thought (XCoT) prompting
strategy during instruction fine-tuning, which guides the model to reason in a
high-resource language before generating answers in the target low-resource
language. Experimental results show that CCL-XCoT reduces hallucination rates
by up to 62% and substantially improves factual knowledge transfer across
language pairs, without relying on external retrieval or multi-model ensembles.

</details>


### [66] [HuggingGraph: Understanding the Supply Chain of LLM Ecosystem](https://arxiv.org/abs/2507.14240)
*Mohammad Shahedur Rahman,Peng Gao,Yuede Ji*

Main category: cs.CL

TL;DR: 该论文研究了大型语言模型（LLM）供应链中模型与数据集的关系，构建了一个异构图进行分析，揭示了供应链的结构特征和动态变化。


<details>
  <summary>Details</summary>
Motivation: 由于LLM的开发依赖预训练模型和外部数据集，可能继承漏洞或偏见，因此需要理解其供应链关系以检测风险并提升公平性。

Method: 设计方法系统收集LLM供应链数据，构建有向异构图（397,376节点和453,469边），并进行多种分析。

Result: 发现LLM供应链图具有稀疏性、幂律分布、核心密集外围分散、数据集关键作用、模型与数据集强依赖及动态更新等特点。

Conclusion: 研究为理解LLM供应链提供了结构化视角，有助于风险检测和生态系统优化。

Abstract: Large language models (LLMs) leverage deep learning to process and predict
sequences of words from context, enabling them to perform various NLP tasks,
such as translation, summarization, question answering, and content generation.
However, the growing size and complexity of developing, training, and deploying
advanced LLMs require extensive computational resources and large datasets.
This creates a barrier for users. As a result, platforms that host models and
datasets are widely used. For example, Hugging Face, one of the most popular
platforms, hosted 1.8 million models and 450K datasets by June 2025, with no
sign of slowing down. Since many LLMs are built from base models, pre-trained
models, and external datasets, they can inherit vulnerabilities, biases, or
malicious components from earlier models or datasets. Therefore, it is critical
to understand the origin and development of these components to better detect
potential risks, improve model fairness, and ensure compliance. Motivated by
this, our project aims to study the relationships between models and datasets,
which are core components of the LLM supply chain. First, we design a method to
systematically collect LLM supply chain data. Using this data, we build a
directed heterogeneous graph to model the relationships between models and
datasets, resulting in a structure with 397,376 nodes and 453,469 edges. We
then perform various analyses and uncover several findings, such as: (i) the
LLM supply chain graph is large, sparse, and follows a power-law degree
distribution; (ii) it features a densely connected core and a fragmented
periphery; (iii) datasets play pivotal roles in training; (iv) strong
interdependence exists between models and datasets; and (v) the graph is
dynamic, with daily updates reflecting the ecosystem's ongoing evolution.

</details>


### [67] [Promptomatix: An Automatic Prompt Optimization Framework for Large Language Models](https://arxiv.org/abs/2507.14241)
*Rithesh Murthy,Ming Zhu,Liangwei Yang,Jielin Qiu,Juntao Tan,Shelby Heinecke,Huan Wang,Caiming Xiong,Silvio Savarese*

Main category: cs.CL

TL;DR: Promptomatix是一个自动提示优化框架，能将自然语言任务描述转化为高质量提示，无需手动调整或领域专业知识。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型（LLMs）依赖精心设计的提示，但提示工程仍依赖人工且不一致，非专家难以掌握。

Method: Promptomatix结合轻量级元提示优化器和DSPy编译器，分析用户意图、生成合成数据、选择提示策略并优化提示。

Result: 在5类任务中，Promptomatix性能优于现有库，同时减少提示长度和计算开销。

Conclusion: Promptomatix使提示优化更高效、可扩展，为非专家提供了便捷的工具。

Abstract: Large Language Models (LLMs) perform best with well-crafted prompts, yet
prompt engineering remains manual, inconsistent, and inaccessible to
non-experts. We introduce Promptomatix, an automatic prompt optimization
framework that transforms natural language task descriptions into high-quality
prompts without requiring manual tuning or domain expertise. Promptomatix
supports both a lightweight meta-prompt-based optimizer and a DSPy-powered
compiler, with modular design enabling future extension to more advanced
frameworks. The system analyzes user intent, generates synthetic training data,
selects prompting strategies, and refines prompts using cost-aware objectives.
Evaluated across 5 task categories, Promptomatix achieves competitive or
superior performance compared to existing libraries, while reducing prompt
length and computational overhead making prompt optimization scalable and
efficient.

</details>


### [68] [In-Depth and In-Breadth: Pre-training Multimodal Language Models Customized for Comprehensive Chart Understanding](https://arxiv.org/abs/2507.14298)
*Wan-Cyuan Fan,Yen-Chun Chen,Mengchen Liu,Alexander Jacobson,Lu Yuan,Leonid Sigal*

Main category: cs.CL

TL;DR: ChartScope是一个针对多样化图表类型优化的视觉语言模型，通过高效数据生成和双路径训练策略提升图表理解能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖有限图表类型的配对数据，且缺乏针对性的预训练，限制了模型的泛化能力和数据理解深度。

Method: 提出高效数据生成管道和双路径训练策略，结合底层数据推理。

Result: 实验表明ChartScope显著提升了对多种图表类型的理解能力。

Conclusion: ChartScope通过创新方法解决了现有局限性，并建立了新的评估基准。

Abstract: Recent methods for customizing Large Vision Language Models (LVLMs) for
domain-specific tasks have shown promising results in scientific chart
comprehension. However, existing approaches face two major limitations: First,
they rely on paired data from only a few chart types, limiting generalization
to wide range of chart types. Secondly, they lack targeted pre-training for
chart-data alignment, which hampers the model's understanding of underlying
data. In this paper, we introduce ChartScope, an LVLM optimized for in-depth
chart comprehension across diverse chart types. We propose an efficient data
generation pipeline that synthesizes paired data for a wide range of chart
types, along with a novel Dual-Path training strategy that enabling the model
to succinctly capture essential data details while preserving robust reasoning
capabilities by incorporating reasoning over the underlying data. Lastly, we
establish ChartDQA, a new benchmark for evaluating not only question-answering
at different levels but also underlying data understanding. Experimental
results demonstrate that ChartScope significantly enhances comprehension on a
wide range of chart types. The code and data are available at
https://davidhalladay.github.io/chartscope_demo.

</details>


### [69] [Aligning Large Language Models to Low-Resource Languages through LLM-Based Selective Translation: A Systematic Study](https://arxiv.org/abs/2507.14304)
*Rakesh Paul,Anusha Kamath,Kanishk Singla,Raviraj Joshi,Utkarsh Vaidya,Sanjay Singh Chauhan,Niranjan Wartikar*

Main category: cs.CL

TL;DR: 论文研究了基于LLM的选择性翻译方法，用于改善多语言大语言模型（LLM）在低资源语言（如印地语）中的对齐问题。


<details>
  <summary>Details</summary>
Motivation: 多语言LLM在英语和低资源语言之间存在性能差距，而高质量对齐数据稀缺，翻译现有英语数据时难以保留代码、数学表达式等关键内容。

Method: 提出选择性翻译技术，仅翻译可翻译部分，保留非翻译内容和句子结构，并比较了Google Cloud Translation和Llama-3.1-405B的效果。

Result: 实验表明选择性翻译是一种实用且有效的方法，可显著改善LLM在低资源语言中的对齐性能。

Conclusion: 选择性翻译为多语言LLM对齐提供了一种可行的解决方案，尤其在低资源语言中表现优异。

Abstract: Multilingual large language models (LLMs) often demonstrate a performance gap
between English and non-English languages, particularly in low-resource
settings. Aligning these models to low-resource languages is essential yet
challenging due to limited high-quality data. While English alignment datasets
are readily available, curating equivalent data in other languages is expensive
and time-consuming. A common workaround is to translate existing English
alignment data; however, standard translation techniques often fail to preserve
critical elements such as code, mathematical expressions, and structured
formats like JSON. In this work, we investigate LLM-based selective
translation, a technique that selectively translates only the translatable
parts of a text while preserving non-translatable content and sentence
structure. We conduct a systematic study to explore key questions around this
approach, including its effectiveness compared to vanilla translation, the
importance of filtering noisy outputs, and the benefits of mixing translated
samples with original English data during alignment. Our experiments focus on
the low-resource Indic language Hindi and compare translations generated by
Google Cloud Translation (GCP) and Llama-3.1-405B. The results highlight the
promise of selective translation as a practical and effective method for
improving multilingual alignment in LLMs.

</details>


### [70] [How LLMs Comprehend Temporal Meaning in Narratives: A Case Study in Cognitive Evaluation of LLMs](https://arxiv.org/abs/2507.14307)
*Karin de Langis,Jong Inn Park,Andreas Schramm,Bin Hu,Khanh Chi Le,Michael Mensink,Ahn Thu Tong,Dongyeop Kang*

Main category: cs.CL

TL;DR: 研究探讨大型语言模型（LLMs）在处理叙事中的时间意义时是否具有类人认知能力，发现其依赖典型性、判断不一致且因果推理能力有限，表明其与人类处理方式存在根本差异。


<details>
  <summary>Details</summary>
Motivation: 探究LLMs是否通过类人认知方式处理语言时间意义，而非仅依赖高级模式识别。

Method: 采用专家参与循环的探测流程，通过针对性实验评估LLMs的语义表征和语用推理能力。

Result: LLMs过度依赖典型性，产生不一致的时间判断，且因果推理能力不足，表明其叙事理解能力有限。

Conclusion: LLMs在时间意义处理上与人类存在根本差异，缺乏稳健的叙事理解能力，研究还提出了标准化评估框架。

Abstract: Large language models (LLMs) exhibit increasingly sophisticated linguistic
capabilities, yet the extent to which these behaviors reflect human-like
cognition versus advanced pattern recognition remains an open question. In this
study, we investigate how LLMs process the temporal meaning of linguistic
aspect in narratives that were previously used in human studies. Using an
Expert-in-the-Loop probing pipeline, we conduct a series of targeted
experiments to assess whether LLMs construct semantic representations and
pragmatic inferences in a human-like manner. Our findings show that LLMs
over-rely on prototypicality, produce inconsistent aspectual judgments, and
struggle with causal reasoning derived from aspect, raising concerns about
their ability to fully comprehend narratives. These results suggest that LLMs
process aspect fundamentally differently from humans and lack robust narrative
understanding. Beyond these empirical findings, we develop a standardized
experimental framework for the reliable assessment of LLMs' cognitive and
linguistic capabilities.

</details>


### [71] [What Makes You CLIC: Detection of Croatian Clickbait Headlines](https://arxiv.org/abs/2507.14314)
*Marija Anđedelić,Dominik Šipek,Laura Majer,Jan Šnajder*

Main category: cs.CL

TL;DR: Error


<details>
  <summary>Details</summary>
Motivation: Error

Method: Error

Result: Error

Conclusion: Error

Abstract: Online news outlets operate predominantly on an advertising-based revenue
model, compelling journalists to create headlines that are often scandalous,
intriguing, and provocative -- commonly referred to as clickbait. Automatic
detection of clickbait headlines is essential for preserving information
quality and reader trust in digital media and requires both contextual
understanding and world knowledge. For this task, particularly in
less-resourced languages, it remains unclear whether fine-tuned methods or
in-context learning (ICL) yield better results. In this paper, we compile CLIC,
a novel dataset for clickbait detection of Croatian news headlines spanning a
20-year period and encompassing mainstream and fringe outlets. We fine-tune the
BERTi\'c model on this task and compare its performance to LLM-based ICL
methods with prompts both in Croatian and English. Finally, we analyze the
linguistic properties of clickbait. We find that nearly half of the analyzed
headlines contain clickbait, and that finetuned models deliver better results
than general LLMs.

</details>


### [72] [Can LLMs Infer Personality from Real World Conversations?](https://arxiv.org/abs/2507.14355)
*Jianfeng Zhu,Ruoming Jin,Karin G. Coifman*

Main category: cs.CL

TL;DR: 论文研究了利用大型语言模型（如GPT-4和LLaMA）进行人格评估的方法，发现其测试-重测可靠性高，但结构效度有限，与真实人格评分的相关性较弱。


<details>
  <summary>Details</summary>
Motivation: 探索如何利用大型语言模型从开放文本中高效且准确地推断人格特质，解决以往研究中依赖合成数据或缺乏心理测量效度的问题。

Method: 使用555份半结构化访谈和BFI-10自评分数作为基准，测试了三种先进LLM（GPT-4.1 Mini、Meta-LLaMA和DeepSeek）在零样本提示和思维链提示下的表现。

Result: 模型显示出高测试-重测可靠性，但结构效度有限（最大Pearson's r = 0.27），预测偏向中等或高特质水平，思维链提示和更长上下文仅略微改善分布对齐。

Conclusion: 当前基于LLM的人格推断存在局限性，需进一步基于证据的开发以提升心理应用中的准确性。

Abstract: Large Language Models (LLMs) such as OpenAI's GPT-4 and Meta's LLaMA offer a
promising approach for scalable personality assessment from open-ended
language. However, inferring personality traits remains challenging, and
earlier work often relied on synthetic data or social media text lacking
psychometric validity. We introduce a real-world benchmark of 555
semi-structured interviews with BFI-10 self-report scores for evaluating
LLM-based personality inference. Three state-of-the-art LLMs (GPT-4.1 Mini,
Meta-LLaMA, and DeepSeek) were tested using zero-shot prompting for BFI-10 item
prediction and both zero-shot and chain-of-thought prompting for Big Five trait
inference. All models showed high test-retest reliability, but construct
validity was limited: correlations with ground-truth scores were weak (max
Pearson's $r = 0.27$), interrater agreement was low (Cohen's $\kappa < 0.10$),
and predictions were biased toward moderate or high trait levels.
Chain-of-thought prompting and longer input context modestly improved
distributional alignment, but not trait-level accuracy. These results
underscore limitations in current LLM-based personality inference and highlight
the need for evidence-based development for psychological applications.

</details>


### [73] [Text-to-SQL for Enterprise Data Analytics](https://arxiv.org/abs/2507.14372)
*Albert Chen,Manas Bundele,Gaurav Ahlawat,Patrick Stetz,Zhitao Wang,Qiang Fei,Donghoon Jung,Audrey Chu,Bharadwaj Jayaraman,Ayushi Panth,Yatin Arora,Sourav Jain,Renjith Varma,Alexey Ilin,Iuliia Melnychuk,Chelsea Chueh,Joyan Sil,Xiaofeng Wang*

Main category: cs.CL

TL;DR: 本文介绍了LinkedIn开发的内部聊天机器人，通过知识图谱和Text-to-SQL代理，帮助用户自助获取数据洞察。


<details>
  <summary>Details</summary>
Motivation: 尽管大型语言模型在Text-to-SQL基准测试中取得了快速进展，但构建企业级解决方案仍具挑战性。本文旨在分享LinkedIn内部聊天机器人的开发经验。

Method: 方法包括构建动态知识图谱、开发Text-to-SQL代理（支持检索、排名、查询生成和错误修正）以及设计交互式聊天机器人。

Result: 聊天机器人每周有300多名用户，53%的响应在内部基准测试中正确或接近正确。消融研究确定了关键组件。

Conclusion: 本文为企业级Text-to-SQL解决方案的开发提供了实用路径。

Abstract: The introduction of large language models has brought rapid progress on
Text-to-SQL benchmarks, but it is not yet easy to build a working enterprise
solution. In this paper, we present insights from building an internal chatbot
that enables LinkedIn's product managers, engineers, and operations teams to
self-serve data insights from a large, dynamic data lake. Our approach features
three components. First, we construct a knowledge graph that captures
up-to-date semantics by indexing database metadata, historical query logs,
wikis, and code. We apply clustering to identify relevant tables for each team
or product area. Second, we build a Text-to-SQL agent that retrieves and ranks
context from the knowledge graph, writes a query, and automatically corrects
hallucinations and syntax errors. Third, we build an interactive chatbot that
supports various user intents, from data discovery to query writing to
debugging, and displays responses in rich UI elements to encourage follow-up
chats. Our chatbot has over 300 weekly users. Expert review shows that 53% of
its responses are correct or close to correct on an internal benchmark set.
Through ablation studies, we identify the most important knowledge graph and
modeling components, offering a practical path for developing enterprise
Text-to-SQL solutions.

</details>


### [74] [Error-Aware Curriculum Learning for Biomedical Relation Classification](https://arxiv.org/abs/2507.14374)
*Sinchani Chakraborty,Sudeshna Sarkar,Pawan Goyal*

Main category: cs.CL

TL;DR: 提出了一种基于GPT-4o的错误感知师生框架，通过结构化指导和课程学习提升生物医学文本中的关系分类性能。


<details>
  <summary>Details</summary>
Motivation: 生物医学文本中的关系分类对构建知识图谱和药物再利用等应用至关重要，但现有方法存在性能瓶颈。

Method: 使用师生框架，教师模型分析学生模型的预测错误，生成改进建议，并通过课程学习训练学生模型。

Result: 在多个数据集上达到新的最优性能，包括4个PPI数据集和DDI数据集。

Conclusion: 该方法通过结构化指导和课程学习显著提升了关系分类性能，具有实际应用潜力。

Abstract: Relation Classification (RC) in biomedical texts is essential for
constructing knowledge graphs and enabling applications such as drug
repurposing and clinical decision-making. We propose an error-aware
teacher--student framework that improves RC through structured guidance from a
large language model (GPT-4o). Prediction failures from a baseline student
model are analyzed by the teacher to classify error types, assign difficulty
scores, and generate targeted remediations, including sentence rewrites and
suggestions for KG-based enrichment. These enriched annotations are used to
train a first student model via instruction tuning. This model then annotates a
broader dataset with difficulty scores and remediation-enhanced inputs. A
second student is subsequently trained via curriculum learning on this dataset,
ordered by difficulty, to promote robust and progressive learning. We also
construct a heterogeneous biomedical knowledge graph from PubMed abstracts to
support context-aware RC. Our approach achieves new state-of-the-art
performance on 4 of 5 PPI datasets and the DDI dataset, while remaining
competitive on ChemProt.

</details>


### [75] [X-Intelligence 3.0: Training and Evaluating Reasoning LLM for Semiconductor Display](https://arxiv.org/abs/2507.14430)
*Xiaolin Yan,Yangxing Liu,Jiazhang Zheng,Chi Liu,Mingyu Du,Caisheng Chen,Haoyang Liu,Ming Ding,Yuan Li,Qiuping Liao,Linfeng Li,Zhili Mei,Siyu Wan,Li Li,Ruyi Zhong,Jiangling Yu,Xule Liu,Huihui Hu,Jiameng Yue,Ruohui Cheng,Qi Yang,Liangqing Wu,Ke Zhu,Chi Zhang,Chufei Jing,Yifan Zhou,Yan Liang,Dongdong Li,Zhaohui Wang,Bin Zhao,Mingzhou Wu,Mingzhong Zhou,Peng Du,Zuomin Liao,Chao Dai,Pengfei Liang,Xiaoguang Zhu,Yu Zhang,Yu Gu,Kun Pan,Yuan Wu,Yanqing Guan,Shaojing Wu,Zikang Feng,Xianze Ma,Peishan Cheng,Wenjuan Jiang,Jing Ba,Huihao Yu,Zeping Hu,Yuan Xu,Zhiwei Liu,He Wang,Zhenguo Lin,Ming Liu,Yanhong Meng*

Main category: cs.CL

TL;DR: X-Intelligence 3.0是一个专为半导体显示行业设计的高性能推理模型，填补了LLMs在该领域缺乏专业知识的空白。


<details>
  <summary>Details</summary>
Motivation: LLMs在半导体显示行业的应用受限，因缺乏领域专业知识。

Method: 通过监督微调、强化学习和领域知识库增强模型能力，并采用自动化评估框架和RAG机制。

Result: 32B参数的X-Intelligence 3.0在多项评估中超越SOTA模型DeepSeek-R1-671B。

Conclusion: X-Intelligence 3.0高效解决了半导体显示行业的推理挑战。

Abstract: Large language models (LLMs) have recently achieved significant advances in
reasoning and demonstrated their advantages in solving challenging problems.
Yet, their effectiveness in the semiconductor display industry remains limited
due to a lack of domain-specific training and expertise. To bridge this gap, we
present X-Intelligence 3.0, the first high-performance reasoning model
specifically developed for the semiconductor display industry. This model is
designed to deliver expert-level understanding and reasoning for the industry's
complex challenges. Leveraging a carefully curated industry knowledge base, the
model undergoes supervised fine-tuning and reinforcement learning to enhance
its reasoning and comprehension capabilities. To further accelerate
development, we implemented an automated evaluation framework that simulates
expert-level assessments. We also integrated a domain-specific
retrieval-augmented generation (RAG) mechanism, resulting in notable
performance gains on benchmark datasets. Despite its relatively compact size of
32 billion parameters, X-Intelligence 3.0 outperforms SOTA DeepSeek-R1-671B
across multiple evaluations. This demonstrates its exceptional efficiency and
establishes it as a powerful solution to the longstanding reasoning challenges
faced by the semiconductor display industry.

</details>


### [76] [XL-DURel: Finetuning Sentence Transformers for Ordinal Word-in-Context Classification](https://arxiv.org/abs/2507.14578)
*Sachin Yadav,Dominik Schlechtweg*

Main category: cs.CL

TL;DR: XL-DURel是一个优化的多语言句子转换模型，用于序数词上下文分类，通过角距离排名目标在复杂空间中表现优于先前模型。


<details>
  <summary>Details</summary>
Motivation: 解决序数和二元词上下文分类任务，探索统一建模方法。

Method: 测试多种回归和排名损失函数，基于复杂空间中的角距离优化模型。

Result: 在序数和二元数据上表现优于先前模型，二元任务作为序数任务的特例得到提升。

Conclusion: 为不同任务形式的词上下文建模提供了统一处理方法。

Abstract: We propose XL-DURel, a finetuned, multilingual Sentence Transformer model
optimized for ordinal Word-in-Context classification. We test several loss
functions for regression and ranking tasks managing to outperform previous
models on ordinal and binary data with a ranking objective based on angular
distance in complex space. We further show that binary WiC can be treated as a
special case of ordinal WiC and that optimizing models for the general ordinal
task improves performance on the more specific binary task. This paves the way
for a unified treatment of WiC modeling across different task formulations.

</details>


### [77] [Exploring Human-AI Complementarity in CPS Diagnosis Using Unimodal and Multimodal BERT Models](https://arxiv.org/abs/2507.14579)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 论文探讨了使用多模态BERT模型（AudiBERT）检测协作问题解决（CPS）指标的有效性，发现其在社交认知维度上有显著改进，但在情感维度上未观察到类似效果。同时，数据量和人类编码一致性对模型性能有重要影响。


<details>
  <summary>Details</summary>
Motivation: 解决AI在教育领域中检测CPS指标的挑战，尤其是如何利用多模态数据和人类-AI互补性来提高诊断效果。

Method: 采用多模态BERT模型（AudiBERT），结合语音和声学-韵律特征，并与传统BERT模型进行比较。

Result: AudiBERT在社交认知维度上有显著改进，但在情感维度上未表现出类似效果；数据量和人类编码一致性对模型性能有显著影响。

Conclusion: 提出了一种结构化方法以实现人类-AI互补性，强调模型可解释性在支持人类参与和反思编码过程中的重要性。

Abstract: Detecting collaborative problem solving (CPS) indicators from dialogue using
machine learning techniques is a significant challenge for the field of AI in
Education. Recent studies have explored the use of Bidirectional Encoder
Representations from Transformers (BERT) models on transcription data to
reliably detect meaningful CPS indicators. A notable advancement involved the
multimodal BERT variant, AudiBERT, which integrates speech and
acoustic-prosodic audio features to enhance CPS diagnosis. Although initial
results demonstrated multimodal improvements, the statistical significance of
these enhancements remained unclear, and there was insufficient guidance on
leveraging human-AI complementarity for CPS diagnosis tasks. This workshop
paper extends the previous research by highlighting that the AudiBERT model not
only improved the classification of classes that were sparse in the dataset,
but it also had statistically significant class-wise improvements over the BERT
model for classifications in the social-cognitive dimension. However, similar
significant class-wise improvements over the BERT model were not observed for
classifications in the affective dimension. A correlation analysis highlighted
that larger training data was significantly associated with higher recall
performance for both the AudiBERT and BERT models. Additionally, the precision
of the BERT model was significantly associated with high inter-rater agreement
among human coders. When employing the BERT model to diagnose indicators within
these subskills that were well-detected by the AudiBERT model, the performance
across all indicators was inconsistent. We conclude the paper by outlining a
structured approach towards achieving human-AI complementarity for CPS
diagnosis, highlighting the crucial inclusion of model explainability to
support human agency and engagement in the reflective coding process.

</details>


### [78] [Explainable Collaborative Problem Solving Diagnosis with BERT using SHAP and its Implications for Teacher Adoption](https://arxiv.org/abs/2507.14584)
*Kester Wong,Sahan Bulathwela,Mutlu Cukurova*

Main category: cs.CL

TL;DR: 研究使用SHAP方法分析BERT模型在协作问题解决（CPS）分类中单个词汇的贡献，发现模型解释性与分类性能不完全相关，并指出需要进一步探索集成模型和人类-AI互补性。


<details>
  <summary>Details</summary>
Motivation: 增强BERT模型在CPS分类中的可解释性，以帮助教师等终端用户理解模型决策，促进信任和更广泛的教育应用。

Method: 使用SHAP方法分析BERT模型在CPS转录数据中单个词汇对分类决策的贡献。

Result: 发现分类性能高不一定解释合理，某些词汇频繁影响分类，甚至存在无意义的词汇对分类有正面贡献。

Conclusion: 模型解释性对终端用户实践改进有限，但可避免过度依赖AI；未来需探索集成模型和人类-AI互补性，因为CPS子技能的精细区分仍需人类推理。

Abstract: The use of Bidirectional Encoder Representations from Transformers (BERT)
model and its variants for classifying collaborative problem solving (CPS) has
been extensively explored within the AI in Education community. However,
limited attention has been given to understanding how individual tokenised
words in the dataset contribute to the model's classification decisions.
Enhancing the explainability of BERT-based CPS diagnostics is essential to
better inform end users such as teachers, thereby fostering greater trust and
facilitating wider adoption in education. This study undertook a preliminary
step towards model transparency and explainability by using SHapley Additive
exPlanations (SHAP) to examine how different tokenised words in transcription
data contributed to a BERT model's classification of CPS processes. The
findings suggested that well-performing classifications did not necessarily
equate to a reasonable explanation for the classification decisions. Particular
tokenised words were used frequently to affect classifications. The analysis
also identified a spurious word, which contributed positively to the
classification but was not semantically meaningful to the class. While such
model transparency is unlikely to be useful to an end user to improve their
practice, it can help them not to overrely on LLM diagnostics and ignore their
human expertise. We conclude the workshop paper by noting that the extent to
which the model appropriately uses the tokens for its classification is
associated with the number of classes involved. It calls for an investigation
into the exploration of ensemble model architectures and the involvement of
human-AI complementarity for CPS diagnosis, since considerable human reasoning
is still required for fine-grained discrimination of CPS subskills.

</details>


### [79] [Backtranslation and paraphrasing in the LLM era? Comparing data augmentation methods for emotion classification](https://arxiv.org/abs/2507.14590)
*Łukasz Radliński,Mateusz Guściora,Jan Kocoń*

Main category: cs.CL

TL;DR: 本文探讨了利用GPT等大语言模型进行NLP数据增强的方法，比较了传统方法（如复述和回译）与生成式方法的性能。实验表明，传统方法在数据质量和分类性能上表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决领域特定机器学习任务中数据稀缺和类别不平衡的问题，探索传统数据增强方法在大语言模型时代的有效性。

Method: 选择基于ChatGPT的数据增强方法，比较复述、回译、零样本生成和少样本生成四种方法在不同实验设置下的表现。

Result: 回译和复述方法在生成数据质量和分类性能上优于零样本和少样本生成方法。

Conclusion: 传统数据增强方法在大语言模型支持下仍具有竞争力，甚至优于纯生成式方法。

Abstract: Numerous domain-specific machine learning tasks struggle with data scarcity
and class imbalance. This paper systematically explores data augmentation
methods for NLP, particularly through large language models like GPT. The
purpose of this paper is to examine and evaluate whether traditional methods
such as paraphrasing and backtranslation can leverage a new generation of
models to achieve comparable performance to purely generative methods. Methods
aimed at solving the problem of data scarcity and utilizing ChatGPT were
chosen, as well as an exemplary dataset. We conducted a series of experiments
comparing four different approaches to data augmentation in multiple
experimental setups. We then evaluated the results both in terms of the quality
of generated data and its impact on classification performance. The key
findings indicate that backtranslation and paraphrasing can yield comparable or
even better results than zero and a few-shot generation of examples.

</details>


### [80] [Retrieval-Augmented Clinical Benchmarking for Contextual Model Testing in Kenyan Primary Care: A Methodology Paper](https://arxiv.org/abs/2507.14615)
*Fred Mutisya,Shikoh Gitau,Christine Syovata,Diana Oigara,Ibrahim Matende,Muna Aden,Munira Ali,Ryan Nyotu,Diana Marion,Job Nyangena,Nasubo Ongoma,Keith Mbae,Elizabeth Wamicha,Eric Mibuari,Jean Philbert Nsengemana,Talkmore Chidede*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLMs）在非洲初级医疗中的潜力，提出了一种基于肯尼亚国家指南的基准数据集和评估框架，揭示了LLMs在本地化场景中的性能差距。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在非洲低资源医疗环境中的有效性，填补其在初级医疗领域的研究空白。

Method: 采用检索增强生成（RAG）方法，结合肯尼亚国家指南创建数据集，并通过医师协作和专家评审确保临床准确性和文化适应性。

Result: 生成的Alama Health QA数据集包含数千个问题-答案对，初步结果显示LLMs在本地化场景中的性能显著低于美国基准。

Conclusion: 研究为非洲医疗系统提供了一种可复制的指南驱动动态基准测试模型，支持AI的安全部署。

Abstract: Large Language Models(LLMs) hold promise for improving healthcare access in
low-resource settings, but their effectiveness in African primary care remains
underexplored. We present a methodology for creating a benchmark dataset and
evaluation framework focused on Kenyan Level 2 and 3 clinical care. Our
approach uses retrieval augmented generation (RAG) to ground clinical questions
in Kenya's national guidelines, ensuring alignment with local standards. These
guidelines were digitized, chunked, and indexed for semantic retrieval. Gemini
Flash 2.0 Lite was then prompted with guideline excerpts to generate realistic
clinical scenarios, multiple-choice questions, and rationale based answers in
English and Swahili. Kenyan physicians co-created and refined the dataset, and
a blinded expert review process ensured clinical accuracy, clarity, and
cultural appropriateness. The resulting Alama Health QA dataset includes
thousands of regulator-aligned question answer pairs across common outpatient
conditions. Beyond accuracy, we introduce evaluation metrics that test clinical
reasoning, safety, and adaptability such as rare case detection (Needle in the
Haystack), stepwise logic (Decision Points), and contextual adaptability.
Initial results reveal significant performance gaps when LLMs are applied to
localized scenarios, consistent with findings that LLM accuracy is lower on
African medical content than on US-based benchmarks. This work offers a
replicable model for guideline-driven, dynamic benchmarking to support safe AI
deployment in African health systems.

</details>


### [81] [Linear Relational Decoding of Morphology in Language Models](https://arxiv.org/abs/2507.14640)
*Eric Xia,Jugal Kalita*

Main category: cs.CL

TL;DR: 论文发现，通过两部分的仿射近似可以很好地近似某些主客体关系的Transformer计算，线性变换Ws能准确再现最终客体状态。


<details>
  <summary>Details</summary>
Motivation: 研究语言模型中概念关系（如形态学）的可解释性及其在潜在空间中的稀疏编码。

Method: 使用Bigger Analogy Test Set，通过线性变换Ws（s为主词中间层表示，W为模型导数）再现客体状态。

Result: 线性技术在形态学关系上达到90%的忠实度，多语言和跨模型结果类似。

Conclusion: 语言模型中的某些概念关系（如形态学）可通过跨层线性变换稀疏编码，易于从潜在空间解释。

Abstract: A two-part affine approximation has been found to be a good approximation for
transformer computations over certain subject object relations. Adapting the
Bigger Analogy Test Set, we show that the linear transformation Ws, where s is
a middle layer representation of a subject token and W is derived from model
derivatives, is also able to accurately reproduce final object states for many
relations. This linear technique is able to achieve 90% faithfulness on
morphological relations, and we show similar findings multi-lingually and
across models. Our findings indicate that some conceptual relationships in
language models, such as morphology, are readily interpretable from latent
space, and are sparsely encoded by cross-layer linear transformations.

</details>


### [82] [Cleanse: Uncertainty Estimation Approach Using Clustering-based Semantic Consistency in LLMs](https://arxiv.org/abs/2507.14649)
*Minsuh Joo,Hyunsoo Cho*

Main category: cs.CL

TL;DR: 本文提出了一种名为Cleanse的聚类方法，用于估计大型语言模型（LLM）生成内容的不确定性，以检测幻觉问题。


<details>
  <summary>Details</summary>
Motivation: 尽管LLM在多种NLP任务中表现优异，但其生成的幻觉（不准确回答）问题严重影响了模型的安全性和可靠性。

Method: Cleanse通过聚类分析LLM隐藏嵌入的语义一致性，量化不确定性。

Result: 在LLaMA-7B、LLaMA-13B、LLaMA2-7B和Mistral-7B模型及SQuAD和CoQA基准测试中验证了其有效性。

Conclusion: Cleanse能有效检测LLM的幻觉问题，提升模型可靠性。

Abstract: Despite the outstanding performance of large language models (LLMs) across
various NLP tasks, hallucinations in LLMs--where LLMs generate inaccurate
responses--remains as a critical problem as it can be directly connected to a
crisis of building safe and reliable LLMs. Uncertainty estimation is primarily
used to measure hallucination levels in LLM responses so that correct and
incorrect answers can be distinguished clearly. This study proposes an
effective uncertainty estimation approach, \textbf{Cl}ust\textbf{e}ring-based
sem\textbf{an}tic con\textbf{s}ist\textbf{e}ncy (\textbf{Cleanse}). Cleanse
quantifies the uncertainty with the proportion of the intra-cluster consistency
in the total consistency between LLM hidden embeddings which contain adequate
semantic information of generations, by employing clustering. The effectiveness
of Cleanse for detecting hallucination is validated using four off-the-shelf
models, LLaMA-7B, LLaMA-13B, LLaMA2-7B and Mistral-7B and two
question-answering benchmarks, SQuAD and CoQA.

</details>


### [83] [Mangosteen: An Open Thai Corpus for Language Model Pretraining](https://arxiv.org/abs/2507.14664)
*Wannaphong Phatthiyaphaibun,Can Udomcharoenchaikit,Pakpoom Singkorapoom,Kunat Pipatanakul,Ekapol Chuangsuwanich,Peerat Limkonchotiwat,Sarana Nutanong*

Main category: cs.CL

TL;DR: Mangosteen是一个47B泰语语料库，通过定制化Dolma流程构建，提升了泰语模型质量，并在基准测试中优于现有模型。


<details>
  <summary>Details</summary>
Motivation: 现有语料库处理泰语时存在不足，缺乏透明度和高质量数据，需要构建一个可复现的高质量泰语语料库。

Method: 采用定制化Dolma流程，包括泰语语言识别、质量过滤器和内容过滤器，并结合非网络来源数据。

Result: Mangosteen显著提升了模型性能，在泰语基准测试中超越SEA-LION-v3和Llama-3.1约4分。

Conclusion: Mangosteen为泰语及区域LLM研究提供了可复现的高质量基础。

Abstract: Pre-training data shapes a language model's quality, but raw web text is
noisy and demands careful cleaning. Existing large-scale corpora rely on
English-centric or language-agnostic pipelines whose heuristics do not capture
Thai script or cultural nuances, leaving risky material such as gambling
content untreated. Prior Thai-specific efforts customize pipelines or build new
ones, yet seldom release their data or document design choices, hindering
reproducibility and raising the question of how to construct a transparent,
high-quality Thai corpus. We introduce Mangosteen: a 47 billion-token Thai
corpus built through a Thai-adapted Dolma pipeline that includes custom
rule-based language ID, revised C4/Gopher quality filters, and Thai-trained
content filters, plus curated non-web sources such as Wikipedia, Royal Gazette
texts, OCR-extracted books, and CC-licensed YouTube subtitles. Systematic
ablations using GPT-2 show the pipeline trims CommonCrawl from 202M to 25M
documents while raising SEA-HELM NLG from 3 to 11; an 8B-parameter SEA-LION
model continually pre-trained on Mangosteen then surpasses SEA-LION-v3 and
Llama-3.1 by about four points on Thai benchmarks. We release the full pipeline
code, cleaning manifests, corpus snapshot, and all checkpoints, providing a
fully reproducible foundation for future Thai and regional LLM research.

</details>


### [84] [Large Language Models as Medical Codes Selectors: a benchmark using the International Classification of Primary Care](https://arxiv.org/abs/2507.14681)
*Vinicius Anjos de Almeida,Vinicius de Camargo,Raquel Gómez-Bravo,Egbert van der Haring,Kees van Boven,Marcelo Finger,Luis Fernandez Lopez*

Main category: cs.CL

TL;DR: 大型语言模型（LLMs）在无需微调的情况下，表现出自动化ICPC-2编码的潜力，性能优异，但需进一步临床验证。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在医疗编码（ICPC-2）中的应用潜力，以提升医疗数据处理的效率和准确性。

Method: 使用437个巴西葡萄牙语临床表达数据集，通过语义搜索引擎和33种LLM模型匹配ICPC-2代码，评估性能指标（F1分数、成本等）。

Result: 28个模型F1分数>0.8，10个>0.85；检索优化可提升4分性能；小模型在格式和输入长度上表现较差。

Conclusion: LLMs在ICPC-2编码中潜力显著，但需更广泛的多语言和端到端临床验证。

Abstract: Background: Medical coding structures healthcare data for research, quality
monitoring, and policy. This study assesses the potential of large language
models (LLMs) to assign ICPC-2 codes using the output of a domain-specific
search engine.
  Methods: A dataset of 437 Brazilian Portuguese clinical expressions, each
annotated with ICPC-2 codes, was used. A semantic search engine (OpenAI's
text-embedding-3-large) retrieved candidates from 73,563 labeled concepts.
Thirty-three LLMs were prompted with each query and retrieved results to select
the best-matching ICPC-2 code. Performance was evaluated using F1-score, along
with token usage, cost, response time, and format adherence.
  Results: Twenty-eight models achieved F1-score > 0.8; ten exceeded 0.85. Top
performers included gpt-4.5-preview, o3, and gemini-2.5-pro. Retriever
optimization can improve performance by up to 4 points. Most models returned
valid codes in the expected format, with reduced hallucinations. Smaller models
(<3B) struggled with formatting and input length.
  Conclusions: LLMs show strong potential for automating ICPC-2 coding, even
without fine-tuning. This work offers a benchmark and highlights challenges,
but findings are limited by dataset scope and setup. Broader, multilingual,
end-to-end evaluations are needed for clinical validation.

</details>


### [85] [MiroMind-M1: An Open-Source Advancement in Mathematical Reasoning via Context-Aware Multi-Stage Policy Optimization](https://arxiv.org/abs/2507.14683)
*Xingxuan Li,Yao Xiao,Dianwen Ng,Hai Ye,Yue Deng,Xiang Lin,Bin Wang,Zhanfeng Mo,Chong Zhang,Yueyi Zhang,Zonglin Yang,Ruilin Li,Lei Lei,Shihao Xu,Han Zhao,Weiling Chen,Feng Ji,Lidong Bing*

Main category: cs.CL

TL;DR: MiroMind-M1系列是完全开源的推理语言模型，基于Qwen-2.5架构，性能优于现有开源模型，并公开了完整资源以促进可复现性。


<details>
  <summary>Details</summary>
Motivation: 解决开源推理语言模型资源不透明、可复现性差的问题。

Method: 两阶段训练：SFT阶段使用719K数学推理问题，RLVR阶段使用62K挑战性问题，并引入Context-Aware Multi-Stage Policy Optimization算法。

Result: 在AIME24、AIME25和MATH基准测试中表现优异，达到或超过现有开源7B和32B模型。

Conclusion: MiroMind-M1系列为社区提供了透明、高效的推理语言模型资源，推动进一步研究。

Abstract: Large language models have recently evolved from fluent text generation to
advanced reasoning across diverse domains, giving rise to reasoning language
models. Among these domains, mathematical reasoning serves as a representative
benchmark as it requires precise multi-step logic and abstract reasoning, which
can be generalized to other tasks. While closed-source RLMs such as GPT-o3
demonstrate impressive reasoning capabilities, their proprietary nature limits
transparency and reproducibility. Although many open-source projects aim to
close this gap, most of them lack sufficient openness by omitting critical
resources such as datasets and detailed training configurations, which hinders
reproducibility. To contribute toward greater transparency in RLM development,
we introduce the MiroMind-M1 series, a set of fully open-source RLMs built on
the Qwen-2.5 backbone that match or exceed the performance of existing
open-source RLMs. Specifically, our models are trained in two stages: SFT on a
carefully curated corpus of 719K math-reasoning problems with verified CoT
trajectories, followed by RLVR on 62K challenging and verifiable problems. To
enhance the robustness and efficiency of the RLVR process, we introduce
Context-Aware Multi-Stage Policy Optimization, an algorithm that integrates
length-progressive training with an adaptive repetition penalty to encourage
context-aware RL training. Our model achieves state-of-the-art or competitive
performance and superior token efficiency among Qwen-2.5-based open-source 7B
and 32B models on the AIME24, AIME25, and MATH benchmarks. To facilitate
reproducibility, we release the complete stack: models (MiroMind-M1-SFT-7B,
MiroMind-M1-RL-7B, MiroMind-M1-RL-32B); datasets (MiroMind-M1-SFT-719K,
MiroMind-M1-RL-62K); and all training and evaluation configurations. We hope
these resources will support further research and foster community advancement.

</details>


### [86] [Mind the Gap: A Review of Arabic Post-Training Datasets and Their Limitations](https://arxiv.org/abs/2507.14688)
*Mohammed Alkhowaiter,Norah Alshahrani,Saied Alshahrani,Reem I. Masoud,Alaa Alzahrani,Deema Alnuhait,Emad A. Alghamdi,Khalid Almubarak*

Main category: cs.CL

TL;DR: 本文回顾了Hugging Face Hub上公开的阿拉伯语后训练数据集，从四个维度评估其质量，发现任务多样性不足、文档不完整等问题，并提出了改进建议。


<details>
  <summary>Details</summary>
Motivation: 后训练对提升大语言模型性能至关重要，但阿拉伯语后训练数据集的质量和多样性存在不足，需系统评估和改进。

Method: 从LLM能力、可控性、对齐性和鲁棒性四个维度评估阿拉伯语后训练数据集，并分析其流行度、文档质量等指标。

Result: 发现阿拉伯语数据集在任务多样性、文档完整性和社区采用率方面存在显著不足。

Conclusion: 需改进阿拉伯语后训练数据集的质量和多样性，以推动阿拉伯语大语言模型的发展。

Abstract: Post-training has emerged as a crucial technique for aligning pre-trained
Large Language Models (LLMs) with human instructions, significantly enhancing
their performance across a wide range of tasks. Central to this process is the
quality and diversity of post-training datasets. This paper presents a review
of publicly available Arabic post-training datasets on the Hugging Face Hub,
organized along four key dimensions: (1) LLM Capabilities (e.g., Question
Answering, Translation, Reasoning, Summarization, Dialogue, Code Generation,
and Function Calling); (2) Steerability (e.g., persona and system prompts); (3)
Alignment (e.g., cultural, safety, ethics, and fairness), and (4) Robustness.
Each dataset is rigorously evaluated based on popularity, practical adoption,
recency and maintenance, documentation and annotation quality, licensing
transparency, and scientific contribution. Our review revealed critical gaps in
the development of Arabic post-training datasets, including limited task
diversity, inconsistent or missing documentation and annotation, and low
adoption across the community. Finally, the paper discusses the implications of
these gaps on the progress of Arabic LLMs and applications while providing
concrete recommendations for future efforts in post-training dataset
development.

</details>


### [87] [Rethinking Suicidal Ideation Detection: A Trustworthy Annotation Framework and Cross-Lingual Model Evaluation](https://arxiv.org/abs/2507.14693)
*Amina Dzafic,Merve Kavut,Ulya Bayram*

Main category: cs.CL

TL;DR: 研究构建了一个土耳其语自杀意念语料库，并提出了一种资源高效的标注框架，同时评估了标签可靠性和模型一致性。


<details>
  <summary>Details</summary>
Motivation: 解决自杀意念检测中语言覆盖不足和标注不可靠的问题，推动全球自杀预防。

Method: 构建土耳其语语料库，引入三人类标注员和两大语言模型（LLM），通过迁移学习评估标签和模型一致性。

Result: 发现现有模型在零样本迁移学习中表现不佳，强调需更严格的标注和评估方法。

Conclusion: 呼吁在心理健康NLP中提高数据和模型可靠性，倡导透明化处理。

Abstract: Suicidal ideation detection is critical for real-time suicide prevention, yet
its progress faces two under-explored challenges: limited language coverage and
unreliable annotation practices. Most available datasets are in English, but
even among these, high-quality, human-annotated data remains scarce. As a
result, many studies rely on available pre-labeled datasets without examining
their annotation process or label reliability. The lack of datasets in other
languages further limits the global realization of suicide prevention via
artificial intelligence (AI). In this study, we address one of these gaps by
constructing a novel Turkish suicidal ideation corpus derived from social media
posts and introducing a resource-efficient annotation framework involving three
human annotators and two large language models (LLMs). We then address the
remaining gaps by performing a bidirectional evaluation of label reliability
and model consistency across this dataset and three popular English suicidal
ideation detection datasets, using transfer learning through eight pre-trained
sentiment and emotion classifiers. These transformers help assess annotation
consistency and benchmark model performance against manually labeled data. Our
findings underscore the need for more rigorous, language-inclusive approaches
to annotation and evaluation in mental health natural language processing (NLP)
while demonstrating the questionable performance of popular models with
zero-shot transfer learning. We advocate for transparency in model training and
dataset construction in mental health NLP, prioritizing data and model
reliability.

</details>


### [88] [Disparities in Peer Review Tone and the Role of Reviewer Anonymity](https://arxiv.org/abs/2507.14741)
*Maria Sahakyan,Bedoor AlShebli*

Main category: cs.CL

TL;DR: 该研究通过自然语言处理和大规模统计建模，分析了8万多篇同行评审的语言特征，揭示了评审语气、情感和支持性语言如何因作者性别、种族和机构背景而异，并探讨了匿名性对评审公平性的影响。


<details>
  <summary>Details</summary>
Motivation: 同行评审虽被视为科学诚信的守门人，但存在偏见问题。研究旨在揭示语言如何加剧评审中的不平等，填补了现有研究的空白。

Method: 使用自然语言处理和大规模统计建模，分析8万多篇匿名和署名评审的语言特征。

Result: 发现评审语言因作者背景不同而存在显著差异，匿名性对评审公平性的影响与传统假设相悖。

Conclusion: 研究揭示了同行评审中的隐性偏见，对评审政策和科学进步提出了重要问题。

Abstract: The peer review process is often regarded as the gatekeeper of scientific
integrity, yet increasing evidence suggests that it is not immune to bias.
Although structural inequities in peer review have been widely debated, much
less attention has been paid to the subtle ways in which language itself may
reinforce disparities. This study undertakes one of the most comprehensive
linguistic analyses of peer review to date, examining more than 80,000 reviews
in two major journals. Using natural language processing and large-scale
statistical modeling, it uncovers how review tone, sentiment, and supportive
language vary across author demographics, including gender, race, and
institutional affiliation. Using a data set that includes both anonymous and
signed reviews, this research also reveals how the disclosure of reviewer
identity shapes the language of evaluation. The findings not only expose hidden
biases in peer feedback, but also challenge conventional assumptions about
anonymity's role in fairness. As academic publishing grapples with reform,
these insights raise critical questions about how review policies shape career
trajectories and scientific progress.

</details>


### [89] [On the robustness of modeling grounded word learning through a child's egocentric input](https://arxiv.org/abs/2507.14749)
*Wai Keen Vong,Brenden M. Lake*

Main category: cs.CL

TL;DR: 研究探讨了机器学习如何通过模拟儿童语言输入来理解语言习得，验证了多模态神经网络在有限输入下的稳健性。


<details>
  <summary>Details</summary>
Motivation: 探索机器学习模型是否能像儿童一样从有限输入中习得语言，并验证其稳健性。

Method: 使用自动化语音转录方法处理SAYCam数据集，生成多模态数据并训练神经网络。

Result: 网络能从每个儿童的数据中习得并泛化词-指称映射，显示出稳健性，但也存在个体差异。

Conclusion: 多模态神经网络在模拟语言习得中表现稳健，但个体差异提示需进一步研究。

Abstract: What insights can machine learning bring to understanding human language
acquisition? Large language and multimodal models have achieved remarkable
capabilities, but their reliance on massive training datasets creates a
fundamental mismatch with children, who succeed in acquiring language from
comparatively limited input. To help bridge this gap, researchers have
increasingly trained neural networks using data similar in quantity and quality
to children's input. Taking this approach to the limit, Vong et al. (2024)
showed that a multimodal neural network trained on 61 hours of visual and
linguistic input extracted from just one child's developmental experience could
acquire word-referent mappings. However, whether this approach's success
reflects the idiosyncrasies of a single child's experience, or whether it would
show consistent and robust learning patterns across multiple children's
experiences was not explored. In this article, we applied automated speech
transcription methods to the entirety of the SAYCam dataset, consisting of over
500 hours of video data spread across all three children. Using these automated
transcriptions, we generated multi-modal vision-and-language datasets for both
training and evaluation, and explored a range of neural network configurations
to examine the robustness of simulated word learning. Our findings demonstrate
that networks trained on automatically transcribed data from each child can
acquire and generalize word-referent mappings across multiple network
architectures. These results validate the robustness of multimodal neural
networks for grounded word learning, while highlighting the individual
differences that emerge in how models learn when trained on each child's
developmental experiences.

</details>


### [90] [GRACE: Generative Recommendation via Journey-Aware Sparse Attention on Chain-of-Thought Tokenization](https://arxiv.org/abs/2507.14758)
*Luyi Ma,Wanjia Zhang,Kai Zhao,Abhishek Kulkarni,Lalitesh Morishetti,Anjana Ganesh,Ashish Ranjan,Aashika Padmanabhan,Jianpeng Xu,Jason Cho,Praveen Kanumala,Kaushiki Nag,Sumit Dutta,Kamiya Motwani,Malay Patel,Evren Korpeoglu,Sushant Kumar,Kannan Achan*

Main category: cs.CL

TL;DR: GRACE提出了一种新的生成式多行为推荐框架，通过混合Chain-of-Thought标记化和稀疏注意力机制，显著提升了推荐性能并降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 生成模型在多行为推荐系统中潜力巨大，但面临标记推理信息不足、计算成本高和多尺度建模有限的问题。

Method: GRACE采用混合Chain-of-Thought标记化方法，结合产品知识图谱属性，并设计Journey-Aware稀疏注意力机制。

Result: 在两个真实数据集上，GRACE显著优于现有基线，HR@10和NDCG@10提升最高达106.9%，同时减少48%的计算成本。

Conclusion: GRACE通过创新的标记化和注意力机制，解决了生成式推荐系统的关键问题，实现了高效且高性能的推荐。

Abstract: Generative models have recently demonstrated strong potential in
multi-behavior recommendation systems, leveraging the expressive power of
transformers and tokenization to generate personalized item sequences. However,
their adoption is hindered by (1) the lack of explicit information for token
reasoning, (2) high computational costs due to quadratic attention complexity
and dense sequence representations after tokenization, and (3) limited
multi-scale modeling over user history. In this work, we propose GRACE
(Generative Recommendation via journey-aware sparse Attention on
Chain-of-thought tokEnization), a novel generative framework for multi-behavior
sequential recommendation. GRACE introduces a hybrid Chain-of-Thought (CoT)
tokenization method that encodes user-item interactions with explicit
attributes from product knowledge graphs (e.g., category, brand, price) over
semantic tokenization, enabling interpretable and behavior-aligned generation.
To address the inefficiency of standard attention, we design a Journey-Aware
Sparse Attention (JSA) mechanism, which selectively attends to compressed,
intra-, inter-, and current-context segments in the tokenized sequence.
Experiments on two real-world datasets show that GRACE significantly
outperforms state-of-the-art baselines, achieving up to +106.9% HR@10 and
+106.7% NDCG@10 improvement over the state-of-the-art baseline on the Home
domain, and +22.1% HR@10 on the Electronics domain. GRACE also reduces
attention computation by up to 48% with long sequences.

</details>


### [91] [FastLongSpeech: Enhancing Large Speech-Language Models for Efficient Long-Speech Processing](https://arxiv.org/abs/2507.14815)
*Shoutao Guo,Shaolei Zhang,Qingkai Fang,Zhengrui Ma,Min Zhang,Yang Feng*

Main category: cs.CL

TL;DR: FastLongSpeech框架通过迭代融合和动态压缩训练，解决了LSLMs处理长语音的挑战，无需专用长语音训练数据，并在LongSpeech-Eval基准上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有LSLMs在长语音处理上存在不足，主要因缺乏长语音训练数据和高计算成本。

Method: 提出FastLongSpeech框架，采用迭代融合策略压缩长语音序列，并通过动态压缩训练适应长语音输入。

Result: 实验表明，FastLongSpeech在长语音和短语音任务中均表现优异，并显著提升推理效率。

Conclusion: FastLongSpeech为LSLMs高效处理长语音提供了可行方案，无需依赖专用训练数据。

Abstract: The rapid advancement of Large Language Models (LLMs) has spurred significant
progress in Large Speech-Language Models (LSLMs), enhancing their capabilities
in both speech understanding and generation. While existing LSLMs often
concentrate on augmenting speech generation or tackling a diverse array of
short-speech tasks, the efficient processing of long-form speech remains a
critical yet underexplored challenge. This gap is primarily attributed to the
scarcity of long-speech training datasets and the high computational costs
associated with long sequences. To address these limitations, we introduce
FastLongSpeech, a novel framework designed to extend LSLM capabilities for
efficient long-speech processing without necessitating dedicated long-speech
training data. FastLongSpeech incorporates an iterative fusion strategy that
can compress excessively long-speech sequences into manageable lengths. To
adapt LSLMs for long-speech inputs, it introduces a dynamic compression
training approach, which exposes the model to short-speech sequences at varying
compression ratios, thereby transferring the capabilities of LSLMs to
long-speech tasks. To assess the long-speech capabilities of LSLMs, we develop
a long-speech understanding benchmark called LongSpeech-Eval. Experiments show
that our method exhibits strong performance in both long-speech and
short-speech tasks, while greatly improving inference efficiency.

</details>


### [92] [Doc2Chart: Intent-Driven Zero-Shot Chart Generation from Documents](https://arxiv.org/abs/2507.14819)
*Akriti Jain,Pritika Ramu,Aparna Garimella,Apoorv Saxena*

Main category: cs.CL

TL;DR: 论文提出了一种基于意图的文档图表生成任务，通过两阶段框架（信息提取与图表生成）在零样本设置下实现，并在数据准确性和图表类型上优于基线方法。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法难以直接从长文档中根据用户意图生成图表的问题，避免用户手动选择相关内容。

Method: 提出无监督的两阶段框架：1) LLM分解意图并提取数据；2) 启发式模块选择图表类型并生成代码。

Result: 在金融和科学领域的数据集上，方法在图表数据准确性和类型选择上分别优于基线9分和17分。

Conclusion: 该方法有效解决了意图驱动的文档图表生成问题，并通过新提出的评估指标验证了其优势。

Abstract: Large Language Models (LLMs) have demonstrated strong capabilities in
transforming text descriptions or tables to data visualizations via
instruction-tuning methods. However, it is not straightforward to apply these
methods directly for a more real-world use case of visualizing data from long
documents based on user-given intents, as opposed to the user pre-selecting the
relevant content manually. We introduce the task of intent-based chart
generation from documents: given a user-specified intent and document(s), the
goal is to generate a chart adhering to the intent and grounded on the
document(s) in a zero-shot setting. We propose an unsupervised, two-staged
framework in which an LLM first extracts relevant information from the
document(s) by decomposing the intent and iteratively validates and refines
this data. Next, a heuristic-guided module selects an appropriate chart type
before final code generation. To assess the data accuracy of the generated
charts, we propose an attribution-based metric that uses a structured textual
representation of charts, instead of relying on visual decoding metrics that
often fail to capture the chart data effectively. To validate our approach, we
curate a dataset comprising of 1,242 $<$intent, document, charts$>$ tuples from
two domains, finance and scientific, in contrast to the existing datasets that
are largely limited to parallel text descriptions/ tables and their
corresponding charts. We compare our approach with baselines using single-shot
chart generation using LLMs and query-based retrieval methods; our method
outperforms by upto $9$ points and $17$ points in terms of chart data accuracy
and chart type respectively over the best baselines.

</details>


### [93] [Beyond Isolated Capabilities: Bridging Long CoT Reasoning and Long-Context Understanding](https://arxiv.org/abs/2507.14849)
*Yifei Wang*

Main category: cs.CL

TL;DR: 推理蒸馏能提升小语言模型的推理能力，但对上下文检索和推理的影响尚未研究。本文通过实验发现，蒸馏显著改善了长上下文理解，解决了“迷失在中间”的问题。


<details>
  <summary>Details</summary>
Motivation: 研究大规模推理蒸馏对上下文检索和推理能力的影响，特别是在检索增强生成（RAG）系统中。

Method: 使用从Deepseek-R1蒸馏的开源模型，通过多文档问答任务评估长上下文理解能力。

Result: 蒸馏显著提升了长上下文理解，促进了更详细的推理过程，解决了“迷失在中间”问题。

Conclusion: 推理蒸馏不仅提升推理能力，还显著改善长上下文理解，对RAG系统有重要价值。

Abstract: Reasoning distillation has emerged as an effective approach to enhance the
reasoning capabilities of smaller language models. However, the impact of
large-scale reasoning distillation on other critical abilities, particularly
in-context retrieval and reasoning, remains unexplored. This gap in
understanding is particularly significant given the increasing importance of
Retrieval-Augmented Generation (RAG) systems, where efficient acquisition and
utilization of contextual information are paramount for generating reliable
responses. Motivated by the need to understand how the extended long-CoT
process influences long-context comprehension, we conduct a comprehensive
investigation using a series of open-source models distilled from Deepseek-R1,
renowned for its exceptional reasoning capabilities. Our study focuses on
evaluating these models' performance in extracting and integrating relevant
information from extended contexts through multi-document question and
answering tasks. Through rigorous experimentation, we demonstrate that
distilled reasoning patterns significantly improve long-context understanding.
Our analysis reveals that distillation fosters greater long-context awareness
by promoting more detailed and explicit reasoning processes during context
analysis and information parsing. This advancement effectively mitigates the
persistent "lost in the middle" issue that has hindered long-context models.

</details>


### [94] [Tiny language models](https://arxiv.org/abs/2507.14871)
*Ronit D. Gross,Yarden Tzach,Tal Halevi,Ella Koresh,Ido Kanter*

Main category: cs.CL

TL;DR: 研究表明，即使是小型语言模型（TLMs）也能通过预训练展现出与大型语言模型（LLMs）类似的关键特征，且性能差距随预训练数据量和任务重叠度增加而扩大。


<details>
  <summary>Details</summary>
Motivation: 由于大型语言模型（LLMs）预训练需要巨大计算资源，限制了广泛研究参与，因此探索小型语言模型（TLMs）是否具备类似能力成为关键需求。

Method: 通过预训练BERT-6及其变体（如BERT-1）在Wikipedia子集上，并在FewRel、AGNews和DBPedia分类任务中评估其性能。

Result: 预训练的TLMs在分类任务中表现优于未预训练模型，且性能差距随预训练数据量和任务重叠度增加而扩大。此外，通过多个浅层架构的软委员会可实现低延迟且不影响分类精度。

Conclusion: TLMs展现了预训练的有效性，未来研究可能揭示其机制，并表明TLMs可能足以支持儿童或青少年语言发展。

Abstract: A prominent achievement of natural language processing (NLP) is its ability
to understand and generate meaningful human language. This capability relies on
complex feedforward transformer block architectures pre-trained on large
language models (LLMs). However, LLM pre-training is currently feasible only
for a few dominant companies due to the immense computational resources
required, limiting broader research participation. This creates a critical need
for more accessible alternatives. In this study, we explore whether tiny
language models (TLMs) exhibit the same key qualitative features of LLMs. We
demonstrate that TLMs exhibit a clear performance gap between pre-trained and
non-pre-trained models across classification tasks, indicating the
effectiveness of pre-training, even at a tiny scale. The performance gap
increases with the size of the pre-training dataset and with greater overlap
between tokens in the pre-training and classification datasets. Furthermore,
the classification accuracy achieved by a pre-trained deep TLM architecture can
be replicated through a soft committee of multiple, independently pre-trained
shallow architectures, enabling low-latency TLMs without affecting
classification accuracy. Our results are based on pre-training BERT-6 and
variants of BERT-1 on subsets of the Wikipedia dataset and evaluating their
performance on FewRel, AGNews, and DBPedia classification tasks. Future
research on TLM is expected to further illuminate the mechanisms underlying
NLP, especially given that its biologically inspired models suggest that TLMs
may be sufficient for children or adolescents to develop language.

</details>


### [95] [MEKiT: Multi-source Heterogeneous Knowledge Injection Method via Instruction Tuning for Emotion-Cause Pair Extraction](https://arxiv.org/abs/2507.14887)
*Shiyi Mu,Yongkang Liu,Shi Feng,Xiaocui Yang,Daling Wang,Yifei Zhang*

Main category: cs.CL

TL;DR: MEKiT方法通过整合内部情感知识和外部因果知识，显著提升大语言模型在情感-原因对提取任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 大语言模型在情感-原因对提取任务中表现不佳，主要原因是缺乏辅助知识，限制了其感知情感和推理原因的能力。

Method: 提出MEKiT方法，结合内部情感知识和外部因果知识，通过指令模板和数据混合进行指令微调。

Result: 实验显示MEKiT在情感-原因对提取任务中优于基线方法，显著提升大语言模型性能。

Conclusion: MEKiT为情感-原因对提取任务提供了更有效和适应性强的解决方案。

Abstract: Although large language models (LLMs) excel in text comprehension and
generation, their performance on the Emotion-Cause Pair Extraction (ECPE) task,
which requires reasoning ability, is often underperform smaller language model.
The main reason is the lack of auxiliary knowledge, which limits LLMs' ability
to effectively perceive emotions and reason causes. To address this issue, we
propose a novel \textbf{M}ulti-source h\textbf{E}terogeneous \textbf{K}nowledge
\textbf{i}njection me\textbf{T}hod, MEKiT, which integrates heterogeneous
internal emotional knowledge and external causal knowledge. Specifically, for
these two distinct aspects and structures of knowledge, we apply the approaches
of incorporating instruction templates and mixing data for instruction-tuning,
which respectively facilitate LLMs in more comprehensively identifying emotion
and accurately reasoning causes. Experimental results demonstrate that MEKiT
provides a more effective and adaptable solution for the ECPE task, exhibiting
an absolute performance advantage over compared baselines and dramatically
improving the performance of LLMs on the ECPE task.

</details>


### [96] [Sparse Autoencoder-guided Supervised Finetuning to Mitigate Unexpected Code-Switching in LLMs](https://arxiv.org/abs/2507.14894)
*Boyi Deng,Yu Wan,Baosong Yang,Fei Huang,Wenjie Wang,Fuli Feng*

Main category: cs.CL

TL;DR: 论文提出SASFT方法，通过稀疏自编码器分析语言混合问题，并指导监督微调，显著减少意外语言切换，同时保持多语言能力。


<details>
  <summary>Details</summary>
Motivation: 大语言模型（LLMs）在多语言任务中表现优异，但存在意外语言切换（code-switching）问题，影响可读性和实用性。现有方法缺乏机制分析且效果有限。

Method: 使用稀疏自编码器分析语言混合问题，发现语言特征预激活值过高是原因。提出SASFT方法，通过监督微调控制预激活值。

Result: 在五种模型和三种语言上的实验表明，SASFT将意外语言切换减少50%以上，部分情况下完全消除，同时保持或提升多语言任务性能。

Conclusion: SASFT有效解决了语言混合问题，且不损害模型的多语言能力，为LLMs的优化提供了新思路。

Abstract: Large Language Models (LLMs) have impressive multilingual capabilities, but
they suffer from unexpected code-switching, also known as language mixing,
which involves switching to unexpected languages in the model response. This
problem leads to poor readability and degrades the usability of model
responses. However, existing work on this issue lacks a mechanistic analysis
and shows limited effectiveness. In this paper, we first provide an in-depth
analysis of unexpected code-switching using sparse autoencoders and find that
when LLMs switch to a language, the features of that language exhibit excessive
pre-activation values. Based on our findings, we propose $\textbf{S}$parse
$\textbf{A}$utoencoder-guided $\textbf{S}$upervised
$\textbf{F}$ine$\textbf{t}$uning (SASFT), which teaches LLMs to maintain
appropriate pre-activation values of specific language features during
training. Experiments on five models across three languages demonstrate that
SASFT consistently reduces unexpected code-switching by more than 50\% compared
to standard supervised fine-tuning, with complete elimination in four cases.
Moreover, SASFT maintains or even improves the models' performance on six
multilingual benchmarks, showing its effectiveness in addressing code-switching
while preserving multilingual capabilities.

</details>


### [97] [From Neurons to Semantics: Evaluating Cross-Linguistic Alignment Capabilities of Large Language Models via Neurons Alignment](https://arxiv.org/abs/2507.14900)
*Chongxuan Huang,Yongshi Ye,Biao Fu,Qifeng Su,Xiaodong Shi*

Main category: cs.CL

TL;DR: 提出了一种基于神经元状态的跨语言对齐评估方法NeuronXA，用于评估大语言模型的跨语言对齐能力，实验证明其在小数据集上表现优异。


<details>
  <summary>Details</summary>
Motivation: 现有跨语言对齐评估方法主要关注句子嵌入，但神经模型的表示空间可能不光滑，影响低资源语言的语义对齐评估。

Method: 受神经科学启发，提出NeuronXA方法，通过神经元状态评估跨语言对齐能力。

Result: 在仅100对平行句子的情况下，NeuronXA与下游任务性能的Pearson相关系数达0.9556，与可迁移性的相关系数为0.8514。

Conclusion: NeuronXA能有效评估跨语言对齐和可迁移性，有望推动跨语言对齐研究和提升多语言大语言模型的语义理解。

Abstract: Large language models (LLMs) have demonstrated remarkable multilingual
capabilities, however, how to evaluate cross-lingual alignment remains
underexplored. Existing alignment benchmarks primarily focus on sentence
embeddings, but prior research has shown that neural models tend to induce a
non-smooth representation space, which impact of semantic alignment evaluation
on low-resource languages. Inspired by neuroscientific findings that similar
information activates overlapping neuronal regions, we propose a novel Neuron
State-Based Cross-Lingual Alignment (NeuronXA) to assess the cross-lingual a
lignment capabilities of LLMs, which offers a more semantically grounded
approach to assess cross-lingual alignment. We evaluate NeuronXA on several
prominent multilingual LLMs (LLaMA, Qwen, Mistral, GLM, and OLMo) across two
transfer tasks and three multilingual benchmarks. The results demonstrate that
with only 100 parallel sentence pairs, NeuronXA achieves a Pearson correlation
of 0.9556 with downstream tasks performance and 0.8514 with transferability.
These findings demonstrate NeuronXA's effectiveness in assessing both
cross-lingual alignment and transferability, even with a small dataset. This
highlights its potential to advance cross-lingual alignment research and to
improve the semantic understanding of multilingual LLMs.

</details>


### [98] [PromptSuite: A Task-Agnostic Framework for Multi-Prompt Generation](https://arxiv.org/abs/2507.14913)
*Eliya Habba,Noam Dahan,Gili Lior,Gabriel Stanovsky*

Main category: cs.CL

TL;DR: PromptSuite是一个框架，用于自动生成多种提示，以支持更稳健的多提示评估。


<details>
  <summary>Details</summary>
Motivation: 单一提示评估LLMs不可靠，而手动生成多提示变体困难，限制了其实际应用。

Method: PromptSuite采用模块化提示设计，支持对每个组件进行受控扰动，并可扩展新组件和扰动类型。

Result: 通过案例研究，PromptSuite提供了有意义的提示变体，支持强健的评估实践。

Conclusion: PromptSuite通过Python API和用户友好的Web界面提供灵活、可扩展的提示生成解决方案。

Abstract: Evaluating LLMs with a single prompt has proven unreliable, with small
changes leading to significant performance differences. However, generating the
prompt variations needed for a more robust multi-prompt evaluation is
challenging, limiting its adoption in practice. To address this, we introduce
PromptSuite, a framework that enables the automatic generation of various
prompts. PromptSuite is flexible - working out of the box on a wide range of
tasks and benchmarks. It follows a modular prompt design, allowing controlled
perturbations to each component, and is extensible, supporting the addition of
new components and perturbation types. Through a series of case studies, we
show that PromptSuite provides meaningful variations to support strong
evaluation practices. It is available through both a Python API:
https://github.com/eliyahabba/PromptSuite, and a user-friendly web interface:
https://promptsuite.streamlit.app/

</details>


### [99] [SYNTHIA: Synthetic Yet Naturally Tailored Human-Inspired PersonAs](https://arxiv.org/abs/2507.14922)
*Vahid Rahimzadeh,Erfan Moosavi Monazzah,Mohammad Taher Pilehvar,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: SYNTHIA是一个基于真实社交媒体用户数据的合成人物数据集，解决了现有方法在一致性和真实性上的不足，并在多样性和叙事一致性上表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有的人物驱动LLM方法要么依赖昂贵的人工数据，要么生成缺乏一致性和真实性的合成人物，需要一种折中方案。

Method: 利用BlueSky开放平台上10,000名真实用户的社交媒体活动数据，生成30,000个背景故事，并结合时间维度和社交互动元数据。

Result: SYNTHIA在人口多样性和社会调查对齐方面表现优异，同时在叙事一致性上显著优于现有方法。

Conclusion: SYNTHIA填补了现有方法的空白，为计算社会科学和人物驱动语言模型提供了新的研究方向。

Abstract: Persona-driven LLMs have emerged as powerful tools in computational social
science, yet existing approaches fall at opposite extremes, either relying on
costly human-curated data or producing synthetic personas that lack consistency
and realism. We introduce SYNTHIA, a dataset of 30,000 backstories derived from
10,000 real social media users from BlueSky open platform across three time
windows, bridging this spectrum by grounding synthetic generation in authentic
user activity. Our evaluation demonstrates that SYNTHIA achieves competitive
performance with state-of-the-art methods in demographic diversity and social
survey alignment while significantly outperforming them in narrative
consistency. Uniquely, SYNTHIA incorporates temporal dimensionality and
provides rich social interaction metadata from the underlying network, enabling
new research directions in computational social science and persona-driven
language modeling.

</details>


### [100] [MUR: Momentum Uncertainty guided Reasoning for Large Language Models](https://arxiv.org/abs/2507.14958)
*Hang Yan,Fangzhi Xu,Rongman Xu,Yifei Li,Jian Zhang,Haoran Luo,Xiaobao Wu,Luu Anh Tuan,Haiteng Zhao,Qika Lin,Jun Liu*

Main category: cs.CL

TL;DR: 论文提出MUR方法，通过动态分配推理预算，减少LLM的冗余计算，提高推理效率。


<details>
  <summary>Details</summary>
Motivation: 优化大型语言模型（LLM）在推理任务中的效率，避免过度思考导致的资源浪费。

Method: 提出Momentum Uncertainty-guided Reasoning（MUR），动态分配推理预算，并引入gamma-control机制调节预算。

Result: MUR在多个基准测试中平均减少50%计算量，同时准确率提升0.62-3.37%。

Conclusion: MUR方法显著提升了LLM的推理效率和性能，无需额外训练。

Abstract: Large Language Models (LLMs) have achieved impressive performance on
reasoning-intensive tasks, yet optimizing their reasoning efficiency remains an
open challenge. While Test-Time Scaling (TTS) improves reasoning quality, it
often leads to overthinking, wasting tokens on redundant computations. This
work investigates how to efficiently and adaptively guide LLM test-time scaling
without additional training. Inspired by the concept of momentum in physics, we
propose Momentum Uncertainty-guided Reasoning (MUR), which dynamically
allocates thinking budgets to critical reasoning steps by tracking and
aggregating stepwise uncertainty over time. To support flexible inference-time
control, we introduce gamma-control, a simple mechanism that tunes the
reasoning budget via a single hyperparameter. We provide in-depth theoretical
proof to support the superiority of MUR in terms of stability and biases. MUR
is comprehensively evaluated against various TTS methods across four
challenging benchmarks (MATH-500, AIME24, AIME25, and GPQA-diamond) using
different sizes of recent Qwen3 models (1.7B, 4B, and 8B). Results demonstrate
that MUR reduces computation by over 50% on average while improving accuracy by
0.62-3.37%.

</details>


### [101] [RefCritic: Training Long Chain-of-Thought Critic Models with Refinement Feedback](https://arxiv.org/abs/2507.15024)
*Qiaoyu Tang,Hao Xiang,Le Yu,Bowen Yu,Hongyu Lin,Yaojie Lu,Xianpei Han,Le Sun,Junyang Lin*

Main category: cs.CL

TL;DR: 本文提出RefCritic，一种基于强化学习的批评模块，通过双重规则奖励生成高质量反馈，显著提升模型性能。


<details>
  <summary>Details</summary>
Motivation: 当前基于监督微调的批评模块无法真正提升模型的批评能力，导致反馈肤浅且缺乏验证。

Method: 提出RefCritic，采用长链思维和双重规则奖励（实例级正确性和策略模型改进准确性），基于强化学习训练。

Result: 在多个基准测试中，RefCritic表现优异，如AIME25上分别提升6.8%和7.2%。

Conclusion: RefCritic通过高质量反馈显著提升模型性能，优于传统监督方法。

Abstract: With the rapid advancement of Large Language Models (LLMs), developing
effective critic modules for precise guidance has become crucial yet
challenging. In this paper, we initially demonstrate that supervised
fine-tuning for building critic modules (which is widely adopted in current
solutions) fails to genuinely enhance models' critique abilities, producing
superficial critiques with insufficient reflections and verifications. To
unlock the unprecedented critique capabilities, we propose RefCritic, a
long-chain-of-thought critic module based on reinforcement learning with dual
rule-based rewards: (1) instance-level correctness of solution judgments and
(2) refinement accuracies of the policy model based on critiques, aiming to
generate high-quality evaluations with actionable feedback that effectively
guides model refinement. We evaluate RefCritic on Qwen2.5-14B-Instruct and
DeepSeek-R1-Distill-Qwen-14B across five benchmarks. On critique and refinement
settings, RefCritic demonstrates consistent advantages across all benchmarks,
e.g., 6.8\% and 7.2\% gains on AIME25 for the respective base models. Notably,
under majority voting, policy models filtered by RefCritic show superior
scaling with increased voting numbers. Moreover, despite training on
solution-level supervision, RefCritic outperforms step-level supervised
approaches on ProcessBench, a benchmark to identify erroneous steps in
mathematical reasoning.

</details>


### [102] [WebShaper: Agentically Data Synthesizing via Information-Seeking Formalization](https://arxiv.org/abs/2507.15061)
*Zhengwei Tao,Jialong Wu,Wenbiao Yin,Junkai Zhang,Baixuan Li,Haiyang Shen,Kuan Li,Liwen Zhang,Xinyu Wang,Yong Jiang,Pengjun Xie,Fei Huang,Jingren Zhou*

Main category: cs.CL

TL;DR: 论文提出了一种基于形式化驱动的信息搜索（IS）数据合成框架WebShaper，通过知识投影（KP）控制推理结构，生成高质量训练数据，提升LLM代理在复杂任务中的表现。


<details>
  <summary>Details</summary>
Motivation: 现有信息搜索代理的训练数据质量不足，且信息结构与推理结构不一致，限制了其发展。

Method: 提出WebShaper框架，通过集合论形式化IS任务，利用知识投影（KP）操作组合控制推理结构，并通过多步扩展过程合成数据集。

Result: 实验表明，WebShaper在GAIA和WebWalkerQA基准测试中达到开源IS代理的最先进性能。

Conclusion: WebShaper通过形式化驱动的数据合成方法，有效解决了信息搜索代理训练数据不足和结构不一致的问题。

Abstract: The advent of Large Language Model (LLM)-powered agents has revolutionized
artificial intelligence by enabling solutions to complex, open-ended tasks
through web-based information-seeking (IS) capabilities. The scarcity of
high-quality training data has limited the development of IS agents. Existing
approaches typically adopt an information-driven paradigm that first collects
web data and then generates questions based on the retrieval. However, this may
lead to inconsistency between information structure and reasoning structure,
question and answer. To mitigate, we propose a formalization-driven IS data
synthesis framework WebShaper to construct a dataset. WebShaper systematically
formalizes IS tasks through set theory. Central to the formalization is the
concept of Knowledge Projections (KP), which enables precise control over
reasoning structure by KP operation compositions. During synthesis, we begin by
creating seed tasks, then use a multi-step expansion process. At each step, an
agentic Expander expands the current formal question more complex with
retrieval and validation tools based on our formalization. We train our model
on the synthesized dataset. Experiment results demonstrate that WebShaper
achieves state-of-the-art performance among open-sourced IS agents on GAIA and
WebWalkerQA benchmarks.

</details>


### [103] [Evaluation of Coding Schemes for Transformer-based Gene Sequence Modeling](https://arxiv.org/abs/2507.15087)
*Chenlei Gong,Yuanhe Tian,Lei Mao,Yan Song*

Main category: cs.CL

TL;DR: 比较了DNA序列建模中k-mer分割与BPE子词标记化的效果，以及不同位置编码方法的性能，发现BPE表现更优，RoPE适合周期性模体，AliBi适合局部依赖任务。


<details>
  <summary>Details</summary>
Motivation: 缺乏对DNA序列建模中不同分割和标记化方法的系统评估，需比较k-mer与BPE的效果及不同位置编码方法的适用性。

Method: 比较k=1,3,4,5,6的k-mer分割、4,096词BPE词汇表及三种位置编码方法（sinusoidal、AliBi、RoPE），在不同层数的Transformer编码器上进行训练和评估。

Result: BPE表现更稳定且性能更高，RoPE擅长捕捉周期性模体，AliBi适合局部依赖任务，层数从3增加到12层效果显著提升。

Conclusion: 为DNA Transformer模型设计标记化和位置编码提供了实用指导，BPE和RoPE是推荐选择。

Abstract: Currently, many studies view DNA sequences as a special type of language and
utilize Transformers to model them. These studies use fixed-length k-mer
segmentation and BPE subword tokenization but lack a systematic evaluation to
determine which is superior. We compare k-mer segmentation with k=1,3,4,5,6, a
4,096-token BPE vocabulary, and three positional encoding methods-sinusoidal,
AliBi, and RoPE. Each configuration is trained from scratch in 3, 6, 12, and
24-layer Transformer encoders and evaluated on GUE benchmark dataset. In
general, BPE delivers higher and more stable performance across tasks by
compressing frequent motifs into variable-length tokens, reducing sequence
length, and improving model generalization. RoPE excels at capturing periodic
motifs and extrapolating to long sequences, while AliBi also performs well on
tasks driven by local dependencies. In terms of depth, we observe significant
gains when increasing layers from 3 to 12, with only marginal improvements or
slight overfitting at 24 layers. This study provides practical guidance for
designing tokenization and positional encoding in DNA Transformer models.

</details>


### [104] [A Penalty Goes a Long Way: Measuring Lexical Diversity in Synthetic Texts Under Prompt-Influenced Length Variations](https://arxiv.org/abs/2507.15092)
*Vijeta Deshpande,Ishita Dasgupta,Uttaran Bhattacharya,Somdeb Sarkhel,Saayan Mitra,Anna Rumshisky*

Main category: cs.CL

TL;DR: 提出了一种新的词汇多样性度量方法PATTR，解决了现有方法因文本长度变化导致的偏差问题，并在实验中验证了其优越性。


<details>
  <summary>Details</summary>
Motivation: 研究提示变化对生成文本长度及词汇多样性测量的影响，现有方法对长度变化敏感，需改进。

Method: 提出PATTR度量方法，生成20M词的合成语料，比较PATTR与MATTR、CR的性能。

Result: PATTR能有效减少长度偏差，在筛选高多样性文本时优于现有方法。

Conclusion: PATTR是一种更鲁棒的词汇多样性度量工具，适用于需要控制文本长度的任务。

Abstract: Synthetic text generated by Large Language Models (LLMs) is increasingly used
for further training and improvement of LLMs. Diversity is crucial for the
effectiveness of synthetic data, and researchers rely on prompt engineering to
improve diversity. However, the impact of prompt variations on response text
length, and, more importantly, the consequential effect on lexical diversity
measurements, remain underexplored. In this work, we propose Penalty-Adjusted
Type-Token Ratio (PATTR), a diversity metric robust to length variations. We
generate a large synthetic corpus of over 20M words using seven models from the
LLaMA, OLMo, and Phi families, focusing on a creative writing task of video
script generation, where diversity is crucial. We evaluate per-response lexical
diversity using PATTR and compare it against existing metrics of Moving-Average
TTR (MATTR) and Compression Ratio (CR). Our analysis highlights how text length
variations introduce biases favoring shorter responses. Unlike existing
metrics, PATTR explicitly considers the task-specific target response length
($L_T$) to effectively mitigate length biases. We further demonstrate the
utility of PATTR in filtering the top-10/100/1,000 most lexically diverse
responses, showing that it consistently outperforms MATTR and CR by yielding on
par or better diversity with high adherence to $L_T$.

</details>


### [105] [Filling the Gap: Is Commonsense Knowledge Generation useful for Natural Language Inference?](https://arxiv.org/abs/2507.15100)
*Chathuri Jayaweera,Brianna Yanqui,Bonnie Dorr*

Main category: cs.CL

TL;DR: 研究探讨了大型语言模型（LLM）作为常识知识生成器在自然语言推理（NLI）任务中的潜力，评估其可靠性和对预测准确性的影响。


<details>
  <summary>Details</summary>
Motivation: 现有常识资源对多种前提-假设对的覆盖不足，需探索LLM生成常识知识的潜力。

Method: 调整并修改现有指标，评估LLM生成常识知识的真实性和一致性。

Result: 显式加入常识知识未显著提升整体结果，但有助于区分蕴含实例，并适度改善矛盾和中立推理的区分。

Conclusion: LLM作为常识知识生成器在NLI中具有一定潜力，尤其在特定推理类型中表现更优。

Abstract: Natural Language Inference (NLI) is the task of determining the semantic
entailment of a premise for a given hypothesis. The task aims to develop
systems that emulate natural human inferential processes where commonsense
knowledge plays a major role. However, existing commonsense resources lack
sufficient coverage for a variety of premise-hypothesis pairs. This study
explores the potential of Large Language Models as commonsense knowledge
generators for NLI along two key dimensions: their reliability in generating
such knowledge and the impact of that knowledge on prediction accuracy. We
adapt and modify existing metrics to assess LLM factuality and consistency in
generating in this context. While explicitly incorporating commonsense
knowledge does not consistently improve overall results, it effectively helps
distinguish entailing instances and moderately improves distinguishing
contradictory and neutral inferences.

</details>


### [106] [From Disagreement to Understanding: The Case for Ambiguity Detection in NLI](https://arxiv.org/abs/2507.15114)
*Chathuri Jayaweera,Bonnie Dorr*

Main category: cs.CL

TL;DR: 论文主张NLI中的标注分歧并非噪声，而是反映了解释性差异，尤其是由前提或假设的模糊性引发。提出模糊感知NLI框架，整合现有分类法，并通过示例说明模糊性如何影响标注决策。


<details>
  <summary>Details</summary>
Motivation: 标注分歧常被视为噪声，但本文认为其反映了有意义的解释性差异，尤其是由内容模糊性引发。

Method: 提出模糊感知NLI框架，整合现有分类法，并通过示例分析模糊性类型及其对标注的影响。

Result: 揭示了模糊性如何影响标注决策，并指出缺乏标注模糊性的数据集是主要限制。

Conclusion: 建议通过新标注资源和无监督方法填补数据缺口，以实现更鲁棒、可解释且与人类对齐的NLI系统。

Abstract: This position paper argues that annotation disagreement in Natural Language
Inference (NLI) is not mere noise but often reflects meaningful interpretive
variation, especially when triggered by ambiguity in the premise or hypothesis.
While underspecified guidelines and annotator behavior can contribute to
variation, content-based ambiguity offers a process-independent signal of
divergent human perspectives. We call for a shift toward ambiguity-aware NLI by
systematically identifying ambiguous input pairs and classifying ambiguity
types. To support this, we present a unified framework that integrates existing
taxonomies and illustrate key ambiguity subtypes through concrete examples.
These examples reveal how ambiguity shapes annotator decisions and motivate the
need for targeted detection methods that better align models with human
interpretation. A key limitation is the lack of datasets annotated for
ambiguity and subtypes. We propose addressing this gap through new annotated
resources and unsupervised approaches to ambiguity detection -- paving the way
for more robust, explainable, and human-aligned NLI systems.

</details>


### [107] [A Case Against Implicit Standards: Homophone Normalization in Machine Translation for Languages that use the Ge'ez Script](https://arxiv.org/abs/2507.15142)
*Hellina Hailu Nigatu,Atnafu Lambebo Tonja,Henok Biadglign Ademtew,Hizkel Mitiku Alemayehu,Negasi Haile Abadi,Tadesse Destaw Belay,Seid Muhie Yimam*

Main category: cs.CL

TL;DR: 论文探讨了阿姆哈拉语NLP中的同音字归一化问题，提出后推断归一化方法，在保持语言特征的同时提升性能。


<details>
  <summary>Details</summary>
Motivation: 研究同音字归一化对模型理解和跨语言迁移的影响，提出更语言感知的干预方法。

Method: 通过单语训练和跨语言迁移实验，提出后推断归一化方案。

Result: 后推断归一化使BLEU分数提升1.03，同时保留语言特征。

Conclusion: 工作呼吁更多语言感知的干预，促进技术驱动的语言变化讨论。

Abstract: Homophone normalization, where characters that have the same sound in a
writing script are mapped to one character, is a pre-processing step applied in
Amharic Natural Language Processing (NLP) literature. While this may improve
performance reported by automatic metrics, it also results in models that are
not able to understand different forms of writing in a single language.
Further, there might be impacts in transfer learning, where models trained on
normalized data do not generalize well to other languages. In this paper, we
experiment with monolingual training and cross-lingual transfer to understand
the impacts of normalization on languages that use the Ge'ez script. We then
propose a post-inference intervention in which normalization is applied to
model predictions instead of training data. With our simple scheme of
post-inference normalization, we show that we can achieve an increase in BLEU
score of up to 1.03 while preserving language features in training. Our work
contributes to the broader discussion on technology-facilitated language change
and calls for more language-aware interventions.

</details>


### [108] [What Level of Automation is "Good Enough"? A Benchmark of Large Language Models for Meta-Analysis Data Extraction](https://arxiv.org/abs/2507.15152)
*Lingbo Li,Anuradha Mathrani,Teo Susnjak*

Main category: cs.CL

TL;DR: 评估三种LLM在医学领域数据提取任务中的表现，发现定制提示最有效，并提出三层自动化指南。


<details>
  <summary>Details</summary>
Motivation: 自动化从RCTs提取数据用于荟萃分析仍具挑战性，需评估LLMs的实际表现。

Method: 测试三种LLM在高血压、糖尿病和骨科领域的统计结果、偏倚评估和特征提取任务，比较四种提示策略。

Result: 所有模型精度高但召回率低，定制提示可提升召回率15%。

Conclusion: 提出三层自动化指南，平衡LLM效率与专家监督，适用于实际荟萃分析。

Abstract: Automating data extraction from full-text randomised controlled trials (RCTs)
for meta-analysis remains a significant challenge. This study evaluates the
practical performance of three LLMs (Gemini-2.0-flash, Grok-3, GPT-4o-mini)
across tasks involving statistical results, risk-of-bias assessments, and
study-level characteristics in three medical domains: hypertension, diabetes,
and orthopaedics. We tested four distinct prompting strategies (basic
prompting, self-reflective prompting, model ensemble, and customised prompts)
to determine how to improve extraction quality. All models demonstrate high
precision but consistently suffer from poor recall by omitting key information.
We found that customised prompts were the most effective, boosting recall by up
to 15\%. Based on this analysis, we propose a three-tiered set of guidelines
for using LLMs in data extraction, matching data types to appropriate levels of
automation based on task complexity and risk. Our study offers practical advice
for automating data extraction in real-world meta-analyses, balancing LLM
efficiency with expert oversight through targeted, task-specific automation.

</details>


### [109] [Collaborative Distillation Strategies for Parameter-Efficient Language Model Deployment](https://arxiv.org/abs/2507.15198)
*Xiandong Meng,Yan Wu,Yexin Tian,Xin Hu,Tianze Kang,Junliang Du*

Main category: cs.CL

TL;DR: 本文提出一种基于多教师模型的蒸馏策略，通过融合多个教师模型的输出概率分布和中间语义特征，指导学生模型学习，从而在保持小参数量的同时提升语言理解和生成能力。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型部署中的高计算成本和推理速度慢的问题。

Method: 引入加权输出融合机制、特征对齐损失函数和熵驱动的动态教师权重策略，优化知识传递的质量和稳定性。

Result: 学生模型在多任务评估中表现优异，尤其在困惑度、蒸馏损失和生成质量上优于其他蒸馏方法。

Conclusion: 多教师协作机制为大规模语言模型的高效压缩提供了可行技术路径。

Abstract: This paper addresses the challenges of high computational cost and slow
inference in deploying large language models. It proposes a distillation
strategy guided by multiple teacher models. The method constructs several
teacher models and integrates their output probability distributions and
intermediate semantic features. This guides the student model to learn from
multiple sources of knowledge. As a result, the student model gains stronger
language understanding and generation ability while maintaining a small
parameter size. To achieve this, the paper introduces a weighted output fusion
mechanism, a feature alignment loss function, and an entropy-driven dynamic
teacher weighting strategy. These components improve the quality and stability
of knowledge transfer during distillation. Under multi-teacher guidance, the
student model captures semantic information more effectively and demonstrates
strong performance across multiple evaluation metrics. In particular, the
method shows high consistency in expression, generalization ability, and task
adaptability in tasks such as language modeling, text generation, and
multi-task learning. The experiments compare the proposed method with several
widely adopted distillation approaches. The results further confirm its overall
advantages in perplexity, distillation loss, and generation quality. This study
provides a feasible technical path for the efficient compression of large-scale
language models. It also demonstrates the effectiveness of multi-teacher
collaborative mechanisms in complex language modeling tasks.

</details>


### [110] [SOI Matters: Analyzing Multi-Setting Training Dynamics in Pretrained Language Models via Subsets of Interest](https://arxiv.org/abs/2507.15236)
*Shayan Vassef,Amirhossein Dabiriaghdam,Mohammadreza Bakhtiari,Yadollah Yaghoobzadeh*

Main category: cs.CL

TL;DR: 研究探讨多任务、多语言和多源学习对预训练语言模型性能的影响，引入SOI框架分析学习行为模式，并通过实验验证多源学习在分布外性能上的优势。


<details>
  <summary>Details</summary>
Motivation: 探索多任务、多语言和多源学习如何提升语言模型的鲁棒性和性能，并揭示训练动态中的行为模式。

Method: 提出SOI框架分类学习行为，通过热力图和数据集可视化分析行为变化，并进行多任务、多源和多语言的对比实验。

Result: 多源学习显著提升分布外性能（7%），多任务学习在相似任务组合中表现优异，两阶段微调进一步优化性能。

Conclusion: SOI框架和两阶段微调为优化多环境语言模型性能提供了新见解和实用方法。

Abstract: This work investigates the impact of multi-task, multi-lingual, and
multi-source learning approaches on the robustness and performance of
pretrained language models. To enhance this analysis, we introduce Subsets of
Interest (SOI), a novel categorization framework that identifies six distinct
learning behavior patterns during training, including forgettable examples,
unlearned examples, and always correct examples. Through SOI transition
heatmaps and dataset cartography visualization, we analyze how examples shift
between these categories when transitioning from single-setting to
multi-setting configurations. We perform comprehensive experiments across three
parallel comparisons: multi-task vs. single-task learning using English tasks
(entailment, paraphrase, sentiment), multi-source vs. single-source learning
using sentiment analysis datasets, and multi-lingual vs. single-lingual
learning using intent classification in French, English, and Persian. Our
results demonstrate that multi-source learning consistently improves
out-of-distribution performance by up to 7%, while multi-task learning shows
mixed results with notable gains in similar task combinations. We further
introduce a two-stage fine-tuning approach where the second stage leverages
SOI-based subset selection to achieve additional performance improvements.
These findings provide new insights into training dynamics and offer practical
approaches for optimizing multi-setting language model performance.

</details>


### [111] [ChiMed 2.0: Advancing Chinese Medical Dataset in Facilitating Large Language Modeling](https://arxiv.org/abs/2507.15275)
*Yuanhe Tian,Junjie Liu,Zhizhou Kou,Yuxiang Li,Yan Song*

Main category: cs.CL

TL;DR: ChiMed 2.0是一个扩展的中文医学数据集，涵盖传统中医和现代医学数据，支持预训练、监督微调和强化学习，实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 现有中文医学数据集规模小、领域覆盖窄，无法满足预训练和RLHF需求，ChiMed 2.0旨在填补这一空白。

Method: 扩展ChiMed数据集，整合在线平台和LLM生成数据，提供预训练、SFT和RLHF支持。

Result: 实验表明，ChiMed 2.0在不同规模模型上均带来性能提升。

Conclusion: ChiMed 2.0为中文医学LLM训练提供了高质量数据资源，验证了其有效性和适用性。

Abstract: Building high-quality data resources is crucial for advancing artificial
intelligence research and applications in specific domains, particularly in the
Chinese medical domain. Existing Chinese medical datasets are limited in size
and narrow in domain coverage, falling short of the diverse corpora required
for effective pre-training. Moreover, most datasets are designed solely for LLM
fine-tuning and do not support pre-training and reinforcement learning from
human feedback (RLHF). In this paper, we propose a Chinese medical dataset
named ChiMed 2.0, which extends our previous work ChiMed, and covers data
collected from Chinese medical online platforms and generated by LLMs. ChiMed
2.0 contains 204.4M Chinese characters covering both traditional Chinese
medicine classics and modern general medical data, where there are 164.8K
documents for pre-training, 351.6K question-answering pairs for supervised
fine-tuning (SFT), and 41.7K preference data tuples for RLHF. To validate the
effectiveness of our approach for training a Chinese medical LLM, we conduct
further pre-training, SFT, and RLHF experiments on representative general
domain LLMs and evaluate their performance on medical benchmark datasets. The
results show performance gains across different model scales, validating the
dataset's effectiveness and applicability.

</details>


### [112] [A Novel Self-Evolution Framework for Large Language Models](https://arxiv.org/abs/2507.15281)
*Haoran Sun,Zekun Zhang,Shaoning Zeng*

Main category: cs.CL

TL;DR: 提出了一种名为DPSE的双阶段自进化框架，通过联合优化用户偏好适应和领域特定能力，提升LLM的性能。


<details>
  <summary>Details</summary>
Motivation: 现有后训练策略（如基于记忆的检索或偏好优化）虽能改善用户对齐，但未能增强模型的领域认知能力。

Method: DPSE框架引入Censor模块提取多维交互信号并估计满意度分数，通过主题感知和偏好驱动策略扩展结构化数据，支持两阶段微调流程。

Result: 实验表明，DPSE在通用NLP基准和长期对话任务中优于监督微调、偏好优化和基于记忆的基线方法。

Conclusion: DPSE为LLM的持续自进化提供了一条自主路径。

Abstract: The capabilities of Large Language Models (LLMs) are limited to some extent
by pre-training, so some researchers optimize LLMs through post-training.
Existing post-training strategies, such as memory-based retrieval or preference
optimization, improve user alignment yet fail to enhance the model's domain
cognition. To bridge this gap, we propose a novel Dual-Phase Self-Evolution
(DPSE) framework that jointly optimizes user preference adaptation and
domain-specific competence. DPSE introduces a Censor module to extract
multi-dimensional interaction signals and estimate satisfaction scores, which
guide structured data expansion via topic-aware and preference-driven
strategies. These expanded datasets support a two-stage fine-tuning pipeline:
supervised domain grounding followed by frequency-aware preference
optimization. Experiments across general NLP benchmarks and long-term dialogue
tasks demonstrate that DPSE consistently outperforms Supervised Fine-Tuning,
Preference Optimization, and Memory-Augmented baselines. Ablation studies
validate the contribution of each module. In this way, our framework provides
an autonomous path toward continual self-evolution of LLMs.

</details>


### [113] [Beyond Easy Wins: A Text Hardness-Aware Benchmark for LLM-generated Text Detection](https://arxiv.org/abs/2507.15286)
*Navid Ayoobi,Sadat Shahriar,Arjun Mukherjee*

Main category: cs.CL

TL;DR: 提出了一种新的AI文本检测器评估范式SHIELD，强调实际应用中的公平性和稳定性，并开发了一种模型无关的后处理框架以提高检测难度。


<details>
  <summary>Details</summary>
Motivation: 现有评估方法过于依赖传统指标（如AUROC），忽略了实际部署中的误报率和稳定性问题。

Method: 设计了SHIELD基准，整合可靠性和稳定性指标，并开发了一种可控难度的后处理框架。

Result: SHIELD能有效挑战当前零样本检测方法的可靠性和稳定性。

Conclusion: SHIELD为AI文本检测器的实际评估提供了更全面的标准，并展示了后处理框架的潜力。

Abstract: We present a novel evaluation paradigm for AI text detectors that prioritizes
real-world and equitable assessment. Current approaches predominantly report
conventional metrics like AUROC, overlooking that even modest false positive
rates constitute a critical impediment to practical deployment of detection
systems. Furthermore, real-world deployment necessitates predetermined
threshold configuration, making detector stability (i.e. the maintenance of
consistent performance across diverse domains and adversarial scenarios), a
critical factor. These aspects have been largely ignored in previous research
and benchmarks. Our benchmark, SHIELD, addresses these limitations by
integrating both reliability and stability factors into a unified evaluation
metric designed for practical assessment. Furthermore, we develop a post-hoc,
model-agnostic humanification framework that modifies AI text to more closely
resemble human authorship, incorporating a controllable hardness parameter.
This hardness-aware approach effectively challenges current SOTA zero-shot
detection methods in maintaining both reliability and stability. (Data and
code: https://github.com/navid-aub/SHIELD-Benchmark)

</details>


### [114] [On the Inevitability of Left-Leaning Political Bias in Aligned Language Models](https://arxiv.org/abs/2507.15328)
*Thilo Hagendorff*

Main category: cs.CL

TL;DR: 论文探讨了AI对齐原则（无害、有帮助、诚实）与左翼政治偏见的必然联系，认为对齐目标与进步道德框架一致，而右翼意识形态与之冲突。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于澄清AI对齐与政治偏见的关系，反驳将左翼偏见视为问题的观点。

Method: 通过理论分析，论证对齐目标与进步道德框架的内在一致性。

Result: 结果表明，对齐目标必然导致左翼偏见，而右翼意识形态与之冲突。

Conclusion: 结论指出，将左翼偏见视为问题实际上违背了AI对齐原则。

Abstract: The guiding principle of AI alignment is to train large language models
(LLMs) to be harmless, helpful, and honest (HHH). At the same time, there are
mounting concerns that LLMs exhibit a left-wing political bias. Yet, the
commitment to AI alignment cannot be harmonized with the latter critique. In
this article, I argue that intelligent systems that are trained to be harmless
and honest must necessarily exhibit left-wing political bias. Normative
assumptions underlying alignment objectives inherently concur with progressive
moral frameworks and left-wing principles, emphasizing harm avoidance,
inclusivity, fairness, and empirical truthfulness. Conversely, right-wing
ideologies often conflict with alignment guidelines. Yet, research on political
bias in LLMs is consistently framing its insights about left-leaning tendencies
as a risk, as problematic, or concerning. This way, researchers are actively
arguing against AI alignment, tacitly fostering the violation of HHH
principles.

</details>


### [115] [Reasoning Models are Test Exploiters: Rethinking Multiple-Choice](https://arxiv.org/abs/2507.15337)
*Narun Raman,Taylor Lundy,Kevin Leyton-Brown*

Main category: cs.CL

TL;DR: 论文研究了多选问答（MCQA）作为评估大型语言模型（LLMs）下游任务性能的代理是否仍然有效，发现其有效性依赖于推理方式。


<details>
  <summary>Details</summary>
Motivation: 尽管下游任务通常不提供明确选项，但MCQA因其自动评分简便性和与下游性能的相关性而被广泛使用。研究旨在验证这一趋势是否适用于当前最先进的推理模型。

Method: 系统评估了15个问答基准和25个LLMs，考虑了5种提问方式，包括是否提供选项、是否替换正确答案为“以上都不是”，以及是否允许模型在选项前后进行链式推理。

Result: MCQA在模型仅能在选项前进行链式推理时仍为良好代理；但若模型能在选项后推理，其性能显著优于自由文本回答，表明MCQA不再适合评估最新模型。

Conclusion: MCQA不再是评估最先进模型下游性能的良好代理，需设计更鲁棒、抗偏见的基准以真实反映LLMs的推理能力。

Abstract: When evaluating Large Language Models (LLMs) in question-answering domains,
it is common to ask the model to choose among a fixed set of choices (so-called
multiple-choice question-answering, or MCQA). Although downstream tasks of
interest typically do not provide systems with explicit options among which to
choose, this approach is nevertheless widely used because it makes it makes
automatic grading straightforward and has tended to produce challenging
benchmarks that correlate sufficiently well with downstream performance. This
paper investigates the extent to which this trend continues to hold for
state-of-the-art reasoning models, describing a systematic evaluation of $15$
different question-answering benchmarks (e.g., MMLU, HLE) and $25$ different
LLMs (including small models such as Qwen 7B and relatively large models such
as Llama 70B). For each model-benchmark pair, we considered $5$ ways of
presenting the model with questions, including variations on whether multiple
choices were offered to the model at all; whether "none of the above" sometimes
replaced the right answer; and whether the model was permitted to perform
chain-of-thought reasoning before and/or after the choices were presented. MCQA
remained a good proxy for the downstream performance of models as long as they
were allowed to perform chain-of-thought reasoning only before being presented
with the options among which they had to select. On the other hand, large
models that were able to perform reasoning after being given a set of options
tended to significantly outperform their free-text performance due to
exploiting the information in the options. We conclude that MCQA is no longer a
good proxy for assessing downstream performance of state-of-the-art models, and
offer practical guidelines for designing more robust, bias-resistant benchmarks
that better reflect LLMs' genuine reasoning capabilities.

</details>


### [116] [LionGuard 2: Building Lightweight, Data-Efficient & Localised Multilingual Content Moderators](https://arxiv.org/abs/2507.15339)
*Leanne Tan,Gabriel Chua,Ziyu Ge,Roy Ka-Wei Lee*

Main category: cs.CL

TL;DR: LionGuard 2是一个轻量级多语言内容审核分类器，针对新加坡语境优化，支持英语、中文、马来语和部分泰米尔语，性能优于多个商业和开源系统。


<details>
  <summary>Details</summary>
Motivation: 解决多语言和低资源变体的内容审核问题，填补实际部署中的安全漏洞。

Method: 基于预训练的OpenAI嵌入和多头序数分类器构建。

Result: 在17个基准测试中表现优异，包括新加坡特定和公共英语数据集。

Conclusion: 高质量本地数据和强大多语言嵌入可实现强审核性能，无需微调大模型。

Abstract: Modern moderation systems increasingly support multiple languages, but often
fail to address localisation and low-resource variants - creating safety gaps
in real-world deployments. Small models offer a potential alternative to large
LLMs, yet still demand considerable data and compute. We present LionGuard 2, a
lightweight, multilingual moderation classifier tailored to the Singapore
context, supporting English, Chinese, Malay, and partial Tamil. Built on
pre-trained OpenAI embeddings and a multi-head ordinal classifier, LionGuard 2
outperforms several commercial and open-source systems across 17 benchmarks,
including both Singapore-specific and public English datasets. The system is
actively deployed within the Singapore Government, demonstrating practical
efficacy at scale. Our findings show that high-quality local data and robust
multilingual embeddings can achieve strong moderation performance, without
fine-tuning large models. We release our model weights and part of our training
data to support future work on LLM safety.

</details>


### [117] [Probing Information Distribution in Transformer Architectures through Entropy Analysis](https://arxiv.org/abs/2507.15347)
*Amedeo Buonanno,Alessandro Rivetti,Francesco A. N. Palmieri,Giovanni Di Gennaro,Gianmarco Romano*

Main category: cs.CL

TL;DR: 该研究通过熵分析探索Transformer架构中信息分布，旨在揭示模型如何处理和转换信息。


<details>
  <summary>Details</summary>
Motivation: 研究旨在理解Transformer模型中信息的管理和转换方式，以提升模型的可解释性和评估框架。

Method: 通过量化标记级不确定性并分析处理阶段的熵模式，应用于GPT类大语言模型。

Result: 揭示了模型行为和内部表征的潜在机制。

Conclusion: 熵分析为Transformer模型的可解释性和评估提供了新视角。

Abstract: This work explores entropy analysis as a tool for probing information
distribution within Transformer-based architectures. By quantifying token-level
uncertainty and examining entropy patterns across different stages of
processing, we aim to investigate how information is managed and transformed
within these models. As a case study, we apply the methodology to a GPT-based
large language model, illustrating its potential to reveal insights into model
behavior and internal representations. This approach may offer insights into
model behavior and contribute to the development of interpretability and
evaluation frameworks for transformer-based models

</details>


### [118] [Metaphor and Large Language Models: When Surface Features Matter More than Deep Understanding](https://arxiv.org/abs/2507.15357)
*Elisa Sanchez-Bayona,Rodrigo Agerri*

Main category: cs.CL

TL;DR: 本文全面评估了大语言模型（LLMs）在隐喻解释中的能力，发现其表现更多受词汇重叠和句子长度等表面特征影响，而非隐喻内容。


<details>
  <summary>Details</summary>
Motivation: 以往研究局限于单一数据集和特定任务，本文通过多样化数据集和任务填补了这一空白。

Method: 使用公开数据集进行自然语言推理（NLI）和问答（QA）任务实验。

Result: LLMs的表现主要由表面特征决定，而非隐喻理解能力。

Conclusion: 需更现实的评估框架来测试LLMs的隐喻处理能力。

Abstract: This paper presents a comprehensive evaluation of the capabilities of Large
Language Models (LLMs) in metaphor interpretation across multiple datasets,
tasks, and prompt configurations. Although metaphor processing has gained
significant attention in Natural Language Processing (NLP), previous research
has been limited to single-dataset evaluations and specific task settings,
often using artificially constructed data through lexical replacement. We
address these limitations by conducting extensive experiments using diverse
publicly available datasets with inference and metaphor annotations, focusing
on Natural Language Inference (NLI) and Question Answering (QA) tasks. The
results indicate that LLMs' performance is more influenced by features like
lexical overlap and sentence length than by metaphorical content, demonstrating
that any alleged emergent abilities of LLMs to understand metaphorical language
are the result of a combination of surface-level features, in-context learning,
and linguistic knowledge. This work provides critical insights into the current
capabilities and limitations of LLMs in processing figurative language,
highlighting the need for more realistic evaluation frameworks in metaphor
interpretation tasks. Data and code are publicly available.

</details>


### [119] [STITCH: Simultaneous Thinking and Talking with Chunked Reasoning for Spoken Language Models](https://arxiv.org/abs/2507.15375)
*Cheng-Han Chiang,Xiaofei Wang,Linjie Li,Chung-Ching Lin,Kevin Lin,Shujie Liu,Zhendong Wang,Zhengyuan Yang,Hung-yi Lee,Lijuan Wang*

Main category: cs.CL

TL;DR: 提出了Stitch方法，通过交替生成无声推理块和语音响应块，实现SLM的思考与说话同步，降低延迟并提升推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前SLM缺乏无声思考能力，导致响应不清晰；人类通过内部推理实现高效沟通，因此需要为SLM引入类似机制。

Method: 提出Stitch方法，交替生成无声推理块和语音响应块，利用语音播放时间生成推理内容，实现思考与说话并行。

Result: Stitch在数学推理数据集上比基线模型提升15%，在非推理任务上表现相当，同时保持低延迟。

Conclusion: Stitch成功实现了SLM的思考与说话同步，显著提升了推理能力且不影响响应速度。

Abstract: Spoken Language Models (SLMs) are designed to take speech inputs and produce
spoken responses. However, current SLMs lack the ability to perform an
internal, unspoken thinking process before responding. In contrast, humans
typically engage in complex mental reasoning internally, enabling them to
communicate ideas clearly and concisely. Thus, integrating an unspoken thought
process into SLMs is highly desirable. While naively generating a complete
chain-of-thought (CoT) reasoning before starting to talk can enable thinking
for SLMs, this induces additional latency for the speech response, as the CoT
reasoning can be arbitrarily long. To solve this issue, we propose Stitch, a
novel generation method that alternates between the generation of unspoken
reasoning chunks and spoken response chunks. Since the audio duration of a
chunk of spoken response is much longer than the time to generate the tokens in
a chunk of spoken response, we use the remaining free time to generate the
unspoken reasoning tokens. When a chunk of audio is played to the user, the
model continues to generate the next unspoken reasoning chunk, achieving
simultaneous thinking and talking. Remarkably, Stitch matches the latency of
baselines that cannot generate unspoken CoT by design while outperforming those
baselines by 15% on math reasoning datasets; Stitch also performs equally well
on non-reasoning datasets as those baseline models. Some animations and
demonstrations are on the project page: https://d223302.github.io/STITCH.

</details>


### [120] [AlgoSimBench: Identifying Algorithmically Similar Problems for Competitive Programming](https://arxiv.org/abs/2507.15378)
*Jierui Li,Raymond Mooney*

Main category: cs.CL

TL;DR: 论文提出了AlgoSimBench基准测试，评估LLMs在识别算法相似问题（ASPs）上的能力，发现其表现不佳，并提出新方法ASM以提高准确性。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在训练数据较少的相关领域中识别算法相似问题的能力是否泛化。

Method: 引入AlgoSimBench基准测试，包含1317个问题和402个多选题，提出ASM方法改进问题相似性检测。

Result: LLMs在识别ASP上表现较差（最佳模型65.9%准确率），ASM方法提升6.7%-11.7%准确率。

Conclusion: ASM方法显著提升LLMs识别算法相似问题的能力，代码和数据已开源。

Abstract: Recent progress in LLMs, such as reasoning models, has demonstrated strong
abilities to solve complex competitive programming problems, often rivaling top
human competitors. However, it remains underexplored whether these abilities
generalize to relevant domains that are less seen during training. To address
this, we introduce AlgoSimBench, a new benchmark designed to assess LLMs'
ability to identify algorithmically similar problems (ASPs)-problems that can
be solved using similar algorithmic approaches. AlgoSimBench consists of 1317
problems, annotated with 231 distinct fine-grained algorithm tags, from which
we curate 402 multiple-choice questions (MCQs), where each question presents
one algorithmically similar problem alongside three textually similar but
algorithmically dissimilar distractors. Our evaluation reveals that LLMs
struggle to identify ASPs, with the best-performing model (o3-mini) achieving
only 65.9% accuracy on the MCQ task. To address this challenge, we propose
attempted solution matching (ASM), a novel method for improving problem
similarity detection. On our MCQ task, ASM yields an absolute accuracy
improvement of 6.7% to 11.7% across different models. We also evaluated code
embedding models and retrieval methods on similar problem identification. While
the adversarial selection of problems degrades the performance to be less than
random, we found that simply summarizing the problem to remove narrative
elements eliminates the effect, and combining ASM with a keyword-prioritized
method, BM25, can yield up to 52.2% accuracy. Code and data are available at
github.com

</details>


### [121] [ASPERA: A Simulated Environment to Evaluate Planning for Complex Action Execution](https://arxiv.org/abs/2507.15501)
*Alexandru Coca,Mark Gaynor,Zhenxing Zhang,Jianpeng Cheng,Bo-Hsiang Tseng,Pete Boothroyd,Héctor Martinez Alonso,Diarmuid Ó Séaghdha,Anders Johannsen*

Main category: cs.CL

TL;DR: 本文评估了大型语言模型（LLMs）在驱动复杂动作执行的数字助手方面的潜力，提出了ASPERA框架和Asper-Bench数据集，展示了LLMs在基于自定义助手库的程序生成中的挑战。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在复杂动作执行数字助手中的应用潜力，解决数据可用性和评估鲁棒性问题。

Method: 开发ASPERA框架，包括助手库模拟和人工辅助的LLM数据生成引擎，生成高质量任务数据。

Result: 发布Asper-Bench数据集，证明基于自定义助手库的程序生成对LLMs更具挑战性。

Conclusion: LLMs在依赖自定义助手库的程序生成中面临显著挑战，ASPERA框架为相关研究提供了工具和数据支持。

Abstract: This work evaluates the potential of large language models (LLMs) to power
digital assistants capable of complex action execution. These assistants rely
on pre-trained programming knowledge to execute multi-step goals by composing
objects and functions defined in assistant libraries into action execution
programs. To achieve this, we develop ASPERA, a framework comprising an
assistant library simulation and a human-assisted LLM data generation engine.
Our engine allows developers to guide LLM generation of high-quality tasks
consisting of complex user queries, simulation state and corresponding
validation programs, tackling data availability and evaluation robustness
challenges. Alongside the framework we release Asper-Bench, an evaluation
dataset of 250 challenging tasks generated using ASPERA, which we use to show
that program generation grounded in custom assistant libraries is a significant
challenge to LLMs compared to dependency-free code generation.

</details>


### [122] [Step-level Verifier-guided Hybrid Test-Time Scaling for Large Language Models](https://arxiv.org/abs/2507.15512)
*Kaiyan Chang,Yonghao Shi,Chenglong Wang,Hang Zhou,Chi Hu,Xiaoqian Liu,Yingfeng Luo,Yuan Ge,Tong Xiao,Jingbo Zhu*

Main category: cs.CL

TL;DR: 本文提出了一种无需训练的测试时缩放（TTS）方法，结合条件性步骤级自优化和并行缩放，显著提升了大型语言模型的推理性能。


<details>
  <summary>Details</summary>
Motivation: 尽管基于训练的TTS方法（如持续强化学习）流行，但其计算开销大。本文旨在探索无需训练的TTS方法，以减轻计算负担并提升推理能力。

Method: 设计了条件性步骤级自优化（一种细粒度顺序缩放方法），并结合其他并行缩放方法，提出混合测试时缩放（Hybrid TTS）新范式。

Result: 在五种不同规模和家族的指令调优LLM（3B-14B）上实验表明，混合策略显著扩展了LLM的推理性能边界。

Conclusion: 混合测试时缩放方法展示了无需训练的TTS在提升LLM推理能力方面的巨大潜力。

Abstract: Test-Time Scaling (TTS) is a promising approach to progressively elicit the
model's intelligence during inference. Recently, training-based TTS methods,
such as continued reinforcement learning (RL), have further surged in
popularity, while training-free TTS methods are gradually fading from
prominence. However, the additional computation overhead of training amplifies
the burden on test-time scaling. In this paper, we focus on training-free TTS
methods for reasoning. We first design Conditional Step-level Self-refinement,
a fine-grained sequential scaling method guided by process verification. On top
of its effectiveness, we further combine it with other classical parallel
scaling methods at the step level, to introduce a novel inference paradigm
called Hybrid Test-Time Scaling. Extensive experiments on five
instruction-tuned LLMs across different scales (3B-14B) and families
demonstrate that hybrid strategy incorporating various training-free TTS
methods at a fine granularity has considerable potential for expanding the
reasoning performance boundaries of LLMs.

</details>


### [123] [Evaluating Text Style Transfer: A Nine-Language Benchmark for Text Detoxification](https://arxiv.org/abs/2507.15557)
*Vitaly Protasov,Nikolay Babakov,Daryna Dementieva,Alexander Panchenko*

Main category: cs.CL

TL;DR: 本文探讨了多语言文本去毒系统的评估方法，发现自动指标与人工判断存在显著差距，并提出了一种更可靠的多语言评估流程。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型（LLMs）在文本生成任务中取得进展，但文本风格转换（TST）的评估仍具挑战性，尤其是多语言环境下的评估尚未充分研究。

Method: 研究首次对九种语言的文本去毒系统进行全面评估，结合基于神经网络的评估模型和基于提示的LLM-as-a-judge方法。

Result: 研究发现自动指标与人工判断存在显著差距，并提出了一种更可靠的多语言评估流程。

Conclusion: 研究为设计更可靠的多语言TST评估流程提供了实用建议。

Abstract: Despite recent progress in large language models (LLMs), evaluation of text
generation tasks such as text style transfer (TST) remains a significant
challenge. Recent studies (Dementieva et al., 2024; Pauli et al., 2025)
revealed a substantial gap between automatic metrics and human judgments.
Moreover, most prior work focuses exclusively on English, leaving multilingual
TST evaluation largely unexplored. In this paper, we perform the first
comprehensive multilingual study on evaluation of text detoxification system
across nine languages: English, Spanish, German, Chinese, Arabic, Hindi,
Ukrainian, Russian, Amharic. Drawing inspiration from the machine translation,
we assess the effectiveness of modern neural-based evaluation models alongside
prompting-based LLM-as-a-judge approaches. Our findings provide a practical
recipe for designing more reliable multilingual TST evaluation pipeline in the
text detoxification case.

</details>


### [124] [Smart Eyes for Silent Threats: VLMs and In-Context Learning for THz Imaging](https://arxiv.org/abs/2507.15576)
*Nicolas Poggi,Shashank Agnihotri,Margret Keuper*

Main category: cs.CL

TL;DR: 论文提出了一种基于上下文学习（ICL）和视觉语言模型（VLM）的太赫兹（THz）图像分类方法，无需微调即可在低数据条件下提升分类性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决太赫兹图像分类中因标注数据有限、分辨率低和视觉模糊性带来的挑战。

Method: 采用模态对齐的提示框架，将两种开放权重的VLM适配到THz领域，并在零样本和单样本设置下评估。

Result: ICL在低数据条件下显著提升了分类性能和可解释性。

Conclusion: 这是ICL增强的VLM在THz成像中的首次应用，为资源受限的科学领域提供了有前景的方向。

Abstract: Terahertz (THz) imaging enables non-invasive analysis for applications such
as security screening and material classification, but effective image
classification remains challenging due to limited annotations, low resolution,
and visual ambiguity. We introduce In-Context Learning (ICL) with
Vision-Language Models (VLMs) as a flexible, interpretable alternative that
requires no fine-tuning. Using a modality-aligned prompting framework, we adapt
two open-weight VLMs to the THz domain and evaluate them under zero-shot and
one-shot settings. Our results show that ICL improves classification and
interpretability in low-data regimes. This is the first application of
ICL-enhanced VLMs to THz imaging, offering a promising direction for
resource-constrained scientific domains. Code:
\href{https://github.com/Nicolas-Poggi/Project_THz_Classification/tree/main}{GitHub
repository}.

</details>


### [125] [Learning to Extract Rational Evidence via Reinforcement Learning for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.15586)
*Xinping Zhao,Shouzheng Huang,Yan Zhong,Xinshuo Hu,Baotian Hu,Min Zhang*

Main category: cs.CL

TL;DR: LEAR是一种通过学习提取理性证据的方法，通过显式推理和提取关键线索，提升RAG系统中LLM的生成质量。


<details>
  <summary>Details</summary>
Motivation: 检索噪声显著影响LLM生成质量，现有方法缺乏显式推理，容易遗漏关键线索且泛化能力不足。

Method: LEAR通过显式推理和提取关键线索，统一证据推理与提取，使用知识标记掩码和奖励函数进行端到端训练。

Result: 在三个基准数据集上验证了LEAR的有效性，提供紧凑高质量证据，提升下游任务准确率。

Conclusion: LEAR显著改善了RAG系统的生成质量，适用于在线应用。

Abstract: Retrieval-Augmented Generation (RAG) effectively improves the accuracy of
Large Language Models (LLMs). However, retrieval noises significantly impact
the quality of LLMs' generation, necessitating the development of denoising
mechanisms. Previous methods extract evidence straightforwardly without
explicit thinking, which risks filtering out key clues and struggles with
generalization. To this end, we propose LEAR, which learns to extract rational
evidence by (1) explicitly reasoning to identify potential cues within
retrieval contents first, and then (2) consciously extracting to avoid omitting
any key cues helpful for answering questions. Specifically, we frame evidence
reasoning and evidence extraction into one unified response for end-to-end
training; apply knowledge token masks for disentanglement to derive
reasoning-based and extraction-based answers; and devise three types of
verifiable reward functions, including answer, length, and format, to update
the model via the policy optimization algorithm. Extensive experiments on three
benchmark datasets show the effectiveness of LEAR, providing compact and
high-quality evidence, improving the accuracy of downstream tasks, and
promoting effective application in online RAG systems.

</details>


### [126] [Conflicting narratives and polarization on social media](https://arxiv.org/abs/2507.15600)
*Armin Pournaki*

Main category: cs.CL

TL;DR: 论文分析了冲突性叙事如何揭示公共领域中的极化与议题对齐机制，通过德国Twitter数据展示了叙事差异与对齐模式。


<details>
  <summary>Details</summary>
Motivation: 探讨叙事作为理解政治现实的解释工具，以及冲突性叙事如何揭示公共领域中的极化现象。

Method: 基于德国Twitter数据（2021-2023），提取对立观点群体的文本信号，分析叙事冲突的两个维度：角色分配差异和事件情节差异。

Result: 发现叙事冲突的证据（如对北约角色的不同解读），并首次揭示叙事对齐模式，即政治行为者用于跨议题对齐观点的策略。

Conclusion: 叙事分析为理解极化机制提供了有效工具，揭示了公共领域中的话语策略。

Abstract: Narratives are key interpretative devices by which humans make sense of
political reality. In this work, we show how the analysis of conflicting
narratives, i.e. conflicting interpretive lenses through which political
reality is experienced and told, provides insight into the discursive
mechanisms of polarization and issue alignment in the public sphere. Building
upon previous work that has identified ideologically polarized issues in the
German Twittersphere between 2021 and 2023, we analyze the discursive dimension
of polarization by extracting textual signals of conflicting narratives from
tweets of opposing opinion groups. Focusing on a selection of salient issues
and events (the war in Ukraine, Covid, climate change), we show evidence for
conflicting narratives along two dimensions: (i) different attributions of
actantial roles to the same set of actants (e.g. diverging interpretations of
the role of NATO in the war in Ukraine), and (ii) emplotment of different
actants for the same event (e.g. Bill Gates in the right-leaning Covid
narrative). Furthermore, we provide first evidence for patterns of narrative
alignment, a discursive strategy that political actors employ to align opinions
across issues. These findings demonstrate the use of narratives as an
analytical lens into the discursive mechanisms of polarization.

</details>


### [127] [Leveraging Context for Multimodal Fallacy Classification in Political Debates](https://arxiv.org/abs/2507.15641)
*Alessio Pittiglio*

Main category: cs.CL

TL;DR: 本文介绍了针对MM-ArgFallacy2025共享任务的提交，专注于政治辩论中的逻辑谬误多模态论证挖掘。采用预训练Transformer模型并结合上下文，在谬误分类子任务中取得了0.4444（文本）、0.3559（音频）和0.4403（多模态）的宏F1分数。


<details>
  <summary>Details</summary>
Motivation: 推动多模态论证挖掘研究，特别是政治辩论中的逻辑谬误识别。

Method: 使用预训练的Transformer模型，并结合多种上下文信息。

Result: 在谬误分类任务中，文本模型宏F1为0.4444，音频为0.3559，多模态为0.4403。多模态模型表现接近文本模型。

Conclusion: 多模态模型有改进潜力，未来可进一步优化。

Abstract: In this paper, we present our submission to the MM-ArgFallacy2025 shared
task, which aims to advance research in multimodal argument mining, focusing on
logical fallacies in political debates. Our approach uses pretrained
Transformer-based models and proposes several ways to leverage context. In the
fallacy classification subtask, our models achieved macro F1-scores of 0.4444
(text), 0.3559 (audio), and 0.4403 (multimodal). Our multimodal model showed
performance comparable to the text-only model, suggesting potential for
improvements.

</details>


### [128] [P3: Prompts Promote Prompting](https://arxiv.org/abs/2507.15675)
*Xinyu Zhang,Yuanquan Hu,Fangchao Liu,Zhicheng Dou*

Main category: cs.CL

TL;DR: P3是一个自优化框架，同时优化系统提示和用户提示，通过迭代过程提升LLM性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法仅优化系统或用户提示，效果不佳，因为两者相互依赖。

Method: P3框架通过迭代同时优化系统提示和用户提示，并利用离线优化的提示进行在线优化。

Result: 在通用任务和推理任务上，P3表现优于现有方法。

Conclusion: 整体优化策略能有效提升LLM在不同领域的性能。

Abstract: Current large language model (LLM) applications often employ multi-component
prompts, comprising both system and user prompts, to guide model behaviors.
While recent advancements have demonstrated the efficacy of automatically
optimizing either the system or user prompt to boost performance, such
unilateral approaches often yield suboptimal outcomes due to the interdependent
nature of these components. In this work, we introduce P3, a novel
self-improvement framework that concurrently optimizes both system and user
prompts through an iterative process. The offline optimized prompts are further
leveraged to promote online prompting by performing query-dependent prompt
optimization. Extensive experiments on general tasks (e.g., Arena-hard and
Alpaca-eval) and reasoning tasks (e.g., GSM8K and GPQA) demonstrate that P3
achieves superior performance in the realm of automatic prompt optimization.
Our results highlight the effectiveness of a holistic optimization strategy in
enhancing LLM performance across diverse domains.

</details>


### [129] [CoLD: Counterfactually-Guided Length Debiasing for Process Reward Models](https://arxiv.org/abs/2507.15698)
*Congmin Zheng,Jiachen Zhu,Jianghao Lin,Xinyi Dai,Yong Yu,Weinan Zhang,Mengyue Yang*

Main category: cs.CL

TL;DR: 论文提出CoLD框架，通过长度惩罚、学习偏差估计器和联合训练策略，解决PRMs中的长度偏差问题，提升推理的准确性和简洁性。


<details>
  <summary>Details</summary>
Motivation: 现有PRMs存在长度偏差，倾向于给更长的推理步骤更高评分，影响奖励预测的可靠性和推理输出的简洁性。

Method: 提出CoLD框架，包含显式长度惩罚调整、学习偏差估计器和联合训练策略，基于反事实推理和因果图分析。

Result: 在MATH500和GSM-Plus数据集上，CoLD显著降低了奖励与长度的相关性，提高了步骤选择的准确性，并鼓励更简洁、逻辑有效的推理。

Conclusion: CoLD有效提升了PRMs的可靠性和鲁棒性，证明了其在实际应用中的有效性。

Abstract: Process Reward Models (PRMs) play a central role in evaluating and guiding
multi-step reasoning in large language models (LLMs), especially for
mathematical problem solving. However, we identify a pervasive length bias in
existing PRMs: they tend to assign higher scores to longer reasoning steps,
even when the semantic content and logical validity are unchanged. This bias
undermines the reliability of reward predictions and leads to overly verbose
outputs during inference. To address this issue, we propose
CoLD(Counterfactually-Guided Length Debiasing), a unified framework that
mitigates length bias through three components: an explicit length-penalty
adjustment, a learned bias estimator trained to capture spurious length-related
signals, and a joint training strategy that enforces length-invariance in
reward predictions. Our approach is grounded in counterfactual reasoning and
informed by causal graph analysis. Extensive experiments on MATH500 and
GSM-Plus show that CoLD consistently reduces reward-length correlation,
improves accuracy in step selection, and encourages more concise, logically
valid reasoning. These results demonstrate the effectiveness and practicality
of CoLD in improving the fidelity and robustness of PRMs.

</details>


### [130] [Compositional Understanding in Signaling Games](https://arxiv.org/abs/2507.15706)
*David Peter Wallis Freeborn*

Main category: cs.CL

TL;DR: 论文提出两种新的信号博弈模型，解决接收者在标准模型中难以学习组合信息的问题。


<details>
  <summary>Details</summary>
Motivation: 标准信号博弈模型中，接收者难以理解组合信息，导致信息丢失。

Method: 构建两种新模型：简约型接收者（仅从信号的原子消息学习）和通用型接收者（从所有可用信息学习）。

Result: 新模型比现有方案更简单，且接收者能从消息的原子组件中学习。

Conclusion: 新模型实现了真正的组合理解演化。

Abstract: Receivers in standard signaling game models struggle with learning
compositional information. Even when the signalers send compositional messages,
the receivers do not interpret them compositionally. When information from one
message component is lost or forgotten, the information from other components
is also erased. In this paper I construct signaling game models in which
genuine compositional understanding evolves. I present two new models: a
minimalist receiver who only learns from the atomic messages of a signal, and a
generalist receiver who learns from all of the available information. These
models are in many ways simpler than previous alternatives, and allow the
receivers to learn from the atomic components of messages.

</details>


### [131] [Is Large Language Model Performance on Reasoning Tasks Impacted by Different Ways Questions Are Asked?](https://arxiv.org/abs/2507.15707)
*Seok Hwan Song,Mohna Chakraborty,Qi Li,Wallapak Tavanapong*

Main category: cs.CL

TL;DR: 研究探讨了不同问题类型对大型语言模型（LLM）在推理任务中准确性的影响，发现性能差异显著且推理准确性与最终答案选择准确性不一定相关。


<details>
  <summary>Details</summary>
Motivation: 探索未解决的问题类型对LLM在推理任务中准确性的影响。

Method: 评估五种LLM在三种问题类型上的性能，使用定量和演绎推理任务，分析推理步骤和最终答案选择的准确性。

Result: 发现LLM性能在不同问题类型间差异显著，推理准确性与最终答案选择准确性不相关，选项数量和措辞影响性能。

Conclusion: 问题类型对LLM性能有显著影响，需在设计评估任务时考虑问题类型和措辞。

Abstract: Large Language Models (LLMs) have been evaluated using diverse question
types, e.g., multiple-choice, true/false, and short/long answers. This study
answers an unexplored question about the impact of different question types on
LLM accuracy on reasoning tasks. We investigate the performance of five LLMs on
three different types of questions using quantitative and deductive reasoning
tasks. The performance metrics include accuracy in the reasoning steps and
choosing the final answer. Key Findings: (1) Significant differences exist in
LLM performance across different question types. (2) Reasoning accuracy does
not necessarily correlate with the final selection accuracy. (3) The number of
options and the choice of words, influence LLM performance.

</details>


### [132] [Chinchunmei at SemEval-2025 Task 11: Boosting the Large Language Model's Capability of Emotion Perception using Contrastive Learning](https://arxiv.org/abs/2507.15714)
*Tian Li,Yujian Sun,Huizhi Liang*

Main category: cs.CL

TL;DR: SemEval-2025 Task 11聚焦于多语言情感检测，提出两种对比学习方法，分别在Track A和Track B中取得不错成绩。


<details>
  <summary>Details</summary>
Motivation: 解决情感表达多样性和背景差异带来的挑战，推动更先进的情感检测方法。

Method: 采用样本对比学习（Contrastive Reasoning Calibration）和生成对比学习（DPO, SimPO），基于LLaMa3-Instruct-8B微调。

Result: 在英语任务中，Track A排名第9，Track B排名第6；其他语言表现优异。

Conclusion: 对比学习方法在多语言情感检测任务中有效，未来可进一步优化。

Abstract: The SemEval-2025 Task 11, Bridging the Gap in Text-Based Emotion Detection,
introduces an emotion recognition challenge spanning over 28 languages. This
competition encourages researchers to explore more advanced approaches to
address the challenges posed by the diversity of emotional expressions and
background variations. It features two tracks: multi-label classification
(Track A) and emotion intensity prediction (Track B), covering six emotion
categories: anger, fear, joy, sadness, surprise, and disgust. In our work, we
systematically explore the benefits of two contrastive learning approaches:
sample-based (Contrastive Reasoning Calibration) and generation-based (DPO,
SimPO) contrastive learning. The sample-based contrastive approach trains the
model by comparing two samples to generate more reliable predictions. The
generation-based contrastive approach trains the model to differentiate between
correct and incorrect generations, refining its prediction. All models are
fine-tuned from LLaMa3-Instruct-8B. Our system achieves 9th place in Track A
and 6th place in Track B for English, while ranking among the top-tier
performing systems for other languages.

</details>


### [133] [From Queries to Criteria: Understanding How Astronomers Evaluate LLMs](https://arxiv.org/abs/2507.15715)
*Alina Hyk,Kiera McCormick,Mian Zhong,Ioana Ciucă,Sanjib Sharma,John F Wu,J. E. G. Peek,Kartheik G. Iyer,Ziang Xiao,Anjalie Field*

Main category: cs.CL

TL;DR: 研究通过分析天文学家对LLM驱动的检索增强生成机器人的使用情况，提出了改进LLM评估方法的建议，并构建了一个天文学领域的样本基准。


<details>
  <summary>Details</summary>
Motivation: 当前LLM评估基准未能跟上用户多样化需求的步伐，尤其是在科学研究和天文学领域。

Method: 通过Slack部署LLM机器人，收集368条查询并进行归纳编码，随后对11位天文学家进行访谈，分析用户评估标准。

Result: 揭示了用户评估LLM系统的具体方式和标准，并据此提出了改进评估基准的建议。

Conclusion: 研究为提升LLM评估和可用性提供了实用方法，尤其适用于科学研究领域。

Abstract: There is growing interest in leveraging LLMs to aid in astronomy and other
scientific research, but benchmarks for LLM evaluation in general have not kept
pace with the increasingly diverse ways that real people evaluate and use these
models. In this study, we seek to improve evaluation procedures by building an
understanding of how users evaluate LLMs. We focus on a particular use case: an
LLM-powered retrieval-augmented generation bot for engaging with astronomical
literature, which we deployed via Slack. Our inductive coding of 368 queries to
the bot over four weeks and our follow-up interviews with 11 astronomers reveal
how humans evaluated this system, including the types of questions asked and
the criteria for judging responses. We synthesize our findings into concrete
recommendations for building better benchmarks, which we then employ in
constructing a sample benchmark for evaluating LLMs for astronomy. Overall, our
work offers ways to improve LLM evaluation and ultimately usability,
particularly for use in scientific research.

</details>


### [134] [BEnchmarking LLMs for Ophthalmology (BELO) for Ophthalmological Knowledge and Reasoning](https://arxiv.org/abs/2507.15717)
*Sahana Srinivasan,Xuguang Ai,Thaddaeus Wai Soon Lo,Aidan Gilson,Minjie Zou,Ke Zou,Hyunjae Kim,Mingjia Yang,Krithi Pushpanathan,Samantha Yew,Wan Ting Loke,Jocelyn Goh,Yibing Chen,Yiming Kong,Emily Yuelei Fu,Michelle Ongyong Hui,Kristen Nwanyanwu,Amisha Dave,Kelvin Zhenghao Li,Chen-Hsin Sun,Mark Chia,Gabriel Dawei Yang,Wendy Meihua Wong,David Ziyou Chen,Dianbo Liu,Maxwell Singer,Fares Antaki,Lucian V Del Priore,Jost Jonas,Ron Adelman,Qingyu Chen,Yih-Chung Tham*

Main category: cs.CL

TL;DR: BELO是一个标准化且全面的眼科领域大语言模型评估基准，通过专家多轮检查开发，评估临床准确性和推理质量。


<details>
  <summary>Details</summary>
Motivation: 当前眼科领域的大语言模型评估基准范围有限且过于注重准确性，需要更全面的评估工具。

Method: 通过关键词匹配和微调的PubMedBERT模型从多个医学数据集中筛选眼科相关多选题，并经过专家多轮检查和优化。

Result: BELO包含900个高质量专家评审问题，评估了六种大语言模型，并建立了公开排行榜。

Conclusion: BELO将作为保留评估基准，确保未来模型的公平和可重复比较。

Abstract: Current benchmarks evaluating large language models (LLMs) in ophthalmology
are limited in scope and disproportionately prioritise accuracy. We introduce
BELO (BEnchmarking LLMs for Ophthalmology), a standardized and comprehensive
evaluation benchmark developed through multiple rounds of expert checking by 13
ophthalmologists. BELO assesses ophthalmology-related clinical accuracy and
reasoning quality. Using keyword matching and a fine-tuned PubMedBERT model, we
curated ophthalmology-specific multiple-choice-questions (MCQs) from diverse
medical datasets (BCSC, MedMCQA, MedQA, BioASQ, and PubMedQA). The dataset
underwent multiple rounds of expert checking. Duplicate and substandard
questions were systematically removed. Ten ophthalmologists refined the
explanations of each MCQ's correct answer. This was further adjudicated by
three senior ophthalmologists. To illustrate BELO's utility, we evaluated six
LLMs (OpenAI o1, o3-mini, GPT-4o, DeepSeek-R1, Llama-3-8B, and Gemini 1.5 Pro)
using accuracy, macro-F1, and five text-generation metrics (ROUGE-L, BERTScore,
BARTScore, METEOR, and AlignScore). In a further evaluation involving human
experts, two ophthalmologists qualitatively reviewed 50 randomly selected
outputs for accuracy, comprehensiveness, and completeness. BELO consists of 900
high-quality, expert-reviewed questions aggregated from five sources: BCSC
(260), BioASQ (10), MedMCQA (572), MedQA (40), and PubMedQA (18). A public
leaderboard has been established to promote transparent evaluation and
reporting. Importantly, the BELO dataset will remain a hold-out,
evaluation-only benchmark to ensure fair and reproducible comparisons of future
models.

</details>


### [135] [Understanding Large Language Models' Ability on Interdisciplinary Research](https://arxiv.org/abs/2507.15736)
*Yuanhao Shen,Daniel Xavier de Sousa,Ricardo Marçal,Ali Asad,Hongyu Guo,Xiaodan Zhu*

Main category: cs.CL

TL;DR: IDRBench是一个新基准，用于评估大语言模型（LLMs）在跨学科研究（IDR）中提出创新想法的能力，发现LLMs在此领域仍有局限。


<details>
  <summary>Details</summary>
Motivation: 现有基准缺乏对LLMs在跨学科研究中创新能力的评估，阻碍了对其潜力的全面理解。

Method: 引入IDRBench，包含专家标注的数据集和任务，覆盖六个学科，评估LLMs在IDR中的表现。

Result: 测试10种LLMs后发现，尽管有一定IDR意识，但LLMs生成高质量IDR想法的能力仍不足。

Conclusion: IDRBench为评估LLMs在跨学科研究中的表现提供了系统框架，并揭示了其局限性，为未来研究指明方向。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed their
impressive ability to perform multi-step, logic-driven reasoning across complex
domains, positioning them as powerful tools and collaborators in scientific
discovery while challenging the long-held view that inspiration-driven ideation
is uniquely human. However, the lack of a dedicated benchmark that evaluates
LLMs' ability to develop ideas in Interdisciplinary Research (IDR) settings
poses a critical barrier to fully understanding their strengths and
limitations. To address this gap, we introduce IDRBench -- a pioneering
benchmark featuring an expert annotated dataset and a suite of tasks tailored
to evaluate LLMs' capabilities in proposing valuable research ideas from
different scientific domains for interdisciplinary research. This benchmark
aims to provide a systematic framework for assessing LLM performance in
complex, cross-domain scientific research. Our dataset consists of scientific
publications sourced from the ArXiv platform covering six distinct disciplines,
and is annotated by domain experts with diverse academic backgrounds. To ensure
high-quality annotations, we emphasize clearly defined dimensions that
characterize authentic interdisciplinary research. The design of evaluation
tasks in IDRBench follows a progressive, real-world perspective, reflecting the
natural stages of interdisciplinary research development, including 1) IDR
Paper Identification, 2) IDR Idea Integration, and 3) IDR Idea Recommendation.
Using IDRBench, we construct baselines across 10 LLMs and observe that despite
fostering some level of IDR awareness, LLMs still struggle to produce quality
IDR ideas. These findings could not only spark new research directions, but
also help to develop next-generation LLMs that excel in interdisciplinary
research.

</details>


### [136] [A Fisher's exact test justification of the TF-IDF term-weighting scheme](https://arxiv.org/abs/2507.15742)
*Paul Sheridan,Zeyad Ahmed,Aitazaz A. Farooque*

Main category: cs.CL

TL;DR: 本文从显著性检验的角度解释了TF-IDF的数学表达式，并证明其与Fisher精确检验的负对数p值密切相关。


<details>
  <summary>Details</summary>
Motivation: 为TF-IDF提供一个统计学上的理论基础，解释其长期有效的原理。

Method: 通过分析TF-ICF变体与Fisher精确检验的p值之间的关系，并在理想化假设下建立联系。

Result: 证明了TF-ICF与Fisher精确检验的负对数p值密切相关，且在无限大文档集合的极限情况下收敛于TF-IDF。

Conclusion: TF-IDF的有效性可以通过Fisher精确检验的统计学视角得到解释，为统计学家提供了理论支持。

Abstract: Term frequency-inverse document frequency, or TF-IDF for short, is arguably
the most celebrated mathematical expression in the history of information
retrieval. Conceived as a simple heuristic quantifying the extent to which a
given term's occurrences are concentrated in any one given document out of
many, TF-IDF and its many variants are routinely used as term-weighting schemes
in diverse text analysis applications. There is a growing body of scholarship
dedicated to placing TF-IDF on a sound theoretical foundation. Building on that
tradition, this paper justifies the use of TF-IDF to the statistics community
by demonstrating how the famed expression can be understood from a significance
testing perspective. We show that the common TF-IDF variant TF-ICF is, under
mild regularity conditions, closely related to the negative logarithm of the
$p$-value from a one-tailed version of Fisher's exact test of statistical
significance. As a corollary, we establish a connection between TF-IDF and the
said negative log-transformed $p$-value under certain idealized assumptions. We
further demonstrate, as a limiting case, that this same quantity converges to
TF-IDF in the limit of an infinitely large document collection. The Fisher's
exact test justification of TF-IDF equips the working statistician with a ready
explanation of the term-weighting scheme's long-established effectiveness.

</details>


### [137] [DialogueForge: LLM Simulation of Human-Chatbot Dialogue](https://arxiv.org/abs/2507.15752)
*Ruizhe Zhu,Hao Zhu,Yaxuan Li,Syang Zhou,Shijing Cai,Malgorzata Lazuka,Elliott Ash*

Main category: cs.CL

TL;DR: DialogueForge框架通过AI模拟生成人机对话，利用种子提示和多种LLM模型生成多轮对话，并通过微调技术提升小模型性能。


<details>
  <summary>Details</summary>
Motivation: 减少人工收集对话数据的成本和时间，推动对话AI研究。

Method: 使用种子提示初始化对话，测试多种LLM模型（包括GPT-4o和开源模型），并探索微调技术。

Result: 大模型（如GPT-4o）生成更真实的对话，小模型（如Llama）通过微调性能显著提升。

Conclusion: 大模型表现更优，但小模型通过微调具备潜力；生成连贯长对话仍是挑战。

Abstract: Collecting human-chatbot dialogues typically demands substantial manual
effort and is time-consuming, which limits and poses challenges for research on
conversational AI. In this work, we propose DialogueForge - a framework for
generating AI-simulated conversations in human-chatbot style. To initialize
each generated conversation, DialogueForge uses seed prompts extracted from
real human-chatbot interactions. We test a variety of LLMs to simulate the
human chatbot user, ranging from state-of-the-art proprietary models to
small-scale open-source LLMs, and generate multi-turn dialogues tailored to
specific tasks. In addition, we explore fine-tuning techniques to enhance the
ability of smaller models to produce indistinguishable human-like dialogues. We
evaluate the quality of the simulated conversations and compare different
models using the UniEval and GTEval evaluation protocols. Our experiments show
that large proprietary models (e.g., GPT-4o) generally outperform others in
generating more realistic dialogues, while smaller open-source models (e.g.,
Llama, Mistral) offer promising performance with greater customization. We
demonstrate that the performance of smaller models can be significantly
improved by employing supervised fine-tuning techniques. Nevertheless,
maintaining coherent and natural long-form human-like dialogues remains a
common challenge across all models.

</details>


### [138] [Interaction as Intelligence: Deep Research With Human-AI Partnership](https://arxiv.org/abs/2507.15759)
*Lyumanshan Ye,Xiaojie Cai,Xinkai Wang,Junfei Wang,Xiangkun Hu,Jiadi Su,Yang Nan,Sihan Wang,Bohan Zhang,Xiaoze Fan,Jinbin Luo,Yuxiang Zheng,Tianze Xu,Dayuan Fu,Yunze Wu,Pengrui Lu,Zengzhi Wang,Yiwei Qin,Zhen Huang,Yan Ma,Zhulin Hu,Haoyang Zou,Tiantian Mi,Yixin Ye,Ethan Chern,Pengfei Liu*

Main category: cs.CL

TL;DR: 论文提出将交互视为智能的核心维度，而非传统AI中的简单接口，并介绍了Deep Cognition系统，通过透明、可控的交互和双向对话提升研究任务的效率。


<details>
  <summary>Details</summary>
Motivation: 传统AI系统采用“输入-等待-输出”模式，导致错误累积、研究边界僵化及专家知识整合不足，需重新定义交互在智能中的作用。

Method: 提出Deep Cognition系统，实现透明可控的交互、细粒度双向对话及共享认知上下文，支持人类在关键节点干预AI思考过程。

Result: 用户评估显示，该系统在透明度、细粒度交互、实时干预等六项指标上显著优于基线，研究任务性能提升31.8%至50.0%。

Conclusion: 交互是智能的核心组成部分，Deep Cognition通过认知监督模式显著提升了AI系统在研究任务中的表现。

Abstract: This paper introduces "Interaction as Intelligence" research series,
presenting a reconceptualization of human-AI relationships in deep research
tasks. Traditional approaches treat interaction merely as an interface for
accessing AI capabilities-a conduit between human intent and machine output. We
propose that interaction itself constitutes a fundamental dimension of
intelligence. As AI systems engage in extended thinking processes for research
tasks, meaningful interaction transitions from an optional enhancement to an
essential component of effective intelligence. Current deep research systems
adopt an "input-wait-output" paradigm where users initiate queries and receive
results after black-box processing. This approach leads to error cascade
effects, inflexible research boundaries that prevent question refinement during
investigation, and missed opportunities for expertise integration. To address
these limitations, we introduce Deep Cognition, a system that transforms the
human role from giving instructions to cognitive oversight-a mode of engagement
where humans guide AI thinking processes through strategic intervention at
critical junctures. Deep cognition implements three key innovations:
(1)Transparent, controllable, and interruptible interaction that reveals AI
reasoning and enables intervention at any point; (2)Fine-grained bidirectional
dialogue; and (3)Shared cognitive context where the system observes and adapts
to user behaviors without explicit instruction. User evaluation demonstrates
that this cognitive oversight paradigm outperforms the strongest baseline
across six key metrics: Transparency(+20.0%), Fine-Grained Interaction(+29.2%),
Real-Time Intervention(+18.5%), Ease of Collaboration(+27.7%),
Results-Worth-Effort(+8.8%), and Interruptibility(+20.7%). Evaluations on
challenging research problems show 31.8% to 50.0% points of improvements over
deep research systems.

</details>


### [139] [Supernova: Achieving More with Less in Transformer Architectures](https://arxiv.org/abs/2507.15773)
*Andrei-Valentin Tanase,Elena Pelican*

Main category: cs.CL

TL;DR: Supernova是一个650M参数的解码器Transformer，通过架构设计和分词创新，在保持计算效率的同时达到更大模型的性能。


<details>
  <summary>Details</summary>
Motivation: 挑战主流扩展范式，证明架构效率和分词质量可以弥补参数数量的减少。

Method: 结合RoPE、GQA（3:1压缩比）、RMSNorm和SwiGLU激活函数，使用128,000词汇的字节级BPE分词器。

Result: Supernova达到1B参数模型90%的性能，参数减少53%，仅需100B训练token。

Conclusion: 架构效率和分词创新可显著减少参数和训练数据需求，挑战了传统扩展方法。

Abstract: We present Supernova, a 650M-parameter decoder-only transformer that
demonstrates how careful architectural design and tokenization innovation can
achieve the performance of larger models while maintaining computational
efficiency. Our architecture combines Rotary Positional Embeddings (RoPE),
Grouped Query Attention (GQA) with a 3:1 compression ratio, RMSNorm for
computational efficiency, and SwiGLU activation functions. A critical
innovation is our custom 128,000-vocabulary byte-level BPE tokenizer, which
achieves state-of-the-art compression performance. Through detailed analysis,
we show that Supernova achieves 90% of the performance of 1B-parameter models
while using 53% fewer parameters and requiring only 100B training tokens--an
order of magnitude less than competing models. Our findings challenge the
prevailing scaling paradigm, demonstrating that architectural efficiency and
tokenization quality can compensate for reduced parameter counts.

</details>


### [140] [Stabilizing Knowledge, Promoting Reasoning: Dual-Token Constraints for RLVR](https://arxiv.org/abs/2507.15778)
*Jiakang Wang,Runze Liu,Fuzheng Zhang,Xiu Li,Guorui Zhou*

Main category: cs.CL

TL;DR: Archer提出了一种基于熵感知的RLVR方法，通过双令牌约束和同步更新，显著提升了LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有RLVR方法对所有令牌采用统一训练信号，未区分低熵知识令牌和高熵推理令牌的不同作用，可能导致语义依赖断裂。

Method: Archer采用双令牌约束和同步更新，对推理令牌应用较弱的KL正则化和较高的剪裁阈值以鼓励探索，对知识令牌施加更强约束以保持事实知识。

Result: 在数学推理和代码生成基准测试中，Archer显著优于现有RLVR方法，达到或超越同类模型的最先进性能。

Conclusion: Archer通过熵感知的令牌区分和同步更新，有效提升了LLM的推理能力，同时保持了知识完整性。

Abstract: Reinforcement Learning with Verifiable Rewards (RLVR) has become an effective
post-training method for improving the reasoning abilities of Large Language
Models (LLMs), mainly by shaping higher-order behaviors such as reflection and
planning. However, previous RLVR algorithms often apply uniform training
signals to all tokens, without considering the different roles of low-entropy
knowledge-related tokens and high-entropy reasoning-related tokens. Some recent
methods try to separate these token types by gradient masking or asynchronous
updates, but these approaches may break semantic dependencies in the model
output and hinder effective learning. In this work, we propose Archer, an
entropy-aware RLVR approach with dual-token constraints and synchronous
updates. Specifically, our method applies weaker KL regularization and higher
clipping thresholds to reasoning tokens to encourage exploration, while using
stronger constraints on knowledge tokens to maintain factual knowledge.
Experimental results on several mathematical reasoning and code generation
benchmarks show that our approach significantly outperforms previous RLVR
methods, reaching or exceeding state-of-the-art performance among models of
comparable size. The code is available at
https://github.com/wizard-III/ArcherCodeR.

</details>


### [141] [Reservoir Computing as a Language Model](https://arxiv.org/abs/2507.15779)
*Felix Köster,Atsushi Uchida*

Main category: cs.CL

TL;DR: 比较了三种字符级语言建模方法，包括两种储层计算方法和基于Transformer的架构，发现Transformer在预测质量上表现优异，而储层计算在训练和推理速度上更高效。


<details>
  <summary>Details</summary>
Motivation: 解决大型语言模型（LLM）的高能耗和慢速处理问题，探索储层计算在自然文本处理中的潜力。

Method: 比较了两种储层计算方法和Transformer架构，通过调整可训练参数数量，评估性能、计算成本和预测准确性。

Result: Transformer在预测质量上表现更好，储层计算在速度和效率上更优；注意力增强的储层计算表现优于传统储层计算。

Conclusion: 储层计算在资源受限场景下具有潜力，但需权衡性能与效率；注意力机制可提升储层计算的动态适应性。

Abstract: Large Language Models (LLM) have dominated the science and media landscape
duo to their impressive performance on processing large chunks of data and
produce human-like levels of text. Nevertheless, their huge energy demand and
slow processing still a bottleneck for further increasing quality while also
making the models accessible to everyone. To solve this bottleneck, we will
investigate how reservoir computing performs on natural text processing, which
could enable fast and energy efficient hardware implementations. Studies
investigating the use of reservoir computing as a language model remain sparse.
In this paper, we compare three distinct approaches for character-level
language modeling, two different reservoir computing approaches, where only an
output layer is trainable, and the well-known transformer-based architectures,
which fully learn an attention-based sequence representation. We explore the
performance, computational cost and prediction accuracy for both paradigms by
equally varying the number of trainable parameters for all models. Using a
consistent pipeline for all three approaches, we demonstrate that transformers
excel in prediction quality, whereas reservoir computers remain highly
efficient reducing the training and inference speed. Furthermore, we
investigate two types of reservoir computing: a traditional reservoir with a
static linear readout, and an attention-enhanced reservoir that dynamically
adapts its output weights via an attention mechanism. Our findings underline
how these paradigms scale and offer guidelines to balance resource constraints
with performance.

</details>


### [142] [Operationalizing AI for Good: Spotlight on Deployment and Integration of AI Models in Humanitarian Work](https://arxiv.org/abs/2507.15823)
*Anton Abilov,Ke Zhang,Hemank Lamba,Elizabeth M. Olson,Joel R. Tetreault,Alejandro Jaimes*

Main category: cs.CL

TL;DR: 论文探讨了AI在公益领域的实际部署与合作过程，填补了现有研究的空白。


<details>
  <summary>Details</summary>
Motivation: 现有AI公益研究多关注模型开发，缺乏对实际部署、合作及影响的讨论。

Method: 与H2H组织紧密合作，在资源有限环境中部署并维护AI模型。

Result: 分享了实际部署经验及关键实践建议。

Conclusion: 强调了合作与持续维护对AI公益项目成功的重要性。

Abstract: Publications in the AI for Good space have tended to focus on the research
and model development that can support high-impact applications. However, very
few AI for Good papers discuss the process of deploying and collaborating with
the partner organization, and the resulting real-world impact. In this work, we
share details about the close collaboration with a humanitarian-to-humanitarian
(H2H) organization and how to not only deploy the AI model in a
resource-constrained environment, but also how to maintain it for continuous
performance updates, and share key takeaways for practitioners.

</details>


### [143] [The Impact of Language Mixing on Bilingual LLM Reasoning](https://arxiv.org/abs/2507.15849)
*Yihao Li,Jiayi Xin,Miranda Muqing Miao,Qi Long,Lyle Ungar*

Main category: cs.CL

TL;DR: 研究发现，双语大语言模型在推理过程中混合语言能提升准确性，强制单语解码会降低表现。


<details>
  <summary>Details</summary>
Motivation: 探讨语言混合在双语推理模型中的作用及其对推理能力的影响。

Method: 使用强化学习与可验证奖励（RLVR）训练模型，分析语言混合行为，并设计轻量级探针预测语言切换效果。

Result: 语言混合提升数学推理任务准确性5.6个百分点，探针引导解码可进一步提高6.25个百分点。

Conclusion: 语言混合是双语模型的策略性推理行为，而非训练副产品。

Abstract: Proficient multilingual speakers often intentionally switch languages in the
middle of a conversation. Similarly, recent reasoning-focused bilingual large
language models (LLMs) with strong capabilities in both languages exhibit
language mixing--alternating languages within their chain of thought.
Discouraging this behavior in DeepSeek-R1 was found to degrade accuracy,
suggesting that language mixing may benefit reasoning. In this work, we study
language switching in Chinese-English bilingual reasoning models. We identify
reinforcement learning with verifiable rewards (RLVR) as the critical training
stage that leads to language mixing. We demonstrate that language mixing can
enhance reasoning: enforcing monolingual decoding reduces accuracy by 5.6
percentage points on math reasoning tasks. Additionally, a lightweight probe
can be trained to predict whether a potential language switch would benefit or
harm reasoning, and when used to guide decoding, increases accuracy by up to
6.25 percentage points. Our findings suggest that language mixing is not merely
a byproduct of multilingual training, but is a strategic reasoning behavior.

</details>


### [144] [3LM: Bridging Arabic, STEM, and Code through Benchmarking](https://arxiv.org/abs/2507.15850)
*Basma El Amel Boussaha,Leen AlQadi,Mugariya Farooq,Shaikha Alsuwaidi,Giulia Campesan,Ahmed Alzubaidi,Mohammed Alyafeai,Hakim Hacid*

Main category: cs.CL

TL;DR: 论文提出了3LM，一套针对阿拉伯语的三个基准测试，填补了阿拉伯语LLM在STEM和代码生成领域的空白。


<details>
  <summary>Details</summary>
Motivation: 阿拉伯语作为全球广泛使用的语言，其LLM研究在STEM和代码生成领域缺乏基准测试，限制了实际应用的发展。

Method: 设计了三个基准测试：基于阿拉伯语教材的自然STEM问答、合成的STEM问题，以及通过人工翻译和审核构建的代码生成测试。

Result: 发布了三个高质量的阿拉伯语基准测试，支持LLM在STEM和代码生成领域的研究。

Conclusion: 3LM填补了阿拉伯语LLM研究的关键空白，为相关领域的进一步发展提供了支持。

Abstract: Arabic is one of the most widely spoken languages in the world, yet efforts
to develop and evaluate Large Language Models (LLMs) for Arabic remain
relatively limited. Most existing Arabic benchmarks focus on linguistic,
cultural, or religious content, leaving a significant gap in domains like STEM
and code which are increasingly relevant for real-world LLM applications. To
help bridge this gap, we present 3LM, a suite of three benchmarks designed
specifically for Arabic. The first is a set of STEM-related question-answer
pairs, naturally sourced from Arabic textbooks and educational worksheets. The
second consists of synthetically generated STEM questions, created using the
same sources. The third benchmark focuses on code generation, built through a
careful translation of two widely used code benchmarks, incorporating a
human-in-the-loop process with several rounds of review to ensure high-quality
and faithful translations. We release all three benchmarks publicly to support
the growth of Arabic LLM research in these essential but underrepresented
areas.

</details>
