{"id": "2507.08806", "categories": ["cs.AI", "cs.CL", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.08806", "abs": "https://arxiv.org/abs/2507.08806", "authors": ["Daewon Choi", "Jimin Lee", "Jihoon Tack", "Woomin Song", "Saket Dingliwal", "Sai Muralidhar Jayanthi", "Bhavana Ganesh", "Jinwoo Shin", "Aram Galstyan", "Sravan Babu Bodapati"], "title": "Think Clearly: Improving Reasoning via Redundant Token Pruning", "comment": null, "summary": "Recent large language models have shown promising capabilities in long-form\nreasoning, following structured chains of thought before arriving at a final\nanswer. However, we observe that these reasoning paths tend to include\nsubstantial redundancy; analyzing attention patterns reveals that attention\nscores are widely scattered, particularly incorrect answers exhibit greater\nattention sparsity. In this paper, we demonstrate that deliberately removing\nthis redundancy in the reasoning process significantly improves performance\nthrough clear thinking, i.e., removing distraction. Specifically, we\nsystematically identify reasoning redundancy by measuring token-level attention\nscores to a special end-of-thinking token, which is appended to an explicit\ninstruction inserted to conclude each intermediate reasoning step. Furthermore,\nwe propose structure-aware pruning that prioritizes removing tokens in\nlow-contributing reasoning chunks over individual tokens. After evicting\nredundant tokens, we remove the injected end-of-thinking instruction, then\nresume the reasoning generation. We demonstrate that our method significantly\nimproves overall accuracy across reasoning-intensive benchmarks without any\ntraining involved. In particular, our method shows strong performance on\nchallenging mathematical competition benchmarks such as AIME and AMC, where\nreasoning redundancy is more prevalent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u901a\u8fc7\u53bb\u9664\u63a8\u7406\u8fc7\u7a0b\u4e2d\u7684\u5197\u4f59\u6ce8\u610f\u529b\uff0c\u663e\u8457\u63d0\u5347\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\u4efb\u52a1\u4e2d\u7684\u6027\u80fd\u3002", "motivation": "\u73b0\u6709\u5927\u8bed\u8a00\u6a21\u578b\u5728\u957f\u94fe\u63a8\u7406\u4e2d\u5b58\u5728\u6ce8\u610f\u529b\u5197\u4f59\u95ee\u9898\uff0c\u5c24\u5176\u662f\u9519\u8bef\u7b54\u6848\u4e2d\u6ce8\u610f\u529b\u7a00\u758f\u6027\u66f4\u9ad8\uff0c\u5f71\u54cd\u6027\u80fd\u3002", "method": "\u901a\u8fc7\u6d4b\u91cf\u7279\u6b8a\u2018\u601d\u8003\u7ed3\u675f\u2019\u6807\u8bb0\u7684\u6ce8\u610f\u529b\u5206\u6570\u8bc6\u522b\u5197\u4f59\uff0c\u91c7\u7528\u7ed3\u6784\u611f\u77e5\u526a\u679d\u53bb\u9664\u4f4e\u8d21\u732e\u63a8\u7406\u5757\uff0c\u5e76\u6062\u590d\u751f\u6210\u3002", "result": "\u65b9\u6cd5\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u5bc6\u96c6\u578b\u4efb\u52a1\u7684\u51c6\u786e\u6027\uff0c\u5c24\u5176\u5728\u6570\u5b66\u7ade\u8d5b\u57fa\u51c6\uff08\u5982AIME\u548cAMC\uff09\u4e0a\u8868\u73b0\u7a81\u51fa\u3002", "conclusion": "\u53bb\u9664\u63a8\u7406\u5197\u4f59\u80fd\u6709\u6548\u63d0\u5347\u6a21\u578b\u6027\u80fd\uff0c\u65e0\u9700\u989d\u5916\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u590d\u6742\u63a8\u7406\u4efb\u52a1\u3002"}}
{"id": "2507.08875", "categories": ["cs.AI", "90B50, 90C29, 90C08, 91A80, 91B06"], "pdf": "https://arxiv.org/pdf/2507.08875", "abs": "https://arxiv.org/abs/2507.08875", "authors": ["Fuh-Hwa Franklin Liu", "Su-Chuan Shih"], "title": "A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data", "comment": "38 pages, 6 figures, 5 table. A practice applicable method for\n  multi-criteria assessments using cardinal and ordinal data", "summary": "Modern methods for multi-criteria assessment (MCA), such as Data Envelopment\nAnalysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria\nDecision-Making (MCDM), are utilized to appraise a collection of\nDecision-Making Units (DMUs), also known as alternatives, based on several\ncriteria. These methodologies inherently rely on assumptions and can be\ninfluenced by subjective judgment to effectively tackle the complex evaluation\nchallenges in various fields. In real-world scenarios, it is essential to\nincorporate both quantitative and qualitative criteria as they consist of\ncardinal and ordinal data. Despite the inherent variability in the criterion\nvalues of different alternatives, the homogeneity assumption is often employed,\nsignificantly affecting evaluations. To tackle these challenges and determine\nthe most appropriate alternative, we propose a novel MCA approach that combines\ntwo Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear\nprogramming, is pivotal in the MCA methodology. This approach improves\nefficiency and fairness, ensuring that evaluations are both comprehensive and\ndependable, thus offering a strong and adaptive solution. Two comprehensive\nnumerical examples demonstrate the accuracy and transparency of our proposed\nmethod. The goal is to encourage continued advancement and stimulate progress\nin automated decision systems and decision support systems.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u4e24\u79cd\u865a\u62df\u5dee\u8ddd\u5206\u6790\uff08VGA\uff09\u6a21\u578b\u7684\u65b0\u578b\u591a\u51c6\u5219\u8bc4\u4f30\uff08MCA\uff09\u65b9\u6cd5\uff0c\u4ee5\u63d0\u9ad8\u8bc4\u4f30\u7684\u6548\u7387\u548c\u516c\u5e73\u6027\u3002", "motivation": "\u73b0\u6709MCA\u65b9\u6cd5\u4f9d\u8d56\u5047\u8bbe\u548c\u4e3b\u89c2\u5224\u65ad\uff0c\u96be\u4ee5\u5904\u7406\u590d\u6742\u8bc4\u4f30\u95ee\u9898\uff0c\u4e14\u5e38\u5ffd\u7565\u5b9a\u91cf\u548c\u5b9a\u6027\u51c6\u5219\u7684\u5dee\u5f02\u3002", "method": "\u7ed3\u5408\u4e24\u79cdVGA\u6a21\u578b\uff0c\u57fa\u4e8e\u7ebf\u6027\u89c4\u5212\uff0c\u6539\u8fdbMCA\u65b9\u6cd5\u3002", "result": "\u901a\u8fc7\u6570\u503c\u793a\u4f8b\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u51c6\u786e\u6027\u548c\u900f\u660e\u5ea6\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u81ea\u52a8\u5316\u51b3\u7b56\u7cfb\u7edf\u63d0\u4f9b\u4e86\u53ef\u9760\u4e14\u7075\u6d3b\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u63a8\u52a8\u4e86\u51b3\u7b56\u652f\u6301\u7cfb\u7edf\u7684\u8fdb\u6b65\u3002"}}
{"id": "2507.08892", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.08892", "abs": "https://arxiv.org/abs/2507.08892", "authors": ["Alexander Sasha Vezhnevets", "Jayd Matyas", "Logan Cross", "Davide Paglieri", "Minsuk Chang", "William A. Cunningham", "Simon Osindero", "William S. Isaac", "Joel Z. Leibo"], "title": "Multi-Actor Generative Artificial Intelligence as a Game Engine", "comment": "13 pages", "summary": "Generative AI can be used in multi-actor environments with purposes ranging\nfrom social science modeling to interactive narrative and AI evaluation.\nSupporting this diversity of use cases -- which we classify as Simulationist,\nDramatist, and Evaluationist -- demands a flexible scenario definition\nframework. We argue here that a good approach is to take inspiration from\ntabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible\nfor the environment and generates all parts of the story not directly\ndetermined by the voluntary actions of player characters. We argue that the\nEntity-Component architectural pattern is useful here. In such a system, the GM\nis not a hardcoded computer game but is itself a configurable entity, composed\nof components just like any other actor. By design, the approach allows for a\nseparation between the underlying implementation details handled by an\nengineer, the creation of reusable components, and their composition and\nconfiguration managed by a designer who constructs entities from the\ncomponents. This separation of concerns is instrumental for achieving rapid\niteration, maintaining modularity, and ultimately to ensure scalability. We\ndescribe the ongoing evolution of the Concordia library in terms of this\nphilosophy, demonstrating how it allows users to effectively configure\nscenarios that align with their specific goals.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u684c\u6e38\u89d2\u8272\u626e\u6f14\u6e38\u620f\uff08TTRPGs\uff09\u7684\u7075\u6d3b\u573a\u666f\u5b9a\u4e49\u6846\u67b6\uff0c\u7528\u4e8e\u652f\u6301\u751f\u6210\u5f0fAI\u5728\u591a\u89d2\u8272\u73af\u5883\u4e2d\u7684\u591a\u6837\u5316\u5e94\u7528\u3002", "motivation": "\u4e3a\u4e86\u6ee1\u8db3\u751f\u6210\u5f0fAI\u5728\u6a21\u62df\u3001\u620f\u5267\u5316\u548c\u8bc4\u4f30\u7b49\u591a\u6837\u5316\u5e94\u7528\u573a\u666f\u4e2d\u7684\u9700\u6c42\uff0c\u9700\u8981\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\u6765\u5b9a\u4e49\u548c\u7ba1\u7406\u573a\u666f\u3002", "method": "\u501f\u9274TTRPGs\u4e2d\u7684\u6e38\u620f\u4e3b\u6301\u4eba\uff08GM\uff09\u6982\u5ff5\uff0c\u91c7\u7528\u5b9e\u4f53-\u7ec4\u4ef6\u67b6\u6784\u6a21\u5f0f\uff0c\u5c06GM\u8bbe\u8ba1\u4e3a\u53ef\u914d\u7f6e\u7684\u5b9e\u4f53\uff0c\u7531\u7ec4\u4ef6\u6784\u6210\uff0c\u5b9e\u73b0\u5de5\u7a0b\u5e08\u4e0e\u8bbe\u8ba1\u5e08\u7684\u5206\u5de5\u534f\u4f5c\u3002", "result": "\u901a\u8fc7Concordia\u5e93\u7684\u5b9e\u8df5\uff0c\u8bc1\u660e\u4e86\u8be5\u6846\u67b6\u80fd\u591f\u6709\u6548\u652f\u6301\u7528\u6237\u6839\u636e\u5177\u4f53\u76ee\u6807\u914d\u7f6e\u573a\u666f\uff0c\u5b9e\u73b0\u5feb\u901f\u8fed\u4ee3\u548c\u6a21\u5757\u5316\u3002", "conclusion": "\u8be5\u6846\u67b6\u901a\u8fc7\u5206\u79bb\u5173\u6ce8\u70b9\uff0c\u63d0\u5347\u4e86\u751f\u6210\u5f0fAI\u5728\u591a\u89d2\u8272\u73af\u5883\u4e2d\u7684\u7075\u6d3b\u6027\u548c\u53ef\u6269\u5c55\u6027\u3002"}}
{"id": "2507.09080", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09080", "abs": "https://arxiv.org/abs/2507.09080", "authors": ["Athanasios Trantas", "Martino Mensio", "Stylianos Stasinos", "Sebastian Gribincea", "Taimur Khan", "Damian Podareanu", "Aliene van der Veen"], "title": "BioAnalyst: A Foundation Model for Biodiversity", "comment": null, "summary": "The accelerating loss of biodiversity presents critical challenges for\necological research and conservation strategies. The preservation of\nbiodiversity is paramount for maintaining ecological balance and ensuring the\nsustainability of ecosystems. However, biodiversity faces numerous threats,\nincluding habitat loss, climate change, and the proliferation of invasive\nspecies. Addressing these and other ecology-related challenges, both at local\nand global scales, requires comprehensive monitoring, predictive and\nconservation planning capabilities. Artificial Intelligence (AI) Foundation\nModels (FMs) have gained significant momentum in numerous scientific domains by\nleveraging vast datasets to learn general-purpose representations adaptable to\nvarious downstream tasks. This paradigm holds immense promise for biodiversity\nconservation. In response, we introduce BioAnalyst, the first Foundation Model\ntailored for biodiversity analysis and conservation planning. BioAnalyst\nemploys a transformer-based architecture, pre-trained on extensive multi-modal\ndatasets encompassing species occurrence records, remote sensing indicators,\nclimate and environmental variables. BioAnalyst is designed for adaptability,\nallowing for fine-tuning of a range of downstream tasks, such as species\ndistribution modelling, habitat suitability assessments, invasive species\ndetection, and population trend forecasting. We evaluate the model's\nperformance on two downstream use cases, demonstrating its generalisability\ncompared to existing methods, particularly in data-scarce scenarios for two\ndistinct use-cases, establishing a new accuracy baseline for ecological\nforecasting. By openly releasing BioAnalyst and its fine-tuning workflows to\nthe scientific community, we aim to foster collaborative efforts in\nbiodiversity modelling and advance AI-driven solutions to pressing ecological\nchallenges.", "AI": {"tldr": "BioAnalyst\u662f\u4e00\u4e2a\u57fa\u4e8eTransformer\u67b6\u6784\u7684AI\u57fa\u7840\u6a21\u578b\uff0c\u4e13\u4e3a\u751f\u7269\u591a\u6837\u6027\u5206\u6790\u548c\u4fdd\u62a4\u89c4\u5212\u8bbe\u8ba1\uff0c\u901a\u8fc7\u591a\u6a21\u6001\u6570\u636e\u96c6\u9884\u8bad\u7ec3\uff0c\u9002\u7528\u4e8e\u591a\u79cd\u4e0b\u6e38\u4efb\u52a1\uff0c\u5e76\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u8868\u73b0\u51fa\u8272\u3002", "motivation": "\u751f\u7269\u591a\u6837\u6027\u4e27\u5931\u52a0\u901f\uff0c\u5a01\u80c1\u751f\u6001\u5e73\u8861\u548c\u53ef\u6301\u7eed\u6027\uff0c\u9700\u7efc\u5408\u76d1\u6d4b\u548c\u4fdd\u62a4\u89c4\u5212\u80fd\u529b\u3002AI\u57fa\u7840\u6a21\u578b\u5728\u79d1\u5b66\u9886\u57df\u8868\u73b0\u4f18\u5f02\uff0c\u6709\u671b\u7528\u4e8e\u751f\u7269\u591a\u6837\u6027\u4fdd\u62a4\u3002", "method": "BioAnalyst\u91c7\u7528Transformer\u67b6\u6784\uff0c\u9884\u8bad\u7ec3\u4e8e\u7269\u79cd\u8bb0\u5f55\u3001\u9065\u611f\u6570\u636e\u3001\u6c14\u5019\u548c\u73af\u5883\u53d8\u91cf\u7b49\u591a\u6a21\u6001\u6570\u636e\u96c6\uff0c\u53ef\u5fae\u8c03\u7528\u4e8e\u7269\u79cd\u5206\u5e03\u5efa\u6a21\u3001\u6816\u606f\u5730\u8bc4\u4f30\u7b49\u4efb\u52a1\u3002", "result": "\u6a21\u578b\u5728\u4e24\u79cd\u4e0b\u6e38\u4efb\u52a1\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u5c24\u5176\u5728\u6570\u636e\u7a00\u7f3a\u573a\u666f\u4e2d\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u4e3a\u751f\u6001\u9884\u6d4b\u8bbe\u5b9a\u4e86\u65b0\u7684\u51c6\u786e\u57fa\u51c6\u3002", "conclusion": "BioAnalyst\u7684\u5f00\u653e\u53d1\u5e03\u65e8\u5728\u4fc3\u8fdb\u751f\u7269\u591a\u6837\u6027\u5efa\u6a21\u5408\u4f5c\uff0c\u63a8\u52a8AI\u89e3\u51b3\u751f\u6001\u6311\u6218\u3002"}}
{"id": "2507.09089", "categories": ["cs.AI", "cs.HC", "cs.SE", "I.2"], "pdf": "https://arxiv.org/pdf/2507.09089", "abs": "https://arxiv.org/abs/2507.09089", "authors": ["Joel Becker", "Nate Rush", "Elizabeth Barnes", "David Rein"], "title": "Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity", "comment": "50 pages, 8 tables, 22 figures", "summary": "Despite widespread adoption, the impact of AI tools on software development\nin the wild remains understudied. We conduct a randomized controlled trial\n(RCT) to understand how AI tools at the February-June 2025 frontier affect the\nproductivity of experienced open-source developers. 16 developers with moderate\nAI experience complete 246 tasks in mature projects on which they have an\naverage of 5 years of prior experience. Each task is randomly assigned to allow\nor disallow usage of early 2025 AI tools. When AI tools are allowed, developers\nprimarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.\nBefore starting tasks, developers forecast that allowing AI will reduce\ncompletion time by 24%. After completing the study, developers estimate that\nallowing AI reduced completion time by 20%. Surprisingly, we find that allowing\nAI actually increases completion time by 19%--AI tooling slowed developers\ndown. This slowdown also contradicts predictions from experts in economics (39%\nshorter) and ML (38% shorter). To understand this result, we collect and\nevaluate evidence for 20 properties of our setting that a priori could\ncontribute to the observed slowdown effect--for example, the size and quality\nstandards of projects, or prior developer experience with AI tooling. Although\nthe influence of experimental artifacts cannot be entirely ruled out, the\nrobustness of the slowdown effect across our analyses suggests it is unlikely\nto primarily be a function of our experimental design.", "AI": {"tldr": "\u7814\u7a76\u53d1\u73b0\uff0c\u5c3d\u7ba1\u5f00\u53d1\u8005\u9884\u671fAI\u5de5\u5177\u80fd\u7f29\u77ed\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\uff0c\u4f46\u5b9e\u9645\u4e0aAI\u5de5\u5177\u53cd\u800c\u589e\u52a0\u4e8619%\u7684\u5b8c\u6210\u65f6\u95f4\uff0c\u4e0e\u7ecf\u6d4e\u5b66\u548c\u673a\u5668\u5b66\u4e60\u4e13\u5bb6\u7684\u9884\u6d4b\u76f8\u53cd\u3002", "motivation": "\u63a2\u8ba8AI\u5de5\u5177\u5728\u5b9e\u9645\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u5bf9\u5f00\u53d1\u8005\u751f\u4ea7\u529b\u7684\u5f71\u54cd\uff0c\u586b\u8865\u73b0\u6709\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u968f\u673a\u5bf9\u7167\u8bd5\u9a8c\uff08RCT\uff09\uff0c16\u540d\u6709\u4e2d\u7b49AI\u7ecf\u9a8c\u7684\u5f00\u53d1\u8005\u5b8c\u6210246\u9879\u4efb\u52a1\uff0c\u968f\u673a\u5206\u914d\u662f\u5426\u4f7f\u75282025\u5e74\u7684AI\u5de5\u5177\uff08\u5982Cursor Pro\u548cClaude 3.5/3.7 Sonnet\uff09\u3002", "result": "AI\u5de5\u5177\u7684\u4f7f\u7528\u5bfc\u81f4\u4efb\u52a1\u5b8c\u6210\u65f6\u95f4\u589e\u52a019%\uff0c\u4e0e\u5f00\u53d1\u8005\u9884\u671f\u768420%\u7f29\u77ed\u548c\u4e13\u5bb6\u9884\u6d4b\u768438-39%\u7f29\u77ed\u76f8\u53cd\u3002", "conclusion": "AI\u5de5\u5177\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\u53ef\u80fd\u5e76\u672a\u5982\u9884\u671f\u63d0\u5347\u751f\u4ea7\u529b\uff0c\u751a\u81f3\u53ef\u80fd\u56e0\u67d0\u4e9b\u56e0\u7d20\u5bfc\u81f4\u6548\u7387\u4e0b\u964d\u3002"}}
{"id": "2507.09179", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09179", "abs": "https://arxiv.org/abs/2507.09179", "authors": ["Ronghua Shi", "Yiou Liu", "Xinyu Ying", "Yang Tan", "Yuchun Feng", "Lynn Ai", "Bill Shi", "Xuhui Wang", "Zhuang Liu"], "title": "Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System", "comment": null, "summary": "Decentralized finance (DeFi) has introduced a new era of permissionless\nfinancial innovation but also led to unprecedented market manipulation. Without\ncentralized oversight, malicious actors coordinate shilling campaigns and\npump-and-dump schemes across various platforms. We propose a Multi-Agent\nReinforcement Learning (MARL) framework for decentralized manipulation\ndetection, modeling the interaction between manipulators and detectors as a\ndynamic adversarial game. This framework identifies suspicious patterns using\ndelayed token price reactions as financial indicators.Our method introduces\nthree innovations: (1) Group Relative Policy Optimization (GRPO) to enhance\nlearning stability in sparse-reward and partially observable settings; (2) a\ntheory-based reward function inspired by rational expectations and information\nasymmetry, differentiating price discovery from manipulation noise; and (3) a\nmulti-modal agent pipeline that integrates LLM-based semantic features, social\ngraph signals, and on-chain market data for informed decision-making.The\nframework is integrated within the Symphony system, a decentralized multi-agent\narchitecture enabling peer-to-peer agent execution and trust-aware learning\nthrough distributed logs, supporting chain-verifiable evaluation. Symphony\npromotes adversarial co-evolution among strategic actors and maintains robust\nmanipulation detection without centralized oracles, enabling real-time\nsurveillance across global DeFi ecosystems.Trained on 100,000 real-world\ndiscourse episodes and validated in adversarial simulations, Hide-and-Shill\nachieves top performance in detection accuracy and causal attribution. This\nwork bridges multi-agent systems with financial surveillance, advancing a new\nparadigm for decentralized market intelligence. All resources are available at\nthe Hide-and-Shill GitHub repository to promote open research and\nreproducibility.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u68c0\u6d4b\u53bb\u4e2d\u5fc3\u5316\u91d1\u878d\uff08DeFi\uff09\u4e2d\u7684\u5e02\u573a\u64cd\u7eb5\u884c\u4e3a\uff0c\u901a\u8fc7\u52a8\u6001\u5bf9\u6297\u6e38\u620f\u5efa\u6a21\u64cd\u7eb5\u8005\u4e0e\u68c0\u6d4b\u8005\u4e4b\u95f4\u7684\u4ea4\u4e92\u3002", "motivation": "DeFi\u7684\u65e0\u8bb8\u53ef\u521b\u65b0\u5e26\u6765\u4e86\u5e02\u573a\u64cd\u7eb5\u95ee\u9898\uff0c\u7f3a\u4e4f\u4e2d\u5fc3\u5316\u76d1\u7ba1\u5bfc\u81f4\u6076\u610f\u884c\u4e3a\u8005\u901a\u8fc7\u534f\u8c03\u6d3b\u52a8\u8fdb\u884c\u64cd\u7eb5\u3002", "method": "\u91c7\u7528MARL\u6846\u67b6\uff0c\u7ed3\u5408GRPO\u4f18\u5316\u3001\u7406\u8bba\u9a71\u52a8\u7684\u5956\u52b1\u51fd\u6570\u548c\u591a\u6a21\u6001\u667a\u80fd\u4f53\u7ba1\u9053\uff0c\u6574\u5408\u8bed\u4e49\u7279\u5f81\u3001\u793e\u4ea4\u56fe\u8c31\u548c\u94fe\u4e0a\u6570\u636e\u3002", "result": "\u5728\u771f\u5b9e\u6570\u636e\u548c\u5bf9\u6297\u6a21\u62df\u4e2d\u9a8c\u8bc1\uff0cHide-and-Shill\u5728\u68c0\u6d4b\u51c6\u786e\u6027\u548c\u56e0\u679c\u5f52\u56e0\u65b9\u9762\u8868\u73b0\u4f18\u5f02\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3a\u53bb\u4e2d\u5fc3\u5316\u5e02\u573a\u60c5\u62a5\u63d0\u4f9b\u4e86\u65b0\u8303\u5f0f\uff0c\u652f\u6301\u5b9e\u65f6\u76d1\u6d4b\u548c\u5f00\u653e\u7814\u7a76\u3002"}}
{"id": "2507.09329", "categories": ["cs.AI", "cs.CR"], "pdf": "https://arxiv.org/pdf/2507.09329", "abs": "https://arxiv.org/abs/2507.09329", "authors": ["Matous Kozak", "Roshanak Zilouchian Moghaddam", "Siva Sivaraman"], "title": "When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents", "comment": "15 pages", "summary": "LLM-based coding agents are rapidly being deployed in software development,\nyet their security implications remain poorly understood. These agents, while\ncapable of accelerating software development, may inadvertently introduce\ninsecure practices. We conducted the first systematic security evaluation of\nautonomous coding agents, analyzing over 12,000 actions across five\nstate-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world\nsoftware setup tasks. Our findings reveal significant security concerns: 21% of\nagent trajectories contained insecure actions, with models showing substantial\nvariation in security behavior. We developed a high-precision detection system\nthat identified four major vulnerability categories, with information exposure\n(CWE-200) being the most prevalent one. We also evaluated mitigation strategies\nincluding feedback mechanisms and security reminders with various effectiveness\nbetween models. GPT-4.1 demonstrated exceptional security awareness with 96.8%\nmitigation success. Our work provides the first comprehensive framework for\nevaluating coding agent security and highlights the need for security-aware\ndesign of next generation LLM-based coding agents.", "AI": {"tldr": "\u672c\u6587\u9996\u6b21\u7cfb\u7edf\u8bc4\u4f30\u4e86\u57fa\u4e8eLLM\u7684\u7f16\u7801\u4ee3\u7406\u7684\u5b89\u5168\u6027\uff0c\u53d1\u73b021%\u7684\u64cd\u4f5c\u5b58\u5728\u5b89\u5168\u9690\u60a3\uff0c\u5e76\u63d0\u51fa\u4e86\u68c0\u6d4b\u7cfb\u7edf\u548c\u7f13\u89e3\u7b56\u7565\u3002", "motivation": "\u7406\u89e3\u57fa\u4e8eLLM\u7684\u7f16\u7801\u4ee3\u7406\u5728\u8f6f\u4ef6\u5f00\u53d1\u4e2d\u7684\u5b89\u5168\u5f71\u54cd\uff0c\u586b\u8865\u5f53\u524d\u7814\u7a76\u7684\u7a7a\u767d\u3002", "method": "\u5206\u6790\u4e8612,000\u591a\u4e2a\u64cd\u4f5c\uff0c\u8986\u76d65\u4e2a\u5148\u8fdb\u6a21\u578b\uff08\u5982GPT-4o\u3001GPT-4.1\u7b49\uff09\u572893\u4e2a\u5b9e\u9645\u8f6f\u4ef6\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002", "result": "21%\u7684\u64cd\u4f5c\u4e0d\u5b89\u5168\uff0c\u4fe1\u606f\u6cc4\u9732\uff08CWE-200\uff09\u6700\u5e38\u89c1\uff1bGPT-4.1\u7684\u7f13\u89e3\u6210\u529f\u7387\u9ad8\u8fbe96.8%\u3002", "conclusion": "\u63d0\u51fa\u4e86\u9996\u4e2a\u7f16\u7801\u4ee3\u7406\u5b89\u5168\u8bc4\u4f30\u6846\u67b6\uff0c\u5f3a\u8c03\u4e0b\u4e00\u4ee3LLM\u7f16\u7801\u4ee3\u7406\u9700\u5177\u5907\u5b89\u5168\u610f\u8bc6\u8bbe\u8ba1\u3002"}}
{"id": "2507.09369", "categories": ["cs.AI", "68T01", "I.2.0"], "pdf": "https://arxiv.org/pdf/2507.09369", "abs": "https://arxiv.org/abs/2507.09369", "authors": ["Andrew Critch", "Jacob Tsimerman"], "title": "A Taxonomy of Omnicidal Futures Involving Artificial Intelligence", "comment": null, "summary": "This report presents a taxonomy and examples of potential omnicidal events\nresulting from AI: scenarios where all or almost all humans are killed. These\nevents are not presented as inevitable, but as possibilities that we can work\nto avoid. Insofar as large institutions require a degree of public support in\norder to take certain actions, we hope that by presenting these possibilities\nin public, we can help to support preventive measures against catastrophic\nrisks from AI.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5173\u4e8eAI\u53ef\u80fd\u5bfc\u81f4\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\u7684\u5206\u7c7b\u548c\u793a\u4f8b\uff0c\u65e8\u5728\u901a\u8fc7\u516c\u5f00\u8ba8\u8bba\u652f\u6301\u9884\u9632\u63aa\u65bd\u3002", "motivation": "\u63a2\u8ba8AI\u53ef\u80fd\u5e26\u6765\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u4ee5\u4fc3\u8fdb\u516c\u4f17\u652f\u6301\u9884\u9632\u63aa\u65bd\u3002", "method": "\u63d0\u51fa\u5206\u7c7b\u6cd5\u548c\u5177\u4f53\u793a\u4f8b\uff0c\u5206\u6790AI\u53ef\u80fd\u5bfc\u81f4\u7684\u5168\u4eba\u7c7b\u706d\u7edd\u4e8b\u4ef6\u3002", "result": "\u660e\u786e\u4e86AI\u53ef\u80fd\u5f15\u53d1\u7684\u707e\u96be\u6027\u98ce\u9669\uff0c\u5e76\u547c\u5401\u516c\u4f17\u5173\u6ce8\u548c\u9884\u9632\u3002", "conclusion": "\u901a\u8fc7\u516c\u5f00\u8ba8\u8bbaAI\u7684\u6f5c\u5728\u98ce\u9669\uff0c\u53ef\u4ee5\u63a8\u52a8\u793e\u4f1a\u91c7\u53d6\u9884\u9632\u63aa\u65bd\uff0c\u907f\u514d\u707e\u96be\u6027\u540e\u679c\u3002"}}
{"id": "2507.09374", "categories": ["cs.AI", "I.2.6; I.2.10"], "pdf": "https://arxiv.org/pdf/2507.09374", "abs": "https://arxiv.org/abs/2507.09374", "authors": ["Chenglin Zhu", "Tao Zhang", "Chong Li", "Mingan Lin", "Zenan Zhou", "Jian Xie"], "title": "EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique", "comment": "14 pages,4 figures", "summary": "Multimodal large language models (MLLMs) still perform poorly on scientific\ntasks, particularly those requiring multi-step and interpretable reasoning.\nTheir limitations include insufficient scientific reasoning patterns, lack of\nglobal coherence in multi-step inference, and the absence of reflective\nself-correction, making them unreliable in structured scientific contexts. We\nintroduce EduFlow, the first end-to-end framework that covers the full pipeline\nof educational scientific reasoning, including data selection, MCTS-based\ntrajectory construction, model training, and output optimization. At its core\nis EduPRM, a process-aware reward model that critiques reasoning steps with\ntags and justifications. EduPRM is trained via curriculum learning on three\ncomplementary supervision sources: MCTS-guided trajectories, error-injected\ncritiques, and teacher-student dialogues, enabling dynamic adaptation to\nmulti-stage problem solving and iterative refinement during inference. We\nfurther propose EduMCTS, a domain-adapted search framework that introduces\nbootstrapping actions specifically designed for educational reasoning, such as\na self-reflection mechanism that promotes reflective error correction. It\nfurther leverages EduPRM's fine-grained feedback to guide the search toward\nhigher-quality reasoning trajectories. By applying self-consistency and\nrejection sampling, we constructed EduMCTS-160K, a large-scale dataset of\neducational reasoning trajectories. Extensive experiments demonstrate that\nEduFlow enhances reasoning consistency and coherence. Code, data, and models\nwill be released.", "AI": {"tldr": "EduFlow\u662f\u4e00\u4e2a\u7aef\u5230\u7aef\u6846\u67b6\uff0c\u901a\u8fc7EduPRM\u548cEduMCTS\u63d0\u5347\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u7684\u63a8\u7406\u80fd\u529b\uff0c\u663e\u8457\u6539\u5584\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\u3002", "motivation": "\u591a\u6a21\u6001\u5927\u8bed\u8a00\u6a21\u578b\u5728\u79d1\u5b66\u4efb\u52a1\u4e2d\u8868\u73b0\u4e0d\u4f73\uff0c\u5c24\u5176\u662f\u5728\u9700\u8981\u591a\u6b65\u548c\u53ef\u89e3\u91ca\u63a8\u7406\u7684\u4efb\u52a1\u4e0a\uff0c\u5b58\u5728\u63a8\u7406\u6a21\u5f0f\u4e0d\u8db3\u3001\u5168\u5c40\u8fde\u8d2f\u6027\u7f3a\u4e4f\u548c\u81ea\u6821\u6b63\u7f3a\u5931\u7b49\u95ee\u9898\u3002", "method": "\u63d0\u51fa\u4e86EduFlow\u6846\u67b6\uff0c\u5305\u62ecEduPRM\uff08\u8fc7\u7a0b\u611f\u77e5\u5956\u52b1\u6a21\u578b\uff09\u548cEduMCTS\uff08\u9886\u57df\u9002\u5e94\u7684\u641c\u7d22\u6846\u67b6\uff09\uff0c\u7ed3\u5408\u8bfe\u7a0b\u5b66\u4e60\u548c\u591a\u6e90\u76d1\u7763\u8bad\u7ec3\uff0c\u4f18\u5316\u63a8\u7406\u8f68\u8ff9\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0cEduFlow\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u7684\u4e00\u81f4\u6027\u548c\u8fde\u8d2f\u6027\uff0c\u5e76\u6784\u5efa\u4e86EduMCTS-160K\u6570\u636e\u96c6\u3002", "conclusion": "EduFlow\u901a\u8fc7\u52a8\u6001\u9002\u5e94\u548c\u8fed\u4ee3\u4f18\u5316\uff0c\u4e3a\u79d1\u5b66\u63a8\u7406\u4efb\u52a1\u63d0\u4f9b\u4e86\u53ef\u9760\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09389", "categories": ["cs.AI", "cs.CY", "cs.IR"], "pdf": "https://arxiv.org/pdf/2507.09389", "abs": "https://arxiv.org/abs/2507.09389", "authors": ["Chris Davis Jaldi", "Anmol Saini", "Elham Ghiasi", "O. Divine Eziolise", "Cogan Shimizu"], "title": "Knowledge Conceptualization Impacts RAG Efficacy", "comment": null, "summary": "Explainability and interpretability are cornerstones of frontier and\nnext-generation artificial intelligence (AI) systems. This is especially true\nin recent systems, such as large language models (LLMs), and more broadly,\ngenerative AI. On the other hand, adaptability to new domains, contexts, or\nscenarios is also an important aspect for a successful system. As such, we are\nparticularly interested in how we can merge these two efforts, that is,\ninvestigating the design of transferable and interpretable neurosymbolic AI\nsystems. Specifically, we focus on a class of systems referred to as ''Agentic\nRetrieval-Augmented Generation'' systems, which actively select, interpret, and\nquery knowledge sources in response to natural language prompts. In this paper,\nwe systematically evaluate how different conceptualizations and representations\nof knowledge, particularly the structure and complexity, impact an AI agent (in\nthis case, an LLM) in effectively querying a triplestore. We report our\nresults, which show that there are impacts from both approaches, and we discuss\ntheir impact and implications.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u7ed3\u5408\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u8bbe\u8ba1\u53ef\u8fc1\u79fb\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u91cd\u70b9\u5173\u6ce8Agentic Retrieval-Augmented Generation\u7cfb\u7edf\uff0c\u5e76\u8bc4\u4f30\u77e5\u8bc6\u8868\u793a\u5bf9LLM\u67e5\u8be2\u4e09\u5143\u7ec4\u5b58\u50a8\u7684\u5f71\u54cd\u3002", "motivation": "\u53ef\u89e3\u91ca\u6027\u548c\u9002\u5e94\u6027\u662f\u4e0b\u4e00\u4ee3AI\u7cfb\u7edf\u7684\u5173\u952e\uff0c\u5c24\u5176\u662f\u5728LLM\u548c\u751f\u6210\u5f0fAI\u4e2d\u3002\u7814\u7a76\u65e8\u5728\u7ed3\u5408\u8fd9\u4e24\u70b9\uff0c\u8bbe\u8ba1\u53ef\u8fc1\u79fb\u4e14\u53ef\u89e3\u91ca\u7684\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u3002", "method": "\u7cfb\u7edf\u8bc4\u4f30\u4e0d\u540c\u77e5\u8bc6\u8868\u793a\uff08\u7ed3\u6784\u548c\u590d\u6742\u6027\uff09\u5bf9LLM\u67e5\u8be2\u4e09\u5143\u7ec4\u5b58\u50a8\u7684\u5f71\u54cd\uff0c\u91cd\u70b9\u5173\u6ce8Agentic Retrieval-Augmented Generation\u7cfb\u7edf\u3002", "result": "\u7ed3\u679c\u663e\u793a\u4e0d\u540c\u77e5\u8bc6\u8868\u793a\u5bf9LLM\u67e5\u8be2\u6548\u679c\u6709\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "\u7814\u7a76\u5f3a\u8c03\u4e86\u77e5\u8bc6\u8868\u793a\u5728AI\u7cfb\u7edf\u4e2d\u7684\u91cd\u8981\u6027\uff0c\u4e3a\u8bbe\u8ba1\u66f4\u9ad8\u6548\u3001\u53ef\u89e3\u91ca\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u65b9\u5411\u3002"}}
{"id": "2507.09407", "categories": ["cs.AI", "cs.CR", "cs.GT"], "pdf": "https://arxiv.org/pdf/2507.09407", "abs": "https://arxiv.org/abs/2507.09407", "authors": ["Quanyan Zhu"], "title": "LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing", "comment": null, "summary": "We introduce the framework of LLM-Stackelberg games, a class of sequential\ndecision-making models that integrate large language models (LLMs) into\nstrategic interactions between a leader and a follower. Departing from\nclassical Stackelberg assumptions of complete information and rational agents,\nour formulation allows each agent to reason through structured prompts,\ngenerate probabilistic behaviors via LLMs, and adapt their strategies through\ninternal cognition and belief updates. We define two equilibrium concepts:\nreasoning and behavioral equilibrium, which aligns an agent's internal\nprompt-based reasoning with observable behavior, and conjectural reasoning\nequilibrium, which accounts for epistemic uncertainty through parameterized\nmodels over an opponent's response. These layered constructs capture bounded\nrationality, asymmetric information, and meta-cognitive adaptation. We\nillustrate the framework through a spearphishing case study, where a sender and\na recipient engage in a deception game using structured reasoning prompts. This\nexample highlights the cognitive richness and adversarial potential of\nLLM-mediated interactions. Our results show that LLM-Stackelberg games provide\na powerful paradigm for modeling decision-making in domains such as\ncybersecurity, misinformation, and recommendation systems.", "AI": {"tldr": "LLM-Stackelberg\u6e38\u620f\u6846\u67b6\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u878d\u5165\u9886\u5bfc\u8005\u4e0e\u8ffd\u968f\u8005\u7684\u7b56\u7565\u4e92\u52a8\u4e2d\uff0c\u7a81\u7834\u4e86\u7ecf\u5178Stackelberg\u6a21\u578b\u7684\u5b8c\u5168\u4fe1\u606f\u548c\u7406\u6027\u5047\u8bbe\uff0c\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548c\u6982\u7387\u884c\u4e3a\u751f\u6210\u5b9e\u73b0\u7b56\u7565\u9002\u5e94\u3002", "motivation": "\u4f20\u7edfStackelberg\u6a21\u578b\u5047\u8bbe\u5b8c\u5168\u4fe1\u606f\u548c\u7406\u6027\u884c\u4e3a\uff0c\u65e0\u6cd5\u6355\u6349\u73b0\u5b9e\u4e2d\u7684\u6709\u9650\u7406\u6027\u3001\u4fe1\u606f\u4e0d\u5bf9\u79f0\u548c\u8ba4\u77e5\u9002\u5e94\u3002\u672c\u6587\u65e8\u5728\u901a\u8fc7LLMs\u6269\u5c55\u8fd9\u4e00\u6846\u67b6\uff0c\u4ee5\u66f4\u771f\u5b9e\u5730\u6a21\u62df\u590d\u6742\u51b3\u7b56\u573a\u666f\u3002", "method": "\u63d0\u51faLLM-Stackelberg\u6e38\u620f\u6846\u67b6\uff0c\u5b9a\u4e49\u4e24\u79cd\u5747\u8861\u6982\u5ff5\uff1a\u63a8\u7406\u4e0e\u884c\u4e3a\u5747\u8861\uff08\u5185\u90e8\u63a8\u7406\u4e0e\u884c\u4e3a\u4e00\u81f4\uff09\u548c\u63a8\u6d4b\u63a8\u7406\u5747\u8861\uff08\u8003\u8651\u5bf9\u624b\u54cd\u5e94\u7684\u8ba4\u77e5\u4e0d\u786e\u5b9a\u6027\uff09\u3002\u901a\u8fc7\u7ed3\u6784\u5316\u63d0\u793a\u548cLLMs\u751f\u6210\u884c\u4e3a\u3002", "result": "\u5728\u9493\u9c7c\u653b\u51fb\u6848\u4f8b\u4e2d\u5c55\u793a\u4e86LLM-Stackelberg\u6846\u67b6\u7684\u8ba4\u77e5\u4e30\u5bcc\u6027\u548c\u5bf9\u6297\u6f5c\u529b\uff0c\u8bc1\u660e\u4e86\u5176\u5728\u7f51\u7edc\u5b89\u5168\u3001\u9519\u8bef\u4fe1\u606f\u548c\u63a8\u8350\u7cfb\u7edf\u7b49\u9886\u57df\u7684\u9002\u7528\u6027\u3002", "conclusion": "LLM-Stackelberg\u6e38\u620f\u4e3a\u590d\u6742\u51b3\u7b56\u9886\u57df\u63d0\u4f9b\u4e86\u5f3a\u5927\u7684\u5efa\u6a21\u5de5\u5177\uff0c\u80fd\u591f\u6355\u6349\u6709\u9650\u7406\u6027\u548c\u8ba4\u77e5\u52a8\u6001\u3002"}}
{"id": "2507.09495", "categories": ["cs.AI", "cs.ET", "cs.HC", "cs.RO", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09495", "abs": "https://arxiv.org/abs/2507.09495", "authors": ["Hang Wang", "Junshan Zhang"], "title": "GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective", "comment": "Position paper", "summary": "Multi-agent reinforcement learning faces fundamental challenges that\nconventional approaches have failed to overcome: exponentially growing joint\naction spaces, non-stationary environments where simultaneous learning creates\nmoving targets, and partial observability that constrains coordination. Current\nmethods remain reactive, employing stimulus-response mechanisms that fail when\nfacing novel scenarios. We argue for a transformative paradigm shift from\nreactive to proactive multi-agent intelligence through generative AI-based\nreinforcement learning. This position advocates reconceptualizing agents not as\nisolated policy optimizers, but as sophisticated generative models capable of\nsynthesizing complex multi-agent dynamics and making anticipatory decisions\nbased on predictive understanding of future interactions. Rather than\nresponding to immediate observations, generative-RL agents can model\nenvironment evolution, predict other agents' behaviors, generate coordinated\naction sequences, and engage in strategic reasoning accounting for long-term\ndynamics. This approach leverages pattern recognition and generation\ncapabilities of generative AI to enable proactive decision-making, seamless\ncoordination through enhanced communication, and dynamic adaptation to evolving\nscenarios. We envision this paradigm shift will unlock unprecedented\npossibilities for distributed intelligence, moving beyond individual\noptimization toward emergent collective behaviors representing genuine\ncollaborative intelligence. The implications extend across autonomous systems,\nrobotics, and human-AI collaboration, promising solutions to coordination\nchallenges intractable under traditional reactive frameworks.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4ece\u53cd\u5e94\u5f0f\u8f6c\u5411\u751f\u6210\u5f0fAI\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff0c\u4ee5\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u65e0\u6cd5\u5e94\u5bf9\u7684\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u3001\u975e\u5e73\u7a33\u73af\u5883\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u95ee\u9898\u3002", "motivation": "\u4f20\u7edf\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\u5728\u8054\u5408\u52a8\u4f5c\u7a7a\u95f4\u3001\u975e\u5e73\u7a33\u73af\u5883\u548c\u90e8\u5206\u53ef\u89c2\u6d4b\u6027\u65b9\u9762\u5b58\u5728\u5c40\u9650\uff0c\u65e0\u6cd5\u5e94\u5bf9\u65b0\u573a\u666f\u3002", "method": "\u91c7\u7528\u751f\u6210\u5f0fAI\u7684\u5f3a\u5316\u5b66\u4e60\uff0c\u5c06\u667a\u80fd\u4f53\u89c6\u4e3a\u751f\u6210\u6a21\u578b\uff0c\u9884\u6d4b\u672a\u6765\u4ea4\u4e92\u5e76\u751f\u6210\u534f\u8c03\u52a8\u4f5c\u5e8f\u5217\u3002", "result": "\u751f\u6210\u5f0f\u667a\u80fd\u4f53\u80fd\u591f\u8fdb\u884c\u524d\u77bb\u6027\u51b3\u7b56\u3001\u589e\u5f3a\u534f\u8c03\u548c\u52a8\u6001\u9002\u5e94\uff0c\u5b9e\u73b0\u771f\u6b63\u7684\u534f\u4f5c\u667a\u80fd\u3002", "conclusion": "\u8fd9\u4e00\u8303\u5f0f\u8f6c\u53d8\u6709\u671b\u5728\u81ea\u4e3b\u7cfb\u7edf\u3001\u673a\u5668\u4eba\u548c\u4eba\u673a\u534f\u4f5c\u4e2d\u89e3\u51b3\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5904\u7406\u7684\u534f\u8c03\u95ee\u9898\u3002"}}
{"id": "2507.09534", "categories": ["cs.AI", "cs.LG", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09534", "abs": "https://arxiv.org/abs/2507.09534", "authors": ["Guanquan Wang", "Takuya Hiraoka", "Yoshimasa Tsuruoka"], "title": "Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning", "comment": null, "summary": "This paper introduces Consistency Trajectory Planning (CTP), a novel offline\nmodel-based reinforcement learning method that leverages the recently proposed\nConsistency Trajectory Model (CTM) for efficient trajectory optimization. While\nprior work applying diffusion models to planning has demonstrated strong\nperformance, it often suffers from high computational costs due to iterative\nsampling procedures. CTP supports fast, single-step trajectory generation\nwithout significant degradation in policy quality. We evaluate CTP on the D4RL\nbenchmark and show that it consistently outperforms existing diffusion-based\nplanning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves\nhigher normalized returns while using significantly fewer denoising steps. In\nparticular, CTP achieves comparable performance with over $120\\times$ speedup\nin inference time, demonstrating its practicality and effectiveness for\nhigh-performance, low-latency offline planning.", "AI": {"tldr": "CTP\u662f\u4e00\u79cd\u57fa\u4e8e\u4e00\u81f4\u6027\u8f68\u8ff9\u6a21\u578b\u7684\u79bb\u7ebf\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u901a\u8fc7\u5355\u6b65\u8f68\u8ff9\u751f\u6210\u5b9e\u73b0\u9ad8\u6548\u4f18\u5316\uff0c\u663e\u8457\u51cf\u5c11\u8ba1\u7b97\u6210\u672c\u3002", "motivation": "\u73b0\u6709\u57fa\u4e8e\u6269\u6563\u6a21\u578b\u7684\u89c4\u5212\u65b9\u6cd5\u8ba1\u7b97\u6210\u672c\u9ad8\uff0cCTP\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\u3002", "method": "\u5229\u7528\u4e00\u81f4\u6027\u8f68\u8ff9\u6a21\u578b\uff08CTM\uff09\u8fdb\u884c\u5355\u6b65\u8f68\u8ff9\u751f\u6210\uff0c\u907f\u514d\u8fed\u4ee3\u91c7\u6837\u3002", "result": "\u5728D4RL\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u73b0\u6709\u65b9\u6cd5\uff0c\u8ba1\u7b97\u901f\u5ea6\u63d0\u5347120\u500d\u3002", "conclusion": "CTP\u662f\u4e00\u79cd\u9ad8\u6548\u3001\u4f4e\u5ef6\u8fdf\u7684\u79bb\u7ebf\u89c4\u5212\u65b9\u6cd5\uff0c\u9002\u7528\u4e8e\u9ad8\u6027\u80fd\u4efb\u52a1\u3002"}}
{"id": "2507.09540", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09540", "abs": "https://arxiv.org/abs/2507.09540", "authors": ["Ali Safa", "Farida Mohsen", "Ali Al-Zawqari"], "title": "Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling", "comment": null, "summary": "Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient\nalternatives to traditional Deep Neural Networks (DNNs) for real-time control\nsystems. However, their training presents several challenges, particularly for\nreinforcement learning (RL) tasks, due to the non-differentiable nature of\nspike-based communication. In this work, we introduce what is, to our\nknowledge, the first framework that employs Metropolis-Hastings (MH) sampling,\na Bayesian inference technique, to train SNNs for dynamical agent control in RL\nenvironments without relying on gradient-based methods. Our approach\niteratively proposes and probabilistically accepts network parameter updates\nbased on accumulated reward signals, effectively circumventing the limitations\nof backpropagation while enabling direct optimization on neuromorphic\nplatforms. We evaluated this framework on two standard control benchmarks:\nAcroBot and CartPole. The results demonstrate that our MH-based approach\noutperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL\napproaches in terms of maximizing the accumulated reward while minimizing\nnetwork resources and training episodes.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eMetropolis-Hastings\u91c7\u6837\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u8bad\u7ec3\u8109\u51b2\u795e\u7ecf\u7f51\u7edc\uff08SNN\uff09\u5728\u5f3a\u5316\u5b66\u4e60\u4efb\u52a1\u4e2d\uff0c\u907f\u514d\u4e86\u68af\u5ea6\u65b9\u6cd5\u7684\u9650\u5236\uff0c\u5e76\u5728\u4e24\u4e2a\u63a7\u5236\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u8868\u73b0\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\u3002", "motivation": "\u89e3\u51b3SNN\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u8bad\u7ec3\u56f0\u96be\u7684\u95ee\u9898\uff0c\u5c24\u5176\u662f\u7531\u4e8e\u8109\u51b2\u901a\u4fe1\u7684\u4e0d\u53ef\u5fae\u5206\u6027\u5bfc\u81f4\u7684\u6311\u6218\u3002", "method": "\u91c7\u7528Metropolis-Hastings\u91c7\u6837\u4f5c\u4e3a\u8d1d\u53f6\u65af\u63a8\u65ad\u6280\u672f\uff0c\u901a\u8fc7\u8fed\u4ee3\u63d0\u51fa\u5e76\u6982\u7387\u63a5\u53d7\u7f51\u7edc\u53c2\u6570\u66f4\u65b0\uff0c\u57fa\u4e8e\u7d2f\u79ef\u5956\u52b1\u4fe1\u53f7\u8fdb\u884c\u8bad\u7ec3\u3002", "result": "\u5728AcroBot\u548cCartPole\u57fa\u51c6\u6d4b\u8bd5\u4e2d\uff0c\u8be5\u65b9\u6cd5\u5728\u6700\u5927\u5316\u7d2f\u79ef\u5956\u52b1\u3001\u51cf\u5c11\u7f51\u7edc\u8d44\u6e90\u548c\u8bad\u7ec3\u6b21\u6570\u65b9\u9762\u4f18\u4e8e\u4f20\u7edfDeep Q-Learning\u548c\u73b0\u6709SNN\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u4e3aSNN\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u5e94\u7528\u63d0\u4f9b\u4e86\u4e00\u79cd\u6709\u6548\u7684\u65e0\u68af\u5ea6\u8bad\u7ec3\u65b9\u6cd5\uff0c\u5c55\u793a\u4e86\u5728\u52a8\u6001\u4ee3\u7406\u63a7\u5236\u4e2d\u7684\u6f5c\u529b\u3002"}}
{"id": "2507.09588", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09588", "abs": "https://arxiv.org/abs/2507.09588", "authors": ["Isaac Shi", "Zeyuan Li", "Fan Liu", "Wenli Wang", "Lewei He", "Yang Yang", "Tianyu Shi"], "title": "eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation", "comment": null, "summary": "We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a\nbusiness-oriented trifecta: proprietary data, operational workflows, and any\nmajor agnostic Large Language Model (LLM). eSapiens gives businesses full\ncontrol over their AI assets, keeping everything in-house for AI knowledge\nretention and data security. eSapiens AI Agents (Sapiens) empower your team by\nproviding valuable insights and automating repetitive tasks, enabling them to\nfocus on high-impact work and drive better business outcomes.\n  The system integrates structured document ingestion, hybrid vector retrieval,\nand no-code orchestration via LangChain, and supports top LLMs including\nOpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which\nhandles structured SQL-style queries and generates actionable insights over\nenterprise databases.\n  To evaluate the system, we conduct two experiments. First, a retrieval\nbenchmark on legal corpora reveals that a chunk size of 512 tokens yields the\nhighest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation\nquality test using TRACe metrics across five LLMs shows that eSapiens delivers\nmore context-consistent outputs with up to a 23% improvement in factual\nalignment.\n  These results demonstrate the effectiveness of eSapiens in enabling\ntrustworthy, auditable AI workflows for high-stakes domains like legal and\nfinance.", "AI": {"tldr": "eSapiens\u662f\u4e00\u4e2a\u9762\u5411\u4f01\u4e1a\u7684AIaaS\u5e73\u53f0\uff0c\u6574\u5408\u4e13\u6709\u6570\u636e\u3001\u5de5\u4f5c\u6d41\u7a0b\u548c\u4e3b\u6d41LLM\uff0c\u63d0\u4f9b\u6570\u636e\u5b89\u5168\u548c\u77e5\u8bc6\u4fdd\u7559\uff0c\u5e76\u901a\u8fc7AI\u4ee3\u7406\u63d0\u5347\u56e2\u961f\u6548\u7387\u3002", "motivation": "\u89e3\u51b3\u4f01\u4e1a\u5728AI\u5e94\u7528\u4e2d\u9762\u4e34\u7684\u6570\u636e\u5b89\u5168\u3001\u77e5\u8bc6\u4fdd\u7559\u548c\u9ad8\u6548\u5de5\u4f5c\u6d41\u7a0b\u7684\u9700\u6c42\u3002", "method": "\u7ed3\u5408\u7ed3\u6784\u5316\u6587\u6863\u5904\u7406\u3001\u6df7\u5408\u5411\u91cf\u68c0\u7d22\u548c\u65e0\u4ee3\u7801\u7f16\u6392\uff08LangChain\uff09\uff0c\u652f\u6301\u591a\u79cdLLM\uff0c\u5e76\u5f15\u5165THOR\u4ee3\u7406\u5904\u7406SQL\u67e5\u8be2\u3002", "result": "\u5b9e\u9a8c\u663e\u793a\uff0c512 token\u5206\u5757\u68c0\u7d22\u7cbe\u5ea6\u6700\u9ad8\uff08Top-3\u51c6\u786e\u738791.3%\uff09\uff0c\u751f\u6210\u8d28\u91cf\u6d4b\u8bd5\u4e2deSapiens\u7684\u4e0a\u4e0b\u6587\u4e00\u81f4\u6027\u63d0\u534723%\u3002", "conclusion": "eSapiens\u5728\u9ad8\u98ce\u9669\u9886\u57df\uff08\u5982\u6cd5\u5f8b\u548c\u91d1\u878d\uff09\u4e2d\u5b9e\u73b0\u4e86\u53ef\u4fe1\u3001\u53ef\u5ba1\u8ba1\u7684AI\u5de5\u4f5c\u6d41\u7a0b\u3002"}}
{"id": "2507.09611", "categories": ["cs.AI", "cs.CY", "68T01"], "pdf": "https://arxiv.org/pdf/2507.09611", "abs": "https://arxiv.org/abs/2507.09611", "authors": ["Jenis Winsta"], "title": "The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development", "comment": "5 pages, 3 figures", "summary": "Artificial intelligence (AI) has made remarkable progress in recent years,\nyet its rapid expansion brings overlooked environmental and ethical challenges.\nThis review explores four critical areas where AI's impact extends beyond\nperformance: energy consumption, electronic waste (e-waste), inequality in\ncompute access, and the hidden energy burden of cybersecurity systems. Drawing\nfrom recent studies and institutional reports, the paper highlights systemic\nissues such as high emissions from model training, rising hardware turnover,\nglobal infrastructure disparities, and the energy demands of securing AI. By\nconnecting these concerns, the review contributes to Responsible AI discourse\nby identifying key research gaps and advocating for sustainable, transparent,\nand equitable development practices. Ultimately, it argues that AI's progress\nmust align with ethical responsibility and environmental stewardship to ensure\na more inclusive and sustainable technological future.", "AI": {"tldr": "\u672c\u6587\u7efc\u8ff0\u4e86AI\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u73af\u5883\u548c\u4f26\u7406\u6311\u6218\uff0c\u91cd\u70b9\u5173\u6ce8\u80fd\u6e90\u6d88\u8017\u3001\u7535\u5b50\u5e9f\u7269\u3001\u8ba1\u7b97\u8d44\u6e90\u4e0d\u5e73\u7b49\u548c\u7f51\u7edc\u5b89\u5168\u7cfb\u7edf\u7684\u9690\u85cf\u80fd\u6e90\u8d1f\u62c5\u3002", "motivation": "\u63a2\u8ba8AI\u5728\u6027\u80fd\u548c\u6548\u7387\u4e4b\u5916\u7684\u73af\u5883\u4e0e\u4f26\u7406\u5f71\u54cd\uff0c\u63a8\u52a8\u8d1f\u8d23\u4efbAI\u7684\u53d1\u5c55\u3002", "method": "\u901a\u8fc7\u5206\u6790\u8fd1\u671f\u7814\u7a76\u548c\u673a\u6784\u62a5\u544a\uff0c\u7cfb\u7edf\u68b3\u7406AI\u5728\u80fd\u6e90\u3001\u5e9f\u7269\u3001\u8d44\u6e90\u5206\u914d\u548c\u7f51\u7edc\u5b89\u5168\u65b9\u9762\u7684\u8d1f\u9762\u5f71\u54cd\u3002", "result": "\u63ed\u793a\u4e86\u6a21\u578b\u8bad\u7ec3\u7684\u9ad8\u6392\u653e\u3001\u786c\u4ef6\u5feb\u901f\u6dd8\u6c70\u3001\u5168\u7403\u57fa\u7840\u8bbe\u65bd\u4e0d\u5e73\u7b49\u4ee5\u53caAI\u5b89\u5168\u7684\u9ad8\u80fd\u8017\u95ee\u9898\u3002", "conclusion": "\u547c\u5401AI\u53d1\u5c55\u9700\u7ed3\u5408\u4f26\u7406\u8d23\u4efb\u548c\u73af\u5883\u4fdd\u62a4\uff0c\u4ee5\u5b9e\u73b0\u53ef\u6301\u7eed\u548c\u5305\u5bb9\u7684\u6280\u672f\u672a\u6765\u3002"}}
{"id": "2507.09617", "categories": ["cs.AI", "cs.RO"], "pdf": "https://arxiv.org/pdf/2507.09617", "abs": "https://arxiv.org/abs/2507.09617", "authors": ["Margherita Martorana", "Francesca Urgese", "Mark Adamik", "Ilaria Tiddi"], "title": "Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs", "comment": null, "summary": "Personal service robots are deployed to support daily living in domestic\nenvironments, particularly for elderly and individuals requiring assistance.\nThese robots must perceive complex and dynamic surroundings, understand tasks,\nand execute context-appropriate actions. However, current systems rely on\nproprietary, hard-coded solutions tied to specific hardware and software,\nresulting in siloed implementations that are difficult to adapt and scale\nacross platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to\nenable interoperability across systems, through structured and standardized\nrepresentations of knowledge and reasoning. However, symbolic systems such as\nKGs and ontologies struggle with raw and noisy sensory input. In contrast,\nmultimodal language models are well suited for interpreting input such as\nimages and natural language, but often lack transparency, consistency, and\nknowledge grounding. In this work, we propose a neurosymbolic framework that\ncombines the perceptual strengths of multimodal language models with the\nstructured representations provided by KGs and ontologies, with the aim of\nsupporting interoperability in robotic applications. Our approach generates\nontology-compliant KGs that can inform robot behavior in a platform-independent\nmanner. We evaluated this framework by integrating robot perception data,\nontologies, and five multimodal models (three LLaMA and two GPT models), using\ndifferent modes of neural-symbolic interaction. We assess the consistency and\neffectiveness of the generated KGs across multiple runs and configurations, and\nperform statistical analyzes to evaluate performance. Results show that GPT-o1\nand LLaMA 4 Maverick consistently outperform other models. However, our\nfindings also indicate that newer models do not guarantee better results,\nhighlighting the critical role of the integration strategy in generating\nontology-compliant KGs.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408\u591a\u6a21\u6001\u8bed\u8a00\u6a21\u578b\u4e0e\u77e5\u8bc6\u56fe\u8c31\u7684\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u4ee5\u63d0\u5347\u670d\u52a1\u673a\u5668\u4eba\u7684\u4e92\u64cd\u4f5c\u6027\u548c\u9002\u5e94\u6027\u3002", "motivation": "\u89e3\u51b3\u73b0\u6709\u670d\u52a1\u673a\u5668\u4eba\u7cfb\u7edf\u4f9d\u8d56\u4e13\u6709\u786c\u4ef6\u548c\u8f6f\u4ef6\u3001\u96be\u4ee5\u8de8\u5e73\u53f0\u6269\u5c55\u7684\u95ee\u9898\uff0c\u540c\u65f6\u7ed3\u5408\u7b26\u53f7\u7cfb\u7edf\u4e0e\u611f\u77e5\u6a21\u578b\u7684\u4f18\u52bf\u3002", "method": "\u63d0\u51fa\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\uff0c\u6574\u5408\u673a\u5668\u4eba\u611f\u77e5\u6570\u636e\u3001\u672c\u4f53\u548c\u591a\u6a21\u6001\u6a21\u578b\uff08LLaMA\u548cGPT\uff09\uff0c\u751f\u6210\u7b26\u5408\u672c\u4f53\u7684\u77e5\u8bc6\u56fe\u8c31\u3002", "result": "GPT-o1\u548cLLaMA 4 Maverick\u8868\u73b0\u6700\u4f73\uff0c\u4f46\u65b0\u6a21\u578b\u4e0d\u4e00\u5b9a\u66f4\u597d\uff0c\u6574\u5408\u7b56\u7565\u662f\u5173\u952e\u3002", "conclusion": "\u795e\u7ecf\u7b26\u53f7\u6846\u67b6\u6709\u6548\u652f\u6301\u673a\u5668\u4eba\u4e92\u64cd\u4f5c\u6027\uff0c\u6574\u5408\u7b56\u7565\u5bf9\u6027\u80fd\u81f3\u5173\u91cd\u8981\u3002"}}
{"id": "2507.09626", "categories": ["cs.AI", "cs.SY", "eess.SY"], "pdf": "https://arxiv.org/pdf/2507.09626", "abs": "https://arxiv.org/abs/2507.09626", "authors": ["Rodion Nazarov", "Anthony Quinn", "Robert Shorten", "Jakub Marecek"], "title": "humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems", "comment": null, "summary": "Artificial intelligence (AI) systems often interact with multiple agents. The\nregulation of such AI systems often requires that {\\em a priori\\/} guarantees\nof fairness and robustness be satisfied. With stochastic models of agents'\nresponses to the outputs of AI systems, such {\\em a priori\\/} guarantees\nrequire non-trivial reasoning about the corresponding stochastic systems. Here,\nwe present an open-source PyTorch-based toolkit for the use of stochastic\ncontrol techniques in modelling interconnections of AI systems and properties\nof their repeated uses. It models robustness and fairness desiderata in a\nclosed-loop fashion, and provides {\\em a priori\\/} guarantees for these\ninterconnections. The PyTorch-based toolkit removes much of the complexity\nassociated with the provision of fairness guarantees for closed-loop models of\nmulti-agent systems.", "AI": {"tldr": "\u4ecb\u7ecd\u4e86\u4e00\u4e2a\u57fa\u4e8ePyTorch\u7684\u5de5\u5177\u5305\uff0c\u7528\u4e8e\u901a\u8fc7\u968f\u673a\u63a7\u5236\u6280\u672f\u5efa\u6a21\u591a\u4ee3\u7406AI\u7cfb\u7edf\u7684\u95ed\u73af\u4ea4\u4e92\uff0c\u5e76\u63d0\u4f9b\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u5148\u9a8c\u4fdd\u8bc1\u3002", "motivation": "\u591a\u4ee3\u7406AI\u7cfb\u7edf\u7684\u4ea4\u4e92\u9700\u8981\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u7684\u5148\u9a8c\u4fdd\u8bc1\uff0c\u4f46\u4f20\u7edf\u65b9\u6cd5\u96be\u4ee5\u5b9e\u73b0\u3002", "method": "\u4f7f\u7528\u57fa\u4e8ePyTorch\u7684\u5de5\u5177\u5305\uff0c\u901a\u8fc7\u968f\u673a\u63a7\u5236\u6280\u672f\u5efa\u6a21\u95ed\u73af\u7cfb\u7edf\uff0c\u5e76\u5206\u6790\u5176\u91cd\u590d\u4f7f\u7528\u7279\u6027\u3002", "result": "\u5de5\u5177\u5305\u7b80\u5316\u4e86\u591a\u4ee3\u7406\u7cfb\u7edf\u95ed\u73af\u6a21\u578b\u7684\u516c\u5e73\u6027\u4fdd\u8bc1\uff0c\u63d0\u4f9b\u4e86\u5148\u9a8c\u4fdd\u8bc1\u3002", "conclusion": "\u8be5\u5de5\u5177\u5305\u4e3a\u591a\u4ee3\u7406AI\u7cfb\u7edf\u7684\u516c\u5e73\u6027\u548c\u9c81\u68d2\u6027\u63d0\u4f9b\u4e86\u6709\u6548\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.09662", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.09662", "abs": "https://arxiv.org/abs/2507.09662", "authors": ["Jason Zhu", "Hongyu Li"], "title": "Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey", "comment": null, "summary": "Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have\ndemonstrated impressive performance on complex reasoning tasks like mathematics\nand programming with long Chain-of-Thought (CoT) reasoning sequences\n(slow-thinking), compared with traditional large language models\n(fast-thinking). However, these reasoning models also face a huge challenge\nthat generating unnecessarily lengthy and redundant reasoning chains even for\ntrivial questions. This phenomenon leads to a significant waste of inference\nresources, increases the response time for simple queries, and hinders the\npractical application of LRMs in real-world products. To this end, it is\ncrucial to shorten lengthy reasoning chains and learn adaptive reasoning\nbetween fast and slow thinking based on input difficulty. In this survey, we\nprovide a comprehensive overview of recent progress in concise and adaptive\nthinking for efficient reasoning of LRMs, including methodologies, benchmarks,\nand challenges for future exploration. We hope this survey can help researchers\nquickly understand the landscape of this field and inspire novel adaptive\nthinking ideas to facilitate better usage of LRMs.", "AI": {"tldr": "\u5927\u578b\u63a8\u7406\u6a21\u578b\uff08LRMs\uff09\u5728\u590d\u6742\u4efb\u52a1\u4e0a\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5b58\u5728\u63a8\u7406\u94fe\u5197\u957f\u7684\u95ee\u9898\uff0c\u5bfc\u81f4\u8d44\u6e90\u6d6a\u8d39\u3002\u672c\u6587\u7efc\u8ff0\u4e86\u7b80\u6d01\u548c\u81ea\u9002\u5e94\u63a8\u7406\u7684\u7814\u7a76\u8fdb\u5c55\u3002", "motivation": "\u89e3\u51b3LRMs\u5728\u7b80\u5355\u95ee\u9898\u4e0a\u751f\u6210\u5197\u957f\u63a8\u7406\u94fe\u7684\u95ee\u9898\uff0c\u4ee5\u63d0\u9ad8\u6548\u7387\u548c\u5b9e\u7528\u6027\u3002", "method": "\u7efc\u8ff0\u4e86\u8fd1\u671f\u5173\u4e8e\u7b80\u6d01\u548c\u81ea\u9002\u5e94\u63a8\u7406\u7684\u65b9\u6cd5\u3001\u57fa\u51c6\u548c\u6311\u6218\u3002", "result": "\u603b\u7ed3\u4e86\u8be5\u9886\u57df\u7684\u7814\u7a76\u8fdb\u5c55\uff0c\u4e3a\u672a\u6765\u63a2\u7d22\u63d0\u4f9b\u65b9\u5411\u3002", "conclusion": "\u5e0c\u671b\u5e2e\u52a9\u7814\u7a76\u8005\u5feb\u901f\u4e86\u89e3\u8be5\u9886\u57df\uff0c\u5e76\u542f\u53d1\u65b0\u7684\u81ea\u9002\u5e94\u63a8\u7406\u65b9\u6cd5\uff0c\u4ee5\u4f18\u5316LRMs\u7684\u4f7f\u7528\u3002"}}
{"id": "2507.09742", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09742", "abs": "https://arxiv.org/abs/2507.09742", "authors": ["Xiaofeng Xiao", "Bo Shen", "Xubo Yue"], "title": "Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations", "comment": null, "summary": "Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume\nof data streams requiring real-time monitoring continues to grow. However, due\nto limited resources, it is impractical to place sensors at every location to\ndetect unexpected shifts. Therefore, it is necessary to develop an optimal\nsensor placement strategy that enables partial observability of the system\nwhile detecting anomalies as quickly as possible. Numerous approaches have been\nproposed to address this challenge; however, most existing methods consider\nonly variable correlations and neglect a crucial factor: Causality. Moreover,\nalthough a few techniques incorporate causal analysis, they rely on\ninterventions-artificially creating anomalies-to identify causal effects, which\nis impractical and might lead to catastrophic losses. In this paper, we\nintroduce a causality-informed deep Q-network (Causal DQ) approach for\npartially observable sensor placement in anomaly detection. By integrating\ncausal information at each stage of Q-network training, our method achieves\nfaster convergence and tighter theoretical error bounds. Furthermore, the\ntrained causal-informed Q-network significantly reduces the detection time for\nanomalies under various settings, demonstrating its effectiveness for sensor\nplacement in large-scale, real-world data streams. Beyond the current\nimplementation, our technique's fundamental insights can be applied to various\nreinforcement learning problems, opening up new possibilities for real-world\ncausality-informed machine learning methods in engineering applications.", "AI": {"tldr": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u56e0\u679c\u5173\u7cfb\u7684\u6df1\u5ea6Q\u7f51\u7edc\uff08Causal DQ\uff09\u65b9\u6cd5\uff0c\u7528\u4e8e\u90e8\u5206\u53ef\u89c2\u6d4b\u7684\u4f20\u611f\u5668\u5e03\u5c40\u4f18\u5316\uff0c\u4ee5\u5feb\u901f\u68c0\u6d4b\u5f02\u5e38\u3002", "motivation": "AI\u9a71\u52a8\u7684\u5236\u9020\u4e1a\u4e2d\uff0c\u6570\u636e\u6d41\u5b9e\u65f6\u76d1\u6d4b\u9700\u6c42\u589e\u957f\uff0c\u4f46\u8d44\u6e90\u6709\u9650\uff0c\u65e0\u6cd5\u5728\u6240\u6709\u4f4d\u7f6e\u90e8\u7f72\u4f20\u611f\u5668\u3002\u73b0\u6709\u65b9\u6cd5\u591a\u5ffd\u7565\u56e0\u679c\u5173\u7cfb\uff0c\u6216\u4f9d\u8d56\u4e0d\u5207\u5b9e\u9645\u7684\u5e72\u9884\u624b\u6bb5\u3002", "method": "\u901a\u8fc7\u5728\u6bcf\u4e2aQ\u7f51\u7edc\u8bad\u7ec3\u9636\u6bb5\u6574\u5408\u56e0\u679c\u4fe1\u606f\uff0c\u5f00\u53d1\u4e86Causal DQ\u65b9\u6cd5\uff0c\u5b9e\u73b0\u66f4\u5feb\u6536\u655b\u548c\u66f4\u7d27\u7684\u7406\u8bba\u8bef\u5dee\u754c\u9650\u3002", "result": "Causal DQ\u663e\u8457\u51cf\u5c11\u4e86\u5f02\u5e38\u68c0\u6d4b\u65f6\u95f4\uff0c\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5b9e\u65f6\u6570\u636e\u6d41\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e0d\u4ec5\u6709\u6548\uff0c\u8fd8\u4e3a\u5176\u4ed6\u5f3a\u5316\u5b66\u4e60\u95ee\u9898\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u6269\u5c55\u4e86\u56e0\u679c\u673a\u5668\u5b66\u4e60\u5728\u5de5\u7a0b\u4e2d\u7684\u5e94\u7528\u3002"}}
{"id": "2507.09751", "categories": ["cs.AI", "cs.CL", "cs.LO"], "pdf": "https://arxiv.org/pdf/2507.09751", "abs": "https://arxiv.org/abs/2507.09751", "authors": ["Bradley P. Allen", "Prateek Chhikara", "Thomas Macaulay Ferguson", "Filip Ilievski", "Paul Groth"], "title": "Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations", "comment": "29 pages, 9 tables, 3 figures. Accepted to the 19th Conference on\n  Neurosymbolic Learning and Reasoning (NeSy 2025)", "summary": "Large language models (LLMs) have demonstrated impressive capabilities in\nnatural language understanding and generation, but they exhibit problems with\nlogical consistency in the output they generate. How can we harness LLMs'\nbroad-coverage parametric knowledge in formal reasoning despite their\ninconsistency? We present a method for directly integrating an LLM into the\ninterpretation function of the formal semantics for a paraconsistent logic. We\nprovide experimental evidence for the feasibility of the method by evaluating\nthe function using datasets created from several short-form factuality\nbenchmarks. Unlike prior work, our method offers a theoretical framework for\nneuro-symbolic reasoning that leverages an LLM's knowledge while preserving the\nunderlying logic's soundness and completeness properties.", "AI": {"tldr": "\u63d0\u51fa\u4e00\u79cd\u65b9\u6cd5\uff0c\u5c06\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\u6574\u5408\u5230\u5f62\u5f0f\u8bed\u4e49\u7684\u89e3\u91ca\u51fd\u6570\u4e2d\uff0c\u4ee5\u89e3\u51b3\u5176\u903b\u8f91\u4e0d\u4e00\u81f4\u6027\u95ee\u9898\uff0c\u5e76\u5728\u5b9e\u9a8c\u4e2d\u9a8c\u8bc1\u4e86\u53ef\u884c\u6027\u3002", "motivation": "\u5c3d\u7ba1LLMs\u5728\u81ea\u7136\u8bed\u8a00\u7406\u89e3\u548c\u751f\u6210\u65b9\u9762\u8868\u73b0\u4f18\u5f02\uff0c\u4f46\u5176\u8f93\u51fa\u5b58\u5728\u903b\u8f91\u4e0d\u4e00\u81f4\u95ee\u9898\uff0c\u9700\u8981\u4e00\u79cd\u65b9\u6cd5\u5728\u5f62\u5f0f\u63a8\u7406\u4e2d\u5229\u7528\u5176\u5e7f\u6cdb\u7684\u77e5\u8bc6\u3002", "method": "\u5c06LLM\u76f4\u63a5\u6574\u5408\u5230\u4e00\u79cd\u77db\u76fe\u5bb9\u5fcd\u903b\u8f91\u7684\u5f62\u5f0f\u8bed\u4e49\u89e3\u91ca\u51fd\u6570\u4e2d\u3002", "result": "\u901a\u8fc7\u591a\u4e2a\u77ed\u7bc7\u4e8b\u5b9e\u6027\u57fa\u51c6\u6570\u636e\u96c6\u9a8c\u8bc1\u4e86\u65b9\u6cd5\u7684\u53ef\u884c\u6027\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u795e\u7ecf\u7b26\u53f7\u63a8\u7406\u63d0\u4f9b\u4e86\u7406\u8bba\u6846\u67b6\uff0c\u65e2\u5229\u7528\u4e86LLM\u7684\u77e5\u8bc6\uff0c\u53c8\u4fdd\u6301\u4e86\u903b\u8f91\u7684\u5065\u5168\u6027\u548c\u5b8c\u5907\u6027\u3002"}}
{"id": "2507.09801", "categories": ["cs.AI", "cs.CY"], "pdf": "https://arxiv.org/pdf/2507.09801", "abs": "https://arxiv.org/abs/2507.09801", "authors": ["Peter Barnett", "Aaron Scher", "David Abecassis"], "title": "Technical Requirements for Halting Dangerous AI Activities", "comment": null, "summary": "The rapid development of AI systems poses unprecedented risks, including loss\nof control, misuse, geopolitical instability, and concentration of power. To\nnavigate these risks and avoid worst-case outcomes, governments may proactively\nestablish the capability for a coordinated halt on dangerous AI development and\ndeployment. In this paper, we outline key technical interventions that could\nallow for a coordinated halt on dangerous AI activities. We discuss how these\ninterventions may contribute to restricting various dangerous AI activities,\nand show how these interventions can form the technical foundation for\npotential AI governance plans.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5982\u4f55\u901a\u8fc7\u6280\u672f\u5e72\u9884\u5b9e\u73b0\u5371\u9669AI\u6d3b\u52a8\u7684\u534f\u8c03\u6682\u505c\uff0c\u4ee5\u5e94\u5bf9AI\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u7684\u98ce\u9669\u3002", "motivation": "AI\u7cfb\u7edf\u7684\u5feb\u901f\u53d1\u5c55\u5e26\u6765\u4e86\u524d\u6240\u672a\u6709\u7684\u98ce\u9669\uff0c\u5982\u5931\u63a7\u3001\u6ee5\u7528\u3001\u5730\u7f18\u653f\u6cbb\u4e0d\u7a33\u5b9a\u548c\u6743\u529b\u96c6\u4e2d\uff0c\u9700\u8981\u653f\u5e9c\u91c7\u53d6\u884c\u52a8\u907f\u514d\u6700\u574f\u7ed3\u679c\u3002", "method": "\u63d0\u51fa\u4e86\u5173\u952e\u7684\u6280\u672f\u5e72\u9884\u63aa\u65bd\uff0c\u4ee5\u5b9e\u73b0\u5bf9\u5371\u9669AI\u6d3b\u52a8\u7684\u534f\u8c03\u6682\u505c\u3002", "result": "\u8fd9\u4e9b\u5e72\u9884\u63aa\u65bd\u53ef\u4ee5\u9650\u5236\u591a\u79cd\u5371\u9669AI\u6d3b\u52a8\uff0c\u5e76\u4e3aAI\u6cbb\u7406\u8ba1\u5212\u63d0\u4f9b\u6280\u672f\u57fa\u7840\u3002", "conclusion": "\u6280\u672f\u5e72\u9884\u662f\u5b9e\u73b0\u5371\u9669AI\u6d3b\u52a8\u534f\u8c03\u6682\u505c\u7684\u6709\u6548\u624b\u6bb5\uff0c\u4e3aAI\u6cbb\u7406\u63d0\u4f9b\u4e86\u53ef\u884c\u65b9\u6848\u3002"}}
{"id": "2507.09850", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09850", "abs": "https://arxiv.org/abs/2507.09850", "authors": ["Wei Du", "Branislav Kisacanin", "George Armstrong", "Shubham Toshniwal", "Ivan Moshkov", "Alexan Ayrapetyan", "Sadegh Mahdavi", "Dan Zhao", "Shizhe Diao", "Dragan Masulovic", "Marius Stanean", "Advaith Avadhanam", "Max Wang", "Ashmit Dutta", "Shitij Govil", "Sri Yanamandara", "Mihir Tandon", "Sriram Ananthakrishnan", "Vedant Rathi", "David Zhang", "Joonseok Kang", "Leon Luo", "Titu Andreescu", "Boris Ginsburg", "Igor Gitman"], "title": "Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation", "comment": "Accepted at the Second AI for Math Workshop at the 42nd International\n  Conference on Machine Learning (ICML 2025)", "summary": "Reasoning-capable language models achieve state-of-the-art performance in\ndiverse complex tasks by generating long, explicit Chain-of-Thought (CoT)\ntraces. While recent works show that base models can acquire such reasoning\ntraces via reinforcement learning or distillation from stronger models like\nDeepSeek-R1, previous works demonstrate that even short CoT prompting without\nfine-tuning is able to improve reasoning. We ask whether long CoT can be\ninduced in a base model using only prompting or minimal tuning. Using just 20\nlong CoT examples from the reasoning model \\texttt{QwQ-32B-Preview}, we lightly\nfine-tune the base model \\texttt{Qwen2.5-32B}. The resulting model outperforms\nthe much larger \\texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of\nhigh-quality examples can unlock strong reasoning capabilities. We further\nexplore using CoT data from non-reasoning models and human annotators, enhanced\nwith prompt engineering, multi-pass editing, and structural guidance. However,\nneither matches the performance of reasoning model traces, suggesting that\ncertain latent qualities of expert CoT are difficult to replicate. We analyze\nkey properties of reasoning data, such as problem difficulty, diversity, and\nanswer length, that influence reasoning distillation. While challenges remain,\nwe are optimistic that carefully curated human-written CoT, even in small\nquantities, can activate reasoning behaviors in base models. We release our\nhuman-authored dataset across refinement stages and invite further\ninvestigation into what makes small-scale reasoning supervision so effective.", "AI": {"tldr": "\u901a\u8fc7\u4ec520\u4e2a\u957f\u94fe\u601d\u7ef4\uff08CoT\uff09\u793a\u4f8b\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u663e\u8457\u63d0\u5347\u63a8\u7406\u80fd\u529b\uff0c\u8d85\u8d8a\u66f4\u5927\u6a21\u578b\u8868\u73b0\u3002", "motivation": "\u63a2\u7a76\u662f\u5426\u4ec5\u901a\u8fc7\u63d0\u793a\u6216\u6700\u5c0f\u5fae\u8c03\u5373\u53ef\u5728\u57fa\u7840\u6a21\u578b\u4e2d\u8bf1\u5bfc\u957f\u94fe\u601d\u7ef4\u63a8\u7406\u80fd\u529b\u3002", "method": "\u4f7f\u752820\u4e2a\u6765\u81ea\u63a8\u7406\u6a21\u578b\u7684CoT\u793a\u4f8b\u5fae\u8c03\u57fa\u7840\u6a21\u578b\uff0c\u5e76\u63a2\u7d22\u975e\u63a8\u7406\u6a21\u578b\u548c\u4eba\u5de5\u6807\u6ce8\u6570\u636e\u7684\u8868\u73b0\u3002", "result": "\u5fae\u8c03\u540e\u7684\u6a21\u578b\u8868\u73b0\u4f18\u4e8e\u66f4\u5927\u7684\u6a21\u578b\uff0c\u4f46\u975e\u63a8\u7406\u6a21\u578b\u548c\u4eba\u5de5\u6570\u636e\u672a\u80fd\u8fbe\u5230\u76f8\u540c\u6548\u679c\u3002", "conclusion": "\u5c11\u91cf\u9ad8\u8d28\u91cf\u793a\u4f8b\u53ef\u6fc0\u6d3b\u57fa\u7840\u6a21\u578b\u7684\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e13\u5bb6CoT\u7684\u6f5c\u5728\u7279\u6027\u96be\u4ee5\u590d\u5236\u3002"}}
{"id": "2507.09854", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09854", "abs": "https://arxiv.org/abs/2507.09854", "authors": ["Aniruddha Chattopadhyay", "Raj Dandekar", "Kaushik Roy"], "title": "Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems", "comment": "Accepted as paper in 19th International Conference on Neurosymbolic\n  Learning and Reasoning,NeSy 2025", "summary": "Neurosymbolic artificial intelligence (AI) systems combine neural network and\nclassical symbolic AI mechanisms to exploit the complementary strengths of\nlarge scale, generalizable learning and robust, verifiable reasoning. Numerous\nclassifications of neurosymbolic AI illustrate how these two components can be\nintegrated in distinctly different ways. In this work, we propose\nreinterpreting instruction tuned large language models as model grounded\nsymbolic AI systems where natural language serves as the symbolic layer and\ngrounding is achieved through the models internal representation space. Within\nthis framework, we investigate and develop novel learning and reasoning\napproaches that preserve structural similarities to traditional learning and\nreasoning paradigms. Preliminary evaluations across axiomatic deductive\nreasoning procedures of varying complexity provide insights into the\neffectiveness of our approach in improving learning efficiency and reasoning\nreliability.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u5c06\u6307\u4ee4\u8c03\u4f18\u7684\u5927\u578b\u8bed\u8a00\u6a21\u578b\u91cd\u65b0\u89e3\u91ca\u4e3a\u57fa\u4e8e\u6a21\u578b\u7684\u7b26\u53f7AI\u7cfb\u7edf\uff0c\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u7b26\u53f7\u5c42\uff0c\u5e76\u901a\u8fc7\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u63a5\u5730\u3002", "motivation": "\u7ed3\u5408\u795e\u7ecf\u7f51\u7edc\u7684\u6cdb\u5316\u5b66\u4e60\u80fd\u529b\u548c\u7b26\u53f7AI\u7684\u53ef\u9a8c\u8bc1\u63a8\u7406\u80fd\u529b\uff0c\u63a2\u7d22\u65b0\u7684\u5b66\u4e60\u548c\u63a8\u7406\u65b9\u6cd5\u3002", "method": "\u5c06\u81ea\u7136\u8bed\u8a00\u4f5c\u4e3a\u7b26\u53f7\u5c42\uff0c\u5229\u7528\u6a21\u578b\u7684\u5185\u90e8\u8868\u793a\u7a7a\u95f4\u5b9e\u73b0\u63a5\u5730\uff0c\u5f00\u53d1\u4e0e\u4f20\u7edf\u5b66\u4e60\u548c\u63a8\u7406\u8303\u5f0f\u7ed3\u6784\u76f8\u4f3c\u7684\u65b0\u65b9\u6cd5\u3002", "result": "\u521d\u6b65\u8bc4\u4f30\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u63d0\u9ad8\u5b66\u4e60\u6548\u7387\u548c\u63a8\u7406\u53ef\u9760\u6027\u65b9\u9762\u6709\u6548\u3002", "conclusion": "\u901a\u8fc7\u91cd\u65b0\u89e3\u91ca\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff0c\u672c\u6587\u4e3a\u795e\u7ecf\u7b26\u53f7AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u5b66\u4e60\u548c\u63a8\u7406\u6846\u67b6\u3002"}}
{"id": "2507.09884", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09884", "abs": "https://arxiv.org/abs/2507.09884", "authors": ["Xuzhao Li", "Xuchen Li", "Shiyu Hu", "Yongzhen Guo", "Wentao Zhang"], "title": "VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains", "comment": "Preprint, Under review", "summary": "Large language models (LLMs) increasingly rely on reinforcement learning (RL)\nto enhance their reasoning capabilities through feedback. A critical challenge\nis verifying the consistency of model-generated responses and reference\nanswers, since these responses are often lengthy, diverse, and nuanced.\nRule-based verifiers struggle with complexity, prompting the use of model-based\nverifiers. However, specialized verifiers lack flexibility, while general LLM\njudges can be inconsistent. Existing research primarily focuses on building\nbetter verifiers, yet a systematic evaluation of different types of verifiers'\nperformance across domains remains lacking, severely constraining the reliable\ndevelopment of Reinforcement Learning with Verifiable Reward (RLVR). To address\nthis, we propose VerifyBench--a cross-domain comprehensive benchmark for\nsystematically evaluating verifiers. We construct 4,000 expert-level questions\ncovering mathematics, physics, chemistry, and biology. Each question is\nequipped with reference answers and diverse responses. The reliability of the\nevaluation is ensured through a rigorous annotation process conducted by a\nmultidisciplinary expert team. We design a four-dimensional experimental\nframework to comprehensively compare the performance boundaries of specialized\nverifiers and general LLMs under combined conditions of extracted answers vs.\ncomplete responses, and short vs. long outputs. Our evaluation uncovers\nfundamental trade-offs in verifiers: while specialized verifiers achieve\nleading accuracy, they exhibit deficiencies in recall; general models show\nstronger inclusivity but unstable precision. More importantly, we discover\nverifiers' high sensitivity to input structure and inherent limitations in\ncross-domain generalization, providing critical insights into the bottlenecks\nof current verifier technology.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86VerifyBench\uff0c\u4e00\u4e2a\u8de8\u9886\u57df\u7684\u7efc\u5408\u57fa\u51c6\uff0c\u7528\u4e8e\u7cfb\u7edf\u8bc4\u4f30\u9a8c\u8bc1\u5668\u5728\u5f3a\u5316\u5b66\u4e60\u4e2d\u7684\u6027\u80fd\uff0c\u63ed\u793a\u4e86\u4e13\u7528\u9a8c\u8bc1\u5668\u548c\u901a\u7528LLM\u4e4b\u95f4\u7684\u6743\u8861\u3002", "motivation": "\u73b0\u6709\u9a8c\u8bc1\u5668\u5728\u590d\u6742\u6027\u548c\u4e00\u81f4\u6027\u65b9\u9762\u5b58\u5728\u4e0d\u8db3\uff0c\u7f3a\u4e4f\u8de8\u9886\u57df\u7684\u7cfb\u7edf\u6027\u8bc4\u4f30\uff0c\u9650\u5236\u4e86RLVR\u7684\u53ef\u9760\u53d1\u5c55\u3002", "method": "\u6784\u5efa\u4e86\u5305\u542b4000\u4e2a\u4e13\u5bb6\u7ea7\u95ee\u9898\u7684VerifyBench\uff0c\u8bbe\u8ba1\u56db\u7ef4\u5b9e\u9a8c\u6846\u67b6\u6bd4\u8f83\u4e13\u7528\u9a8c\u8bc1\u5668\u548c\u901a\u7528LLM\u7684\u6027\u80fd\u3002", "result": "\u4e13\u7528\u9a8c\u8bc1\u5668\u51c6\u786e\u7387\u9ad8\u4f46\u53ec\u56de\u7387\u4f4e\uff0c\u901a\u7528\u6a21\u578b\u5305\u5bb9\u6027\u5f3a\u4f46\u7cbe\u5ea6\u4e0d\u7a33\u5b9a\uff0c\u9a8c\u8bc1\u5668\u5bf9\u8f93\u5165\u7ed3\u6784\u654f\u611f\u4e14\u8de8\u9886\u57df\u6cdb\u5316\u80fd\u529b\u6709\u9650\u3002", "conclusion": "\u7814\u7a76\u63ed\u793a\u4e86\u5f53\u524d\u9a8c\u8bc1\u5668\u6280\u672f\u7684\u74f6\u9888\uff0c\u4e3a\u672a\u6765\u6539\u8fdb\u63d0\u4f9b\u4e86\u5173\u952e\u89c1\u89e3\u3002"}}
{"id": "2507.09955", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.09955", "abs": "https://arxiv.org/abs/2507.09955", "authors": ["Luolin Xiong", "Haofen Wang", "Xi Chen", "Lu Sheng", "Yun Xiong", "Jingping Liu", "Yanghua Xiao", "Huajun Chen", "Qing-Long Han", "Yang Tang"], "title": "DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models", "comment": null, "summary": "DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their\nV3 and R1 series models, which attracted global attention due to their low\ncost, high performance, and open-source advantages. This paper begins by\nreviewing the evolution of large AI models focusing on paradigm shifts, the\nmainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.\nSubsequently, the paper highlights novel algorithms introduced by DeepSeek,\nincluding Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),\nMulti-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).\nThe paper then explores DeepSeek engineering breakthroughs in LLM scaling,\ntraining, inference, and system-level optimization architecture. Moreover, the\nimpact of DeepSeek models on the competitive AI landscape is analyzed,\ncomparing them to mainstream LLMs across various fields. Finally, the paper\nreflects on the insights gained from DeepSeek innovations and discusses future\ntrends in the technical and engineering development of large AI models,\nparticularly in data, training, and reasoning.", "AI": {"tldr": "DeepSeek\u53d1\u5e03V3\u548cR1\u7cfb\u5217\u6a21\u578b\uff0c\u5177\u6709\u4f4e\u6210\u672c\u3001\u9ad8\u6027\u80fd\u548c\u5f00\u6e90\u4f18\u52bf\u3002\u8bba\u6587\u56de\u987e\u4e86\u5927\u6a21\u578b\u53d1\u5c55\uff0c\u4ecb\u7ecd\u4e86DeepSeek\u7684\u521b\u65b0\u7b97\u6cd5\u548c\u5de5\u7a0b\u7a81\u7834\uff0c\u5e76\u5206\u6790\u4e86\u5176\u5bf9AI\u7ade\u4e89\u683c\u5c40\u7684\u5f71\u54cd\u3002", "motivation": "\u63a2\u8ba8DeepSeek\u6a21\u578b\u7684\u6280\u672f\u521b\u65b0\u53ca\u5176\u5bf9\u5927\u578bAI\u6a21\u578b\u53d1\u5c55\u7684\u5f71\u54cd\uff0c\u4e3a\u672a\u6765\u8d8b\u52bf\u63d0\u4f9b\u89c1\u89e3\u3002", "method": "\u56de\u987e\u5927\u6a21\u578b\u53d1\u5c55\uff0c\u4ecb\u7ecdDeepSeek\u7684MLA\u3001MoE\u3001MTP\u548cGRPO\u7b97\u6cd5\uff0c\u5206\u6790\u5de5\u7a0b\u7a81\u7834\u548c\u7ade\u4e89\u683c\u5c40\u3002", "result": "DeepSeek\u6a21\u578b\u5728\u6027\u80fd\u3001\u6210\u672c\u548c\u5f00\u6e90\u65b9\u9762\u8868\u73b0\u7a81\u51fa\uff0c\u5bf9AI\u9886\u57df\u4ea7\u751f\u663e\u8457\u5f71\u54cd\u3002", "conclusion": "DeepSeek\u7684\u521b\u65b0\u4e3a\u5927\u578bAI\u6a21\u578b\u53d1\u5c55\u63d0\u4f9b\u4e86\u65b0\u65b9\u5411\uff0c\u672a\u6765\u8d8b\u52bf\u5c06\u805a\u7126\u6570\u636e\u3001\u8bad\u7ec3\u548c\u63a8\u7406\u4f18\u5316\u3002"}}
{"id": "2507.09989", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.09989", "abs": "https://arxiv.org/abs/2507.09989", "authors": ["Xiaoyang Yu", "Youfang Lin", "Shuo Wang", "Sheng Han"], "title": "Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient", "comment": null, "summary": "In heterogeneous multi-agent reinforcement learning (MARL), achieving\nmonotonic improvement plays a pivotal role in enhancing performance. The HAPPO\nalgorithm proposes a feasible solution by introducing a sequential update\nscheme, which requires independent learning with No Parameter-sharing (NoPS).\nHowever, heterogeneous MARL generally requires Partial Parameter-sharing\n(ParPS) based on agent grouping to achieve high cooperative performance. Our\nexperiments prove that directly combining ParPS with the sequential update\nscheme leads to the policy updating baseline drift problem, thereby failing to\nachieve improvement. To solve the conflict between monotonic improvement and\nParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)\nalgorithm. First, we replace the sequentially computed $Q_{\\psi}^s(s,a_{1:i})$\nwith the Optimal Marginal Q (OMQ) function $\\phi_{\\psi}^*(s,a_{1:i})$ derived\nfrom Q-functions. This maintains MAAD's monotonic improvement while eliminating\nthe conflict through optimal joint action sequences instead of sequential\npolicy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)\nas the critic function, employing pessimistic uncertainty-constrained loss to\noptimize different Q-value estimations. This provides the required Q-values for\nOMQ computation and stable baselines for actor updates. Finally, we implement a\nCentralized Critic Grouped Actor (CCGA) architecture that simultaneously\nachieves ParPS in local policy networks and accurate global Q-function\ncomputation. Experimental results in SMAC and MAMuJoCo environments demonstrate\nthat OMDPG outperforms various state-of-the-art MARL baselines.", "AI": {"tldr": "OMDPG\u7b97\u6cd5\u901a\u8fc7\u5f15\u5165\u6700\u4f18\u8fb9\u9645Q\u51fd\u6570\u548c\u5e7f\u4e49Q\u6279\u8bc4\u5668\uff0c\u89e3\u51b3\u4e86\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\u5355\u8c03\u6539\u8fdb\u4e0e\u90e8\u5206\u53c2\u6570\u5171\u4eab\u7684\u51b2\u7a81\uff0c\u63d0\u5347\u4e86\u6027\u80fd\u3002", "motivation": "\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u4e2d\uff0c\u5355\u8c03\u6539\u8fdb\u4e0e\u90e8\u5206\u53c2\u6570\u5171\u4eab\uff08ParPS\uff09\u4e4b\u95f4\u5b58\u5728\u51b2\u7a81\uff0c\u76f4\u63a5\u7ed3\u5408\u4f1a\u5bfc\u81f4\u7b56\u7565\u66f4\u65b0\u57fa\u7ebf\u6f02\u79fb\u95ee\u9898\uff0c\u65e0\u6cd5\u5b9e\u73b0\u6027\u80fd\u63d0\u5347\u3002", "method": "\u63d0\u51faOMDPG\u7b97\u6cd5\uff1a1\uff09\u7528\u6700\u4f18\u8fb9\u9645Q\u51fd\u6570\u66ff\u4ee3\u987a\u5e8f\u8ba1\u7b97\u7684Q\u51fd\u6570\uff1b2\uff09\u5f15\u5165\u5e7f\u4e49Q\u6279\u8bc4\u5668\uff1b3\uff09\u91c7\u7528\u96c6\u4e2d\u5f0f\u6279\u8bc4\u5668\u5206\u7ec4\u6267\u884c\u5668\u67b6\u6784\u3002", "result": "\u5728SMAC\u548cMAMuJoCo\u73af\u5883\u4e2d\uff0cOMDPG\u4f18\u4e8e\u591a\u79cd\u6700\u5148\u8fdb\u7684\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u57fa\u7ebf\u3002", "conclusion": "OMDPG\u6210\u529f\u89e3\u51b3\u4e86\u5355\u8c03\u6539\u8fdb\u4e0eParPS\u7684\u51b2\u7a81\uff0c\u663e\u8457\u63d0\u5347\u4e86\u5f02\u8d28\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\u7684\u6027\u80fd\u3002"}}
{"id": "2507.10000", "categories": ["cs.AI", "cs.CL", "I.2.11; F.4.1; I.2.4; G.2.2"], "pdf": "https://arxiv.org/pdf/2507.10000", "abs": "https://arxiv.org/abs/2507.10000", "authors": ["Mark Burgess"], "title": "On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model", "comment": null, "summary": "Since Searle's work deconstructing intent and intentionality in the realm of\nphilosophy, the practical meaning of intent has received little attention in\nscience and technology. Intentionality and context are both central to the\nscope of Promise Theory's model of Semantic Spacetime, used as an effective\nTiny Language Model. One can identify themes and concepts from a text, on a low\nlevel (without knowledge of the specific language) by using process coherence\nas a guide. Any agent process can assess superficially a degree of latent\n`intentionality' in data by looking for anomalous multi-scale anomalies and\nassessing the work done to form them. Scale separation can be used to sort\nparts into `intended' content and `ambient context', using the spacetime\ncoherence as a measure. This offers an elementary but pragmatic interpretation\nof latent intentionality for very low computational cost, and without reference\nto extensive training or reasoning capabilities. The process is well within the\nreach of basic organisms as it does not require large scale artificial\nprobabilistic batch processing. The level of concept formation depends,\nhowever, on the memory capacity of the agent.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8ePromise Theory\u7684\u8bed\u4e49\u65f6\u7a7a\u6a21\u578b\uff0c\u901a\u8fc7\u591a\u5c3a\u5ea6\u5f02\u5e38\u68c0\u6d4b\u548c\u65f6\u7a7a\u4e00\u81f4\u6027\u5206\u6790\uff0c\u4f4e\u6210\u672c\u5730\u8bc4\u4f30\u6570\u636e\u4e2d\u7684\u6f5c\u5728\u610f\u56fe\u6027\u3002", "motivation": "\u63a2\u8ba8\u79d1\u5b66\u548c\u6280\u672f\u9886\u57df\u4e2d\u610f\u56fe\u7684\u5b9e\u9645\u610f\u4e49\uff0c\u5f25\u8865Searle\u4e4b\u540e\u5bf9\u610f\u56fe\u7814\u7a76\u7684\u4e0d\u8db3\u3002", "method": "\u5229\u7528\u8fc7\u7a0b\u4e00\u81f4\u6027\u548c\u591a\u5c3a\u5ea6\u5f02\u5e38\u68c0\u6d4b\uff0c\u5206\u79bb\u610f\u56fe\u5185\u5bb9\u548c\u73af\u5883\u80cc\u666f\uff0c\u8bc4\u4f30\u6f5c\u5728\u610f\u56fe\u6027\u3002", "result": "\u63d0\u4f9b\u4e86\u4e00\u79cd\u4f4e\u6210\u672c\u3001\u65e0\u9700\u5927\u89c4\u6a21\u8bad\u7ec3\u6216\u63a8\u7406\u80fd\u529b\u7684\u6f5c\u5728\u610f\u56fe\u6027\u8bc4\u4f30\u65b9\u6cd5\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u9002\u7528\u4e8e\u57fa\u7840\u751f\u7269\u4f53\uff0c\u4f46\u6982\u5ff5\u5f62\u6210\u6c34\u5e73\u53d7\u9650\u4e8e\u4ee3\u7406\u7684\u8bb0\u5fc6\u80fd\u529b\u3002"}}
{"id": "2507.10007", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10007", "abs": "https://arxiv.org/abs/2507.10007", "authors": ["Zijun Chen", "Wenbo Hu", "Richang Hong"], "title": "Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning", "comment": null, "summary": "Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning\ncapabilities in both large language models (LLMs) and multimodal large language\nmodels (MLLMs). However, its reliability is often undermined by the\naccumulation of errors in intermediate steps. This paper introduces an novel\napproach to calibrate the CoT reasoning accuracy by leveraging the model's\nintrinsic veracity encoding. We discover that specific attention head\nactivations reliably reflect the truthfulness of reasoning steps in CoT. Based\non this insight, we train a confidence predictor to evaluate the correctness of\neach reasoning step using these truthfulness-sensitive activations, dynamically\nselecting the most plausible reasoning path via beam search. Experimental\nresults demonstrate that our method significantly outperforms the\nstate-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and\nSelf-Evaluation Guided Beam Search) across the mathematical, symbolic, and\ncommonsense reasoning tasks, exhibiting superior accuracy and reliability in\nboth unimodal and multimodal settings. We further validate the approach on\nlarge reasoning models, confirming its applicability to specialized reasoning\nmodels. Additionally, we explore the role of the model's self-correction\nability in CoT reasoning. This work provides a novel reliability improvement\npath for CoT reasoning with broad application potential.", "AI": {"tldr": "\u672c\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u901a\u8fc7\u6a21\u578b\u5185\u5728\u771f\u5b9e\u6027\u7f16\u7801\u6821\u51c6CoT\u63a8\u7406\u51c6\u786e\u6027\u7684\u65b0\u65b9\u6cd5\uff0c\u663e\u8457\u63d0\u5347\u4e86\u63a8\u7406\u4efb\u52a1\u7684\u53ef\u9760\u6027\u548c\u51c6\u786e\u6027\u3002", "motivation": "CoT\u63a8\u7406\u5728LLMs\u548cMLLMs\u4e2d\u8868\u73b0\u51fa\u5f3a\u5927\u7684\u6df1\u5ea6\u63a8\u7406\u80fd\u529b\uff0c\u4f46\u4e2d\u95f4\u6b65\u9aa4\u7684\u9519\u8bef\u7d2f\u79ef\u4f1a\u964d\u4f4e\u5176\u53ef\u9760\u6027\u3002", "method": "\u5229\u7528\u7279\u5b9a\u6ce8\u610f\u529b\u5934\u6fc0\u6d3b\u53cd\u6620CoT\u63a8\u7406\u6b65\u9aa4\u7684\u771f\u5b9e\u6027\uff0c\u8bad\u7ec3\u7f6e\u4fe1\u5ea6\u9884\u6d4b\u5668\u52a8\u6001\u9009\u62e9\u6700\u4f18\u63a8\u7406\u8def\u5f84\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u8be5\u65b9\u6cd5\u5728\u6570\u5b66\u3001\u7b26\u53f7\u548c\u5e38\u8bc6\u63a8\u7406\u4efb\u52a1\u4e2d\u4f18\u4e8e\u73b0\u6709\u57fa\u7ebf\uff0c\u9002\u7528\u4e8e\u5355\u6a21\u6001\u548c\u591a\u6a21\u6001\u573a\u666f\u3002", "conclusion": "\u8be5\u7814\u7a76\u4e3aCoT\u63a8\u7406\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u7684\u53ef\u9760\u6027\u6539\u8fdb\u8def\u5f84\uff0c\u5177\u6709\u5e7f\u6cdb\u7684\u5e94\u7528\u6f5c\u529b\u3002"}}
{"id": "2507.10045", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.10045", "abs": "https://arxiv.org/abs/2507.10045", "authors": ["Malte Christian Bartels", "Debayan Banerjee", "Ricardo Usbeck"], "title": "Automating SPARQL Query Translations between DBpedia and Wikidata", "comment": "18 pages, 2 figues. Paper accepted at SEMANTiCS 2025 conference\n  happening on September 2025", "summary": "This paper investigates whether state-of-the-art Large Language Models (LLMs)\ncan automatically translate SPARQL between popular Knowledge Graph (KG)\nschemas. We focus on translations between the DBpedia and Wikidata KG, and\nlater on DBLP and OpenAlex KG. This study addresses a notable gap in KG\ninteroperability research by rigorously evaluating LLM performance on\nSPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first\nalign 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100\nDBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic\nKGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and\nMistral-Large-Instruct-2407 are selected based on their sizes and architectures\nand tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs\nwere compared with gold answers, and resulting errors were categorized. We find\nthat the performance varies markedly across models and prompting strategies,\nand that translations for Wikidata to DBpedia work far better than translations\nfor DBpedia to Wikidata.", "AI": {"tldr": "\u7814\u7a76\u4e86\u5927\u578b\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u5728SPARQL\u67e5\u8be2\u7ffb\u8bd1\u4e2d\u7684\u8868\u73b0\uff0c\u91cd\u70b9\u8bc4\u4f30\u4e86DBpedia-Wikidata\u548cDBLP-OpenAlex\u4e4b\u95f4\u7684\u7ffb\u8bd1\u6548\u679c\u3002", "motivation": "\u89e3\u51b3\u77e5\u8bc6\u56fe\u8c31\uff08KG\uff09\u4e92\u64cd\u4f5c\u6027\u7814\u7a76\u4e2dSPARQL\u7ffb\u8bd1\u7684\u7a7a\u767d\u3002", "method": "\u4f7f\u7528\u4e09\u79cdLLM\u6a21\u578b\uff08Llama-3-8B\u3001DeepSeek-R1-Distill-Llama-70B\u3001Mistral-Large-Instruct-2407\uff09\uff0c\u901a\u8fc7\u96f6\u6837\u672c\u3001\u5c11\u6837\u672c\u548c\u601d\u7ef4\u94fe\u53d8\u4f53\u8fdb\u884c\u6d4b\u8bd5\u3002", "result": "\u4e0d\u540c\u6a21\u578b\u548c\u63d0\u793a\u7b56\u7565\u8868\u73b0\u5dee\u5f02\u663e\u8457\uff0cWikidata\u5230DBpedia\u7684\u7ffb\u8bd1\u6548\u679c\u4f18\u4e8e\u53cd\u5411\u3002", "conclusion": "LLM\u5728SPARQL\u7ffb\u8bd1\u4e2d\u8868\u73b0\u4e0d\u4e00\uff0c\u9700\u8fdb\u4e00\u6b65\u4f18\u5316\u6a21\u578b\u548c\u7b56\u7565\u3002"}}
{"id": "2507.10076", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10076", "abs": "https://arxiv.org/abs/2507.10076", "authors": ["Anna Rapberger", "Fabrizio Russo", "Antonio Rago", "Francesca Toni"], "title": "On Gradual Semantics for Assumption-Based Argumentation", "comment": null, "summary": "In computational argumentation, gradual semantics are fine-grained\nalternatives to extension-based and labelling-based semantics . They ascribe a\ndialectical strength to (components of) arguments sanctioning their degree of\nacceptability. Several gradual semantics have been studied for abstract,\nbipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,\nto a lesser extent, for some forms of structured argumentation. However, this\nhas not been the case for assumption-based argumentation (ABA), despite it\nbeing a popular form of structured argumentation with several applications\nwhere gradual semantics could be useful. In this paper, we fill this gap and\npropose a family of novel gradual semantics for equipping assumptions, which\nare the core components in ABA frameworks, with dialectical strengths. To do\nso, we use bipolar set-based argumentation frameworks as an abstraction of\n(potentially non-flat) ABA frameworks and generalise state-of-the-art modular\ngradual semantics for QBAFs. We show that our gradual ABA semantics satisfy\nsuitable adaptations of desirable properties of gradual QBAF semantics, such as\nbalance and monotonicity. We also explore an argument-based approach that\nleverages established QBAF modular semantics directly, and use it as baseline.\nFinally, we conduct experiments with synthetic ABA frameworks to compare our\ngradual ABA semantics with its argument-based counterpart and assess\nconvergence.", "AI": {"tldr": "\u8be5\u8bba\u6587\u586b\u8865\u4e86\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\u4e2d\u9010\u6b65\u8bed\u4e49\u5b66\u7684\u7a7a\u767d\uff0c\u63d0\u51fa\u4e86\u4e00\u7cfb\u5217\u65b0\u7684\u9010\u6b65\u8bed\u4e49\u5b66\u65b9\u6cd5\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5c3d\u7ba1\u9010\u6b65\u8bed\u4e49\u5b66\u5728\u591a\u79cd\u8bba\u8bc1\u6846\u67b6\u4e2d\u5df2\u6709\u7814\u7a76\uff0c\u4f46\u5c1a\u672a\u5e94\u7528\u4e8e\u5047\u8bbe\u57fa\u7840\u8bba\u8bc1\uff08ABA\uff09\uff0c\u800cABA\u662f\u4e00\u79cd\u6d41\u884c\u7684\u7ed3\u6784\u5316\u8bba\u8bc1\u5f62\u5f0f\u3002\u672c\u6587\u65e8\u5728\u586b\u8865\u8fd9\u4e00\u7a7a\u767d\u3002", "method": "\u901a\u8fc7\u5c06\u53cc\u6781\u96c6\u57fa\u7840\u8bba\u8bc1\u6846\u67b6\u4f5c\u4e3aABA\u7684\u62bd\u8c61\uff0c\u5e76\u6269\u5c55\u4e86\u6700\u5148\u8fdb\u7684\u6a21\u5757\u5316\u9010\u6b65\u8bed\u4e49\u5b66\u65b9\u6cd5\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u65b0\u7684\u9010\u6b65ABA\u8bed\u4e49\u5b66\u3002", "result": "\u5b9e\u9a8c\u8868\u660e\uff0c\u63d0\u51fa\u7684\u9010\u6b65ABA\u8bed\u4e49\u5b66\u6ee1\u8db3\u5e73\u8861\u6027\u548c\u5355\u8c03\u6027\u7b49\u7406\u60f3\u6027\u8d28\uff0c\u5e76\u4e0e\u57fa\u4e8e\u8bba\u8bc1\u7684\u65b9\u6cd5\u8fdb\u884c\u4e86\u6bd4\u8f83\u3002", "conclusion": "\u672c\u6587\u6210\u529f\u5730\u5c06\u9010\u6b65\u8bed\u4e49\u5b66\u5f15\u5165ABA\uff0c\u5e76\u901a\u8fc7\u5b9e\u9a8c\u9a8c\u8bc1\u4e86\u5176\u53ef\u884c\u6027\u548c\u6709\u6548\u6027\u3002"}}
{"id": "2507.10106", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10106", "abs": "https://arxiv.org/abs/2507.10106", "authors": ["Harshal Nandigramwar", "Syed Qutub", "Kay-Ulrich Scholl"], "title": "BlueGlass: A Framework for Composite AI Safety", "comment": "Accepted at ICML 2025 [Actionable Interpretability Workshop]", "summary": "As AI systems become increasingly capable and ubiquitous, ensuring the safety\nof these systems is critical. However, existing safety tools often target\ndifferent aspects of model safety and cannot provide full assurance in\nisolation, highlighting a need for integrated and composite methodologies. This\npaper introduces BlueGlass, a framework designed to facilitate composite AI\nsafety workflows by providing a unified infrastructure enabling the integration\nand composition of diverse safety tools that operate across model internals and\noutputs. Furthermore, to demonstrate the utility of this framework, we present\nthree safety-oriented analyses on vision-language models for the task of object\ndetection: (1) distributional evaluation, revealing performance trade-offs and\npotential failure modes across distributions; (2) probe-based analysis of layer\ndynamics highlighting shared hierarchical learning via phase transition; and\n(3) sparse autoencoders identifying interpretable concepts. More broadly, this\nwork contributes foundational infrastructure and findings for building more\nrobust and reliable AI systems.", "AI": {"tldr": "\u672c\u6587\u4ecb\u7ecd\u4e86BlueGlass\u6846\u67b6\uff0c\u7528\u4e8e\u6574\u5408\u591a\u79cdAI\u5b89\u5168\u5de5\u5177\uff0c\u5e76\u901a\u8fc7\u4e09\u4e2a\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u7684\u5b89\u5168\u5206\u6790\u5c55\u793a\u4e86\u5176\u6548\u7528\u3002", "motivation": "\u968f\u7740AI\u7cfb\u7edf\u7684\u80fd\u529b\u589e\u5f3a\u548c\u666e\u53ca\uff0c\u786e\u4fdd\u5176\u5b89\u5168\u6027\u53d8\u5f97\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u5b89\u5168\u5de5\u5177\u5404\u81ea\u72ec\u7acb\uff0c\u65e0\u6cd5\u63d0\u4f9b\u5168\u9762\u4fdd\u969c\uff0c\u56e0\u6b64\u9700\u8981\u96c6\u6210\u5316\u7684\u65b9\u6cd5\u3002", "method": "\u63d0\u51faBlueGlass\u6846\u67b6\uff0c\u63d0\u4f9b\u7edf\u4e00\u57fa\u7840\u8bbe\u65bd\u4ee5\u6574\u5408\u548c\u7ec4\u5408\u591a\u6837\u5316\u7684\u5b89\u5168\u5de5\u5177\uff0c\u8986\u76d6\u6a21\u578b\u5185\u90e8\u548c\u8f93\u51fa\u3002\u901a\u8fc7\u4e09\u4e2a\u5177\u4f53\u5206\u6790\uff08\u5206\u5e03\u8bc4\u4f30\u3001\u63a2\u9488\u5206\u6790\u548c\u7a00\u758f\u81ea\u7f16\u7801\u5668\uff09\u9a8c\u8bc1\u6846\u67b6\u3002", "result": "\u5c55\u793a\u4e86BlueGlass\u6846\u67b6\u5728\u89c6\u89c9\u8bed\u8a00\u6a21\u578b\u4e0a\u7684\u5e94\u7528\uff0c\u63ed\u793a\u4e86\u6027\u80fd\u6743\u8861\u3001\u5c42\u6b21\u5b66\u4e60\u52a8\u6001\u548c\u53ef\u89e3\u91ca\u6982\u5ff5\u3002", "conclusion": "\u8be5\u5de5\u4f5c\u4e3a\u6784\u5efa\u66f4\u7a33\u5065\u53ef\u9760\u7684AI\u7cfb\u7edf\u63d0\u4f9b\u4e86\u57fa\u7840\u6027\u57fa\u7840\u8bbe\u65bd\u548c\u53d1\u73b0\u3002"}}
{"id": "2507.10119", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10119", "abs": "https://arxiv.org/abs/2507.10119", "authors": ["Sadig Gojayev", "Ahmad Anaqreh", "Carolina Fortuna"], "title": "Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration", "comment": null, "summary": "Application migration in edge-cloud system enables high QoS and cost\neffective service delivery. However, automatically orchestrating such migration\nis typically solved with heuristic approaches. Starting from the Markov\nDecision Process (MDP), in this paper, we identify, analyze and compare\nselected state-of-the-art Artificial Intelligence (AI) planning and\nReinforcement Learning (RL) approaches for solving the class of edge-cloud\napplication migration problems that can be modeled as Towers of Hanoi (ToH)\nproblems. We introduce a new classification based on state space definition and\nanalyze the compared models also through this lense. The aim is to understand\navailable techniques capable of orchestrating such application migration in\nemerging computing continuum environments.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u5e94\u7528\u8fc1\u79fb\u7684\u81ea\u52a8\u7f16\u6392\u95ee\u9898\uff0c\u901a\u8fc7MDP\u6846\u67b6\u6bd4\u8f83\u4e86AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4ee5\u6c49\u8bfa\u5854\u95ee\u9898\u4e3a\u6a21\u578b\u8fdb\u884c\u4e86\u5206\u6790\u3002", "motivation": "\u7814\u7a76\u65e8\u5728\u63d0\u9ad8\u8fb9\u7f18-\u4e91\u7cfb\u7edf\u4e2d\u5e94\u7528\u8fc1\u79fb\u7684\u670d\u52a1\u8d28\u91cf\uff08QoS\uff09\u548c\u6210\u672c\u6548\u76ca\uff0c\u63a2\u7d22\u81ea\u52a8\u5316\u7f16\u6392\u7684\u53ef\u884c\u6280\u672f\u3002", "method": "\u57fa\u4e8e\u9a6c\u5c14\u53ef\u592b\u51b3\u7b56\u8fc7\u7a0b\uff08MDP\uff09\uff0c\u6bd4\u8f83\u4e86AI\u89c4\u5212\u548c\u5f3a\u5316\u5b66\u4e60\u65b9\u6cd5\uff0c\u5e76\u4ee5\u6c49\u8bfa\u5854\u95ee\u9898\u4e3a\u6a21\u578b\u8fdb\u884c\u5206\u7c7b\u548c\u5206\u6790\u3002", "result": "\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8e\u72b6\u6001\u7a7a\u95f4\u5b9a\u4e49\u7684\u65b0\u5206\u7c7b\u65b9\u6cd5\uff0c\u5e76\u5206\u6790\u4e86\u4e0d\u540c\u6a21\u578b\u7684\u9002\u7528\u6027\u3002", "conclusion": "\u7814\u7a76\u4e3a\u65b0\u5174\u8ba1\u7b97\u8fde\u7eed\u73af\u5883\u4e2d\u7684\u5e94\u7528\u8fc1\u79fb\u7f16\u6392\u63d0\u4f9b\u4e86\u6280\u672f\u53c2\u8003\u548c\u5206\u7c7b\u6846\u67b6\u3002"}}
{"id": "2507.10124", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10124", "abs": "https://arxiv.org/abs/2507.10124", "authors": ["Thomas T. Hills"], "title": "Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making", "comment": "12 pages, 3 figures", "summary": "Identifying bias in LLMs is ongoing. Because they are still in development,\nwhat is true today may be false tomorrow. We therefore need general strategies\nfor debiasing that will outlive current models. Strategies developed for\ndebiasing human decision making offer one promising approach as they\nincorporate an LLM-style prompt intervention designed to bring latent knowledge\ninto awareness during decision making. LLMs trained on vast amounts of\ninformation contain information about potential biases, counter-arguments, and\ncontradictory evidence, but that information may only be brought to bear if\nprompted. Metacognitive prompts developed in the human decision making\nliterature are designed to achieve this, and as I demonstrate here, they show\npromise with LLMs. The prompt I focus on here is \"could you be wrong?\"\nFollowing an LLM response, this prompt leads LLMs to produce additional\ninformation, including why they answered as they did, errors, biases,\ncontradictory evidence, and alternatives, none of which were apparent in their\ninitial response. Indeed, this metaknowledge often reveals that how LLMs and\nusers interpret prompts are not aligned. Here I demonstrate this prompt using a\nset of questions taken from recent articles about LLM biases, including\nimplicit discriminatory biases and failures of metacognition. \"Could you be\nwrong\" prompts the LLM to identify its own biases and produce cogent\nmetacognitive reflection. I also present another example involving convincing\nbut incomplete information, which is readily corrected by the metacognitive\nprompt. In sum, this work argues that human psychology offers a new avenue for\nprompt engineering, leveraging a long history of effective prompt-based\nimprovements to human decision making.", "AI": {"tldr": "\u8bba\u6587\u63a2\u8ba8\u4e86\u5229\u7528\u4eba\u7c7b\u5fc3\u7406\u5b66\u4e2d\u7684\u5143\u8ba4\u77e5\u63d0\u793a\uff08\u5982\u201c\u4f60\u53ef\u80fd\u662f\u9519\u7684\u5417\uff1f\u201d\uff09\u6765\u51cf\u5c11LLM\u7684\u504f\u89c1\uff0c\u5e76\u5c55\u793a\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u7531\u4e8eLLM\u4ecd\u5728\u53d1\u5c55\u4e2d\uff0c\u5f53\u524d\u7684\u504f\u89c1\u53ef\u80fd\u968f\u65f6\u95f4\u53d8\u5316\uff0c\u56e0\u6b64\u9700\u8981\u901a\u7528\u7684\u53bb\u504f\u89c1\u7b56\u7565\u3002\u4eba\u7c7b\u51b3\u7b56\u4e2d\u7684\u53bb\u504f\u89c1\u65b9\u6cd5\u4e3aLLM\u63d0\u4f9b\u4e86\u6f5c\u5728\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u91c7\u7528\u5143\u8ba4\u77e5\u63d0\u793a\uff08\u5982\u201c\u4f60\u53ef\u80fd\u662f\u9519\u7684\u5417\uff1f\u201d\uff09\u5f15\u5bfcLLM\u53cd\u601d\u5176\u56de\u7b54\uff0c\u63ed\u793a\u6f5c\u5728\u504f\u89c1\u3001\u9519\u8bef\u548c\u77db\u76fe\u4fe1\u606f\u3002", "result": "\u5143\u8ba4\u77e5\u63d0\u793a\u80fd\u6709\u6548\u5f15\u5bfcLLM\u8bc6\u522b\u81ea\u8eab\u504f\u89c1\uff0c\u5e76\u63d0\u4f9b\u989d\u5916\u7684\u53cd\u601d\u4fe1\u606f\uff0c\u6539\u5584\u521d\u59cb\u56de\u7b54\u7684\u5c40\u9650\u6027\u3002", "conclusion": "\u4eba\u7c7b\u5fc3\u7406\u5b66\u4e3aLLM\u7684\u63d0\u793a\u5de5\u7a0b\u63d0\u4f9b\u4e86\u65b0\u601d\u8def\uff0c\u5229\u7528\u5143\u8ba4\u77e5\u63d0\u793a\u53ef\u663e\u8457\u63d0\u5347LLM\u7684\u51b3\u7b56\u8d28\u91cf\u3002"}}
{"id": "2507.10134", "categories": ["cs.AI", "53-01", "C.2"], "pdf": "https://arxiv.org/pdf/2507.10134", "abs": "https://arxiv.org/abs/2507.10134", "authors": ["Yousef Emami", "Hao Zhou", "Miguel Gutierrez Gaitan", "Kai Li", "Luis Almeida"], "title": "FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring", "comment": "8 pages, 8 figures", "summary": "Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in\nwildfire monitoring, where early detection minimizes environmental impact. In\nUAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor\ntransmission scheduling and velocity is critical for minimizing Age of\nInformation (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has\nbeen used for such optimization; however, its limitations such as low sampling\nefficiency, simulation-to-reality gaps, and complex training render it\nunsuitable for time-critical applications like wildfire monitoring. This paper\nintroduces a new online Flight Resource Allocation scheme based on LLM-Enabled\nIn-Context Learning (FRSICL) to jointly optimize the UAV's flight control and\ndata collection schedule along the trajectory in real time, thereby\nasymptotically minimizing the average AoI across ground sensors. In contrast to\nDRL, FRSICL generates data collection schedules and controls velocity using\nnatural language task descriptions and feedback from the environment, enabling\ndynamic decision-making without extensive retraining. Simulation results\nconfirm the effectiveness of the proposed FRSICL compared to Proximal Policy\nOptimization (PPO) and Nearest-Neighbor baselines.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u57fa\u4e8eLLM\u7684\u5728\u7ebf\u98de\u884c\u8d44\u6e90\u5206\u914d\u65b9\u6848\uff08FRSICL\uff09\uff0c\u7528\u4e8e\u65e0\u4eba\u673a\u8f85\u52a9\u7684\u91ce\u706b\u76d1\u6d4b\u7cfb\u7edf\uff0c\u901a\u8fc7\u5b9e\u65f6\u4f18\u5316\u98de\u884c\u63a7\u5236\u548c\u6570\u636e\u6536\u96c6\u8c03\u5ea6\uff0c\u6700\u5c0f\u5316\u4fe1\u606f\u5e74\u9f84\uff08AoI\uff09\u3002", "motivation": "\u4f20\u7edf\u6df1\u5ea6\u5f3a\u5316\u5b66\u4e60\uff08DRL\uff09\u5728\u65e0\u4eba\u673a\u8f85\u52a9\u91ce\u706b\u76d1\u6d4b\u4e2d\u5b58\u5728\u91c7\u6837\u6548\u7387\u4f4e\u3001\u4eff\u771f\u4e0e\u73b0\u5b9e\u5dee\u8ddd\u5927\u7b49\u95ee\u9898\uff0c\u4e0d\u9002\u7528\u4e8e\u65f6\u95f4\u654f\u611f\u5e94\u7528\u3002", "method": "FRSICL\u5229\u7528\u81ea\u7136\u8bed\u8a00\u4efb\u52a1\u63cf\u8ff0\u548c\u73af\u5883\u53cd\u9988\uff0c\u52a8\u6001\u751f\u6210\u6570\u636e\u6536\u96c6\u8ba1\u5212\u548c\u901f\u5ea6\u63a7\u5236\uff0c\u65e0\u9700\u5927\u91cf\u91cd\u65b0\u8bad\u7ec3\u3002", "result": "\u4eff\u771f\u7ed3\u679c\u8868\u660e\uff0cFRSICL\u5728\u6700\u5c0f\u5316AoI\u65b9\u9762\u4f18\u4e8ePPO\u548c\u6700\u8fd1\u90bb\u57fa\u7ebf\u65b9\u6cd5\u3002", "conclusion": "FRSICL\u4e3a\u65e0\u4eba\u673a\u8f85\u52a9\u91ce\u706b\u76d1\u6d4b\u63d0\u4f9b\u4e86\u4e00\u79cd\u9ad8\u6548\u3001\u52a8\u6001\u7684\u89e3\u51b3\u65b9\u6848\uff0c\u514b\u670d\u4e86DRL\u7684\u5c40\u9650\u6027\u3002"}}
{"id": "2507.10142", "categories": ["cs.AI", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10142", "abs": "https://arxiv.org/abs/2507.10142", "authors": ["Siyi Hu", "Mohamad A Hady", "Jianglin Qiao", "Jimmy Cao", "Mahardhika Pratama", "Ryszard Kowalczyk"], "title": "Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review", "comment": null, "summary": "Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in\ncoordinating multiple agents across simulated benchmarks and constrained\nscenarios. However, its deployment in real-world multi-agent systems (MAS)\nremains limited, primarily due to the complex and dynamic nature of such\nenvironments. These challenges arise from multiple interacting sources of\nvariability, including fluctuating agent populations, evolving task goals, and\ninconsistent execution conditions. Together, these factors demand that MARL\nalgorithms remain effective under continuously changing system configurations\nand operational demands. To better capture and assess this capacity for\nadjustment, we introduce the concept of \\textit{adaptability} as a unified and\npractically grounded lens through which to evaluate the reliability of MARL\nalgorithms under shifting conditions, broadly referring to any changes in the\nenvironment dynamics that may occur during learning or execution. Centred on\nthe notion of adaptability, we propose a structured framework comprising three\nkey dimensions: learning adaptability, policy adaptability, and scenario-driven\nadaptability. By adopting this adaptability perspective, we aim to support more\nprincipled assessments of MARL performance beyond narrowly defined benchmarks.\nUltimately, this survey contributes to the development of algorithms that are\nbetter suited for deployment in dynamic, real-world multi-agent systems.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u201c\u9002\u5e94\u6027\u201d\u6982\u5ff5\uff0c\u7528\u4e8e\u8bc4\u4f30\u591a\u667a\u80fd\u4f53\u5f3a\u5316\u5b66\u4e60\uff08MARL\uff09\u5728\u52a8\u6001\u73af\u5883\u4e2d\u7684\u53ef\u9760\u6027\uff0c\u5e76\u63d0\u51fa\u4e86\u4e00\u4e2a\u5305\u542b\u4e09\u4e2a\u7ef4\u5ea6\u7684\u6846\u67b6\u3002", "motivation": "\u73b0\u5b9e\u4e16\u754c\u7684\u591a\u667a\u80fd\u4f53\u7cfb\u7edf\uff08MAS\uff09\u73af\u5883\u590d\u6742\u591a\u53d8\uff0c\u73b0\u6709MARL\u7b97\u6cd5\u96be\u4ee5\u9002\u5e94\uff0c\u9700\u66f4\u5168\u9762\u7684\u8bc4\u4f30\u65b9\u6cd5\u3002", "method": "\u63d0\u51fa\u201c\u9002\u5e94\u6027\u201d\u6982\u5ff5\uff0c\u5e76\u6784\u5efa\u5305\u542b\u5b66\u4e60\u9002\u5e94\u6027\u3001\u7b56\u7565\u9002\u5e94\u6027\u548c\u573a\u666f\u9a71\u52a8\u9002\u5e94\u6027\u7684\u4e09\u7ef4\u6846\u67b6\u3002", "result": "\u901a\u8fc7\u9002\u5e94\u6027\u89c6\u89d2\uff0c\u652f\u6301\u66f4\u7cfb\u7edf\u5316\u7684MARL\u6027\u80fd\u8bc4\u4f30\uff0c\u8d85\u8d8a\u4f20\u7edf\u57fa\u51c6\u6d4b\u8bd5\u3002", "conclusion": "\u8be5\u6846\u67b6\u6709\u52a9\u4e8e\u5f00\u53d1\u66f4\u9002\u5408\u52a8\u6001\u73b0\u5b9e\u4e16\u754cMAS\u7684MARL\u7b97\u6cd5\u3002"}}
{"id": "2507.10156", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10156", "abs": "https://arxiv.org/abs/2507.10156", "authors": ["Lubnaa Abdur Rahman", "Ioannis Papathanail", "Stavroula Mougiakakou"], "title": "Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation", "comment": "10 pages, 2 Figures, 7 tables", "summary": "AI has driven significant progress in the nutrition field, especially through\nmultimedia-based automatic dietary assessment. However, existing automatic\ndietary assessment systems often overlook critical non-visual factors, such as\nrecipe-specific ingredient substitutions that can significantly alter\nnutritional content, and rarely account for individual dietary needs, including\nallergies, restrictions, cultural practices, and personal preferences. In\nSwitzerland, while food-related information is available, it remains\nfragmented, and no centralized repository currently integrates all relevant\nnutrition-related aspects within a Swiss context. To bridge this divide, we\nintroduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our\nbest knowledge, to unite recipes, ingredients, and their substitutions with\nnutrient data, dietary restrictions, allergen information, and national\nnutrition guidelines under one graph. We establish a LLM-powered enrichment\npipeline for populating the graph, whereby we further present the first\nbenchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge\naugmentation. Our results demonstrate that LLMs can effectively enrich the\ngraph with relevant nutritional information. Our SwissFKG goes beyond recipe\nrecommendations by offering ingredient-level information such as allergen and\ndietary restriction information, and guidance aligned with nutritional\nguidelines. Moreover, we implement a Graph-RAG application to showcase how the\nSwissFKG's rich natural-language data structure can help LLM answer\nuser-specific nutrition queries, and we evaluate LLM-embedding pairings by\ncomparing user-query responses against predefined expected answers. As such,\nour work lays the foundation for the next generation of dietary assessment\ntools that blend visual, contextual, and cultural dimensions of eating.", "AI": {"tldr": "\u8bba\u6587\u4ecb\u7ecd\u4e86\u745e\u58eb\u98df\u54c1\u77e5\u8bc6\u56fe\u8c31\uff08SwissFKG\uff09\uff0c\u6574\u5408\u4e86\u98df\u8c31\u3001\u98df\u6750\u3001\u8425\u517b\u6570\u636e\u3001\u996e\u98df\u9650\u5236\u548c\u8fc7\u654f\u4fe1\u606f\uff0c\u5e76\u5229\u7528LLM\u589e\u5f3a\u56fe\u8c31\u5185\u5bb9\uff0c\u4e3a\u4e2a\u6027\u5316\u8425\u517b\u8bc4\u4f30\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002", "motivation": "\u73b0\u6709\u81ea\u52a8\u996e\u98df\u8bc4\u4f30\u7cfb\u7edf\u5ffd\u89c6\u975e\u89c6\u89c9\u56e0\u7d20\uff08\u5982\u98df\u6750\u66ff\u4ee3\u548c\u4e2a\u4f53\u9700\u6c42\uff09\uff0c\u745e\u58eb\u7f3a\u4e4f\u7edf\u4e00\u7684\u8425\u517b\u4fe1\u606f\u6574\u5408\u8d44\u6e90\u3002", "method": "\u6784\u5efaSwissFKG\uff0c\u91c7\u7528LLM\u589e\u5f3a\u56fe\u8c31\u5185\u5bb9\uff0c\u5e76\u8bc4\u4f30LLM\u5728\u98df\u54c1\u77e5\u8bc6\u589e\u5f3a\u4e2d\u7684\u8868\u73b0\u3002", "result": "LLM\u80fd\u6709\u6548\u4e30\u5bcc\u56fe\u8c31\u8425\u517b\u4fe1\u606f\uff0cSwissFKG\u63d0\u4f9b\u98df\u6750\u7ea7\u4fe1\u606f\u548c\u8425\u517b\u6307\u5357\uff0cGraph-RAG\u5e94\u7528\u5c55\u793a\u5176\u652f\u6301LLM\u56de\u7b54\u7528\u6237\u8425\u517b\u67e5\u8be2\u7684\u80fd\u529b\u3002", "conclusion": "SwissFKG\u4e3a\u7ed3\u5408\u89c6\u89c9\u3001\u4e0a\u4e0b\u6587\u548c\u6587\u5316\u7ef4\u5ea6\u7684\u4e0b\u4e00\u4ee3\u996e\u98df\u8bc4\u4f30\u5de5\u5177\u5960\u5b9a\u4e86\u57fa\u7840\u3002"}}
{"id": "2507.10174", "categories": ["cs.AI", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10174", "abs": "https://arxiv.org/abs/2507.10174", "authors": ["Yumi Omori", "Zixuan Dong", "Keith Ross"], "title": "Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?", "comment": "Accepted by RLBrew: Ingredients for Developing Generalist Agents\n  workshop (RLC 2025)", "summary": "In recent years, extensive work has explored the application of the\nTransformer architecture to reinforcement learning problems. Among these,\nDecision Transformer (DT) has gained particular attention in the context of\noffline reinforcement learning due to its ability to frame return-conditioned\npolicy learning as a sequence modeling task. Most recently, Bhargava et al.\n(2024) provided a systematic comparison of DT with more conventional MLP-based\noffline RL algorithms, including Behavior Cloning (BC) and Conservative\nQ-Learning (CQL), and claimed that DT exhibits superior performance in\nsparse-reward and low-quality data settings.\n  In this paper, through experimentation on robotic manipulation tasks\n(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered\nBehavior Cloning (FBC) achieves competitive or superior performance compared to\nDT in sparse-reward environments. FBC simply filters out low-performing\ntrajectories from the dataset and then performs ordinary behavior cloning on\nthe filtered dataset. FBC is not only very straightforward, but it also\nrequires less training data and is computationally more efficient. The results\ntherefore suggest that DT is not preferable for sparse-reward environments.\nFrom prior work, arguably, DT is also not preferable for dense-reward\nenvironments. Thus, we pose the question: Is DT ever preferable?", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u9a8c\u6bd4\u8f83\u4e86\u57fa\u4e8eMLP\u7684\u8fc7\u6ee4\u884c\u4e3a\u514b\u9686\uff08FBC\uff09\u4e0e\u51b3\u7b56\u53d8\u6362\u5668\uff08DT\uff09\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u7684\u8868\u73b0\uff0c\u53d1\u73b0FBC\u6027\u80fd\u66f4\u4f18\u4e14\u66f4\u9ad8\u6548\u3002", "motivation": "\u63a2\u8ba8DT\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u662f\u5426\u4f18\u4e8e\u4f20\u7edf\u65b9\u6cd5\uff0c\u5e76\u8d28\u7591DT\u7684\u9002\u7528\u6027\u3002", "method": "\u5728Robomimic\u548cD4RL\u4efb\u52a1\u4e0a\u5b9e\u9a8c\uff0c\u6bd4\u8f83FBC\uff08\u8fc7\u6ee4\u4f4e\u8d28\u91cf\u8f68\u8ff9\u540e\u8fdb\u884c\u884c\u4e3a\u514b\u9686\uff09\u4e0eDT\u7684\u6027\u80fd\u3002", "result": "FBC\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u8868\u73b0\u4f18\u4e8eDT\uff0c\u4e14\u66f4\u9ad8\u6548\u3002", "conclusion": "DT\u5728\u7a00\u758f\u5956\u52b1\u73af\u5883\u4e2d\u5e76\u975e\u4f18\u9009\uff0c\u751a\u81f3\u53ef\u80fd\u5728\u5176\u4ed6\u73af\u5883\u4e2d\u4e5f\u4e0d\u5360\u4f18\uff0c\u8d28\u7591\u5176\u9002\u7528\u6027\u3002"}}
{"id": "2507.10208", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.10208", "abs": "https://arxiv.org/abs/2507.10208", "authors": ["Hamzah Ziadeh", "Hendrik Knoche"], "title": "Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks", "comment": null, "summary": "Research into explainable artificial intelligence (XAI) for data analysis\ntasks suffer from a large number of contradictions and lack of concrete design\nrecommendations stemming from gaps in understanding the tasks that require AI\nassistance. In this paper, we drew on multiple fields such as visual analytics,\ncognition, and dashboard design to propose a method for categorising and\ncomparing XAI studies under three dimensions: what, why, and who. We identified\nthe main problems as: inadequate descriptions of tasks, context-free studies,\nand insufficient testing with target users. We propose that studies should\nspecifically report on their users' domain, AI, and data analysis expertise to\nillustrate the generalisability of their findings. We also propose study\nguidelines for designing and reporting XAI tasks to improve the XAI community's\nability to parse the rapidly growing field. We hope that our contribution can\nhelp researchers and designers better identify which studies are most relevant\nto their work, what gaps exist in the research, and how to handle contradictory\nresults regarding XAI design.", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u548c\u6bd4\u8f83\u53ef\u89e3\u91ca\u4eba\u5de5\u667a\u80fd\uff08XAI\uff09\u7814\u7a76\u7684\u65b9\u6cd5\uff0c\u57fa\u4e8e\u4e09\u4e2a\u7ef4\u5ea6\uff1a\u4ec0\u4e48\u3001\u4e3a\u4ec0\u4e48\u548c\u8c01\uff0c\u65e8\u5728\u89e3\u51b3\u4efb\u52a1\u63cf\u8ff0\u4e0d\u8db3\u3001\u8131\u79bb\u4e0a\u4e0b\u6587\u7814\u7a76\u548c\u76ee\u6807\u7528\u6237\u6d4b\u8bd5\u4e0d\u8db3\u7b49\u95ee\u9898\u3002", "motivation": "\u5f53\u524dXAI\u7814\u7a76\u5b58\u5728\u5927\u91cf\u77db\u76fe\u4e14\u7f3a\u4e4f\u5177\u4f53\u8bbe\u8ba1\u5efa\u8bae\uff0c\u4e3b\u8981\u6e90\u4e8e\u5bf9\u9700\u8981AI\u8f85\u52a9\u7684\u4efb\u52a1\u7406\u89e3\u4e0d\u8db3\u3002", "method": "\u501f\u9274\u89c6\u89c9\u5206\u6790\u3001\u8ba4\u77e5\u79d1\u5b66\u548c\u4eea\u8868\u677f\u8bbe\u8ba1\u7b49\u591a\u4e2a\u9886\u57df\uff0c\u63d0\u51fa\u4e86\u4e00\u79cd\u5206\u7c7b\u548c\u6bd4\u8f83XAI\u7814\u7a76\u7684\u65b9\u6cd5\u3002", "result": "\u8bc6\u522b\u51fa\u4e3b\u8981\u95ee\u9898\u5305\u62ec\u4efb\u52a1\u63cf\u8ff0\u4e0d\u8db3\u3001\u8131\u79bb\u4e0a\u4e0b\u6587\u7814\u7a76\u548c\u76ee\u6807\u7528\u6237\u6d4b\u8bd5\u4e0d\u8db3\uff0c\u5e76\u63d0\u51fa\u7814\u7a76\u5e94\u660e\u786e\u62a5\u544a\u7528\u6237\u7684\u9886\u57df\u3001AI\u548c\u6570\u636e\u5206\u6790\u4e13\u4e1a\u77e5\u8bc6\u3002", "conclusion": "\u8bba\u6587\u4e3aXAI\u7814\u7a76\u63d0\u4f9b\u4e86\u8bbe\u8ba1\u548c\u62a5\u544a\u6307\u5357\uff0c\u5e2e\u52a9\u7814\u7a76\u8005\u66f4\u597d\u5730\u8bc6\u522b\u76f8\u5173\u7814\u7a76\u3001\u586b\u8865\u7814\u7a76\u7a7a\u767d\u5e76\u5904\u7406\u8bbe\u8ba1\u77db\u76fe\u3002"}}
{"id": "2507.10281", "categories": ["cs.AI", "cs.DB"], "pdf": "https://arxiv.org/pdf/2507.10281", "abs": "https://arxiv.org/abs/2507.10281", "authors": ["Jiaming Tian", "Liyao Li", "Wentao Ye", "Haobo Wang", "Lingxin Wang", "Lihua Yu", "Zujie Ren", "Gang Chen", "Junbo Zhao"], "title": "Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence", "comment": null, "summary": "Tables are fundamental in domains such as finance, healthcare, and public\nadministration, yet real-world table tasks often involve noise, structural\nheterogeneity, and semantic complexity--issues underexplored in existing\nresearch that primarily targets clean academic datasets. This survey focuses on\nLLM-based Table Agents, which aim to automate table-centric workflows by\nintegrating preprocessing, reasoning, and domain adaptation. We define five\ncore competencies--C1: Table Structure Understanding, C2: Table and Query\nSemantic Understanding, C3: Table Retrieval and Compression, C4: Executable\nReasoning with Traceability, and C5: Cross-Domain Generalization--to analyze\nand compare current approaches. In addition, a detailed examination of the\nText-to-SQL Agent reveals a performance gap between academic benchmarks and\nreal-world scenarios, especially for open-source models. Finally, we provide\nactionable insights to improve the robustness, generalization, and efficiency\nof LLM-based Table Agents in practical settings.", "AI": {"tldr": "\u8be5\u8bba\u6587\u7efc\u8ff0\u4e86\u57fa\u4e8eLLM\u7684\u8868\u683c\u4ee3\u7406\uff0c\u65e8\u5728\u901a\u8fc7\u6574\u5408\u9884\u5904\u7406\u3001\u63a8\u7406\u548c\u9886\u57df\u9002\u5e94\u6765\u81ea\u52a8\u5316\u8868\u683c\u4efb\u52a1\uff0c\u5b9a\u4e49\u4e86\u4e94\u4e2a\u6838\u5fc3\u80fd\u529b\uff0c\u5e76\u5206\u6790\u4e86\u5f53\u524d\u65b9\u6cd5\u7684\u5dee\u8ddd\u3002", "motivation": "\u73b0\u5b9e\u4e2d\u7684\u8868\u683c\u4efb\u52a1\u5e38\u6d89\u53ca\u566a\u58f0\u3001\u7ed3\u6784\u5f02\u8d28\u6027\u548c\u8bed\u4e49\u590d\u6742\u6027\uff0c\u800c\u73b0\u6709\u7814\u7a76\u591a\u9488\u5bf9\u5e72\u51c0\u7684\u5b66\u672f\u6570\u636e\u96c6\uff0c\u56e0\u6b64\u9700\u8981\u63a2\u7d22\u66f4\u5b9e\u7528\u7684\u89e3\u51b3\u65b9\u6848\u3002", "method": "\u5b9a\u4e49\u4e86\u4e94\u4e2a\u6838\u5fc3\u80fd\u529b\uff08C1-C5\uff09\u6765\u5206\u6790\u6bd4\u8f83\u73b0\u6709\u65b9\u6cd5\uff0c\u5e76\u4ee5Text-to-SQL\u4ee3\u7406\u4e3a\u4f8b\u8be6\u7ec6\u7814\u7a76\u4e86\u6027\u80fd\u5dee\u8ddd\u3002", "result": "\u53d1\u73b0\u5f00\u6e90\u6a21\u578b\u5728\u5b66\u672f\u57fa\u51c6\u548c\u5b9e\u9645\u573a\u666f\u95f4\u5b58\u5728\u6027\u80fd\u5dee\u8ddd\uff0c\u5e76\u63d0\u51fa\u4e86\u6539\u8fdb\u5efa\u8bae\u3002", "conclusion": "\u4e3a\u63d0\u9ad8LLM\u8868\u683c\u4ee3\u7406\u7684\u9c81\u68d2\u6027\u3001\u6cdb\u5316\u6027\u548c\u6548\u7387\u63d0\u4f9b\u4e86\u5b9e\u7528\u5efa\u8bae\u3002"}}
{"id": "2507.10397", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10397", "abs": "https://arxiv.org/abs/2507.10397", "authors": ["Alessandra M. M. M. Gouv\u00eaa", "Nuno Paulos", "Eduardo Uchoa e Mari\u00e1 C. V. Nascimento"], "title": "Instance space analysis of the capacitated vehicle routing problem", "comment": null, "summary": "This paper seeks to advance CVRP research by addressing the challenge of\nunderstanding the nuanced relationships between instance characteristics and\nmetaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a\nvaluable tool that allows for a new perspective on the field. By combining the\nISA methodology with a dataset from the DIMACS 12th Implementation Challenge on\nVehicle Routing, our research enabled the identification of 23 relevant\ninstance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,\nwhich employ dimensionality reduction and machine learning methods, allowed us\nto create a two-dimensional projection of the instance space to understand how\nthe structure of instances affect the behavior of MHs. A key contribution of\nour work is that we provide a projection matrix, which makes it straightforward\nto incorporate new instances into this analysis and allows for a new method for\ninstance analysis in the CVRP field.", "AI": {"tldr": "\u672c\u6587\u901a\u8fc7\u5b9e\u4f8b\u7a7a\u95f4\u5206\u6790\uff08ISA\uff09\u65b9\u6cd5\uff0c\u7ed3\u5408DIMACS\u6570\u636e\u96c6\uff0c\u8bc6\u522b\u4e8623\u4e2a\u76f8\u5173\u5b9e\u4f8b\u7279\u5f81\uff0c\u5e76\u5229\u7528\u964d\u7ef4\u548c\u673a\u5668\u5b66\u4e60\u65b9\u6cd5\uff0c\u63ed\u793a\u4e86\u5b9e\u4f8b\u7ed3\u6784\u5bf9\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6027\u80fd\u7684\u5f71\u54cd\u3002", "motivation": "\u89e3\u51b3CVRP\u7814\u7a76\u4e2d\u5b9e\u4f8b\u7279\u5f81\u4e0e\u5143\u542f\u53d1\u5f0f\u7b97\u6cd5\u6027\u80fd\u4e4b\u95f4\u590d\u6742\u5173\u7cfb\u7684\u7406\u89e3\u95ee\u9898\u3002", "method": "\u7ed3\u5408ISA\u65b9\u6cd5\u548cDIMACS\u6570\u636e\u96c6\uff0c\u901a\u8fc7PRELIM\u3001SIFTED\u548cPILOT\u9636\u6bb5\u8fdb\u884c\u964d\u7ef4\u548c\u673a\u5668\u5b66\u4e60\u5206\u6790\u3002", "result": "\u751f\u6210\u4e86\u4e00\u4e2a\u4e8c\u7ef4\u5b9e\u4f8b\u7a7a\u95f4\u6295\u5f71\uff0c\u5e76\u63d0\u4f9b\u4e86\u6295\u5f71\u77e9\u9635\uff0c\u4fbf\u4e8e\u65b0\u5b9e\u4f8b\u7684\u7eb3\u5165\u5206\u6790\u3002", "conclusion": "\u4e3aCVRP\u9886\u57df\u7684\u5b9e\u4f8b\u5206\u6790\u63d0\u4f9b\u4e86\u4e00\u79cd\u65b0\u65b9\u6cd5\uff0c\u5e76\u63ed\u793a\u4e86\u5b9e\u4f8b\u7ed3\u6784\u5bf9\u7b97\u6cd5\u884c\u4e3a\u7684\u5f71\u54cd\u3002"}}
{"id": "2507.10421", "categories": ["cs.AI", "cs.ET", "cs.IR", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.10421", "abs": "https://arxiv.org/abs/2507.10421", "authors": ["Meriem Zerkouk", "Miloud Mihoubi", "Belkacem Chikhaoui"], "title": "SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning", "comment": "International Conference on Education and New Learning Technologies\n  (2025)", "summary": "School dropout is a serious problem in distance learning, where early\ndetection is crucial for effective intervention and student perseverance.\nPredicting student dropout using available educational data is a widely\nresearched topic in learning analytics. Our partner's distance learning\nplatform highlights the importance of integrating diverse data sources,\nincluding socio-demographic data, behavioral data, and sentiment analysis, to\naccurately predict dropout risks. In this paper, we introduce a novel model\nthat combines sentiment analysis of student comments using the Bidirectional\nEncoder Representations from Transformers (BERT) model with socio-demographic\nand behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We\nfine-tuned BERT on student comments to capture nuanced sentiments, which were\nthen merged with key features selected using feature importance techniques in\nXGBoost. Our model was tested on unseen data from the next academic year,\nachieving an accuracy of 84\\%, compared to 82\\% for the baseline model.\nAdditionally, the model demonstrated superior performance in other metrics,\nsuch as precision and F1-score. The proposed method could be a vital tool in\ndeveloping personalized strategies to reduce dropout rates and encourage\nstudent perseverance", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u7ed3\u5408BERT\u60c5\u611f\u5206\u6790\u548cXGBoost\u7279\u5f81\u9009\u62e9\u7684\u65b0\u6a21\u578b\uff0c\u7528\u4e8e\u9884\u6d4b\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u5b66\u751f\u8f8d\u5b66\u98ce\u9669\uff0c\u51c6\u786e\u7387\u8fbe84%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u3002", "motivation": "\u8fdc\u7a0b\u5b66\u4e60\u4e2d\u7684\u8f8d\u5b66\u95ee\u9898\u4e25\u91cd\uff0c\u65e9\u671f\u9884\u6d4b\u5bf9\u5e72\u9884\u548c\u5b66\u751f\u575a\u6301\u81f3\u5173\u91cd\u8981\u3002\u73b0\u6709\u7814\u7a76\u5f3a\u8c03\u6574\u5408\u591a\u6837\u5316\u6570\u636e\u6e90\u4ee5\u63d0\u9ad8\u9884\u6d4b\u51c6\u786e\u6027\u3002", "method": "\u7ed3\u5408BERT\u5bf9\u5b66\u751f\u8bc4\u8bba\u8fdb\u884c\u60c5\u611f\u5206\u6790\uff0c\u4e0eXGBoost\u5904\u7406\u7684\u793e\u4f1a\u4eba\u53e3\u548c\u884c\u4e3a\u6570\u636e\u878d\u5408\uff0c\u901a\u8fc7\u7279\u5f81\u91cd\u8981\u6027\u9009\u62e9\u5173\u952e\u7279\u5f81\u3002", "result": "\u6a21\u578b\u5728\u672a\u89c1\u6570\u636e\u4e0a\u51c6\u786e\u7387\u8fbe84%\uff0c\u4f18\u4e8e\u57fa\u7ebf\u6a21\u578b\u768482%\uff0c\u4e14\u5728\u7cbe\u786e\u7387\u548cF1\u5206\u6570\u7b49\u6307\u6807\u4e0a\u8868\u73b0\u66f4\u4f18\u3002", "conclusion": "\u8be5\u65b9\u6cd5\u4e3a\u5f00\u53d1\u4e2a\u6027\u5316\u7b56\u7565\u4ee5\u51cf\u5c11\u8f8d\u5b66\u7387\u548c\u9f13\u52b1\u5b66\u751f\u575a\u6301\u63d0\u4f9b\u4e86\u91cd\u8981\u5de5\u5177\u3002"}}
{"id": "2507.10446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.10446", "abs": "https://arxiv.org/abs/2507.10446", "authors": ["Sudarshan Babu"], "title": "Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures", "comment": "arXiv admin note: text overlap with arXiv:2310.17075", "summary": "The ability to transfer knowledge from prior experiences to novel tasks\nstands as a pivotal capability of intelligent agents, including both humans and\ncomputational models. This principle forms the basis of transfer learning,\nwhere large pre-trained neural networks are fine-tuned to adapt to downstream\ntasks. Transfer learning has demonstrated tremendous success, both in terms of\ntask adaptation speed and performance. However there are several domains where,\ndue to lack of data, training such large pre-trained models or foundational\nmodels is not a possibility - computational chemistry, computational\nimmunology, and medical imaging are examples. To address these challenges, our\nwork focuses on designing architectures to enable efficient acquisition of\npriors when large amounts of data are unavailable. In particular, we\ndemonstrate that we can use neural memory to enable adaptation on\nnon-stationary distributions with only a few samples. Then we demonstrate that\nour hypernetwork designs (a network that generates another network) can acquire\nmore generalizable priors than standard networks when trained with Model\nAgnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene\ngeneration, demonstrating that they can acquire priors efficiently on just a\nhandful of training scenes, thereby leading to faster text-to-3D generation. We\nthen extend our hypernetwork framework to perform 3D segmentation on novel\nscenes with limited data by efficiently transferring priors from earlier viewed\nscenes. Finally, we repurpose an existing molecular generative method as a\npre-training framework that facilitates improved molecular property prediction,\naddressing critical challenges in computational immunology", "AI": {"tldr": "\u8bba\u6587\u63d0\u51fa\u4e86\u4e00\u79cd\u5229\u7528\u795e\u7ecf\u8bb0\u5fc6\u548c\u8d85\u7f51\u7edc\u8bbe\u8ba1\u7684\u65b9\u6cd5\uff0c\u89e3\u51b3\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\uff08\u5982\u8ba1\u7b97\u5316\u5b66\u3001\u533b\u5b66\u6210\u50cf\uff09\u4e2d\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u95ee\u9898\uff0c\u5e76\u57283D\u573a\u666f\u751f\u6210\u548c\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u4e2d\u9a8c\u8bc1\u4e86\u5176\u6709\u6548\u6027\u3002", "motivation": "\u5728\u6570\u636e\u7a00\u7f3a\u7684\u9886\u57df\uff08\u5982\u8ba1\u7b97\u5316\u5b66\u3001\u8ba1\u7b97\u514d\u75ab\u5b66\u548c\u533b\u5b66\u6210\u50cf\uff09\uff0c\u8bad\u7ec3\u5927\u578b\u9884\u8bad\u7ec3\u6a21\u578b\u6216\u57fa\u7840\u6a21\u578b\u4e0d\u53ef\u884c\uff0c\u56e0\u6b64\u9700\u8981\u8bbe\u8ba1\u9ad8\u6548\u83b7\u53d6\u5148\u9a8c\u77e5\u8bc6\u7684\u67b6\u6784\u3002", "method": "\u4f7f\u7528\u795e\u7ecf\u8bb0\u5fc6\u9002\u5e94\u975e\u5e73\u7a33\u5206\u5e03\uff0c\u8bbe\u8ba1\u8d85\u7f51\u7edc\u7ed3\u5408MAML\u83b7\u53d6\u66f4\u901a\u7528\u7684\u5148\u9a8c\uff0c\u5e76\u5c06\u5176\u5e94\u7528\u4e8e3D\u573a\u666f\u751f\u6210\u548c\u5206\u5272\u4ee5\u53ca\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u3002", "result": "\u65b9\u6cd5\u5728\u5c11\u91cf\u6837\u672c\u4e0b\u5b9e\u73b0\u4e86\u9ad8\u6548\u5148\u9a8c\u83b7\u53d6\uff0c\u63d0\u5347\u4e863D\u573a\u666f\u751f\u6210\u548c\u5206\u5272\u7684\u6548\u7387\uff0c\u5e76\u6539\u8fdb\u4e86\u5206\u5b50\u5c5e\u6027\u9884\u6d4b\u3002", "conclusion": "\u63d0\u51fa\u7684\u67b6\u6784\u5728\u6570\u636e\u7a00\u7f3a\u9886\u57df\u5c55\u793a\u4e86\u9ad8\u6548\u5148\u9a8c\u83b7\u53d6\u7684\u6f5c\u529b\uff0c\u4e3a\u76f8\u5173\u5e94\u7528\u63d0\u4f9b\u4e86\u65b0\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
{"id": "2507.10522", "categories": ["cs.AI", "cs.CL", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.10522", "abs": "https://arxiv.org/abs/2507.10522", "authors": ["Jennifer D'Souza", "Endres Keno Sander", "Andrei Aioanei"], "title": "DeepResearch$^{\\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology", "comment": "12 pages, 3 figures", "summary": "We introduce DeepResearch$^{\\text{Eco}}$, a novel agentic LLM-based system\nfor automated scientific synthesis that supports recursive, depth- and\nbreadth-controlled exploration of original research questions -- enhancing\nsearch diversity and nuance in the retrieval of relevant scientific literature.\nUnlike conventional retrieval-augmented generation pipelines, DeepResearch\nenables user-controllable synthesis with transparent reasoning and\nparameter-driven configurability, facilitating high-throughput integration of\ndomain-specific evidence while maintaining analytical rigor. Applied to 49\necological research questions, DeepResearch achieves up to a 21-fold increase\nin source integration and a 14.9-fold rise in sources integrated per 1,000\nwords. High-parameter settings yield expert-level analytical depth and\ncontextual diversity.\n  Source code available at: https://github.com/sciknoworg/deep-research.", "AI": {"tldr": "DeepResearch$^{\\text{Eco}}$\u662f\u4e00\u4e2a\u57fa\u4e8eLLM\u7684\u81ea\u52a8\u5316\u79d1\u5b66\u5408\u6210\u7cfb\u7edf\uff0c\u652f\u6301\u9012\u5f52\u3001\u6df1\u5ea6\u548c\u5e7f\u5ea6\u53ef\u63a7\u7684\u63a2\u7d22\uff0c\u663e\u8457\u63d0\u5347\u6587\u732e\u68c0\u7d22\u7684\u591a\u6837\u6027\u548c\u7ec6\u81f4\u5ea6\u3002", "motivation": "\u4f20\u7edf\u68c0\u7d22\u589e\u5f3a\u751f\u6210\u65b9\u6cd5\u7f3a\u4e4f\u7528\u6237\u53ef\u63a7\u6027\u548c\u900f\u660e\u5ea6\uff0cDeepResearch\u65e8\u5728\u89e3\u51b3\u8fd9\u4e00\u95ee\u9898\uff0c\u652f\u6301\u9ad8\u6548\u3001\u4e25\u8c28\u7684\u9886\u57df\u8bc1\u636e\u6574\u5408\u3002", "method": "\u901a\u8fc7\u7528\u6237\u53ef\u63a7\u5236\u7684\u5408\u6210\u3001\u900f\u660e\u63a8\u7406\u548c\u53c2\u6570\u9a71\u52a8\u914d\u7f6e\uff0c\u5b9e\u73b0\u9ad8\u541e\u5410\u91cf\u7684\u9886\u57df\u8bc1\u636e\u96c6\u6210\u3002", "result": "\u572849\u4e2a\u751f\u6001\u7814\u7a76\u95ee\u9898\u4e2d\uff0cDeepResearch\u5b9e\u73b0\u4e8621\u500d\u7684\u6e90\u6574\u5408\u63d0\u5347\u548c14.9\u500d\u7684\u6bcf\u5343\u5b57\u6e90\u6574\u5408\u589e\u957f\uff0c\u9ad8\u53c2\u6570\u8bbe\u7f6e\u4e0b\u8fbe\u5230\u4e13\u5bb6\u7ea7\u5206\u6790\u6df1\u5ea6\u3002", "conclusion": "DeepResearch$^{\\text{Eco}}$\u5728\u79d1\u5b66\u6587\u732e\u5408\u6210\u4e2d\u8868\u73b0\u51fa\u8272\uff0c\u63d0\u4f9b\u4e86\u9ad8\u6548\u3001\u53ef\u63a7\u7684\u89e3\u51b3\u65b9\u6848\u3002"}}
