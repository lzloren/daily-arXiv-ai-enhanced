<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 19]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning](https://arxiv.org/abs/2507.00008)
*Hang Wu,Hongkai Chen,Yujun Cai,Chang Liu,Qingwen Ye,Ming-Hsuan Yang,Yiwei Wang*

Main category: cs.AI

TL;DR: DiMo-GUI是一个无需训练的GUI自然语言查询框架，通过动态视觉定位和模态感知优化解决GUI中的视觉多样性和语言模糊性问题。


<details>
  <summary>Details</summary>
Motivation: GUI中的视觉元素多样、空间杂乱以及语言模糊性使得自然语言查询的定位具有挑战性。

Method: 将GUI拆分为文本元素和图标元素，利用通用视觉语言模型独立处理各模态，并通过动态聚焦候选区域逐步细化定位结果。

Result: 在标准GUI定位基准测试中表现优于基线方法，验证了模态分离与区域聚焦推理的有效性。

Conclusion: DiMo-GUI通过模态分离和动态区域聚焦，无需额外训练即可显著提升GUI定位性能。

Abstract: Grounding natural language queries in graphical user interfaces (GUIs) poses
unique challenges due to the diversity of visual elements, spatial clutter, and
the ambiguity of language. In this paper, we introduce DiMo-GUI, a
training-free framework for GUI grounding that leverages two core strategies:
dynamic visual grounding and modality-aware optimization. Instead of treating
the GUI as a monolithic image, our method splits the input into textual
elements and iconic elements, allowing the model to reason over each modality
independently using general-purpose vision-language models. When predictions
are ambiguous or incorrect, DiMo-GUI dynamically focuses attention by
generating candidate focal regions centered on the model's initial predictions
and incrementally zooms into subregions to refine the grounding result. This
hierarchical refinement process helps disambiguate visually crowded layouts
without the need for additional training or annotations. We evaluate our
approach on standard GUI grounding benchmarks and demonstrate consistent
improvements over baseline inference pipelines, highlighting the effectiveness
of combining modality separation with region-focused reasoning.

</details>


### [2] [TalentMine: LLM-Based Extraction and Question-Answering from Multimodal Talent Tables](https://arxiv.org/abs/2507.00041)
*Varun Mannam,Fang Wang,Chaochun Liu,Xin Chen*

Main category: cs.AI

TL;DR: 论文提出TalentMine框架，通过LLM增强的语义表表示解决传统表格提取方法在语义关系理解上的不足，显著提升人才管理文档的查询准确率。


<details>
  <summary>Details</summary>
Motivation: 传统表格提取方法在处理复杂表格时丢失语义关系，导致信息检索和决策支持效果不佳，亟需改进。

Method: 引入TalentMine框架，结合多模态推理生成语义丰富的表格表示，替代传统的CSV或文本线性化方法。

Result: 实验显示TalentMine在查询任务中达到100%准确率，远超AWS Textract（0%）和其视觉问答能力（40%）。

Conclusion: TalentMine通过保留表格结构和语义关系，显著提升人才管理系统的信息检索性能，为端到端系统提供高效解决方案。

Abstract: In talent management systems, critical information often resides in complex
tabular formats, presenting significant retrieval challenges for conventional
language models. These challenges are pronounced when processing Talent
documentation that requires precise interpretation of tabular relationships for
accurate information retrieval and downstream decision-making. Current table
extraction methods struggle with semantic understanding, resulting in poor
performance when integrated into retrieval-augmented chat applications. This
paper identifies a key bottleneck - while structural table information can be
extracted, the semantic relationships between tabular elements are lost,
causing downstream query failures. To address this, we introduce TalentMine, a
novel LLM-enhanced framework that transforms extracted tables into semantically
enriched representations. Unlike conventional approaches relying on CSV or text
linearization, our method employs specialized multimodal reasoning to preserve
both structural and semantic dimensions of tabular data. Experimental
evaluation across employee benefits document collections demonstrates
TalentMine's superior performance, achieving 100% accuracy in query answering
tasks compared to 0% for standard AWS Textract extraction and 40% for AWS
Textract Visual Q&A capabilities. Our comparative analysis also reveals that
the Claude v3 Haiku model achieves optimal performance for talent management
applications. The key contributions of this work include (1) a systematic
analysis of semantic information loss in current table extraction pipelines,
(2) a novel LLM-based method for semantically enriched table representation,
(3) an efficient integration framework for retrieval-augmented systems as
end-to-end systems, and (4) comprehensive benchmarks on talent analytics tasks
showing substantial improvements across multiple categories.

</details>


### [3] [A collaborative digital twin built on FAIR data and compute infrastructure](https://arxiv.org/abs/2507.00048)
*Thomas M. Deucher,Juan C. Verduzco,Michael Titus,Alejandro Strachan*

Main category: cs.AI

TL;DR: 论文提出了一种基于FAIR数据基础设施的分布式自驱动实验室（SDL）框架，结合机器学习和自动化实验，加速科学和工程中的发现与优化任务。


<details>
  <summary>Details</summary>
Motivation: 通过整合FAIR数据基础设施和SDL，促进地理分散的研究者协作，共享数据并利用机器学习模型进行优化。

Method: 利用nanoHUB服务实现在线模拟和FAIR数据管理，研究者通过简单界面提交数据，系统自动处理并更新机器学习模型。

Result: 开发了一个通用的工具框架，适用于多种优化问题，并通过食品染料配方的案例验证了其有效性。

Conclusion: 该框架为科学和工程中的优化任务提供了高效、协作的解决方案，并具有广泛的应用潜力。

Abstract: The integration of machine learning with automated experimentation in
self-driving laboratories (SDL) offers a powerful approach to accelerate
discovery and optimization tasks in science and engineering applications. When
supported by findable, accessible, interoperable, and reusable (FAIR) data
infrastructure, SDLs with overlapping interests can collaborate more
effectively. This work presents a distributed SDL implementation built on
nanoHUB services for online simulation and FAIR data management. In this
framework, geographically dispersed collaborators conducting independent
optimization tasks contribute raw experimental data to a shared central
database. These researchers can then benefit from analysis tools and machine
learning models that automatically update as additional data become available.
New data points are submitted through a simple web interface and automatically
processed using a nanoHUB Sim2L, which extracts derived quantities and indexes
all inputs and outputs in a FAIR data repository called ResultsDB. A separate
nanoHUB workflow enables sequential optimization using active learning, where
researchers define the optimization objective, and machine learning models are
trained on-the-fly with all existing data, guiding the selection of future
experiments. Inspired by the concept of ``frugal twin", the optimization task
seeks to find the optimal recipe to combine food dyes to achieve the desired
target color. With easily accessible and inexpensive materials, researchers and
students can set up their own experiments, share data with collaborators, and
explore the combination of FAIR data, predictive ML models, and sequential
optimization. The tools introduced are generally applicable and can easily be
extended to other optimization problems.

</details>


### [4] [SEZ-HARN: Self-Explainable Zero-shot Human Activity Recognition Network](https://arxiv.org/abs/2507.00050)
*Devin Y. De Silva,Sandareka Wickramanayake,Dulani Meedeniya,Sanka Rasnayaka*

Main category: cs.AI

TL;DR: SEZ-HARN是一种新型的零样本人类活动识别模型，能够识别未训练过的活动并通过骨架视频解释其决策过程，性能接近现有黑盒模型。


<details>
  <summary>Details</summary>
Motivation: 解决现有IMU传感器数据在HAR中缺乏全面数据集和模型透明度的问题。

Method: 提出自解释的零样本HAR模型SEZ-HARN，生成骨架视频以解释决策。

Result: 在四个基准数据集上表现接近最佳黑盒模型，同时提供可理解的解释。

Conclusion: SEZ-HARN在保持高精度的同时提升了模型透明度，适用于实际应用。

Abstract: Human Activity Recognition (HAR), which uses data from Inertial Measurement
Unit (IMU) sensors, has many practical applications in healthcare and assisted
living environments. However, its use in real-world scenarios has been limited
by the lack of comprehensive IMU-based HAR datasets that cover a wide range of
activities and the lack of transparency in existing HAR models. Zero-shot HAR
(ZS-HAR) overcomes the data limitations, but current models struggle to explain
their decisions, making them less transparent. This paper introduces a novel
IMU-based ZS-HAR model called the Self-Explainable Zero-shot Human Activity
Recognition Network (SEZ-HARN). It can recognize activities not encountered
during training and provide skeleton videos to explain its decision-making
process. We evaluate the effectiveness of the proposed SEZ-HARN on four
benchmark datasets PAMAP2, DaLiAc, HTD-MHAD and MHealth and compare its
performance against three state-of-the-art black-box ZS-HAR models. The
experiment results demonstrate that SEZ-HARN produces realistic and
understandable explanations while achieving competitive Zero-shot recognition
accuracy. SEZ-HARN achieves a Zero-shot prediction accuracy within 3\% of the
best-performing black-box model on PAMAP2 while maintaining comparable
performance on the other three datasets.

</details>


### [5] [Enhancing Reasoning Capabilities in SLMs with Reward Guided Dataset Distillation](https://arxiv.org/abs/2507.00054)
*Shreyansh Padarha*

Main category: cs.AI

TL;DR: AdvDistill是一种基于奖励的数据集蒸馏框架，通过多代教师模型响应和规则验证器分配奖励，显著提升小语言模型在数学和复杂推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 传统知识蒸馏方法中学生模型仅模仿教师模型的分布内响应，限制了其泛化能力，尤其在推理任务中表现不佳且计算成本高。

Method: AdvDistill利用教师模型对每个提示的多代响应，并通过规则验证器分配奖励，将这些奖励作为训练学生模型的权重。

Result: AdvDistill显著提升了学生模型在数学和复杂推理任务中的性能。

Conclusion: 引入奖励机制的数据集蒸馏过程有效且有益，能够显著提升小语言模型的性能。

Abstract: The push to compress and impart the proficiency of Large Language Models
(LLMs) into more deployable and efficient Small Language Models (SLMs) has
benefited from improvements in knowledge distillation (KD) techniques. These
techniques allow a smaller student model to learn from a more capable and
larger teacher model's responses. However, distillation often revolves around
the student model merely copying the teacher's in-distribution responses,
limiting its generalisability. This limitation is amplified on reasoning tasks
and can be computationally expensive. In this study, we propose AdvDistill, a
reward-guided dataset distillation framework. We utilise multiple generations
(responses) from a teacher for each prompt and assign rewards based on
rule-based verifiers. These varying and normally distributed rewards serve as
weights when training student models. Our methods and their subsequent
behavioural analysis demonstrate a significant improvement in student model
performance for mathematical and complex reasoning tasks, showcasing the
efficacy and benefits of incorporating a rewarding mechanism in dataset
distillation processes.

</details>


### [6] [VoyagerVision: Investigating the Role of Multi-modal Information for Open-ended Learning Systems](https://arxiv.org/abs/2507.00079)
*Ethan Smyth,Alessandro Suglia*

Main category: cs.AI

TL;DR: 论文提出VoyagerVision，一种多模态模型，通过视觉反馈在Minecraft中创建结构，扩展了Voyager的能力。


<details>
  <summary>Details</summary>
Motivation: 研究视觉输入如何增强模型对空间环境的理解，从而提升其任务执行能力和开放性。

Method: 基于Voyager，开发多模态模型VoyagerVision，利用截图作为视觉反馈在Minecraft中创建结构。

Result: VoyagerVision平均在50次迭代中创建2.75个独特结构，且在平坦世界中成功率为50%。

Conclusion: 视觉输入显著提升了模型的任务执行能力，为开放性研究提供了新方向。

Abstract: Open-endedness is an active field of research in the pursuit of capable
Artificial General Intelligence (AGI), allowing models to pursue tasks of their
own choosing. Simultaneously, recent advancements in Large Language Models
(LLMs) such as GPT-4o [9] have allowed such models to be capable of
interpreting image inputs. Implementations such as OMNI-EPIC [4] have made use
of such features, providing an LLM with pixel data of an agent's POV to parse
the environment and allow it to solve tasks. This paper proposes that providing
these visual inputs to a model gives it greater ability to interpret spatial
environments, and as such, can increase the number of tasks it can successfully
perform, extending its open-ended potential. To this aim, this paper proposes
VoyagerVision -- a multi-modal model capable of creating structures within
Minecraft using screenshots as a form of visual feedback, building on the
foundation of Voyager. VoyagerVision was capable of creating an average of 2.75
unique structures within fifty iterations of the system, as Voyager was
incapable of this, it is an extension in an entirely new direction.
Additionally, in a set of building unit tests VoyagerVision was successful in
half of all attempts in flat worlds, with most failures arising in more complex
structures. Project website is available at
https://esmyth-dev.github.io/VoyagerVision.github.io/

</details>


### [7] [Thinking About Thinking: SAGE-nano's Inverse Reasoning for Self-Aware Language Models](https://arxiv.org/abs/2507.00092)
*Basab Jha,Firoj Paudel,Ujjwal Puri,Zhang Yuting,Choi Donghyuk,Wang Junhao*

Main category: cs.AI

TL;DR: 论文提出了一种名为“逆向推理”的新范式，通过SAGE-nano模型实现LLM的自我解释，提升推理透明度和性能。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在复杂推理任务中决策过程不透明的问题，增强模型的可解释性和安全性。

Method: 采用逆向推理范式，通过元认知结构和注意力机制分解和解释推理链。

Result: SAGE-nano在AQUA-RAT等任务中表现出色（推理准确率74.6%，解释质量92.1%），性能接近Claude-3.5 Sonnet或GPT-4o。

Conclusion: 逆向推理框架为透明AI系统开辟了新途径，填补了AI安全、教育和科学发现中的空白。

Abstract: Large Language Models (LLMs) have demonstrated remarkable capabilities at
solving complex reasoning tasks with Chain-of-Thought (CoT) prompting, but
their decision-making processes remain somewhat blackbox. We introduce
textbfinverse reasoning, a novel paradigm enabling LLMs to decompose and
explain their own reasoning chains post-hoc. Our approach, used in SAGE-nano, a
4-billion-parameter reasoning model, employs a metacognitive structure that
reflects back via attention processes to identify major decision points and
generate explanations of reasoning choices. While typical CoT approaches are
directed towards forward reasoning generation, inverse reasoning provides
insight into why specific reasoning chains were selected over others. Through
thorough testing of logical reasoning puzzles, math problems and ethical
dilemmas from AQUA-RAT, CommonsenseQA, and customized benchmarks, we
demonstrate that SAGE-nano is at the cutting edge both on reasoning accuracy
(74.6% on AQUA-RAT) and explanation quality (92.1% human preference score) for
its task, and offers performance almost on par with models like Claude-3.5
Sonnet or GPT-4o. Our contributions are: (i) the first rigorous framework for
LLM self-reflection via inverse reasoning, (ii) a novel metalearning framework
to reverse the attention flow, (iii) comprehensive evaluation frameworks for
reasoning transparency, and (iv) evidence that increasing reasoning using
inverse reasoning improves interpretability along with reasoning performance.
Our work creates new avenues for transparent AI systems and closes significant
gaps in AI safety, education, and scientific discovery.

</details>


### [8] [BlackBoxToBlueprint: Extracting Interpretable Logic from Legacy Systems using Reinforcement Learning and Counterfactual Analysis](https://arxiv.org/abs/2507.00180)
*Vidhi Rathore*

Main category: cs.AI

TL;DR: 提出了一种基于强化学习的新方法，从黑盒遗留系统中提取可解释的决策逻辑，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如行为克隆）仅复制输入输出行为，无法捕捉底层意图，而遗留系统通常缺乏文档和理解。

Method: 使用强化学习代理探索输入空间，识别关键决策边界，并通过K-Means聚类和决策树提取人类可读规则。

Result: 在三种不同复杂度的遗留系统上验证，提取的规则准确反映了核心逻辑。

Conclusion: 该方法为遗留系统迁移中的规范和测试用例生成提供了有前景的基础。

Abstract: Modernizing legacy software systems is a critical but challenging task, often
hampered by a lack of documentation and understanding of the original system's
intricate decision logic. Traditional approaches like behavioral cloning merely
replicate input-output behavior without capturing the underlying intent. This
paper proposes a novel pipeline to automatically extract interpretable decision
logic from legacy systems treated as black boxes. The approach uses a
Reinforcement Learning (RL) agent to explore the input space and identify
critical decision boundaries by rewarding actions that cause meaningful changes
in the system's output. These counterfactual state transitions, where the
output changes, are collected and clustered using K-Means. Decision trees are
then trained on these clusters to extract human-readable rules that approximate
the system's decision logic near the identified boundaries. I demonstrated the
pipeline's effectiveness on three dummy legacy systems with varying complexity,
including threshold-based, combined-conditional, and non-linear range logic.
Results show that the RL agent successfully focuses exploration on relevant
boundary regions, and the extracted rules accurately reflect the core logic of
the underlying dummy systems, providing a promising foundation for generating
specifications and test cases during legacy migration.

</details>


### [9] [ChatGPT produces more "lazy" thinkers: Evidence of cognitive engagement decline](https://arxiv.org/abs/2507.00181)
*Georgios P. Georgiou*

Main category: cs.AI

TL;DR: 研究探讨ChatGPT对学生写作任务中认知投入的影响，发现AI辅助导致认知投入降低。


<details>
  <summary>Details</summary>
Motivation: 探究生成式AI工具（如ChatGPT）是否影响学生的深度思考和主动学习。

Method: 实验设计，随机分配学生至AI辅助组或对照组，完成写作任务并评估认知投入。

Result: ChatGPT组的认知投入显著低于对照组，表明AI可能导致认知卸载。

Conclusion: 需制定教学策略，确保学生主动反思AI生成内容，避免影响自主学习和深度认知投入。

Abstract: Despite the increasing use of large language models (LLMs) in education,
concerns have emerged about their potential to reduce deep thinking and active
learning. This study investigates the impact of generative artificial
intelligence (AI) tools, specifically ChatGPT, on the cognitive engagement of
students during academic writing tasks. The study employed an experimental
design with participants randomly assigned to either an AI-assisted (ChatGPT)
or a non-assisted (control) condition. Participants completed a structured
argumentative writing task followed by a cognitive engagement scale (CES), the
CES-AI, developed to assess mental effort, attention, deep processing, and
strategic thinking. The results revealed significantly lower cognitive
engagement scores in the ChatGPT group compared to the control group. These
findings suggest that AI assistance may lead to cognitive offloading. The study
contributes to the growing body of literature on the psychological implications
of AI in education and raises important questions about the integration of such
tools into academic practice. It calls for pedagogical strategies that promote
active, reflective engagement with AI-generated content to avoid compromising
self-regulated learning and deep cognitive involvement of students.

</details>


### [10] [Holistic Artificial Intelligence in Medicine; improved performance and explainability](https://arxiv.org/abs/2507.00205)
*Periklis Petridis,Georgios Margaritis,Vasiliki Stoumpou,Dimitris Bertsimas*

Main category: cs.AI

TL;DR: xHAIM是一个基于生成式AI的框架，通过结构化步骤提升医学AI的预测能力和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决HAIM框架在任务无关性和缺乏可解释性方面的局限性。

Method: 通过四个步骤：识别任务相关数据、生成患者摘要、改进预测模型、提供临床解释。

Result: 在HAIM-MIMIC-MM数据集上，AUC从79.9%提升至90.3%。

Conclusion: xHAIM将AI从黑盒预测器转变为可解释的决策支持系统，增强临床实用性。

Abstract: With the increasing interest in deploying Artificial Intelligence in
medicine, we previously introduced HAIM (Holistic AI in Medicine), a framework
that fuses multimodal data to solve downstream clinical tasks. However, HAIM
uses data in a task-agnostic manner and lacks explainability. To address these
limitations, we introduce xHAIM (Explainable HAIM), a novel framework
leveraging Generative AI to enhance both prediction and explainability through
four structured steps: (1) automatically identifying task-relevant patient data
across modalities, (2) generating comprehensive patient summaries, (3) using
these summaries for improved predictive modeling, and (4) providing clinical
explanations by linking predictions to patient-specific medical knowledge.
Evaluated on the HAIM-MIMIC-MM dataset, xHAIM improves average AUC from 79.9%
to 90.3% across chest pathology and operative tasks. Importantly, xHAIM
transforms AI from a black-box predictor into an explainable decision support
system, enabling clinicians to interactively trace predictions back to relevant
patient data, bridging AI advancements with clinical utility.

</details>


### [11] [Learning for routing: A guided review of recent developments and future directions](https://arxiv.org/abs/2507.00218)
*Fangting Zhou,Attila Lischka,Balazs Kulcsar,Jiaming Wu,Morteza Haghir Chehreghani,Gilbert Laporte*

Main category: cs.AI

TL;DR: 综述了机器学习在解决NP难组合优化问题（如TSP和VRP）中的应用进展，提出了一种分类方法，旨在整合传统运筹学方法与现代ML技术。


<details>
  <summary>Details</summary>
Motivation: 由于NP难问题的复杂性，传统精确算法耗时过长，而启发式方法无法保证最优性，机器学习为解决这些问题提供了新思路。

Method: 提出了一种分类法，将基于ML的路由方法分为构造型和改进型，并分析了其适用性。

Result: 综述为未来研究提供了结构化框架，并探讨了如何应对新兴VRP变体。

Conclusion: 机器学习与传统运筹学方法的结合为解决复杂路由问题提供了新的研究方向。

Abstract: This paper reviews the current progress in applying machine learning (ML)
tools to solve NP-hard combinatorial optimization problems, with a focus on
routing problems such as the traveling salesman problem (TSP) and the vehicle
routing problem (VRP). Due to the inherent complexity of these problems, exact
algorithms often require excessive computational time to find optimal
solutions, while heuristics can only provide approximate solutions without
guaranteeing optimality. With the recent success of machine learning models,
there is a growing trend in proposing and implementing diverse ML techniques to
enhance the resolution of these challenging routing problems. We propose a
taxonomy categorizing ML-based routing methods into construction-based and
improvement-based approaches, highlighting their applicability to various
problem characteristics. This review aims to integrate traditional OR methods
with state-of-the-art ML techniques, providing a structured framework to guide
future research and address emerging VRP variants.

</details>


### [12] [ASTRO: Teaching Language Models to Reason by Reflecting and Backtracking In-Context](https://arxiv.org/abs/2507.00417)
*Joongwon Kim,Anirudh Goyal,Liang Tan,Hannaneh Hajishirzi,Srinivasan Iyer,Tianlu Wang*

Main category: cs.AI

TL;DR: ASTRO框架通过自回归搜索教学，提升语言模型的推理能力，结合蒙特卡洛树搜索生成的数据集和强化学习，显著提高了Llama 3在数学问题上的表现。


<details>
  <summary>Details</summary>
Motivation: 现有开源推理模型依赖已有强推理能力的模型，而ASTRO旨在提升非推理模型（如Llama 3）的推理能力。

Method: 利用蒙特卡洛树搜索生成自然语言链式思维数据集，结合强化学习微调模型。

Result: 在MATH-500、AMC 2023和AIME 2024上分别取得16.0%、26.9%和20.0%的性能提升。

Conclusion: 搜索启发的训练方法为开放LLM提供了稳健的推理能力提升途径。

Abstract: We introduce ASTRO, the "Autoregressive Search-Taught Reasoner", a framework
for training language models to reason like search algorithms, explicitly
leveraging self-reflection, backtracking, and exploration in their outputs.
Recently, training large language models (LLMs) via reinforcement learning (RL)
has led to the advent of reasoning models with greatly enhanced reasoning
capabilities. Open-source replications of reasoning models, while successful,
build upon models that already exhibit strong reasoning capabilities along with
search behavior observed even before RL. As a result, it is yet unclear how to
boost the reasoning capabilities of other non-reasoner models including Llama
3. ASTRO teaches such models to internalize structured search behavior through
a synthetic dataset derived from Monte Carlo Tree Search (MCTS) over
mathematical problem-solving trajectories. By converting search traces into
natural language chain-of-thoughts that capture both successes and recoveries
from failure, ASTRO bootstraps models with a rich prior for exploration during
RL. We finetune our models on these search-derived traces and further improve
performance via RL with verifiable rewards. We apply ASTRO to the Llama 3
family of models and achieve absolute performance gains of 16.0% on MATH-500,
26.9% on AMC 2023, and 20.0% on AIME 2024, especially improving upon
challenging problems that require iterative correction. Our results demonstrate
that search-inspired training offers a principled way to instill robust
reasoning capabilities into open LLMs.

</details>


### [13] [Does Math Reasoning Improve General LLM Capabilities? Understanding Transferability of LLM Reasoning](https://arxiv.org/abs/2507.00432)
*Maggie Huan,Yuetai Li,Tuney Zheng,Xiaoyu Xu,Seungone Kim,Minxin Du,Radha Poovendran,Graham Neubig,Xiang Yue*

Main category: cs.AI

TL;DR: 研究发现，尽管数学推理能力在大型语言模型中显著提升，但这种能力并未广泛迁移到其他领域。强化学习调优的模型表现更好，而监督微调则导致能力遗忘。


<details>
  <summary>Details</summary>
Motivation: 探讨数学推理能力的提升是否反映了更广泛的解决问题的能力，还是仅仅是狭隘的过拟合。

Method: 评估20多个开放权重的推理调优模型，并在数学、科学问答、代理规划、编码和指令遵循等任务上进行测试。通过Qwen3-14B模型进行对照实验，比较不同调优方法的效果。

Result: 大多数在数学上成功的模型未能将能力迁移到其他领域。强化学习调优的模型表现更优，监督微调导致能力遗忘。潜在空间和标记空间分析揭示了监督微调引起的表示和输出漂移。

Conclusion: 需要重新思考后训练方法，特别是依赖监督微调蒸馏数据来提升推理模型的策略。

Abstract: Math reasoning has become the poster child of progress in large language
models (LLMs), with new models rapidly surpassing human-level performance on
benchmarks like MATH and AIME. But as math leaderboards improve week by week,
it is worth asking: do these gains reflect broader problem-solving ability or
just narrow overfitting? To answer this question, we evaluate over 20
open-weight reasoning-tuned models across a broad suite of tasks, including
math, scientific QA, agent planning, coding, and standard
instruction-following. We surprisingly find that most models that succeed in
math fail to transfer their gains to other domains. To rigorously study this
phenomenon, we conduct controlled experiments on Qwen3-14B models using
math-only data but different tuning methods. We find that reinforcement
learning (RL)-tuned models generalize well across domains, while supervised
fine-tuning (SFT)-tuned models often forget general capabilities. Latent-space
representation and token-space distribution shift analyses reveal that SFT
induces substantial representation and output drift, while RL preserves
general-domain structure. Our results suggest a need to rethink standard
post-training recipes, particularly the reliance on SFT-distilled data for
advancing reasoning models.

</details>


### [14] [Advancing Local Search in SMT-NRA with MCSAT Integration](https://arxiv.org/abs/2507.00557)
*Tianyi Ding,Haokun Li,Xinpeng Ni,Bican Xia,Tianqi Zhao*

Main category: cs.AI

TL;DR: 提出了一种名为$2d$-cell-jump的二维单元跳跃操作，扩展了SMT-NRA的局部搜索方法，并结合MCSAT框架和sample-cell投影算子提升搜索效率。


<details>
  <summary>Details</summary>
Motivation: 提升SMT-NRA（非线性实数算术的可满足性模理论）的局部搜索效率。

Method: 引入$2d$-cell-jump操作，扩展$2d$-LS框架，结合MCSAT和sample-cell投影算子，设计混合框架。

Result: 实验结果表明局部搜索性能显著提升。

Conclusion: 提出的方法有效提升了SMT-NRA的搜索效率。

Abstract: In this paper, we advance local search for Satisfiability Modulo the Theory
of Nonlinear Real Arithmetic (SMT-NRA for short). First, we introduce a
two-dimensional cell-jump move, called \emph{$2d$-cell-jump}, generalizing the
key operation, cell-jump, of the local search method for SMT-NRA. Then, we
propose an extended local search framework, named \emph{$2d$-LS} (following the
local search framework, LS, for SMT-NRA), integrating the model constructing
satisfiability calculus (MCSAT) framework to improve search efficiency. To
further improve the efficiency of MCSAT, we implement a recently proposed
technique called \emph{sample-cell projection operator} for MCSAT, which is
well suited for CDCL-style search in the real domain and helps guide the search
away from conflicting states. Finally, we design a hybrid framework for SMT-NRA
combining MCSAT, $2d$-LS and OpenCAD, to improve search efficiency through
information exchange. The experimental results demonstrate improvements in
local search performance, highlighting the effectiveness of the proposed
methods.

</details>


### [15] [Can Large Language Models Develop Strategic Reasoning? Post-training Insights from Learning Chess](https://arxiv.org/abs/2507.00726)
*Dongyoon Hwang,Hojoon Lee,Jaegul Choo,Dongmin Park,Jongho Park*

Main category: cs.AI

TL;DR: 研究探讨了通过强化学习（RL）让大语言模型（LLM）在象棋中发展策略推理能力，发现基于知识蒸馏的密集奖励优于稀疏奖励，但模型表现远低于专家水平，原因可能是预训练模型对象棋的理解不足。


<details>
  <summary>Details</summary>
Motivation: 探索LLM是否可以通过RL在象棋中发展策略推理能力，填补这一领域的研究空白。

Method: 利用象棋预训练的动作价值网络为LLM的输出动作质量提供密集奖励（知识蒸馏），并与稀疏奖励进行比较。

Result: 密集奖励通常优于稀疏奖励，但所有模型表现远低于专家水平，RL可能无法完全克服预训练模型对象棋理解的不足。

Conclusion: 预训练模型对象棋的内部理解不足是性能瓶颈，仅靠RL可能无法完全解决。

Abstract: While reinforcement learning (RL) for large language models (LLMs) has shown
promise in mathematical reasoning, strategic reasoning for LLMs using RL
remains largely unexplored. We investigate whether LLMs can develop strategic
reasoning capabilities through RL in chess. To this end, we leverage a
chess-pretrained action-value network to provide dense reward on the LLM's
output move quality, which can be seen as a form of knowledge distillation. Our
experiments show that our distillation-based dense rewards often outperform
sparse binary rewards. However, surprisingly, all models plateau far below
expert levels. We provide SFT and RL ablations on chess reasoning training and
find evidence that this limitation stems from a deficit in the pretrained
models' internal understanding of chess--a deficit which RL alone may not be
able to fully overcome.

</details>


### [16] [A Robust Algorithm for Non-IID Machine Learning Problems with Convergence Analysis](https://arxiv.org/abs/2507.00810)
*Qing Xu,Xiaohua Xuan*

Main category: cs.AI

TL;DR: 提出了一种基于非光滑优化、二次规划和迭代过程的改进数值算法，用于解决极小极大问题，并提供了收敛性证明。


<details>
  <summary>Details</summary>
Motivation: 解决极小极大问题在鲁棒优化和不平衡学习等领域的广泛应用需求。

Method: 结合非光滑优化、二次规划和迭代过程设计改进算法。

Result: 在梯度连续性和有界性等温和假设下，算法具有收敛性。

Conclusion: 该算法在多种领域具有潜在应用价值。

Abstract: In this paper, we propose an improved numerical algorithm for solving minimax
problems based on nonsmooth optimization, quadratic programming and iterative
process. We also provide a rigorous proof of convergence for our algorithm
under some mild assumptions, such as gradient continuity and boundedness. Such
an algorithm can be widely applied in various fields such as robust
optimization, imbalanced learning, etc.

</details>


### [17] [SafeMobile: Chain-level Jailbreak Detection and Automated Evaluation for Multimodal Mobile Agents](https://arxiv.org/abs/2507.00841)
*Siyuan Liang,Tianmeng Fang,Zhe Liu,Aishan Liu,Yan Xiao,Jinyuan He,Ee-Chien Chang,Xiaochun Cao*

Main category: cs.AI

TL;DR: 该论文探讨了多模态基础模型在智能代理系统中的安全风险，提出了一种结合行为序列信息的风险识别机制，并基于大语言模型设计了自动化辅助评估方案，初步验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 随着多模态基础模型在智能代理系统中的广泛应用，系统面临潜在的越狱风险，现有安全措施在复杂交互中仍有局限，缺乏高效的风险评估方法。

Method: 构建了结合行为序列信息的风险识别机制，并设计基于大语言模型的自动化辅助评估方案。

Result: 初步验证表明，该方法能提升风险行为识别能力，降低代理被越狱的概率。

Conclusion: 该研究为多模态智能代理系统的安全风险建模与防护提供了有价值的参考。

Abstract: With the wide application of multimodal foundation models in intelligent
agent systems, scenarios such as mobile device control, intelligent assistant
interaction, and multimodal task execution are gradually relying on such large
model-driven agents. However, the related systems are also increasingly exposed
to potential jailbreak risks. Attackers may induce the agents to bypass the
original behavioral constraints through specific inputs, and then trigger
certain risky and sensitive operations, such as modifying settings, executing
unauthorized commands, or impersonating user identities, which brings new
challenges to system security. Existing security measures for intelligent
agents still have limitations when facing complex interactions, especially in
detecting potentially risky behaviors across multiple rounds of conversations
or sequences of tasks. In addition, an efficient and consistent automated
methodology to assist in assessing and determining the impact of such risks is
currently lacking. This work explores the security issues surrounding mobile
multimodal agents, attempts to construct a risk discrimination mechanism by
incorporating behavioral sequence information, and designs an automated
assisted assessment scheme based on a large language model. Through preliminary
validation in several representative high-risk tasks, the results show that the
method can improve the recognition of risky behaviors to some extent and assist
in reducing the probability of agents being jailbroken. We hope that this study
can provide some valuable references for the security risk modeling and
protection of multimodal intelligent agent systems.

</details>


### [18] [Thinking Beyond Tokens: From Brain-Inspired Intelligence to Cognitive Foundations for Artificial General Intelligence and its Societal Impact](https://arxiv.org/abs/2507.00951)
*Rizwan Qureshi,Ranjan Sapkota,Abbas Shah,Amgad Muneer,Anas Zafar,Ashmal Vayani,Maged Shoman,Abdelrahman B. M. Eldaly,Kai Zhang,Ferhat Sadak,Shaina Raza,Xinqi Fan,Ravid Shwartz-Ziv,Hong Yan,Vinjia Jain,Aman Chadha,Manoj Karkee,Jia Wu,Philip Torr,Seyedali Mirjalili*

Main category: cs.AI

TL;DR: 论文探讨了机器是否能像人类一样思考、推理和行动，分析了当前AI模型的局限性，并提出了多学科融合的AGI发展路径，强调模块化推理、记忆和多智能体协调的重要性。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索如何克服当前AI模型（如GPT-4.5等）的局限性，实现真正的通用人工智能（AGI）。

Method: 方法包括跨学科综合（AI、认知神经科学、心理学等），分析通用智能的架构和认知基础，提出Agentic RAG框架和通用化策略。

Result: 结果指出，通过模块化、记忆与推理的整合以及神经符号系统等方法，可以缩小统计学习与目标导向认知之间的差距。

Conclusion: 结论强调，AGI的实现需要解决科学、技术和伦理挑战，并依赖于模块化、交互式和自我改进组件的整合。

Abstract: Can machines truly think, reason and act in domains like humans? This
enduring question continues to shape the pursuit of Artificial General
Intelligence (AGI). Despite the growing capabilities of models such as GPT-4.5,
DeepSeek, Claude 3.5 Sonnet, Phi-4, and Grok 3, which exhibit multimodal
fluency and partial reasoning, these systems remain fundamentally limited by
their reliance on token-level prediction and lack of grounded agency. This
paper offers a cross-disciplinary synthesis of AGI development, spanning
artificial intelligence, cognitive neuroscience, psychology, generative models,
and agent-based systems. We analyze the architectural and cognitive foundations
of general intelligence, highlighting the role of modular reasoning, persistent
memory, and multi-agent coordination. In particular, we emphasize the rise of
Agentic RAG frameworks that combine retrieval, planning, and dynamic tool use
to enable more adaptive behavior. We discuss generalization strategies,
including information compression, test-time adaptation, and training-free
methods, as critical pathways toward flexible, domain-agnostic intelligence.
Vision-Language Models (VLMs) are reexamined not just as perception modules but
as evolving interfaces for embodied understanding and collaborative task
completion. We also argue that true intelligence arises not from scale alone
but from the integration of memory and reasoning: an orchestration of modular,
interactive, and self-improving components where compression enables adaptive
behavior. Drawing on advances in neurosymbolic systems, reinforcement learning,
and cognitive scaffolding, we explore how recent architectures begin to bridge
the gap between statistical learning and goal-directed cognition. Finally, we
identify key scientific, technical, and ethical challenges on the path to AGI.

</details>


### [19] [Enhancing LLM Agent Safety via Causal Influence Prompting](https://arxiv.org/abs/2507.00979)
*Dongyoon Hahm,Woogyeol Jin,June Suk Choi,Sungsoo Ahn,Kimin Lee*

Main category: cs.AI

TL;DR: 论文提出了一种名为CIP的新技术，利用因果影响图（CID）来识别和减轻自主代理决策中的风险，从而提升安全性。


<details>
  <summary>Details</summary>
Motivation: 随着基于大语言模型（LLM）的自主代理在各种辅助任务中展现潜力，确保其行为安全可靠至关重要，以避免意外后果。

Method: 方法包括三个步骤：1) 基于任务规范初始化CID以描述决策过程；2) 使用CID指导代理与环境交互；3) 根据观察到的行为和结果迭代优化CID。

Result: 实验结果表明，该方法在代码执行和移动设备控制任务中显著提升了安全性。

Conclusion: CIP技术通过因果影响图有效增强了自主代理的安全性，为未来研究提供了实用框架。

Abstract: As autonomous agents powered by large language models (LLMs) continue to
demonstrate potential across various assistive tasks, ensuring their safe and
reliable behavior is crucial for preventing unintended consequences. In this
work, we introduce CIP, a novel technique that leverages causal influence
diagrams (CIDs) to identify and mitigate risks arising from agent
decision-making. CIDs provide a structured representation of cause-and-effect
relationships, enabling agents to anticipate harmful outcomes and make safer
decisions. Our approach consists of three key steps: (1) initializing a CID
based on task specifications to outline the decision-making process, (2)
guiding agent interactions with the environment using the CID, and (3)
iteratively refining the CID based on observed behaviors and outcomes.
Experimental results demonstrate that our method effectively enhances safety in
both code execution and mobile device control tasks.

</details>
