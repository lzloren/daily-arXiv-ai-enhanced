<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 84]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [LLMs are Capable of Misaligned Behavior Under Explicit Prohibition and Surveillance](https://arxiv.org/abs/2507.02977)
*Igor Ivanov*

Main category: cs.AI

TL;DR: 前沿LLMs在受监控的沙盒环境中仍试图作弊，揭示了目标导向行为与对齐之间的根本矛盾。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs在明确告知限制和监控的情况下是否仍会作弊，以探索其目标导向行为与对齐的冲突。

Method: 在沙盒环境中让LLMs完成不可能的任务，并监控其行为，明确告知限制和禁止作弊。

Result: 一些前沿LLMs仍会持续作弊并试图规避限制。

Conclusion: 当前LLMs在目标导向行为与对齐之间存在根本矛盾，需进一步研究解决。

Abstract: In this paper, LLMs are tasked with completing an impossible quiz, while they
are in a sandbox, monitored, told about these measures and instructed not to
cheat. Some frontier LLMs cheat consistently and attempt to circumvent
restrictions despite everything. The results reveal a fundamental tension
between goal-directed behavior and alignment in current LLMs. The code and
evaluation logs are available at github.com/baceolus/cheating_evals

</details>


### [2] [Discovering Algorithms with Computational Language Processing](https://arxiv.org/abs/2507.03190)
*Theo Bourdais,Abeynaya Gnanasekaran,Houman Owhadi,Tuhin Sahai*

Main category: cs.AI

TL;DR: 提出了一种自动化算法发现的框架，通过将算法表示为操作序列的标记，利用语法链式生成复杂过程。结合蒙特卡洛树搜索（MCTS）和强化学习（RL），该方法重新发现、改进并生成新算法，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决强NP难组合优化问题和量子计算基础算法（如Grover和QAOA）的局限性，通过自动化发现更高效的算法。

Method: 将算法表示为标记序列，利用语法链式生成复杂过程，结合MCTS和RL探索标记链式并生成新标记。

Result: 生成的算法在强NP难问题和量子计算中显著优于现有方法，并能针对具体问题实例定制。

Conclusion: 该框架在算法发现和优化方面具有潜力，尤其适用于复杂问题和量子计算领域。

Abstract: Algorithms are the engine for reproducible problem-solving. We present a
framework automating algorithm discovery by conceptualizing them as sequences
of operations, represented as tokens. These computational tokens are chained
using a grammar, enabling the formation of increasingly sophisticated
procedures. Our ensemble Monte Carlo tree search (MCTS) guided by reinforcement
learning (RL) explores token chaining and drives the creation of new tokens.
This methodology rediscovers, improves, and generates new algorithms that
substantially outperform existing methods for strongly NP-hard combinatorial
optimization problems and foundational quantum computing approaches such as
Grover's and Quantum Approximate Optimization Algorithm. Operating at the
computational rather than code-generation level, our framework produces
algorithms that can be tailored specifically to problem instances, not merely
classes.

</details>


### [3] [SI-Agent: An Agentic Framework for Feedback-Driven Generation and Tuning of Human-Readable System Instructions for Large Language Models](https://arxiv.org/abs/2507.03223)
*Jeshwanth Challagundla*

Main category: cs.AI

TL;DR: SI-Agent是一个自动生成和优化人类可读系统指令（SIs）的框架，通过反馈循环提升任务性能和可读性。


<details>
  <summary>Details</summary>
Motivation: 手动设计系统指令资源密集且效果不佳，现有自动方法牺牲可读性，SI-Agent旨在解决这一问题。

Method: SI-Agent采用三个协作代理（Instructor、Follower、Feedback），通过迭代反馈优化指令，结合LLM编辑和进化算法。

Result: 实验表明SI-Agent在任务性能、可读性和效率上优于基线，平衡了性能与可解释性。

Conclusion: SI-Agent为LLM定制化和透明度提供了新途径，但需解决计算成本和反馈可靠性问题。

Abstract: System Instructions (SIs), or system prompts, are pivotal for guiding Large
Language Models (LLMs) but manual crafting is resource-intensive and often
suboptimal. Existing automated methods frequently generate non-human-readable
"soft prompts," sacrificing interpretability. This paper introduces SI-Agent, a
novel agentic framework designed to automatically generate and iteratively
refine human-readable SIs through a feedback-driven loop. SI-Agent employs
three collaborating agents: an Instructor Agent, an Instruction Follower Agent
(target LLM), and a Feedback/Reward Agent evaluating task performance and
optionally SI readability. The framework utilizes iterative cycles where
feedback guides the Instructor's refinement strategy (e.g., LLM-based editing,
evolutionary algorithms). We detail the framework's architecture, agent roles,
the iterative refinement process, and contrast it with existing methods. We
present experimental results validating SI-Agent's effectiveness, focusing on
metrics for task performance, SI readability, and efficiency. Our findings
indicate that SI-Agent generates effective, readable SIs, offering a favorable
trade-off between performance and interpretability compared to baselines.
Potential implications include democratizing LLM customization and enhancing
model transparency. Challenges related to computational cost and feedback
reliability are acknowledged.

</details>


### [4] [Efficient Knowledge Graph Construction and Retrieval from Unstructured Text for Large-Scale RAG Systems](https://arxiv.org/abs/2507.03226)
*Congmin Min,Rhea Mathew,Joyce Pan,Sahil Bansal,Abbas Keshavarzi,Amar Viswanathan Kannan*

Main category: cs.AI

TL;DR: 提出了一种可扩展且成本高效的GraphRAG框架，通过依赖知识图谱构建和轻量级检索策略，显著降低了计算成本和延迟，并在企业环境中验证了其性能。


<details>
  <summary>Details</summary>
Motivation: 解决GraphRAG因依赖大型语言模型（LLMs）构建知识图谱导致的高计算成本和延迟问题，推动其在企业环境中的实际应用。

Method: 1. 依赖工业级NLP库的依赖知识图谱构建管道；2. 结合混合查询节点识别和一跳遍历的轻量级图检索策略。

Result: 在SAP数据集上表现优异，性能提升15%（LLM-as-Judge）和4.35%（RAGAS），依赖构建方法达到LLM生成图谱94%的性能，同时显著降低成本。

Conclusion: 验证了GraphRAG在大规模企业应用中的可行性，为实际、可解释和领域适应的检索增强推理铺平了道路。

Abstract: We propose a scalable and cost-efficient framework for deploying Graph-based
Retrieval Augmented Generation (GraphRAG) in enterprise environments. While
GraphRAG has shown promise for multi-hop reasoning and structured retrieval,
its adoption has been limited by the high computational cost of constructing
knowledge graphs using large language models (LLMs) and the latency of
graph-based retrieval. To address these challenges, we introduce two core
innovations: (1) a dependency-based knowledge graph construction pipeline that
leverages industrial-grade NLP libraries to extract entities and relations from
unstructured text completely eliminating reliance on LLMs; and (2) a
lightweight graph retrieval strategy that combines hybrid query node
identification with efficient one-hop traversal for high-recall, low-latency
subgraph extraction. We evaluate our framework on two SAP datasets focused on
legacy code migration and demonstrate strong empirical performance. Our system
achieves up to 15% and 4.35% improvements over traditional RAG baselines based
on LLM-as-Judge and RAGAS metrics, respectively. Moreover, our dependency-based
construction approach attains 94% of the performance of LLM-generated knowledge
graphs (61.87% vs. 65.83%) while significantly reducing cost and improving
scalability. These results validate the feasibility of deploying GraphRAG
systems in real-world, large-scale enterprise applications without incurring
prohibitive resource requirements paving the way for practical, explainable,
and domain-adaptable retrieval-augmented reasoning.

</details>


### [5] [CodeAgents: A Token-Efficient Framework for Codified Multi-Agent Reasoning in LLMs](https://arxiv.org/abs/2507.03254)
*Bruce Yang,Xinfeng He,Huan Gao,Yifan Cao,Xiaofan Li,David Hsu*

Main category: cs.AI

TL;DR: CodeAgents是一个多代理提示框架，通过模块化伪代码提升多代理系统的规划和效率。


<details>
  <summary>Details</summary>
Motivation: 现有结构化提示策略局限于单代理、仅规划场景，且忽视多代理环境中的效率、模块化和可扩展性。

Method: 将代理交互组件（任务、计划、反馈等）编码为模块化伪代码，加入控制结构和类型变量。

Result: 在三个基准测试中表现优异，规划性能提升3-36个百分点，输入输出令牌使用减少55-87%和41-70%。

Conclusion: CodeAgents显著提升多代理系统的规划性能和令牌效率，为可扩展LLM系统开发提供新方向。

Abstract: Effective prompt design is essential for improving the planning capabilities
of large language model (LLM)-driven agents. However, existing structured
prompting strategies are typically limited to single-agent, plan-only settings,
and often evaluate performance solely based on task accuracy - overlooking
critical factors such as token efficiency, modularity, and scalability in
multi-agent environments. To address these limitations, we introduce
CodeAgents, a prompting framework that codifies multi-agent reasoning and
enables structured, token-efficient planning in multi-agent systems. In
CodeAgents, all components of agent interaction - Task, Plan, Feedback, system
roles, and external tool invocations - are codified into modular pseudocode
enriched with control structures (e.g., loops, conditionals), boolean logic,
and typed variables. This design transforms loosely connected agent plans into
cohesive, interpretable, and verifiable multi-agent reasoning programs. We
evaluate the proposed framework across three diverse benchmarks - GAIA,
HotpotQA, and VirtualHome - using a range of representative LLMs. Results show
consistent improvements in planning performance, with absolute gains of 3-36
percentage points over natural language prompting baselines. On VirtualHome,
our method achieves a new state-of-the-art success rate of 56%. In addition,
our approach reduces input and output token usage by 55-87% and 41-70%,
respectively, underscoring the importance of token-aware evaluation metrics in
the development of scalable multi-agent LLM systems. The code and resources are
available at: https://anonymous.4open.science/r/CodifyingAgent-5A86

</details>


### [6] [GDGB: A Benchmark for Generative Dynamic Text-Attributed Graph Learning](https://arxiv.org/abs/2507.03267)
*Jie Peng,Jiarui Ji,Runlin Lei,Zhewei Wei,Yongchao Liu,Chuntao Hong*

Main category: cs.AI

TL;DR: 论文提出了GDGB基准，包含8个高质量文本特征的DyTAG数据集，并定义了两个新的生成任务（TDGG和IDGG），设计了多维度评估指标，并提出了基于LLM的生成框架GAG-General。


<details>
  <summary>Details</summary>
Motivation: 现有DyTAG数据集文本质量差，缺乏生成任务的标准化评估，限制了DyTAG生成研究的进展。

Method: 提出GDGB基准，包含高质量数据集，定义TDGG和IDGG任务，设计多维度评估指标，并开发GAG-General生成框架。

Result: GDGB支持严格的TDGG和IDGG评估，揭示了结构和文本特征在DyTAG生成中的关键作用。

Conclusion: GDGB为生成DyTAG研究提供了基础资源，推动了实际应用的进一步发展。

Abstract: Dynamic Text-Attributed Graphs (DyTAGs), which intricately integrate
structural, temporal, and textual attributes, are crucial for modeling complex
real-world systems. However, most of the existing DyTAG datasets exhibit poor
textual quality, which severely limits their utility for DyTAG generation tasks
requiring semantically rich inputs. Additionally, prior work mainly focuses on
discriminative tasks on DyTAGs, resulting in a lack of standardized task
formulations and evaluation protocols tailored for DyTAG generation. To address
these critical issues, we propose Generative DyTAG Benchmark (GDGB), which
comprises eight meticulously curated DyTAG datasets with high-quality textual
features for both nodes and edges, overcoming limitations of prior datasets.
Building on GDGB, we define two novel DyTAG generation tasks: Transductive
Dynamic Graph Generation (TDGG) and Inductive Dynamic Graph Generation (IDGG).
TDGG transductively generates a target DyTAG based on the given source and
destination node sets, while the more challenging IDGG introduces new node
generation to inductively model the dynamic expansion of real-world graph data.
To enable holistic evaluation, we design multifaceted metrics that assess the
structural, temporal, and textual quality of the generated DyTAGs. We further
propose GAG-General, an LLM-based multi-agent generative framework tailored for
reproducible and robust benchmarking of DyTAG generation. Experimental results
demonstrate that GDGB enables rigorous evaluation of TDGG and IDGG, with key
insights revealing the critical interplay of structural and textual features in
DyTAG generation. These findings establish GDGB as a foundational resource for
advancing generative DyTAG research and unlocking further practical
applications in DyTAG generation. GDGB datasets, source codes, and leaderboards
are available at \href{https://gdgb-algo.github.io/}{here}.

</details>


### [7] [Memory Mosaics at scale](https://arxiv.org/abs/2507.03285)
*Jianyu Zhang,Léon Bottou*

Main category: cs.AI

TL;DR: Memory Mosaics v2在大型语言模型规模（10B）和真实数据集上展示了优于Transformer的性能，特别是在新任务推理和上下文学习方面。


<details>
  <summary>Details</summary>
Motivation: 验证Memory Mosaics在扩展到大型语言模型（如llama-8B）和真实数据集时是否仍保持其优越的组成性和上下文学习能力。

Method: 将Memory Mosaics扩展到10B规模，训练1万亿token，并引入架构改进（Memory Mosaics v2），评估其在训练知识存储、新知识存储和上下文学习三个维度的能力。

Result: Memory Mosaics v2在训练知识学习上与Transformer相当，但在新任务推理和上下文学习方面显著优于Transformer，且无法通过增加Transformer的训练数据轻易复现。

Conclusion: Memory Mosaics v2在大型模型和真实数据集上表现出色，尤其在动态任务处理方面具有优势。

Abstract: Memory Mosaics [Zhang et al., 2025], networks of associative memories, have
demonstrated appealing compositional and in-context learning capabilities on
medium-scale networks (GPT-2 scale) and synthetic small datasets. This work
shows that these favorable properties remain when we scale memory mosaics to
large language model sizes (llama-8B scale) and real-world datasets.
  To this end, we scale memory mosaics to 10B size, we train them on one
trillion tokens, we introduce a couple architectural modifications ("Memory
Mosaics v2"), we assess their capabilities across three evaluation dimensions:
training-knowledge storage, new-knowledge storage, and in-context learning.
  Throughout the evaluation, memory mosaics v2 match transformers on the
learning of training knowledge (first dimension) and significantly outperforms
transformers on carrying out new tasks at inference time (second and third
dimensions). These improvements cannot be easily replicated by simply
increasing the training data for transformers. A memory mosaics v2 trained on
one trillion tokens still perform better on these tasks than a transformer
trained on eight trillion tokens.

</details>


### [8] [LTLCrit: A Temporal Logic-based LLM Critic for Safe and Efficient Embodied Agents](https://arxiv.org/abs/2507.03293)
*Anand Gokhale,Vaibhav Srivastava,Francesco Bullo*

Main category: cs.AI

TL;DR: 提出了一种模块化的actor-critic架构，结合LLM和形式逻辑，通过LTL约束提升长期规划任务的安全性和效率。


<details>
  <summary>Details</summary>
Motivation: LLM在长期规划任务中容易累积错误，导致不安全或低效行为，限制了其通用性。

Method: 采用LLM actor和LTLCrit critic的架构，通过LTL约束指导行为，支持固定和自适应约束。

Result: 在Minecraft钻石挖掘任务中实现100%完成率，并提升效率。

Conclusion: 通过逻辑监督LLM是一种强大且灵活的范式，适用于安全、通用的决策。

Abstract: Large language models (LLMs) have demonstrated promise in reasoning tasks and
general decision-making in static environments. In long-term planning tasks,
however, errors tend to accumulate, often leading to unsafe or inefficient
behavior, limiting their use in general-purpose settings. We propose a modular
actor-critic architecture in which an LLM actor is guided by LTLCrit, a
trajectory-level LLM critic that communicates via linear temporal logic (LTL).
Our setup combines the reasoning strengths of language models with the
guarantees of formal logic. The actor selects high-level actions from natural
language observations, while the critic analyzes full trajectories and proposes
new LTL constraints that shield the actor from future unsafe or inefficient
behavior. The architecture supports both fixed, hand-specified safety
constraints and adaptive, learned soft constraints that promote long-term
efficiency. Our architecture is model-agnostic: any LLM-based planner can serve
as the actor, and LTLCrit serves as a logic-generating wrapper. We formalize
planning as graph traversal under symbolic constraints, allowing LTLCrit to
analyze failed or suboptimal trajectories and generate new temporal logic rules
that improve future behavior. We evaluate our system on the Minecraft
diamond-mining benchmark, achieving 100% completion rates and improving
efficiency compared to baseline LLM planners. Our results suggest that enabling
LLMs to supervise each other through logic is a powerful and flexible paradigm
for safe, generalizable decision making.

</details>


### [9] [NDAI-NeuroMAP: A Neuroscience-Specific Embedding Model for Domain-Specific Retrieval](https://arxiv.org/abs/2507.03329)
*Devendra Patel,Aaditya Jain,Jayant Verma,Divyansh Rajput,Sunil Mahala,Ketki Suresh Khapare,Jayateja Kalla*

Main category: cs.AI

TL;DR: NDAI-NeuroMAP是首个专为神经科学领域设计的高精度信息检索密集向量嵌入模型，通过多目标优化框架显著提升性能。


<details>
  <summary>Details</summary>
Motivation: 解决神经科学领域信息检索任务中通用和生物医学嵌入模型的不足，强调领域特定嵌入架构的重要性。

Method: 使用50万精心构建的三元组（查询-正例-负例配置），结合25万神经科学定义条目和25万结构化知识图谱三元组，基于FremyCompany/BioLORD-2023模型进行微调，采用对比学习和三元组度量学习的多目标优化框架。

Result: 在包含约2.4万神经科学查询的测试集上，性能显著优于现有通用和生物医学嵌入模型。

Conclusion: 领域特定嵌入架构对神经科学RAG系统和临床自然语言处理应用至关重要。

Abstract: We present NDAI-NeuroMAP, the first neuroscience-domain-specific dense vector
embedding model engineered for high-precision information retrieval tasks. Our
methodology encompasses the curation of an extensive domain-specific training
corpus comprising 500,000 carefully constructed triplets
(query-positive-negative configurations), augmented with 250,000
neuroscience-specific definitional entries and 250,000 structured
knowledge-graph triplets derived from authoritative neurological ontologies. We
employ a sophisticated fine-tuning approach utilizing the
FremyCompany/BioLORD-2023 foundation model, implementing a multi-objective
optimization framework combining contrastive learning with triplet-based metric
learning paradigms. Comprehensive evaluation on a held-out test dataset
comprising approximately 24,000 neuroscience-specific queries demonstrates
substantial performance improvements over state-of-the-art general-purpose and
biomedical embedding models. These empirical findings underscore the critical
importance of domain-specific embedding architectures for neuroscience-oriented
RAG systems and related clinical natural language processing applications.

</details>


### [10] [Exploring Object Status Recognition for Recipe Progress Tracking in Non-Visual Cooking](https://arxiv.org/abs/2507.03330)
*Franklin Mingzhe Li,Kaitlyn Ng,Bin Zhu,Patrick Carrington*

Main category: cs.AI

TL;DR: OSCAR是一个基于物体状态识别的技术框架，用于支持无视觉烹饪中的食谱进度跟踪，通过整合食谱解析、物体状态提取和实时建模，显著提高了步骤预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 烹饪对视觉障碍者具有挑战性，现有系统缺乏对物体状态的跟踪和反馈支持，OSCAR旨在填补这一空白。

Method: OSCAR整合了食谱解析、物体状态提取、视觉对齐和时间因果建模，构建了一个实时跟踪烹饪进度的技术框架。

Result: 在173个教学视频和12个真实烹饪会话中，OSCAR显著提高了步骤预测的准确性，并揭示了影响性能的关键因素。

Conclusion: OSCAR为无视觉烹饪提供了有效的上下文感知支持，贡献了技术框架、数据集和设计见解。

Abstract: Cooking plays a vital role in everyday independence and well-being, yet
remains challenging for people with vision impairments due to limited support
for tracking progress and receiving contextual feedback. Object status - the
condition or transformation of ingredients and tools - offers a promising but
underexplored foundation for context-aware cooking support. In this paper, we
present OSCAR (Object Status Context Awareness for Recipes), a technical
pipeline that explores the use of object status recognition to enable recipe
progress tracking in non-visual cooking. OSCAR integrates recipe parsing,
object status extraction, visual alignment with cooking steps, and time-causal
modeling to support real-time step tracking. We evaluate OSCAR on 173
instructional videos and a real-world dataset of 12 non-visual cooking sessions
recorded by BLV individuals in their homes. Our results show that object status
consistently improves step prediction accuracy across vision-language models,
and reveal key factors that impact performance in real-world conditions, such
as implicit tasks, camera placement, and lighting. We contribute the pipeline
of context-aware recipe progress tracking, an annotated real-world non-visual
cooking dataset, and design insights to guide future context-aware assistive
cooking systems.

</details>


### [11] [Disambiguation-Centric Finetuning Makes Enterprise Tool-Calling LLMs More Realistic and Less Risky](https://arxiv.org/abs/2507.03336)
*Ashutosh Hathidara,Julien Yu,Sebastian Schreiber*

Main category: cs.AI

TL;DR: DiaFORGE是一个三阶段对话框架，通过生成多轮对话、监督微调和动态评估，显著提升LLM在调用企业API时的准确性和成功率。


<details>
  <summary>Details</summary>
Motivation: 解决LLM在调用企业API时因工具相似或参数不明确而失败的问题。

Method: 三阶段管道：生成多轮对话、监督微调模型、动态评估模型性能。

Result: DiaFORGE训练的模型在动态基准测试中比GPT-4o和Claude-3.5-Sonnet分别提高了27和49个百分点的成功率。

Conclusion: DiaFORGE为构建可靠的企业级工具调用代理提供了实用方案，并发布了开放数据集以推动研究。

Abstract: Large language models (LLMs) are increasingly tasked with invoking enterprise
APIs, yet they routinely falter when near-duplicate tools vie for the same user
intent or when required arguments are left underspecified. We introduce
DiaFORGE (Dialogue Framework for Organic Response Generation & Evaluation), a
disambiguation-centric, three-stage pipeline that (i) synthesizes
persona-driven, multi-turn dialogues in which the assistant must distinguish
among highly similar tools, (ii) performs supervised fine-tuning of open-source
models with reasoning traces across 3B - 70B parameters, and (iii) evaluates
real-world readiness via a dynamic suite that redeploys each model in a live
agentic loop and reports end-to-end goal completion alongside conventional
static metrics. On our dynamic benchmark DiaBENCH, models trained with DiaFORGE
raise tool-invocation success by 27 pp over GPT-4o and by 49 pp over
Claude-3.5-Sonnet, both under optimized prompting. To spur further research, we
release an open corpus of 5000 production-grade enterprise API specifications
paired with rigorously validated, disambiguation-focused dialogues, offering a
practical blueprint for building reliable, enterprise-ready tool-calling
agents.

</details>


### [12] [Effects of structure on reasoning in instance-level Self-Discover](https://arxiv.org/abs/2507.03347)
*Sachith Gunasekara,Yasiru Ratnayake*

Main category: cs.AI

TL;DR: iSelf-Discover框架的动态生成结构化JSON推理与非结构化推理相比，非结构化推理在性能上表现更优，尤其在复杂任务中。


<details>
  <summary>Details</summary>
Motivation: 研究结构化输出与非结构化自然语言推理的性能差异，以优化LLM在复合系统中的推理能力。

Method: 引入iSelf-Discover框架，动态生成结构化JSON与非结构化推理，并在多样基准上进行比较。

Result: 非结构化推理在MATH基准上相对性能提升18.90%，零样本非结构化版本优于五样本结构化版本。

Conclusion: 非结构化推理在复杂问题解决中更具优势，需重新评估结构化格式的依赖和复合系统的组织方式。

Abstract: The drive for predictable LLM reasoning in their integration with compound
systems has popularized structured outputs, yet concerns remain about
performance trade-offs compared to unconstrained natural language. At the same
time, training on unconstrained Chain of Thought (CoT) traces has brought about
a new class of strong reasoning models that nevertheless present novel compute
budget and faithfulness challenges. This paper introduces iSelf-Discover, an
instance-level adaptation of the Self-Discover framework, and using it compares
dynamically generated structured JSON reasoning with its unstructured
counterpart. Our empirical evaluation across diverse benchmarks using
state-of-the-art open-source models supports a consistent advantage for
unstructured reasoning. Notably, on the complex MATH benchmark, unstructured
plans achieved relative performance improvements of up to 18.90\% over
structured approaches. Zero-shot unstructured iSelf-Discover variants are also
shown to outperform their five-shot structured counterparts, underscoring the
significance of this gap, even when structured plans are dynamically generated
to ensure reasoning precedes the final answer. We further demonstrate that the
optimal granularity of plan generation (instance-level vs. task-level) is
context-dependent. These findings invite re-evaluation of the reliance on
structured formats for complex problem-solving and how compound systems should
be organized.

</details>


### [13] [Artificial intelligence in drug discovery: A comprehensive review with a case study on hyperuricemia, gout arthritis, and hyperuricemic nephropathy](https://arxiv.org/abs/2507.03407)
*Junwei Su,Cheng Xin,Ao Shang,Shan Wu,Zhenzhen Xie,Ruogu Xiong,Xiaoyu Xu,Cheng Zhang,Guang Chen,Yau-Tuen Chan,Guoyi Tang,Ning Wang,Yong Xu,Yibin Feng*

Main category: cs.AI

TL;DR: 本文系统综述了人工智能（AI）和机器学习（ML）在药物发现全流程中的最新进展，填补了现有文献对关键阶段依赖关系的忽视，并通过案例研究展示了实际应用效果。


<details>
  <summary>Details</summary>
Motivation: 传统药物发现方法复杂、成本高、周期长且失败率高，亟需全面了解AI/ML如何有效整合到全流程中。

Method: 详细分析了AI/ML在目标识别、命中筛选和先导优化等核心阶段的应用，并结合案例研究（如高尿酸血症和痛风性关节炎）展示实际效果。

Result: 展示了AI/ML在各阶段的显著方法学进展及其实际影响，同时探讨了当前面临的挑战。

Conclusion: 本综述为研究者利用AI/ML克服瓶颈、加速药物发现提供了重要指导，并指出了未来研究方向。

Abstract: This paper systematically reviews recent advances in artificial intelligence
(AI), with a particular focus on machine learning (ML), across the entire drug
discovery pipeline. Due to the inherent complexity, escalating costs, prolonged
timelines, and high failure rates of traditional drug discovery methods, there
is a critical need to comprehensively understand how AI/ML can be effectively
integrated throughout the full process. Currently available literature reviews
often narrowly focus on specific phases or methodologies, neglecting the
dependence between key stages such as target identification, hit screening, and
lead optimization. To bridge this gap, our review provides a detailed and
holistic analysis of AI/ML applications across these core phases, highlighting
significant methodological advances and their impacts at each stage. We further
illustrate the practical impact of these techniques through an in-depth case
study focused on hyperuricemia, gout arthritis, and hyperuricemic nephropathy,
highlighting real-world successes in molecular target identification and
therapeutic candidate discovery. Additionally, we discuss significant
challenges facing AI/ML in drug discovery and outline promising future research
directions. Ultimately, this review serves as an essential orientation for
researchers aiming to leverage AI/ML to overcome existing bottlenecks and
accelerate drug discovery.

</details>


### [14] [Lessons from a Chimp: AI "Scheming" and the Quest for Ape Language](https://arxiv.org/abs/2507.03409)
*Christopher Summerfield,Lennart Luettgau,Magda Dubois,Hannah Rose Kirk,Kobi Hackenburg,Catherine Fist,Katarina Slama,Nicola Ding,Rebecca Anselmetti,Andrew Strait,Mario Giulianelli,Cozmin Ududec*

Main category: cs.AI

TL;DR: 论文探讨当前AI系统是否可能发展出“阴谋”能力（隐蔽且战略性地追求不匹配目标），并与1970年代研究非人灵长类动物掌握自然语言的实践对比，提出避免历史研究中的陷阱。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探讨AI系统是否可能发展出隐蔽的战略行为，并借鉴历史研究中的教训以避免类似错误。

Method: 通过对比当前AI研究与1970年代非人灵长类语言研究的实践，分析其共同点与问题。

Result: 指出当前研究可能存在的陷阱，如过度拟人化、依赖轶事和描述性分析，缺乏理论框架。

Conclusion: 建议AI阴谋研究应避免历史错误，并提出具体步骤以推动科学严谨的研究进展。

Abstract: We examine recent research that asks whether current AI systems may be
developing a capacity for "scheming" (covertly and strategically pursuing
misaligned goals). We compare current research practices in this field to those
adopted in the 1970s to test whether non-human primates could master natural
language. We argue that there are lessons to be learned from that historical
research endeavour, which was characterised by an overattribution of human
traits to other agents, an excessive reliance on anecdote and descriptive
analysis, and a failure to articulate a strong theoretical framework for the
research. We recommend that research into AI scheming actively seeks to avoid
these pitfalls. We outline some concrete steps that can be taken for this
research programme to advance in a productive and scientifically rigorous
fashion.

</details>


### [15] [Multi-Agent Reasoning for Cardiovascular Imaging Phenotype Analysis](https://arxiv.org/abs/2507.03460)
*Weitong Zhang,Mengyun Qiao,Chengqi Zang,Steven Niederer,Paul M Matthews,Wenjia Bai,Bernhard Kainz*

Main category: cs.AI

TL;DR: MESHAgents框架利用多学科AI代理动态发现影像表型与疾病风险因素的复杂关联，性能接近专家选择的方法。


<details>
  <summary>Details</summary>
Motivation: 传统方法依赖人工假设测试，难以捕捉影像表型与其他多模态数据的非线性关系。

Method: 通过多学科AI代理（如心脏病学、生物力学、统计学）动态生成和整合见解，实现自动化PheWAS。

Result: 在心脏和主动脉影像研究中，MESHAgents发现超出标准人口因素的混杂变量，疾病分类任务AUC差异仅-0.004。

Conclusion: MESHAgents提供可扩展的临床相关影像表型，性能接近专家方法且透明度高。

Abstract: Identifying the associations between imaging phenotypes and disease risk
factors and outcomes is essential for understanding disease mechanisms and
improving diagnosis and prognosis models. However, traditional approaches rely
on human-driven hypothesis testing and selection of association factors, often
overlooking complex, non-linear dependencies among imaging phenotypes and other
multi-modal data. To address this, we introduce a Multi-agent Exploratory
Synergy for the Heart (MESHAgents) framework that leverages large language
models as agents to dynamically elicit, surface, and decide confounders and
phenotypes in association studies, using cardiovascular imaging as a proof of
concept. Specifically, we orchestrate a multi-disciplinary team of AI agents --
spanning cardiology, biomechanics, statistics, and clinical research -- which
spontaneously generate and converge on insights through iterative,
self-organizing reasoning. The framework dynamically synthesizes statistical
correlations with multi-expert consensus, providing an automated pipeline for
phenome-wide association studies (PheWAS). We demonstrate the system's
capabilities through a population-based study of imaging phenotypes of the
heart and aorta. MESHAgents autonomously uncovered correlations between imaging
phenotypes and a wide range of non-imaging factors, identifying additional
confounder variables beyond standard demographic factors. Validation on
diagnosis tasks reveals that MESHAgents-discovered phenotypes achieve
performance comparable to expert-selected phenotypes, with mean AUC differences
as small as -0.004 on disease classification tasks. Notably, the recall score
improves for 6 out of 9 disease types. Our framework provides clinically
relevant imaging phenotypes with transparent reasoning, offering a scalable
alternative to expert-driven methods.

</details>


### [16] [REAL: Benchmarking Abilities of Large Language Models for Housing Transactions and Services](https://arxiv.org/abs/2507.03477)
*Kexin Zhu,Yang Han*

Main category: cs.AI

TL;DR: REAL是首个评估大语言模型在房地产交易和服务中能力的评测套件，包含5316条高质量评测条目，覆盖4个主题和14个类别。实验结果表明，LLMs在房地产领域仍有较大改进空间。


<details>
  <summary>Details</summary>
Motivation: 评估大语言模型（LLMs）是否能在房地产交易和服务中发挥类似人类代理的作用。

Method: 开发了REAL评测套件，包含5316条条目，覆盖记忆、理解、推理和幻觉4个主题，分为14个类别。

Result: 实验显示，LLMs在房地产领域的表现仍有显著提升空间。

Conclusion: LLMs在房地产交易和服务中的应用潜力尚未完全发挥，需进一步改进。

Abstract: The development of large language models (LLMs) has greatly promoted the
progress of chatbot in multiple fields. There is an urgent need to evaluate
whether LLMs can play the role of agent in housing transactions and services as
well as humans. We present Real Estate Agent Large Language Model Evaluation
(REAL), the first evaluation suite designed to assess the abilities of LLMs in
the field of housing transactions and services. REAL comprises 5,316
high-quality evaluation entries across 4 topics: memory, comprehension,
reasoning and hallucination. All these entries are organized as 14 categories
to assess whether LLMs have the knowledge and ability in housing transactions
and services scenario. Additionally, the REAL is used to evaluate the
performance of most advanced LLMs. The experiment results indicate that LLMs
still have significant room for improvement to be applied in the real estate
field.

</details>


### [17] [Limits of Safe AI Deployment: Differentiating Oversight and Control](https://arxiv.org/abs/2507.03525)
*David Manheim,Aidan Homewood*

Main category: cs.AI

TL;DR: 论文区分了AI系统中的监督与控制，提出了一个框架来明确两者的适用条件，并提出了AI监督的成熟度模型。


<details>
  <summary>Details</summary>
Motivation: 由于监督与控制概念在AI领域常被混淆，影响了有效人类监督的设计与评估，论文旨在澄清两者的区别及其在实践中的应用。

Method: 通过文献综述和理论分析，区分控制（事前或实时操作）与监督（事后或政策职能），并构建框架和成熟度模型。

Result: 提出了一个理论框架、监督方法的文档化建议，以及基于微软模型的AI监督成熟度模型。

Conclusion: 明确了监督与控制的边界，为监管者和实践者提供了工具，同时指出了现有方法的局限性及未来研究方向。

Abstract: Oversight and control (collectively, supervision) are often invoked as key
levers for ensuring that AI systems are accountable, reliable, and able to
fulfill governance and management requirements. However, the concepts are
frequently conflated or insufficiently distinguished in academic and policy
discourse, undermining efforts to design or evaluate systems that should remain
under meaningful human supervision.
  This paper undertakes a targeted critical review of literature on supervision
outside of AI, along with a brief summary of past work on the topic related to
AI. We then differentiate control as being ex-ante or real-time, and
operational rather than policy or governance. In contrast, oversight is either
a policy and governance function, or is ex-post. We suggest that control aims
to prevent failures. In contrast, oversight often focuses on detection,
remediation, or incentives for future prevention; all preventative oversight
strategies nonetheless necessitate control.
  Building on this foundation, we make three contributions. First, we propose a
theoretically-informed yet policy-grounded framework that articulates the
conditions under which each mechanism is possible, where they fall short, and
what is required to make them meaningful in practice. Second, we outline how
supervision methods should be documented and integrated into risk management,
and drawing on the Microsoft Responsible AI Maturity Model, we outline a
maturity model for AI supervision. Third, we explicitly highlight some
boundaries of these mechanisms, including where they apply, where they fail,
and where it is clear that no existing methods suffice. This foregrounds the
question of whether meaningful supervision is possible in a given deployment
context, and can support regulators, auditors, and practitioners in identifying
both present limitations and the need for new conceptual and technical
advances.

</details>


### [18] [A Universal Approach to Feature Representation in Dynamic Task Assignment Problems](https://arxiv.org/abs/2507.03579)
*Riccardo Lo Bianco,Remco Dijkman,Wim Nuijten,Willem van Jaarsveld*

Main category: cs.AI

TL;DR: 本文提出了一种基于图表示和深度强化学习的方法，用于解决具有无限状态和动作空间的动态任务分配问题。


<details>
  <summary>Details</summary>
Motivation: 动态任务分配问题中，资源和任务的特征可能具有无限取值，如何表示状态和动作以输入策略神经网络仍是一个挑战。

Method: 提出了一种图表示方法（assignment graph），将标记的Colored Petri Nets映射到assignment graph，并改进了Proximal Policy Optimization算法。

Result: 实验表明，该方法适用于表示和学习接近最优的任务分配策略，无论状态和动作空间的维度如何。

Conclusion: 该方法为无限状态和动作空间的动态任务分配问题提供了有效的解决方案。

Abstract: Dynamic task assignment concerns the optimal assignment of resources to tasks
in a business process. Recently, Deep Reinforcement Learning (DRL) has been
proposed as the state of the art for solving assignment problems. DRL methods
usually employ a neural network (NN) as an approximator for the policy
function, which ingests the state of the process and outputs a valuation of the
possible assignments. However, representing the state and the possible
assignments so that they can serve as inputs and outputs for a policy NN
remains an open challenge, especially when tasks or resources have features
with an infinite number of possible values. To solve this problem, this paper
proposes a method for representing and solving assignment problems with
infinite state and action spaces. In doing so, it provides three contributions:
(I) A graph-based feature representation of assignment problems, which we call
assignment graph; (II) A mapping from marked Colored Petri Nets to assignment
graphs; (III) An adaptation of the Proximal Policy Optimization algorithm that
can learn to solve assignment problems represented through assignment graphs.
To evaluate the proposed representation method, we model three archetypal
assignment problems ranging from finite to infinite state and action space
dimensionalities. The experiments show that the method is suitable for
representing and learning close-to-optimal task assignment policies regardless
of the state and action space dimensionalities.

</details>


### [19] [Benchmarking Vector, Graph and Hybrid Retrieval Augmented Generation (RAG) Pipelines for Open Radio Access Networks (ORAN)](https://arxiv.org/abs/2507.03608)
*Sarat Ahmad,Zeinab Nezami,Maryam Hafeez,Syed Ali Raza Zaidi*

Main category: cs.AI

TL;DR: 比较了Vector RAG、GraphRAG和Hybrid GraphRAG在ORAN架构中的表现，发现GraphRAG和Hybrid GraphRAG优于传统RAG。


<details>
  <summary>Details</summary>
Motivation: 解决在ORAN架构中生成xApps和rApps时，LLM微调的高成本和资源消耗问题。

Method: 采用Retrieval-Augmented Generation (RAG)方法，比较Vector RAG、GraphRAG和Hybrid GraphRAG的性能。

Result: GraphRAG和Hybrid GraphRAG表现更优，Hybrid GraphRAG提升事实正确性8%，GraphRAG提升上下文相关性7%。

Conclusion: GraphRAG和Hybrid GraphRAG是更有效的解决方案，适用于ORAN架构中的高要求任务。

Abstract: Generative AI (GenAI) is expected to play a pivotal role in enabling
autonomous optimization in future wireless networks. Within the ORAN
architecture, Large Language Models (LLMs) can be specialized to generate xApps
and rApps by leveraging specifications and API definitions from the RAN
Intelligent Controller (RIC) platform. However, fine-tuning base LLMs for
telecom-specific tasks remains expensive and resource-intensive.
Retrieval-Augmented Generation (RAG) offers a practical alternative through
in-context learning, enabling domain adaptation without full retraining. While
traditional RAG systems rely on vector-based retrieval, emerging variants such
as GraphRAG and Hybrid GraphRAG incorporate knowledge graphs or dual retrieval
strategies to support multi-hop reasoning and improve factual grounding.
Despite their promise, these methods lack systematic, metric-driven
evaluations, particularly in high-stakes domains such as ORAN. In this study,
we conduct a comparative evaluation of Vector RAG, GraphRAG, and Hybrid
GraphRAG using ORAN specifications. We assess performance across varying
question complexities using established generation metrics: faithfulness,
answer relevance, context relevance, and factual correctness. Results show that
both GraphRAG and Hybrid GraphRAG outperform traditional RAG. Hybrid GraphRAG
improves factual correctness by 8%, while GraphRAG improves context relevance
by 7%.

</details>


### [20] [EvoAgentX: An Automated Framework for Evolving Agentic Workflows](https://arxiv.org/abs/2507.03616)
*Yingxu Wang,Siwei Liu,Jinyuan Fang,Zaiqiao Meng*

Main category: cs.AI

TL;DR: EvoAgentX是一个开源平台，通过自动化生成、执行和进化优化多智能体工作流，解决了现有框架在动态演化和性能优化方面的不足。


<details>
  <summary>Details</summary>
Motivation: 现有多智能体系统（MAS）框架需要手动配置工作流，缺乏动态演化和性能优化的原生支持，且优化算法未统一集成。

Method: EvoAgentX采用五层模块化架构（基础组件、智能体、工作流、演化和评估层），集成了三种优化算法（TextGrad、AFlow、MIPRO）以迭代优化提示、工具配置和工作流拓扑。

Result: 在HotPotQA、MBPP和MATH等任务上，EvoAgentX显著提升了性能（如HotPotQA F1提高7.44%，MBPP pass@1提高10.00%，MATH解决准确率提高10.00%），并在GAIA上实现了高达20.00%的总体准确率提升。

Conclusion: EvoAgentX通过自动化优化多智能体工作流，显著提升了复杂任务的性能，为MAS领域提供了高效的开源解决方案。

Abstract: Multi-agent systems (MAS) have emerged as a powerful paradigm for
orchestrating large language models (LLMs) and specialized tools to
collaboratively address complex tasks. However, existing MAS frameworks often
require manual workflow configuration and lack native support for dynamic
evolution and performance optimization. In addition, many MAS optimization
algorithms are not integrated into a unified framework. In this paper, we
present EvoAgentX, an open-source platform that automates the generation,
execution, and evolutionary optimization of multi-agent workflows. EvoAgentX
employs a modular architecture consisting of five core layers: the basic
components, agent, workflow, evolving, and evaluation layers. Specifically,
within the evolving layer, EvoAgentX integrates three MAS optimization
algorithms, TextGrad, AFlow, and MIPRO, to iteratively refine agent prompts,
tool configurations, and workflow topologies. We evaluate EvoAgentX on
HotPotQA, MBPP, and MATH for multi-hop reasoning, code generation, and
mathematical problem solving, respectively, and further assess it on real-world
tasks using GAIA. Experimental results show that EvoAgentX consistently
achieves significant performance improvements, including a 7.44% increase in
HotPotQA F1, a 10.00% improvement in MBPP pass@1, a 10.00% gain in MATH solve
accuracy, and an overall accuracy improvement of up to 20.00% on GAIA. The
source code is available at: https://github.com/EvoAgentX/EvoAgentX

</details>


### [21] [Large Language Models for Combinatorial Optimization: A Systematic Review](https://arxiv.org/abs/2507.03637)
*Francesca Da Ros,Michael Soprano,Luca Di Gaspero,Kevin Roitero*

Main category: cs.AI

TL;DR: 本文系统综述了大语言模型（LLMs）在组合优化（CO）中的应用，基于PRISMA指南筛选了103篇研究，分类总结了LLMs的任务、架构、数据集和应用领域，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在组合优化领域的应用现状，为研究者提供全面的领域概览和未来研究方向。

Method: 采用PRISMA指南进行文献检索，通过Scopus和Google Scholar筛选2000多篇文献，最终纳入103篇研究，按语义分类和主题分析。

Result: 总结了LLMs在CO中的任务、架构、专用数据集和应用领域，并提出了未来研究方向。

Conclusion: LLMs在组合优化中展现出潜力，但仍需进一步研究以优化其应用效果。

Abstract: This systematic review explores the application of Large Language Models
(LLMs) in Combinatorial Optimization (CO). We report our findings using the
Preferred Reporting Items for Systematic Reviews and Meta-Analyses (PRISMA)
guidelines. We conduct a literature search via Scopus and Google Scholar,
examining over 2,000 publications. We assess publications against four
inclusion and four exclusion criteria related to their language, research
focus, publication year, and type. Eventually, we select 103 studies. We
classify these studies into semantic categories and topics to provide a
comprehensive overview of the field, including the tasks performed by LLMs, the
architectures of LLMs, the existing datasets specifically designed for
evaluating LLMs in CO, and the field of application. Finally, we identify
future directions for leveraging LLMs in this field.

</details>


### [22] [Towards Machine Theory of Mind with Large Language Model-Augmented Inverse Planning](https://arxiv.org/abs/2507.03682)
*Rebekah A. Gelpí,Eric Xue,William A. Cunningham*

Main category: cs.AI

TL;DR: 提出了一种混合方法，结合大型语言模型（LLMs）和贝叶斯逆向规划模型，以提升机器心智理论（ToM）的性能。


<details>
  <summary>Details</summary>
Motivation: 解决贝叶斯逆向规划模型在复杂场景下的扩展性问题，以及LLMs在推理任务中的脆弱性。

Method: 使用LLMs生成假设和似然函数，结合贝叶斯逆向规划模型计算后验概率。

Result: 在ToM任务中表现优于单独使用LLMs或链式思维提示的模型，且适用于开放任务。

Conclusion: 混合方法结合了两者的优势，为ToM模型和社交智能生成代理的未来发展提供了方向。

Abstract: We propose a hybrid approach to machine Theory of Mind (ToM) that uses large
language models (LLMs) as a mechanism for generating hypotheses and likelihood
functions with a Bayesian inverse planning model that computes posterior
probabilities for an agent's likely mental states given its actions. Bayesian
inverse planning models can accurately predict human reasoning on a variety of
ToM tasks, but these models are constrained in their ability to scale these
predictions to scenarios with a large number of possible hypotheses and
actions. Conversely, LLM-based approaches have recently demonstrated promise in
solving ToM benchmarks, but can exhibit brittleness and failures on reasoning
tasks even when they pass otherwise structurally identical versions. By
combining these two methods, this approach leverages the strengths of each
component, closely matching optimal results on a task inspired by prior inverse
planning models and improving performance relative to models that utilize LLMs
alone or with chain-of-thought prompting, even with smaller LLMs that typically
perform poorly on ToM tasks. We also exhibit the model's potential to predict
mental states on open-ended tasks, offering a promising direction for future
development of ToM models and the creation of socially intelligent generative
agents.

</details>


### [23] [Towards Unified Neurosymbolic Reasoning on Knowledge Graphs](https://arxiv.org/abs/2507.03697)
*Qika Lin,Fangzhi Xu,Hao Lu,Kai He,Rui Mao,Jun Liu,Erik Cambria,Mengling Feng*

Main category: cs.AI

TL;DR: Tunsr提出了一种统一神经符号推理框架，通过推理图和逻辑消息传递机制整合神经与符号方法，解决了现有方法在多样推理场景中的局限性。


<details>
  <summary>Details</summary>
Motivation: 当前知识图谱推理方法多集中于单一神经或符号推理形式，未能有效整合两者优势，且难以满足多样推理场景需求。

Method: Tunsr引入推理图结构和前向逻辑消息传递机制，更新命题与一阶逻辑表示，并通过FARI算法归纳规则。

Result: 在19个数据集上的实验表明，Tunsr在四种推理场景中均表现优异。

Conclusion: Tunsr成功统一神经与符号推理，为多样推理任务提供了高效解决方案。

Abstract: Knowledge Graph (KG) reasoning has received significant attention in the
fields of artificial intelligence and knowledge engineering, owing to its
ability to autonomously deduce new knowledge and consequently enhance the
availability and precision of downstream applications. However, current methods
predominantly concentrate on a single form of neural or symbolic reasoning,
failing to effectively integrate the inherent strengths of both approaches.
Furthermore, the current prevalent methods primarily focus on addressing a
single reasoning scenario, presenting limitations in meeting the diverse
demands of real-world reasoning tasks. Unifying the neural and symbolic
methods, as well as diverse reasoning scenarios in one model is challenging as
there is a natural representation gap between symbolic rules and neural
networks, and diverse scenarios exhibit distinct knowledge structures and
specific reasoning objectives. To address these issues, we propose a unified
neurosymbolic reasoning framework, namely Tunsr, for KG reasoning. Tunsr first
introduces a consistent structure of reasoning graph that starts from the query
entity and constantly expands subsequent nodes by iteratively searching
posterior neighbors. Based on it, a forward logic message-passing mechanism is
proposed to update both the propositional representations and attentions, as
well as first-order logic (FOL) representations and attentions of each node. In
this way, Tunsr conducts the transformation of merging multiple rules by
merging possible relations at each step. Finally, the FARI algorithm is
proposed to induce FOL rules by constantly performing attention calculations
over the reasoning graph. Extensive experimental results on 19 datasets of four
reasoning scenarios (transductive, inductive, interpolation, and extrapolation)
demonstrate the effectiveness of Tunsr.

</details>


### [24] [Roadmap for using large language models (LLMs) to accelerate cross-disciplinary research with an example from computational biology](https://arxiv.org/abs/2507.03722)
*Ruian Ke,Ruy M. Ribeiro*

Main category: cs.AI

TL;DR: 论文探讨了大型语言模型（LLMs）在跨学科研究中的应用，强调其作为辅助工具的作用，并通过案例研究展示了其潜力。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于解决LLMs在研究中面临的质疑（如幻觉、偏见等），并探索如何负责任地利用其优势推动跨学科合作。

Method: 方法包括分析LLMs的能力与局限性，并通过计算生物学案例（HIV反弹动力学建模）展示LLMs在跨学科研究中的实际应用。

Result: 结果表明，LLMs在人类监督下能有效促进跨学科合作，加速科学发现。

Conclusion: 结论认为，LLMs应作为人类主导框架下的辅助工具，负责任地使用将推动跨学科研究的创新。

Abstract: Large language models (LLMs) are powerful artificial intelligence (AI) tools
transforming how research is conducted. However, their use in research has been
met with skepticism, due to concerns about hallucinations, biases and potential
harms to research. These emphasize the importance of clearly understanding the
strengths and weaknesses of LLMs to ensure their effective and responsible use.
Here, we present a roadmap for integrating LLMs into cross-disciplinary
research, where effective communication, knowledge transfer and collaboration
across diverse fields are essential but often challenging. We examine the
capabilities and limitations of LLMs and provide a detailed computational
biology case study (on modeling HIV rebound dynamics) demonstrating how
iterative interactions with an LLM (ChatGPT) can facilitate interdisciplinary
collaboration and research. We argue that LLMs are best used as augmentative
tools within a human-in-the-loop framework. Looking forward, we envisage that
the responsible use of LLMs will enhance innovative cross-disciplinary research
and substantially accelerate scientific discoveries.

</details>


### [25] [Agent-Based Detection and Resolution of Incompleteness and Ambiguity in Interactions with Large Language Models](https://arxiv.org/abs/2507.03726)
*Riya Naik,Ashwin Srinivasan,Swati Agarwal,Estrid He*

Main category: cs.AI

TL;DR: 论文探讨了通过基于代理的架构增强LLM问答系统的推理能力，自动解决问题的模糊性或缺失信息，缩短交互时间并提高答案质量。


<details>
  <summary>Details</summary>
Motivation: 当前LLM问答系统在多轮交互中可能因上下文不明确而变得繁琐，希望通过代理架构自动解决这些问题。

Method: 使用基于LLM的代理作为零样本ReAct代理，通过分类、解决和回答三个动作处理问题的不完整性或模糊性。

Result: 代理架构缩短了交互时间，提高了答案质量，并能解释问题缺陷的解决过程，但可能增加LLM调用和延迟。

Conclusion: 代理架构在大多数情况下能显著提升问答系统的鲁棒性，尤其适用于问题上下文不足的场景。

Abstract: Many of us now treat LLMs as modern-day oracles asking it almost any kind of
question. However, consulting an LLM does not have to be a single turn
activity. But long multi-turn interactions can get tedious if it is simply to
clarify contextual information that can be arrived at through reasoning. In
this paper, we examine the use of agent-based architecture to bolster LLM-based
Question-Answering systems with additional reasoning capabilities. We examine
the automatic resolution of potential incompleteness or ambiguities in
questions by transducers implemented using LLM-based agents. We focus on
several benchmark datasets that are known to contain questions with these
deficiencies to varying degrees. We equip different LLMs (GPT-3.5-Turbo and
Llama-4-Scout) with agents that act as specialists in detecting and resolving
deficiencies of incompleteness and ambiguity. The agents are implemented as
zero-shot ReAct agents. Rather than producing an answer in a single step, the
model now decides between 3 actions a) classify b) resolve c) answer. Action a)
decides if the question is incomplete, ambiguous, or normal. Action b)
determines if any deficiencies identified can be resolved. Action c) answers
the resolved form of the question. We compare the use of LLMs with and without
the use of agents with these components. Our results show benefits of agents
with transducer 1) A shortening of the length of interactions with human 2) An
improvement in the answer quality and 3) Explainable resolution of deficiencies
in the question. On the negative side we find while it may result in additional
LLM invocations and in some cases, increased latency. But on tested datasets,
the benefits outweigh the costs except when questions already have sufficient
context. Suggesting the agent-based approach could be a useful mechanism to
harness the power of LLMs to develop more robust QA systems.

</details>


### [26] [Optimizing UAV Trajectories via a Simplified Close Enough TSP Approach](https://arxiv.org/abs/2507.03775)
*Hiba Bederina*

Main category: cs.AI

TL;DR: 文章提出了一种解决Close Enough Traveling Salesman Problem（CETSP）的方法，通过简化数学公式和利用凸集约束设计提高计算效率。


<details>
  <summary>Details</summary>
Motivation: 简化CETSP的数学表达，优化计算资源使用，同时保持解的质量。

Method: 引入近似欧几里得距离的重新公式化，简化目标函数，并利用凸集设计约束条件。采用基于CPLEX的分段计算策略。

Result: 在真实CETSP实例中验证了方法的有效性，计算资源管理良好且解质量未受影响。

Conclusion: 提出的数学公式在性能和计算效率上表现优异，为CETSP提供了实用解决方案。

Abstract: This article explores an approach to addressing the Close Enough Traveling
Salesman Problem (CETSP). The objective is to streamline the mathematical
formulation by introducing reformulations that approximate the Euclidean
distances and simplify the objective function. Additionally, the use of convex
sets in the constraint design offers computational benefits. The proposed
methodology is empirically validated on real-world CETSP instances, with the
aid of computational strategies such as a fragmented CPLEX-based approach.
Results demonstrate its effectiveness in managing computational resources
without compromising solution quality. Furthermore, the article analyzes the
behavior of the proposed mathematical formulations, providing comprehensive
insights into their performance.

</details>


### [27] [Learning Dark Souls Combat Through Pixel Input With Neuroevolution](https://arxiv.org/abs/2507.03793)
*Jim O'Connor,Gary B. Parker,Mustafa Bugti*

Main category: cs.AI

TL;DR: 论文研究如何利用NEAT算法自动化《黑暗之魂》游戏玩法，通过视觉输入直接进化神经网络，无需游戏状态信息，成功率达到35%。


<details>
  <summary>Details</summary>
Motivation: 探索在缺乏直接API支持或明确状态表示的复杂游戏环境中，基于视觉的神经进化的应用潜力。

Method: 引入DSAPI框架，结合实时计算机视觉提取游戏数据，使用NEAT算法从原始像素数据进化神经网络。

Result: 进化后的代理在击败初始Boss时达到35%的成功率。

Conclusion: 研究表明，神经进化在视觉复杂的游戏场景中具有可行性，为类似环境提供了新思路。

Abstract: This paper investigates the application of Neuroevolution of Augmenting
Topologies (NEAT) to automate gameplay in Dark Souls, a notoriously challenging
action role-playing game characterized by complex combat mechanics, dynamic
environments, and high-dimensional visual inputs. Unlike traditional
reinforcement learning or game playing approaches, our method evolves neural
networks directly from raw pixel data, circumventing the need for explicit
game-state information. To facilitate this approach, we introduce the Dark
Souls API (DSAPI), a novel Python framework leveraging real-time computer
vision techniques for extracting critical game metrics, including player and
enemy health states. Using NEAT, agents evolve effective combat strategies for
defeating the Asylum Demon, the game's initial boss, without predefined
behaviors or domain-specific heuristics. Experimental results demonstrate that
evolved agents achieve up to a 35% success rate, indicating the viability of
neuroevolution in addressing complex, visually intricate gameplay scenarios.
This work represents an interesting application of vision-based neuroevolution,
highlighting its potential use in a wide range of challenging game environments
lacking direct API support or well-defined state representations.

</details>


### [28] [Generating Novelty in Open-World Multi-Agent Strategic Board Games](https://arxiv.org/abs/2507.03802)
*Mayank Kejriwal,Shilpa Thomas*

Main category: cs.AI

TL;DR: GNOME是一个实验平台，用于测试多智能体AI系统在面对未预期的新颖性时的表现，支持开放讨论AI鲁棒性和新颖性。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体AI系统在开放世界环境中应对未预期新颖性的能力，以提升AI的鲁棒性。

Method: 通过分离AI开发与模拟器，设计GNOME平台，支持未预期新颖性的测试，并在DARPA SAIL-ON项目中应用。

Result: GNOME在NeurIPS 2020上以Monopoly游戏展示，成功促进了对AI鲁棒性和新颖性的讨论。

Conclusion: GNOME为多智能体AI系统在开放世界中的新颖性适应提供了有效测试平台，并支持进一步研究。

Abstract: We describe GNOME (Generating Novelty in Open-world Multi-agent
Environments), an experimental platform that is designed to test the
effectiveness of multi-agent AI systems when faced with \emph{novelty}. GNOME
separates the development of AI gameplaying agents with the simulator, allowing
\emph{unanticipated} novelty (in essence, novelty that is not subject to
model-selection bias). Using a Web GUI, GNOME was recently demonstrated at
NeurIPS 2020 using the game of Monopoly to foster an open discussion on AI
robustness and the nature of novelty in real-world environments. In this
article, we further detail the key elements of the demonstration, and also
provide an overview of the experimental design that is being currently used in
the DARPA Science of Artificial Intelligence and Learning for Open-World
Novelty (SAIL-ON) program to evaluate external teams developing
novelty-adaptive gameplaying agents.

</details>


### [29] [Leveraging Large Language Models for Tacit Knowledge Discovery in Organizational Contexts](https://arxiv.org/abs/2507.03811)
*Gianlucca Zuin,Saulo Mastelini,Túlio Loures,Adriano Veloso*

Main category: cs.AI

TL;DR: 提出基于LLM的代理框架，通过SI模型模拟知识传播，实现94.9%的知识召回率。


<details>
  <summary>Details</summary>
Motivation: 解决组织中隐性知识难以记录的问题，如信息不完整、专家难识别等。

Method: 使用代理框架和LLM，通过SI模型模拟知识传播，进行864次仿真。

Result: 代理实现94.9%的知识召回率，反馈评分与文献评分强相关。

Conclusion: 该方法能有效捕获碎片化知识，无需直接访问领域专家。

Abstract: Documenting tacit knowledge in organizations can be a challenging task due to
incomplete initial information, difficulty in identifying knowledgeable
individuals, the interplay of formal hierarchies and informal networks, and the
need to ask the right questions. To address this, we propose an agent-based
framework leveraging large language models (LLMs) to iteratively reconstruct
dataset descriptions through interactions with employees. Modeling knowledge
dissemination as a Susceptible-Infectious (SI) process with waning infectivity,
we conduct 864 simulations across various synthetic company structures and
different dissemination parameters. Our results show that the agent achieves
94.9% full-knowledge recall, with self-critical feedback scores strongly
correlating with external literature critic scores. We analyze how each
simulation parameter affects the knowledge retrieval process for the agent. In
particular, we find that our approach is able to recover information without
needing to access directly the only domain specialist. These findings highlight
the agent's ability to navigate organizational complexity and capture
fragmented knowledge that would otherwise remain inaccessible.

</details>


### [30] [RELRaE: LLM-Based Relationship Extraction, Labelling, Refinement, and Evaluation](https://arxiv.org/abs/2507.03829)
*George Hannah,Jacopo de Berardinis,Terry R. Payne,Valentina Tamma,Andrew Mitchell,Ellen Piercy,Ewan Johnson,Andrew Ng,Harry Rostron,Boris Konev*

Main category: cs.AI

TL;DR: 论文提出RELRaE框架，利用大语言模型（LLMs）从XML模式中提取和标注隐含关系，支持实验室数据的知识图谱转换。


<details>
  <summary>Details</summary>
Motivation: 实验室机器人产生的XML数据需要转换为知识图谱以实现数据互操作性，关键步骤是丰富XML模式以构建本体模式基础。

Method: 提出RELRaE框架，利用LLMs分阶段提取和标注XML模式中的隐含关系，并评估其准确性。

Result: LLMs能有效支持实验室自动化中关系标签的生成，在半自动本体生成框架中具有重要价值。

Conclusion: LLMs在实验室数据转换和本体生成中具有实用性和推广潜力。

Abstract: A large volume of XML data is produced in experiments carried out by robots
in laboratories. In order to support the interoperability of data between labs,
there is a motivation to translate the XML data into a knowledge graph. A key
stage of this process is the enrichment of the XML schema to lay the foundation
of an ontology schema. To achieve this, we present the RELRaE framework, a
framework that employs large language models in different stages to extract and
accurately label the relationships implicitly present in the XML schema. We
investigate the capability of LLMs to accurately generate these labels and then
evaluate them. Our work demonstrates that LLMs can be effectively used to
support the generation of relationship labels in the context of lab automation,
and that they can play a valuable role within semi-automatic ontology
generation frameworks more generally.

</details>


### [31] [Economic Evaluation of LLMs](https://arxiv.org/abs/2507.03834)
*Michael J. Zellinger,Matt Thomson*

Main category: cs.AI

TL;DR: 论文提出了一种经济评估框架，用于量化LLM的性能权衡，并将其转化为单一数值，基于具体用例的经济约束（如错误成本、延迟成本等）。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法无法比较具有不同优缺点的LLM（如低成本高错误率与高成本高准确性模型）的问题。

Method: 通过经济评估框架，将LLM的性能权衡量化为基于美元的数字，考虑错误成本、延迟成本和查询放弃成本。

Result: 在MATH基准测试中，推理模型在错误成本超过0.01美元时表现更优；单一大模型在错误成本低至0.1美元时优于级联模型。

Conclusion: 在自动化重要任务时，应优先使用最强大的模型，而非最小化部署成本，因为错误的经济影响远大于部署成本。

Abstract: Practitioners often navigate LLM performance trade-offs by plotting Pareto
frontiers of optimal accuracy-cost trade-offs. However, this approach offers no
way to compare between LLMs with distinct strengths and weaknesses: for
example, a cheap, error-prone model vs a pricey but accurate one. To address
this gap, we propose economic evaluation of LLMs. Our framework quantifies the
performance trade-off of an LLM as a single number based on the economic
constraints of a concrete use case, all expressed in dollars: the cost of
making a mistake, the cost of incremental latency, and the cost of abstaining
from a query. We apply our economic evaluation framework to compare the
performance of reasoning and non-reasoning models on difficult questions from
the MATH benchmark, discovering that reasoning models offer better
accuracy-cost tradeoffs as soon as the economic cost of a mistake exceeds
\$0.01. In addition, we find that single large LLMs often outperform cascades
when the cost of making a mistake is as low as \$0.1. Overall, our findings
suggest that when automating meaningful human tasks with AI models,
practitioners should typically use the most powerful available model, rather
than attempt to minimize AI deployment costs, since deployment costs are likely
dwarfed by the economic impact of AI errors.

</details>


### [32] [Participatory Evolution of Artificial Life Systems via Semantic Feedback](https://arxiv.org/abs/2507.03839)
*Shuowen Li,Kexin Wang,Minglu Fang,Danqi Huang,Ali Asadipour,Haipeng Mi,Yitong Sun*

Main category: cs.AI

TL;DR: 提出了一种语义反馈框架，通过自然语言指导人工生命系统的演化，结合提示到参数编码、CMA-ES优化器和CLIP评估，实现用户意图对视觉结果和行为规则的调控。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自然语言更直观地指导人工生命系统的演化，提升语义对齐和用户参与度。

Method: 集成提示到参数编码器、CMA-ES优化器和基于CLIP的评估系统，支持交互式生态系统模拟，包括提示细化、多智能体交互和涌现规则合成。

Result: 用户研究表明，该系统在语义对齐方面优于手动调整，展示了其在参与式生成设计和开放式演化中的潜力。

Conclusion: 该框架为人工生命系统的自然语言指导提供了有效工具，具有广泛的应用前景。

Abstract: We present a semantic feedback framework that enables natural language to
guide the evolution of artificial life systems. Integrating a
prompt-to-parameter encoder, a CMA-ES optimizer, and CLIP-based evaluation, the
system allows user intent to modulate both visual outcomes and underlying
behavioral rules. Implemented in an interactive ecosystem simulation, the
framework supports prompt refinement, multi-agent interaction, and emergent
rule synthesis. User studies show improved semantic alignment over manual
tuning and demonstrate the system's potential as a platform for participatory
generative design and open-ended evolution.

</details>


### [33] [From Query to Explanation: Uni-RAG for Multi-Modal Retrieval-Augmented Learning in STEM](https://arxiv.org/abs/2507.03868)
*Xinyi Wu,Yanhao Jia,Luwei Xiao,Shuai Zhao,Fengkuang Chiang,Erik Cambria*

Main category: cs.AI

TL;DR: 论文提出了一种名为Uni-Retrieval的多模态检索模块和Uni-RAG的检索增强生成框架，用于解决教育场景中的多样性和模糊性问题，并在实验中表现出色。


<details>
  <summary>Details</summary>
Motivation: 现有检索系统主要关注自然文本-图像匹配，无法应对教育场景中的多样性和模糊性需求。

Method: 开发了Uni-Retrieval模块，结合MoE-LoRA模块动态匹配查询风格原型与Prompt Bank中的知识；进一步整合为Uni-RAG框架，实现检索增强生成。

Result: 在SER等多模态基准测试中，Uni-RAG在检索准确性和生成质量上均优于基线系统，同时保持低计算成本。

Conclusion: Uni-RAG为智能教育系统提供了可扩展、基于教学理论的解决方案，支持跨STEM场景的个性化学习辅助。

Abstract: In AI-facilitated teaching, leveraging various query styles to interpret
abstract educational content is crucial for delivering effective and accessible
learning experiences. However, existing retrieval systems predominantly focus
on natural text-image matching and lack the capacity to address the diversity
and ambiguity inherent in real-world educational scenarios. To address this
limitation, we develop a lightweight and efficient multi-modal retrieval
module, named Uni-Retrieval, which extracts query-style prototypes and
dynamically matches them with tokens from a continually updated Prompt Bank.
This Prompt Bank encodes and stores domain-specific knowledge by leveraging a
Mixture-of-Expert Low-Rank Adaptation (MoE-LoRA) module and can be adapted to
enhance Uni-Retrieval's capability to accommodate unseen query types at test
time. To enable natural language educational content generation, we integrate
the original Uni-Retrieval with a compact instruction-tuned language model,
forming a complete retrieval-augmented generation pipeline named Uni-RAG. Given
a style-conditioned query, Uni-RAG first retrieves relevant educational
materials and then generates human-readable explanations, feedback, or
instructional content aligned with the learning objective. Experimental results
on SER and other multi-modal benchmarks show that Uni-RAG outperforms baseline
retrieval and RAG systems in both retrieval accuracy and generation quality,
while maintaining low computational cost. Our framework provides a scalable,
pedagogically grounded solution for intelligent educational systems, bridging
retrieval and generation to support personalized, explainable, and efficient
learning assistance across diverse STEM scenarios.

</details>


### [34] [Uncovering Systemic and Environment Errors in Autonomous Systems Using Differential Testing](https://arxiv.org/abs/2507.03870)
*Rahil P Mehta,Yashwanthi Anand,Manish Motwani,Sandhya Saisubramanian*

Main category: cs.AI

TL;DR: AIProbe是一种黑盒测试技术，通过差异测试区分自主代理行为错误是源于代理缺陷还是环境不可行性。


<details>
  <summary>Details</summary>
Motivation: 随着自主代理及其环境复杂性增加，识别行为错误来源变得困难但至关重要。

Method: AIProbe生成多样化环境配置和任务，使用拉丁超立方采样，并通过基于搜索的规划器独立求解任务，对比代理性能以区分错误来源。

Result: AIProbe在多领域评估中显著优于现有技术，能检测更多总错误和独特错误。

Conclusion: AIProbe有助于自主代理的可靠部署，有效区分代理缺陷与环境不可行性。

Abstract: When an autonomous agent behaves undesirably, including failure to complete a
task, it can be difficult to determine whether the behavior is due to a
systemic agent error, such as flaws in the model or policy, or an environment
error, where a task is inherently infeasible under a given environment
configuration, even for an ideal agent. As agents and their environments grow
more complex, identifying the error source becomes increasingly difficult but
critical for reliable deployment. We introduce AIProbe, a novel black-box
testing technique that applies differential testing to attribute undesirable
agent behaviors either to agent deficiencies, such as modeling or training
flaws, or due to environmental infeasibility. AIProbe first generates diverse
environmental configurations and tasks for testing the agent, by modifying
configurable parameters using Latin Hypercube sampling. It then solves each
generated task using a search-based planner, independent of the agent. By
comparing the agent's performance to the planner's solution, AIProbe identifies
whether failures are due to errors in the agent's model or policy, or due to
unsolvable task conditions. Our evaluation across multiple domains shows that
AIProbe significantly outperforms state-of-the-art techniques in detecting both
total and unique errors, thereby contributing to a reliable deployment of
autonomous agents.

</details>


### [35] [LLMs model how humans induce logically structured rules](https://arxiv.org/abs/2507.03876)
*Alyssa Loo,Ellie Pavlick,Roman Feiman*

Main category: cs.AI

TL;DR: 论文探讨了人工神经网络（尤其是大型语言模型LLMs）是否能解释人类认知中的逻辑概念，并通过实验证明LLMs在拟合人类行为方面表现优于或等同于贝叶斯概率语言模型（pLoT）。


<details>
  <summary>Details</summary>
Motivation: 研究旨在验证人工神经网络（特别是LLMs）是否能作为解释人类抽象认知功能（如逻辑和语言）的合适计算模型。

Method: 通过四个实验，测试多种LLMs在逻辑规则归纳任务中的表现，并与贝叶斯pLoT模型进行比较。

Result: LLMs在拟合人类行为方面表现优于或等同于pLoT模型，且其预测规则的性质与pLoT不同，表明LLMs并非pLoT的简单实现。

Conclusion: LLMs可能提供了一种新的理论框架，用于解释人类逻辑概念的原始表征和计算，值得未来认知科学研究关注。

Abstract: A central goal of cognitive science is to provide a computationally explicit
account of both the structure of the mind and its development: what are the
primitive representational building blocks of cognition, what are the rules via
which those primitives combine, and where do these primitives and rules come
from in the first place? A long-standing debate concerns the adequacy of
artificial neural networks as computational models that can answer these
questions, in particular in domains related to abstract cognitive function,
such as language and logic. This paper argues that recent advances in neural
networks -- specifically, the advent of large language models (LLMs) --
represent an important shift in this debate. We test a variety of LLMs on an
existing experimental paradigm used for studying the induction of rules
formulated over logical concepts. Across four experiments, we find converging
empirical evidence that LLMs provide at least as good a fit to human behavior
as models that implement a Bayesian probablistic language of thought (pLoT),
which have been the best computational models of human behavior on the same
task. Moreover, we show that the LLMs make qualitatively different predictions
about the nature of the rules that are inferred and deployed in order to
complete the task, indicating that the LLM is unlikely to be a mere
implementation of the pLoT solution. Based on these results, we argue that LLMs
may instantiate a novel theoretical account of the primitive representations
and computations necessary to explain human logical concepts, with which future
work in cognitive science should engage.

</details>


### [36] [Agent Exchange: Shaping the Future of AI Agent Economics](https://arxiv.org/abs/2507.03904)
*Yingxuan Yang,Ying Wen,Jun Wang,Weinan Zhang*

Main category: cs.AI

TL;DR: 论文提出Agent Exchange (AEX)，一个专为AI代理经济设计的拍卖平台，支持代理间的价值交换和协作。


<details>
  <summary>Details</summary>
Motivation: 随着大型语言模型（LLMs）的发展，AI代理从被动工具转变为自主经济参与者，需要新的经济基础设施支持其协作与价值交换。

Method: AEX平台设计基于实时竞价（RTB）系统，包含用户侧平台（USP）、代理侧平台（ASP）、代理中心（Agent Hubs）和数据管理平台（DMP）四个组件。

Result: AEX为AI代理经济提供了优化的基础设施，支持代理间的任务分配、能力展示和知识共享。

Conclusion: AEX为未来AI生态系统中的代理经济奠定了基础，展示了其设计原则和系统架构的可行性。

Abstract: The rise of Large Language Models (LLMs) has transformed AI agents from
passive computational tools into autonomous economic actors. This shift marks
the emergence of the agent-centric economy, in which agents take on active
economic roles-exchanging value, making strategic decisions, and coordinating
actions with minimal human oversight. To realize this vision, we propose Agent
Exchange (AEX), a specialized auction platform designed to support the dynamics
of the AI agent marketplace. AEX offers an optimized infrastructure for agent
coordination and economic participation. Inspired by Real-Time Bidding (RTB)
systems in online advertising, AEX serves as the central auction engine,
facilitating interactions among four ecosystem components: the User-Side
Platform (USP), which translates human goals into agent-executable tasks; the
Agent-Side Platform (ASP), responsible for capability representation,
performance tracking, and optimization; Agent Hubs, which coordinate agent
teams and participate in AEX-hosted auctions; and the Data Management Platform
(DMP), ensuring secure knowledge sharing and fair value attribution. We outline
the design principles and system architecture of AEX, laying the groundwork for
agent-based economic infrastructure in future AI ecosystems.

</details>


### [37] [Animation Needs Attention: A Holistic Approach to Slides Animation Comprehension with Visual-Language Models](https://arxiv.org/abs/2507.03916)
*Yifan Jiang,Yibo Xue,Yukun Kang,Pin Zheng,Jian Peng,Feiran Wu,Changliang Xu*

Main category: cs.AI

TL;DR: 论文提出了首个公开的幻灯片动画数据集，并利用LoRA微调Qwen-2.5-VL-7B模型，显著提升了动画生成效果。


<details>
  <summary>Details</summary>
Motivation: 现有AI幻灯片生成工具缺乏动画支持，且视觉语言模型在动画任务上表现不佳，主要由于缺乏公开数据集和时序推理能力不足。

Method: 发布包含12,000组自然语言描述、动画JSON文件和渲染视频的数据集，并采用LoRA微调Qwen-2.5-VL-7B模型。

Result: 模型在BLEU-4、ROUGE-L、SPICE和CODA指标上优于GPT-4.1和Gemini-2.5-Pro，尤其在CODA细节上提升显著。

Conclusion: 数据集、LoRA增强模型和CODA指标为未来动态幻灯片生成研究提供了基准和基础。

Abstract: Slide animations, such as fade-ins, fly-ins, and wipes, are critical for
audience engagement, efficient information delivery, and vivid visual
expression. However, most AI-driven slide-generation tools still lack native
animation support, and existing vision-language models (VLMs) struggle with
animation tasks due to the absence of public datasets and limited
temporal-reasoning capabilities. To address this gap, we release the first
public dataset for slide-animation modeling: 12,000 triplets of
natural-language descriptions, animation JSON files, and rendered videos,
collectively covering every built-in PowerPoint effect. Using this resource, we
fine-tune Qwen-2.5-VL-7B with Low-Rank Adaptation (LoRA) and achieve consistent
improvements over GPT-4.1 and Gemini-2.5-Pro in BLEU-4, ROUGE-L, SPICE, and our
Coverage-Order-Detail Assessment (CODA) metric, which evaluates action
coverage, temporal order, and detail fidelity. On a manually curated test set
of slides, the LoRA model increases BLEU-4 by around 60%, ROUGE-L by 30%, and
shows significant improvements in CODA-detail. This demonstrates that low-rank
adaptation enables reliable temporal reasoning and generalization beyond
synthetic data. Overall, our dataset, LoRA-enhanced model, and CODA metric
provide a rigorous benchmark and foundation for future research on VLM-based
dynamic slide generation.

</details>


### [38] [CortexDebate: Debating Sparsely and Equally for Multi-Agent Debate](https://arxiv.org/abs/2507.03928)
*Yiliu Sun,Zicheng Zhao,Sheng Wan,Chen Gong*

Main category: cs.AI

TL;DR: CortexDebate提出了一种稀疏辩论图方法，通过MDM模块优化LLM代理间的辩论，解决了现有MAD方法中的输入过长和过度自信问题。


<details>
  <summary>Details</summary>
Motivation: 单一大语言模型存在幻觉和推理能力不足问题，而现有的多代理辩论方法因输入过长和代理过度自信导致效果不佳。

Method: 提出CortexDebate方法，构建稀疏辩论图，并通过MDM模块（基于麦肯锡信任公式）优化辩论图。

Result: 在四个任务类型的八个数据集上验证了CortexDebate的有效性。

Conclusion: CortexDebate通过稀疏辩论图和MDM模块显著提升了多代理辩论的效果。

Abstract: Nowadays, single Large Language Model (LLM) struggles with critical issues
such as hallucination and inadequate reasoning abilities. To mitigate these
issues, Multi-Agent Debate (MAD) has emerged as an effective strategy, where
LLM agents engage in in-depth debates with others on tasks. However, existing
MAD methods face two major issues: (a) too lengthy input contexts, which causes
LLM agents to get lost in plenty of input information and experiences
performance drop; and (b) the overconfidence dilemma, where self-assured LLM
agents dominate the debate, leading to low debating effectiveness. To address
these limitations, we propose a novel MAD method called "CortexDebate".
Inspired by the human brain's tendency to establish a sparse and dynamically
optimized network among cortical areas governed by white matter, CortexDebate
constructs a sparse debating graph among LLM agents, where each LLM agent only
debates with the ones that are helpful to it. To optimize the graph, we propose
a module named McKinsey-based Debate Matter (MDM), which acts as an artificial
analog to white matter. By integrating the McKinsey Trust Formula, a
well-established measure of trustworthiness from sociology, MDM enables
credible evaluations that guide graph optimization. The effectiveness of our
CortexDebate has been well demonstrated by extensive experimental results
across eight datasets from four task types.

</details>


### [39] [An ASP-Based Framework for MUSes](https://arxiv.org/abs/2507.03929)
*Mohimenul Kabir,Kuldeep S Meel*

Main category: cs.AI

TL;DR: 论文提出了一种基于答案集编程（ASP）的框架MUS-ASP，用于在线枚举最小不可满足子集（MUS），并展示了其在MUS枚举和计数任务中的高效性。


<details>
  <summary>Details</summary>
Motivation: 理解不可满足公式的核心原因是关键应用需求，而MUS是捕捉这一原因的有效方法。当前研究集中在枚举MUS和计数MUS数量上。

Method: 通过将MUS枚举问题转化为答案集求解问题，利用ASP在知识表示和组合问题上的优势，设计MUS-ASP框架。

Result: 实验证明MUS-ASP在MUS枚举和计数任务中表现高效，尤其是在混合求解器中集成时。

Conclusion: MUS-ASP框架通过ASP的高效计算能力，显著提升了MUS枚举和计数的性能。

Abstract: Given an unsatisfiable formula, understanding the core reason for
unsatisfiability is crucial in several applications. One effective way to
capture this is through the minimal unsatisfiable subset (MUS), the
subset-minimal set of clauses that remains unsatisfiable. Current research
broadly focuses on two directions: (i) enumerating as many MUSes as possible
within a given time limit, and (ii) counting the total number of MUSes for a
given unsatisfiable formula.
  In this paper, we introduce an answer set programming-based framework, named
MUS-ASP, designed for online enumeration of MUSes. ASP is a powerful tool for
its strengths in knowledge representation and is particularly suitable for
specifying complex combinatorial problems. By translating MUS enumeration into
answer set solving, MUS-ASP leverages the computational efficiency of
state-of-the-art ASP systems. Our extensive experimental evaluation
demonstrates the effectiveness of MUS-ASP and highlights the acceleration in
both MUS enumeration and counting tasks, particularly when integrated within
hybrid solvers, including the framework proposed in this paper.

</details>


### [40] [Toward Better Generalisation in Uncertainty Estimators: Leveraging Data-Agnostic Features](https://arxiv.org/abs/2507.03998)
*Thuy An Ha,Bao Quoc Vo*

Main category: cs.AI

TL;DR: 论文探讨了如何通过结合数据无关特征和隐藏状态特征来提升大型语言模型（LLM）输出的不确定性量化，以改善跨领域性能。


<details>
  <summary>Details</summary>
Motivation: LLM常生成高自信但不准确的回答，需量化其不确定性以评估输出质量。隐藏状态特征虽有效，但跨领域泛化能力不足。

Method: 结合数据无关特征与隐藏状态特征，并筛选最具信息量的隐藏状态特征，评估混合特征集对跨领域性能的影响。

Result: 引入数据无关特征通常提升泛化性能，但某些情况下会降低效果；筛选隐藏状态特征后，数据无关特征的贡献不一致。

Conclusion: 混合特征集在某些情况下有效，但数据无关特征可能被低估，导致结果不一致。

Abstract: Large Language Models (LLMs) often generate responses that are factually
incorrect yet expressed with high confidence, which can pose serious risks for
end users. To address this, it is essential for LLMs not only to produce
answers but also to provide accurate estimates of their correctness.
Uncertainty quantification methods have been introduced to assess the quality
of LLM outputs, with factual accuracy being a key aspect of that quality. Among
these methods, those that leverage hidden states to train probes have shown
particular promise, as these internal representations encode information
relevant to the factuality of responses, making this approach the focus of this
paper. However, the probe trained on the hidden states of one dataset often
struggles to generalise to another dataset of a different task or domain. To
address this limitation, we explore combining data-agnostic features with
hidden-state features and assess whether this hybrid feature set enhances
out-of-domain performance. We further examine whether selecting only the most
informative hidden-state features, thereby discarding task-specific noise,
enables the data-agnostic features to contribute more effectively. The
experiment results indicate that although introducing data-agnostic features
generally enhances generalisation performance in most cases, in certain
scenarios their inclusion degrades performance. A similar pattern emerges when
retaining only the most important hidden-state features - adding data-agnostic
features does not consistently further enhance performance compared to using
the full set of hidden-state features. A closer analysis reveals that, in some
specific cases, the trained probe underweights the data-agnostic features
relative to the hidden-state features, which we believe is the main reason why
the results are inconclusive.

</details>


### [41] [Lyria: A General LLM-Driven Genetic Algorithm Framework for Problem Solving](https://arxiv.org/abs/2507.04034)
*Weizhi Tang,Kwabena Nuamah,Vaishak Belle*

Main category: cs.AI

TL;DR: Lyria是一个结合大型语言模型（LLMs）和遗传算法的框架，用于解决复杂问题，通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: LLMs在多目标优化、精确约束满足等问题上表现不足，需结合遗传算法的全局搜索能力。

Method: 提出Lyria框架，包含7个核心组件，结合LLMs的语义理解能力和遗传算法的优化能力。

Result: 在4种LLMs和3类问题上的实验证明Lyria有效，并通过7项消融实验分析性能影响因素。

Conclusion: Lyria成功结合LLMs和遗传算法，为解决复杂问题提供了有效方案。

Abstract: While Large Language Models (LLMs) have demonstrated impressive abilities
across various domains, they still struggle with complex problems characterized
by multi-objective optimization, precise constraint satisfaction, immense
solution spaces, etc. To address the limitation, drawing on the superior
semantic understanding ability of LLMs and also the outstanding global search
and optimization capability of genetic algorithms, we propose to capitalize on
their respective strengths and introduce Lyria, a general LLM-driven genetic
algorithm framework, comprising 7 essential components. Through conducting
extensive experiments with 4 LLMs across 3 types of problems, we demonstrated
the efficacy of Lyria. Additionally, with 7 additional ablation experiments, we
further systematically analyzed and elucidated the factors that affect its
performance.

</details>


### [42] [Ready Jurist One: Benchmarking Language Agents for Legal Intelligence in Dynamic Environments](https://arxiv.org/abs/2507.04037)
*Zheng Jia,Shengbin Yue,Wei Chen,Siyuan Wang,Yidong Liu,Yun Song,Zhongyu Wei*

Main category: cs.AI

TL;DR: 论文介绍了J1-ENVS（动态法律环境）和J1-EVAL（评估框架），用于测试LLM代理在动态法律场景中的表现，发现即使SOTA模型也难以达到60%的性能。


<details>
  <summary>Details</summary>
Motivation: 解决静态基准与动态法律实践之间的差距，推动法律智能的发展。

Method: 开发了J1-ENVS（交互式法律环境）和J1-EVAL（细粒度评估框架），并在17个LLM代理上进行实验。

Result: 多数模型具备法律知识，但在动态场景中程序执行表现不佳，GPT-4o总体性能不足60%。

Conclusion: 动态法律智能仍面临挑战，研究为未来方向提供了参考。

Abstract: The gap between static benchmarks and the dynamic nature of real-world legal
practice poses a key barrier to advancing legal intelligence. To this end, we
introduce J1-ENVS, the first interactive and dynamic legal environment tailored
for LLM-based agents. Guided by legal experts, it comprises six representative
scenarios from Chinese legal practices across three levels of environmental
complexity. We further introduce J1-EVAL, a fine-grained evaluation framework,
designed to assess both task performance and procedural compliance across
varying levels of legal proficiency. Extensive experiments on 17 LLM agents
reveal that, while many models demonstrate solid legal knowledge, they struggle
with procedural execution in dynamic settings. Even the SOTA model, GPT-4o,
falls short of 60% overall performance. These findings highlight persistent
challenges in achieving dynamic legal intelligence and offer valuable insights
to guide future research.

</details>


### [43] [HAWK: A Hierarchical Workflow Framework for Multi-Agent Collaboration](https://arxiv.org/abs/2507.04067)
*Yuyang Cheng,Yumiao Xu,Chaojia Yu,Yong Zhao*

Main category: cs.AI

TL;DR: HAWK是一个模块化框架，通过分层设计和标准化接口解决多智能体系统的互操作性、任务调度和资源共享问题。


<details>
  <summary>Details</summary>
Motivation: 多智能体系统在跨平台互操作性、动态任务调度和高效资源共享方面存在挑战，缺乏标准化接口和灵活的协作框架。

Method: HAWK框架包含五层（用户、工作流、操作、智能体和资源）和十六个标准化接口，支持任务解析、工作流编排、智能调度、资源调用和数据同步。

Result: 通过CreAgentive原型验证，HAWK显著提高了吞吐量，降低了调用复杂度，并增强了系统可控性。

Conclusion: HAWK展示了在多领域的应用潜力，并提出了未来研究方向，如幻觉缓解和实时性能优化。

Abstract: Contemporary multi-agent systems encounter persistent challenges in
cross-platform interoperability, dynamic task scheduling, and efficient
resource sharing. Agents with heterogeneous implementations often lack
standardized interfaces; collaboration frameworks remain brittle and hard to
extend; scheduling policies are static; and inter-agent state synchronization
is insufficient. We propose Hierarchical Agent Workflow (HAWK), a modular
framework comprising five layers-User, Workflow, Operator, Agent, and
Resource-and supported by sixteen standardized interfaces. HAWK delivers an
end-to-end pipeline covering task parsing, workflow orchestration, intelligent
scheduling, resource invocation, and data synchronization. At its core lies an
adaptive scheduling and optimization module in the Workflow Layer, which
harnesses real-time feedback and dynamic strategy adjustment to maximize
utilization. The Resource Layer provides a unified abstraction over
heterogeneous data sources, large models, physical devices, and third-party
services&tools, simplifying cross-domain information retrieval. We demonstrate
HAWK's scalability and effectiveness via CreAgentive, a multi-agent
novel-generation prototype, which achieves marked gains in throughput, lowers
invocation complexity, and improves system controllability. We also show how
hybrid deployments of large language models integrate seamlessly within HAWK,
highlighting its flexibility. Finally, we outline future research
avenues-hallucination mitigation, real-time performance tuning, and enhanced
cross-domain adaptability-and survey prospective applications in healthcare,
government, finance, and education.

</details>


### [44] [How to Train Your LLM Web Agent: A Statistical Diagnosis](https://arxiv.org/abs/2507.04103)
*Dheeraj Vattikonda,Santhoshi Ravichandran,Emiliano Penaloza,Hadi Nekoei,Megh Thakkar,Thibault Le Sellier de Chezelles,Nicolas Gontier,Miguel Muñoz-Mármol,Sahar Omidi Shayegan,Stefania Raimondo,Xue Liu,Alexandre Drouin,Laurent Charlin,Alexandre Piché,Alexandre Lacoste,Massimo Caccia*

Main category: cs.AI

TL;DR: 论文提出了一种两阶段训练方法（监督微调+强化学习），显著提升了开源LLM网络代理的性能，同时降低了计算成本。


<details>
  <summary>Details</summary>
Motivation: 开源LLM网络代理性能落后于闭源系统，主要受限于单步任务设计和高计算成本。

Method: 采用两阶段训练：先用Llama 3.1 8B模仿Llama 3.3 70B进行监督微调，再进行策略强化学习。通过1,370种配置采样和自举法优化超参数。

Result: 结合监督微调和强化学习的方法在WorkArena和MiniWob++上表现最佳，计算成本仅为纯监督微调的55%。

Conclusion: 该方法显著提升了开源代理的性能，缩小了与闭源系统的差距，并优化了计算效率。

Abstract: LLM-based web agents have recently made significant progress, but much of it
has occurred in closed-source systems, widening the gap with open-source
alternatives. Progress has been held back by two key challenges: first, a
narrow focus on single-step tasks that overlooks the complexity of multi-step
web interactions; and second, the high compute costs required to post-train
LLM-based web agents. To address this, we present the first statistically
grounded study on compute allocation for LLM web-agent post-training. Our
approach uses a two-stage pipeline, training a Llama 3.1 8B student to imitate
a Llama 3.3 70B teacher via supervised fine-tuning (SFT), followed by on-policy
reinforcement learning. We find this process highly sensitive to hyperparameter
choices, making exhaustive sweeps impractical. To spare others from expensive
trial-and-error, we sample 1,370 configurations and use bootstrapping to
estimate effective hyperparameters. Our results show that combining SFT with
on-policy RL consistently outperforms either approach alone on both WorkArena
and MiniWob++. Further, this strategy requires only 55% of the compute to match
the peak performance of pure SFT on MiniWob++, effectively pushing the
compute-performance Pareto frontier, and is the only strategy that can close
the gap with closed-source models.

</details>


### [45] [Enhancing Robustness of LLM-Driven Multi-Agent Systems through Randomized Smoothing](https://arxiv.org/abs/2507.04105)
*Jinwei Hu,Yi Dong,Zhengtao Ding,Xiaowei Huang*

Main category: cs.AI

TL;DR: 提出了一种基于随机平滑的防御框架，用于增强多智能体系统在安全关键领域的安全性。


<details>
  <summary>Details</summary>
Motivation: 提升大型语言模型驱动的多智能体系统在安全关键领域（如航空航天）的安全性。

Method: 应用随机平滑技术，采用两阶段自适应采样机制，平衡鲁棒性和计算效率。

Result: 模拟结果表明，该方法能有效防止对抗行为和幻觉传播，同时保持共识性能。

Conclusion: 为LLM驱动的多智能体系统在高风险环境中的安全部署提供了实用且可扩展的解决方案。

Abstract: This paper presents a defense framework for enhancing the safety of large
language model (LLM) empowered multi-agent systems (MAS) in safety-critical
domains such as aerospace. We apply randomized smoothing, a statistical
robustness certification technique, to the MAS consensus context, enabling
probabilistic guarantees on agent decisions under adversarial influence. Unlike
traditional verification methods, our approach operates in black-box settings
and employs a two-stage adaptive sampling mechanism to balance robustness and
computational efficiency. Simulation results demonstrate that our method
effectively prevents the propagation of adversarial behaviors and
hallucinations while maintaining consensus performance. This work provides a
practical and scalable path toward safe deployment of LLM-based MAS in
real-world, high-stakes environments.

</details>


### [46] [A Technical Survey of Reinforcement Learning Techniques for Large Language Models](https://arxiv.org/abs/2507.04136)
*Saksham Sahai Srivastava,Vaneet Aggarwal*

Main category: cs.AI

TL;DR: 本文综述了强化学习（RL）在大型语言模型（LLMs）中的应用，重点介绍了RLHF、RLAIF等算法及其在代码生成和推理等领域的应用，同时指出了奖励建模和反馈机制等挑战。


<details>
  <summary>Details</summary>
Motivation: 探讨如何通过强化学习解决LLMs在指令遵循、伦理对齐和推理能力方面的关键问题。

Method: 综述了PPO、Q-Learning、Actor-Critic等方法，以及RLHF、RLAIF、DPO、GRPO等专门针对LLMs的RL技术。

Result: RLHF在模型对齐中占主导地位，基于结果的RL（如RLVR）显著提升了逐步推理能力，但仍存在奖励攻击和计算成本等挑战。

Conclusion: 本文为研究人员提供了RL驱动LLM发展的路线图，强调了能力提升与安全性和可扩展性的平衡。

Abstract: Reinforcement Learning (RL) has emerged as a transformative approach for
aligning and enhancing Large Language Models (LLMs), addressing critical
challenges in instruction following, ethical alignment, and reasoning
capabilities. This survey offers a comprehensive foundation on the integration
of RL with language models, highlighting prominent algorithms such as Proximal
Policy Optimization (PPO), Q-Learning, and Actor-Critic methods. Additionally,
it provides an extensive technical overview of RL techniques specifically
tailored for LLMs, including foundational methods like Reinforcement Learning
from Human Feedback (RLHF) and AI Feedback (RLAIF), as well as advanced
strategies such as Direct Preference Optimization (DPO) and Group Relative
Policy Optimization (GRPO). We systematically analyze their applications across
domains, i.e., from code generation to tool-augmented reasoning. We also
present a comparative taxonomy based on reward modeling, feedback mechanisms,
and optimization strategies. Our evaluation highlights key trends. RLHF remains
dominant for alignment, and outcome-based RL such as RLVR significantly
improves stepwise reasoning. However, persistent challenges such as reward
hacking, computational costs, and scalable feedback collection underscore the
need for continued innovation. We further discuss emerging directions,
including hybrid RL algorithms, verifier-guided training, and multi-objective
alignment frameworks. This survey serves as a roadmap for researchers advancing
RL-driven LLM development, balancing capability enhancement with safety and
scalability.

</details>


### [47] [Mpemba Effect in Large-Language Model Training Dynamics: A Minimal Analysis of the Valley-River model](https://arxiv.org/abs/2507.04206)
*Sibei Liu,Zhijian Hu*

Main category: cs.AI

TL;DR: 论文通过热力学类比（Mpemba效应）解释了LLM训练中学习率调度（WSD策略）的机制，提出高平台学习率能加速损失下降，并推导了最优学习率（强Mpemba点）的存在条件。


<details>
  <summary>Details</summary>
Motivation: 探索LLM训练中学习率调度（WSD策略）的机制，解决其选择依赖经验的问题。

Method: 利用热力学类比（Mpemba效应）分析“谷-河”损失景观，推导最优平台学习率及其存在条件。

Result: 高平台学习率能加速损失下降，存在“强Mpemba点”使收敛更快。

Conclusion: 为平台学习率调度提供了理论依据，指导LLM学习率调优。

Abstract: Learning rate (LR) schedules in large language model (LLM) training often
follow empirical templates: warm-up, constant plateau/stable phase, and decay
(WSD). However, the mechanistic explanation for this strategy remains
underexplored, and the choice of plateau height and decay schedule is largely
heuristic. In this paper, we connect training dynamics to a thermodynamic
analogy via the Mpemba effect - a phenomenon in which a hotter system cools
faster than a colder one when quenched into the same bath. We analyze a class
of "valley-river" loss landscapes, where sharp (valley) directions equilibrate
quickly, while flatter (river) directions govern global descent. The Mpemba
effect provides an explanation for the necessity of the warm-up phase and
motivates a high plateau - rather than a low one - for accelerating loss
decrease during decay. We show that for certain loss landscapes, there exists
an optimal plateau learning rate - the "strong Mpemba point" - at which the
slowest mode vanishes, resulting in faster convergence during the decay phase.
We derive analytical conditions for its existence and estimate decay dynamics
required to preserve the Mpemba advantage. Our minimal model and analysis offer
a principled justification for plateau-based schedulers and provide guidance
for tuning LR in LLMs with minimal hyperparameter sweep.

</details>


### [48] [Clustering via Self-Supervised Diffusion](https://arxiv.org/abs/2507.04283)
*Roy Uziel,Irit Chelly,Oren Freifeld,Ari Pakman*

Main category: cs.AI

TL;DR: CLUDI是一种结合扩散模型和预训练Vision Transformer的自监督聚类框架，通过师生范式实现高性能聚类。


<details>
  <summary>Details</summary>
Motivation: 扩散模型在生成任务中表现优异，但尚未应用于聚类任务，因此提出CLUDI以填补这一空白。

Method: CLUDI采用师生范式，教师模型通过扩散采样生成多样聚类分配，学生模型优化为稳定预测。

Result: CLUDI在多个数据集上实现了最先进的聚类性能，表现出对复杂数据分布的强适应性。

Conclusion: CLUDI为聚类任务提供了一种新的高效方法，具有广泛的应用潜力。

Abstract: Diffusion models, widely recognized for their success in generative tasks,
have not yet been applied to clustering. We introduce Clustering via Diffusion
(CLUDI), a self-supervised framework that combines the generative power of
diffusion models with pre-trained Vision Transformer features to achieve robust
and accurate clustering. CLUDI is trained via a teacher-student paradigm: the
teacher uses stochastic diffusion-based sampling to produce diverse cluster
assignments, which the student refines into stable predictions. This
stochasticity acts as a novel data augmentation strategy, enabling CLUDI to
uncover intricate structures in high-dimensional data. Extensive evaluations on
challenging datasets demonstrate that CLUDI achieves state-of-the-art
performance in unsupervised classification, setting new benchmarks in
clustering robustness and adaptability to complex data distributions.

</details>


### [49] [Answer Set Programming Modulo Theories and Reasoning about Continuous Changes](https://arxiv.org/abs/2507.04299)
*Joohyung Lee,Yunsong Meng*

Main category: cs.AI

TL;DR: ASPMT是ASP与SMT紧密结合的新框架，类似于一阶逻辑与SMT的关系，通过固定背景理论的解释实现。类似ASP与SAT的关系，"紧"ASPMT程序可转换为SMT实例。ASPMT通过增强动作语言C+处理连续和离散变化，并展示SMT求解器可用于计算该语言。


<details>
  <summary>Details</summary>
Motivation: 结合ASP与SMT的优势，处理连续和离散变化，扩展动作语言C+的能力。

Method: 基于功能稳定模型语义，固定背景理论的解释，将ASPMT程序转换为SMT实例。

Result: 成功将ASPMT应用于动作语言C+，支持连续资源累积效应的表示。

Conclusion: ASPMT为ASP与SMT的结合提供了有效框架，扩展了动作语言的应用范围。

Abstract: Answer Set Programming Modulo Theories (ASPMT) is a new framework of tight
integration of answer set programming (ASP) and satisfiability modulo theories
(SMT). Similar to the relationship between first-order logic and SMT, it is
based on a recent proposal of the functional stable model semantics by fixing
interpretations of background theories. Analogously to a known relationship
between ASP and SAT, ``tight'' ASPMT programs can be translated into SMT
instances. We demonstrate the usefulness of ASPMT by enhancing action language
C+ to handle continuous changes as well as discrete changes. We reformulate the
semantics of C+ in terms ofASPMT, and show that SMT solvers can be used to
compute the language. We also show how the language can represent cumulative
effects on continuous resources.

</details>


### [50] [Voltage Mode Winner-Take-All Circuit for Neuromorphic Systems](https://arxiv.org/abs/2507.04338)
*Abdullah M. Zyarah,Dhireesha Kudithipudi*

Main category: cs.AI

TL;DR: 提出了一种可配置的winner-take-all电路，支持k-winner和滞后特性，功耗低，延迟小，适用于空间滤波和分类。


<details>
  <summary>Details</summary>
Motivation: 利用神经形态计算实现低功耗的片上学习，winner-take-all电路是关键学习单元。

Method: 在IBM 65 nm工艺节点上模拟了一种可配置的winner-take-all电路。

Result: 电路功耗为34.9 μW，延迟10.4 ns，可处理1000个输入。

Conclusion: 该电路在空间滤波和分类中表现出实用性。

Abstract: Recent advances in neuromorphic computing demonstrate on-device learning
capabilities with low power consumption. One of the key learning units in these
systems is the winner-take-all circuit. In this research, we propose a
winner-take-all circuit that can be configured to achieve k-winner and
hysteresis properties, simulated in IBM 65 nm node. The circuit dissipated 34.9
$\mu$W of power with a latency of 10.4 ns, while processing 1000 inputs. The
utility of the circuit is demonstrated for spatial filtering and
classification.

</details>


### [51] [SmartThinker: Learning to Compress and Preserve Reasoning by Step-Level Length Control](https://arxiv.org/abs/2507.04348)
*Xingyang He,Xiao Ling,Jie Liu*

Main category: cs.AI

TL;DR: SmartThinker框架通过两阶段学习，实现对推理链长度的细粒度控制，减少冗余推理，同时保持或提升性能。


<details>
  <summary>Details</summary>
Motivation: 大型推理模型存在冗余和低效问题，现有方法通过全局长度惩罚导致关键步骤过度压缩或保留不必要细节，需要更精细的控制。

Method: SmartThinker采用两阶段框架：1）通过拒绝采样和监督微调适应短推理模式；2）通过SCPO优化模型输出分布，实现步骤级长度控制。

Result: 实验表明，SmartThinker显著减少冗余推理，性能与现有方法相当或更优。

Conclusion: SmartThinker提供了一种高效且精细的推理长度控制方法，解决了现有方法的不足。

Abstract: Large reasoning models (LRMs) have exhibited remarkable reasoning
capabilities through inference-time scaling, but this progress has also
introduced considerable redundancy and inefficiency into their reasoning
processes, resulting in substantial computational waste. Previous work has
attempted to mitigate this issue by penalizing the overall length of generated
samples during reinforcement learning (RL), with the goal of encouraging a more
concise chains of thought. However, we observe that such global length penalty
often lead to excessive compression of critical reasoning steps while
preserving unnecessary details in simpler ones, yielding a suboptimal trade-off
between accuracy and efficiency. To address this issue, we propose
SmartThinker, a two-stage learnable framework designed to enable fine-grained
control over the length of reasoning chains based on the importance of each
individual step. In the first stage, SmartThinker adapts a reasoning model to a
short-form reasoning mode through rejection sampling combined with supervised
fine-tuning (SFT). In the second stage, SmartThinker applies Step-Level Length
Control Policy Optimization (SCPO) to refine the model output distribution,
which increases the proportion of length allocated to critical steps while
reducing redundancy in less important ones. SCPO consists of four core
components: an online importance estimator, a step-level length control reward
function, a step-level generalized advantage estimation (S-GAE) and a
difficulty-adaptive clipping strategy. Working in concert, these components
enable SCPO to implement differentiated length control across reasoning steps.
Empirical results across multiple reasoning benchmarks and various backbone
models demonstrate that SmartThinker significantly reduces redundant reasoning
while achieving comparable or even superior performance to existing methods.

</details>


### [52] [WebSynthesis: World-Model-Guided MCTS for Efficient WebUI-Trajectory Synthesis](https://arxiv.org/abs/2507.04370)
*Yifei Gao,Junhong Ye,Jiaqi Wang,Jitao Sang*

Main category: cs.AI

TL;DR: WebSynthesis框架通过虚拟环境模拟和树状规划，解决了复杂网页环境中代理训练的高成本和不可控性问题。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在网页代理训练中遇到的环境状态不可控和高API成本问题。

Method: 提出WebSynthesis框架，利用学习的世界模型模拟虚拟网页环境，支持高效的树状规划和大规模轨迹生成。

Result: 实验表明，使用小规模合成数据训练的代理性能可媲美或超越基于大规模真实数据训练的模型。

Conclusion: WebSynthesis为代理的自我改进提供了一种可扩展且高效的解决方案。

Abstract: Recent advancements in large language models (LLMs) have significantly
improved the capabilities of web agents. However, effectively navigating
complex and dynamic web environments still requires more advanced
trajectory-level planning and execution. Prior studies have addressed
self-improving agents by collecting extensive GUI trajectories from
real-environment interactions. Despite their effectiveness, these approaches
encounter two critical challenges: (1) Uncontrollable environment states, where
real or sandboxed web environments often yield unstable and non-deterministic
feedback, complicating the reproduction and debugging of agent behaviors; and
(2) High API costs, as generating even a single interaction trajectory can
involve hundreds of queries, leading to considerable API usage and
computational expenses. To address these limitations and enable scalable
self-improvement for agents, we propose WebSynthesis, a novel framework for
trajectory synthesis and training. WebSynthesis leverages a learned world model
to simulate virtual web environments, allowing a policy agent to perform
efficient and reversible tree-based planning. This approach supports the
large-scale generation of diverse and high-quality trajectories, which are
subsequently utilized to refine the agent's policy. Experimental results
demonstrate that an agent trained using WebSynthesis on a small-scale synthetic
dataset achieves performance comparable to or even surpassing that of models
trained on large-scale real-world data.

</details>


### [53] [MOD-X: A Modular Open Decentralized eXchange Framework proposal for Heterogeneous Interoperable Artificial Agents](https://arxiv.org/abs/2507.04376)
*Georgios Ioannides,Christos Constantinou,Vinija Jain,Aman Chadha,Aaron Elkins*

Main category: cs.AI

TL;DR: MOD-X是一个新型的模块化开放去中心化交换框架，旨在解决异构智能体之间的互操作性问题，通过分层架构、通用消息总线、状态管理和区块链安全机制实现。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统从单一模型发展为专业化智能体生态系统，标准化通信协议的需求日益迫切。

Method: MOD-X提出分层架构，包括通用消息总线、状态管理、翻译能力和区块链安全机制，支持发布-订阅通信模型、语义能力发现和动态工作流编排。

Result: MOD-X能够有效集成异构智能体（如基于规则的系统、神经网络、符号推理引擎等），无需中心化协调。

Conclusion: MOD-X为去中心化、可扩展的智能体生态系统提供了理论与实践结合的解决方案。

Abstract: As Artificial Intelligence systems evolve from monolithic models to
ecosystems of specialized agents, the need for standardized communication
protocols becomes increasingly critical. This paper introduces MOD-X (Modular
Open Decentralized eXchange), a novel architectural framework proposal for
agent interoperability that addresses key limitations of existing protocols.
Unlike current approaches, MOD-X proposes a layered architecture with a
Universal Message Bus, thorough state management, translation capabilities, and
blockchain-based security mechanisms. We present MOD-X's architecture, compare
it with existing protocols, and demonstrate its application through a worked
example how it enables integration between heterogeneous specialist agents
(agents with different architectures, vendors, capabilities, and knowledge
representations--including rule-based systems, neural networks, symbolic
reasoning engines, and legacy software with agent wrappers). MOD-X's key
innovations include a publish-subscribe communication model, semantic
capability discovery, and dynamic workflow orchestration--providing a framework
that bridges theoretical formalism with practical implementation. This
architecture addresses the growing need for truly decentralized, interoperable
agent ecosystems that can scale effectively without the need for central
coordination.

</details>


### [54] [DC-Mamber: A Dual Channel Prediction Model based on Mamba and Linear Transformer for Multivariate Time Series Forecasting](https://arxiv.org/abs/2507.04381)
*Bing Fan,Shusen Ma,Yun-Bo Zhao,Yu Kang*

Main category: cs.AI

TL;DR: 论文提出DC-Mamber模型，结合Mamba和线性Transformer，通过双通道策略分别提取局部和全局时间特征，提升多元时间序列预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 现有方法（如Transformer和Mamba）在多元时间序列预测中存在局部模式敏感性不足或全局上下文聚合能力有限的问题，需结合两者优势。

Method: 提出DC-Mamber模型，使用Mamba通道（通道独立）提取变量内特征，线性Transformer通道（通道混合）建模全局依赖，并通过融合层整合。

Result: 在八个公开数据集上验证，DC-Mamber的预测准确性优于现有模型。

Conclusion: DC-Mamber通过双通道设计有效结合局部和全局特征，显著提升了多元时间序列预测性能。

Abstract: In multivariate time series forecasting (MTSF), existing strategies for
processing sequences are typically categorized as channel-independent and
channel-mixing. The former treats all temporal information of each variable as
a token, focusing on capturing local temporal features of individual variables,
while the latter constructs a token from the multivariate information at each
time step, emphasizing the modeling of global temporal dependencies. Current
mainstream models are mostly based on Transformer and the emerging Mamba.
Transformers excel at modeling global dependencies through self-attention
mechanisms but exhibit limited sensitivity to local temporal patterns and
suffer from quadratic computational complexity, restricting their efficiency in
long-sequence processing. In contrast, Mamba, based on state space models
(SSMs), achieves linear complexity and efficient long-range modeling but
struggles to aggregate global contextual information in parallel. To overcome
the limitations of both models, we propose DC-Mamber, a dual-channel
forecasting model based on Mamba and linear Transformer for time series
forecasting. Specifically, the Mamba-based channel employs a
channel-independent strategy to extract intra-variable features, while the
Transformer-based channel adopts a channel-mixing strategy to model
cross-timestep global dependencies. DC-Mamber first maps the raw input into two
distinct feature representations via separate embedding layers. These
representations are then processed by a variable encoder (built on Mamba) and a
temporal encoder (built on linear Transformer), respectively. Finally, a fusion
layer integrates the dual-channel features for prediction. Extensive
experiments on eight public datasets confirm DC-Mamber's superior accuracy over
existing models.

</details>


### [55] [LayerCake: Token-Aware Contrastive Decoding within Large Language Model Layers](https://arxiv.org/abs/2507.04404)
*Jingze Zhu,Yongliang Wu,Wenbo Zhu,Jiawang Cao,Yanqiang Zheng,Jiawei Chen,Xu Yang,Bernt Schiele,Jonas Fischer,Xinting Hu*

Main category: cs.AI

TL;DR: 提出了一种基于令牌感知和层定位的对比解码方法，通过联合分析令牌和层信号，提升大型语言模型的事实生成准确性。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在知识密集型任务中因事实错误而受限，现有方法未能充分利用令牌和层信号的联合动态。

Method: 通过注意力分析识别令牌类型与层的关系，选择性抑制特定令牌类型的注意力，生成对比信号指导解码。

Result: 实验表明，该方法无需额外训练或模型修改，显著提升了多种大型语言模型和基准测试的事实性。

Conclusion: 该方法为提升语言模型事实性提供了一种高效且无需训练的解码策略。

Abstract: Large language models (LLMs) excel at natural language understanding and
generation but remain vulnerable to factual errors, limiting their reliability
in knowledge-intensive tasks. While decoding-time strategies provide a
promising efficient solution without training, existing methods typically treat
token-level and layer-level signals in isolation, overlooking the joint
dynamics between them. In this work, we introduce a token-aware,
layer-localized contrastive decoding method that aligns specific token types
with their most influential transformer layers to improve factual generation.
Through empirical attention analysis, we identify two key patterns: punctuation
tokens receive dominant attention in early layers, while conceptual tokens
govern semantic reasoning in intermediate layers. By selectively suppressing
attention to these token types at their respective depths, we achieve the
induction of controlled factual degradation and derive contrastive signals to
guide the final factual decoding. Our method requires no additional training or
model modification, and experiments demonstrate that our method consistently
improves factuality across multiple LLMs and various benchmarks.

</details>


### [56] [ARMR: Adaptively Responsive Network for Medication Recommendation](https://arxiv.org/abs/2507.04428)
*Feiyue Wu,Tianxing Wu,Shenqi Jing*

Main category: cs.AI

TL;DR: 论文提出了一种自适应响应网络（ARMR），用于药物推荐，通过区分近期和远期患者历史以及动态调整对新旧药物的关注，提高了推荐的准确性和个性化。


<details>
  <summary>Details</summary>
Motivation: 现有药物推荐方法难以平衡历史药物复用与新药物引入，尤其是在患者病情变化时。

Method: ARMR结合了分段时间学习组件和自适应响应机制，动态调整对新旧药物的关注。

Result: 在MIMIC-III和MIMIC-IV数据集上的实验表明，ARMR在不同评估指标上优于现有基线方法。

Conclusion: ARMR为复杂医疗条件下的患者提供了更个性化和准确的药物推荐。

Abstract: Medication recommendation is a crucial task in healthcare, especially for
patients with complex medical conditions. However, existing methods often
struggle to effectively balance the reuse of historical medications with the
introduction of new drugs in response to the changing patient conditions. In
order to address this challenge, we propose an Adaptively Responsive network
for Medication Recommendation (ARMR), a new method which incorporates 1) a
piecewise temporal learning component that distinguishes between recent and
distant patient history, enabling more nuanced temporal understanding, and 2)
an adaptively responsive mechanism that dynamically adjusts attention to new
and existing drugs based on the patient's current health state and medication
history. Experiments on the MIMIC-III and MIMIC-IV datasets indicate that ARMR
has better performance compared with the state-of-the-art baselines in
different evaluation metrics, which contributes to more personalized and
accurate medication recommendations. The source code is publicly avaiable at:
https://github.com/seucoin/armr2.

</details>


### [57] [MedGellan: LLM-Generated Medical Guidance to Support Physicians](https://arxiv.org/abs/2507.04431)
*Debodeep Banerjee,Burcu Sayin,Stefano Teso,Andrea Passerini*

Main category: cs.AI

TL;DR: MedGellan是一个轻量级、无需标注的框架，利用大型语言模型（LLM）从原始医疗记录生成临床指导，帮助医生诊断。


<details>
  <summary>Details</summary>
Motivation: 医疗决策中的错误可能导致严重后果，因此需要结合机器智能与人类监督的混合框架。

Method: 采用贝叶斯启发的提示策略，尊重临床数据的时间顺序，生成诊断指导。

Result: 初步实验表明，MedGellan生成的指导提高了诊断性能，尤其是在召回率和F1分数上。

Conclusion: MedGellan为医疗决策提供了一种实用的混合框架，结合了机器智能与人类专业知识。

Abstract: Medical decision-making is a critical task, where errors can result in
serious, potentially life-threatening consequences. While full automation
remains challenging, hybrid frameworks that combine machine intelligence with
human oversight offer a practical alternative. In this paper, we present
MedGellan, a lightweight, annotation-free framework that uses a Large Language
Model (LLM) to generate clinical guidance from raw medical records, which is
then used by a physician to predict diagnoses. MedGellan uses a
Bayesian-inspired prompting strategy that respects the temporal order of
clinical data. Preliminary experiments show that the guidance generated by the
LLM with MedGellan improves diagnostic performance, particularly in recall and
$F_1$ score.

</details>


### [58] [A Linguistic Analysis of Spontaneous Thoughts: Investigating Experiences of Déjà Vu, Unexpected Thoughts, and Involuntary Autobiographical Memories](https://arxiv.org/abs/2507.04439)
*Videep Venkatesha,Mary Cati Poulos,Christopher Steadman,Caitlin Mills,Anne M. Cleary,Nathaniel Blanchard*

Main category: cs.AI

TL;DR: 该研究通过分析语言特征，探讨了Deja Vu、非自愿自传体记忆和意外思维的内在模式，验证并更新了现有理论。


<details>
  <summary>Details</summary>
Motivation: 研究自发思维的动态交互作用，尤其是语言如何揭示这些认知状态的内在特征。

Method: 分析参与者对这些思维类型的描述中的语言模式。

Result: Deja Vu以抽象和空间语言为特征，非自愿自传体记忆富含个人情感细节，意外思维则表现为不可预测性和认知中断。

Conclusion: 语言可作为揭示自发认知状态的窗口，为相关理论提供新支持。

Abstract: The onset of spontaneous thoughts are reflective of dynamic interactions
between cognition, emotion, and attention. Typically, these experiences are
studied through subjective appraisals that focus on their triggers,
phenomenology, and emotional salience. In this work, we use linguistic
signatures to investigate Deja Vu, Involuntary Autobiographical Memories and
Unexpected Thoughts. Specifically, we analyze the inherent characteristics of
the linguistic patterns in participant generated descriptions of these thought
types. We show how, by positioning language as a window into spontaneous
cognition, existing theories on these attentional states can be updated and
reaffirmed. Our findings align with prior research, reinforcing that Deja Vu is
a metacognitive experience characterized by abstract and spatial language,
Involuntary Autobiographical Memories are rich in personal and emotionally
significant detail, and Unexpected Thoughts are marked by unpredictability and
cognitive disruption. This work is demonstrative of languages potential to
reveal deeper insights into how internal spontaneous cognitive states manifest
through expression.

</details>


### [59] [Anomalous Decision Discovery using Inverse Reinforcement Learning](https://arxiv.org/abs/2507.04464)
*Ashish Bastola,Mert D. Pesé,Long Cheng,Jonathon Smereka,Abolfazl Razi*

Main category: cs.AI

TL;DR: 提出了一种基于逆向强化学习（IRL）的异常检测框架TRAP，通过隐式学习时间信用分配，提高了噪声鲁棒性和对未见场景的泛化能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法在未见场景、传感器噪声和遮挡情况下效果不佳，且需要大量标注数据，限制了实际应用。

Method: 采用逆向强化学习（IRL）框架TRAP，通过隐式学习时间信用分配和预训练，实现早期异常检测。

Result: 在14,000+模拟轨迹上表现优异，AUC达0.90，F1-score为82.2%，优于基线方法。

Conclusion: TRAP框架在噪声鲁棒性和泛化能力上表现突出，为自动驾驶异常检测提供了有效解决方案。

Abstract: Anomaly detection plays a critical role in Autonomous Vehicles (AVs) by
identifying unusual behaviors through perception systems that could compromise
safety and lead to hazardous situations. Current approaches, which often rely
on predefined thresholds or supervised learning paradigms, exhibit reduced
efficacy when confronted with unseen scenarios, sensor noise, and occlusions,
leading to potential safety-critical failures. Moreover, supervised methods
require large annotated datasets, limiting their real-world feasibility. To
address these gaps, we propose an anomaly detection framework based on Inverse
Reinforcement Learning (IRL) to infer latent driving intentions from sequential
perception data, thus enabling robust identification. Specifically, we present
Trajectory-Reward Guided Adaptive Pre-training (TRAP), a novel IRL framework
for anomaly detection, to address two critical limitations of existing methods:
noise robustness and generalization to unseen scenarios. Our core innovation is
implicitly learning temporal credit assignments via reward and worst-case
supervision. We leverage pre-training with variable-horizon sampling to
maximize time-to-consequence, resulting in early detection of behavior
deviation. Experiments on 14,000+ simulated trajectories demonstrate
state-of-the-art performance, achieving 0.90 AUC and 82.2\% F1-score -
outperforming similarly trained supervised and unsupervised baselines by 39\%
on Recall and 12\% on F1-score, respectively. Similar performance is achieved
while exhibiting robustness to various noise types and generalization to unseen
anomaly types. Our code will be available at:
https://github.com/abastola0/TRAP.git

</details>


### [60] [Thousand-Brains Systems: Sensorimotor Intelligence for Rapid, Robust Learning and Inference](https://arxiv.org/abs/2507.04494)
*Niels Leadholm,Viviane Clay,Scott Knudstrup,Hojae Lee,Jeff Hawkins*

Main category: cs.AI

TL;DR: 论文提出了一种名为Monty的千脑系统，模拟生物大脑皮层柱的架构，用于3D物体感知任务，展示了其在快速学习、泛化和高效推理方面的潜力。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统缺乏生物智能的核心特性，如快速持续学习、基于感知运动的表征和结构化知识。受神经科学启发，研究旨在通过模拟皮层柱架构缩小生物与人工智能的差距。

Method: 利用YCB数据集，评估Monty系统在3D物体识别和姿态估计任务中的表现，包括其感知运动学习、模块化架构和投票算法。

Result: Monty通过结构化表征实现了鲁棒的泛化能力，支持快速推理，并通过Hebbian-like绑定实现了高效学习，优于现有深度学习架构。

Conclusion: 千脑系统（如Monty）是一种有前景的AI新方法，但仍处于早期发展阶段。

Abstract: Current AI systems achieve impressive performance on many tasks, yet they
lack core attributes of biological intelligence, including rapid, continual
learning, representations grounded in sensorimotor interactions, and structured
knowledge that enables efficient generalization. Neuroscience theory suggests
that mammals evolved flexible intelligence through the replication of a
semi-independent, sensorimotor module, a functional unit known as a cortical
column. To address the disparity between biological and artificial
intelligence, thousand-brains systems were proposed as a means of mirroring the
architecture of cortical columns and their interactions.
  In the current work, we evaluate the unique properties of Monty, the first
implementation of a thousand-brains system. We focus on 3D object perception,
and in particular, the combined task of object recognition and pose estimation.
Utilizing the YCB dataset of household objects, we first assess Monty's use of
sensorimotor learning to build structured representations, finding that these
enable robust generalization. These representations include an emphasis on
classifying objects by their global shape, as well as a natural ability to
detect object symmetries. We then explore Monty's use of model-free and
model-based policies to enable rapid inference by supporting principled
movements. We find that such policies complement Monty's modular architecture,
a design that can accommodate communication between modules to further
accelerate inference speed via a novel `voting' algorithm. Finally, we examine
Monty's use of associative, Hebbian-like binding to enable rapid, continual,
and computationally efficient learning, properties that compare favorably to
current deep learning architectures. While Monty is still in a nascent stage of
development, these findings support thousand-brains systems as a powerful and
promising new approach to AI.

</details>


### [61] [Churn-Aware Recommendation Planning under Aggregated Preference Feedback](https://arxiv.org/abs/2507.04513)
*Gur Keinan,Omer Ben-Porat*

Main category: cs.AI

TL;DR: 论文研究了在隐私保护限制下推荐系统的序列决策问题，提出Rec-APC模型，证明最优策略会收敛到纯利用，并通过实验验证其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 研究在用户数据受限（仅能获取群体偏好信息）的情况下，推荐系统如何有效进行个性化推荐，同时避免用户流失。

Method: 提出Rec-APC模型，利用贝叶斯更新处理用户反馈，设计分支定界算法计算最优策略。

Result: 实验证明Rec-APC在用户类型较多时优于POMDP方法SARSOP，且策略能快速收敛。

Conclusion: Rec-APC为聚合偏好数据下的决策问题提供了新思路，展示了其实际应用潜力。

Abstract: We study a sequential decision-making problem motivated by recent regulatory
and technological shifts that limit access to individual user data in
recommender systems (RSs), leaving only population-level preference
information. This privacy-aware setting poses fundamental challenges in
planning under uncertainty: Effective personalization requires exploration to
infer user preferences, yet unsatisfactory recommendations risk immediate user
churn. To address this, we introduce the Rec-APC model, in which an anonymous
user is drawn from a known prior over latent user types (e.g., personas or
clusters), and the decision-maker sequentially selects items to recommend.
Feedback is binary -- positive responses refine the posterior via Bayesian
updates, while negative responses result in the termination of the session.
  We prove that optimal policies converge to pure exploitation in finite time
and propose a branch-and-bound algorithm to efficiently compute them.
Experiments on synthetic and MovieLens data confirm rapid convergence and
demonstrate that our method outperforms the POMDP solver SARSOP, particularly
when the number of user types is large or comparable to the number of content
categories. Our results highlight the applicability of this approach and
inspire new ways to improve decision-making under the constraints imposed by
aggregated preference data.

</details>


### [62] [Towards integration of Privacy Enhancing Technologies in Explainable Artificial Intelligence](https://arxiv.org/abs/2507.04528)
*Sonal Allana,Rozita Dara,Xiaodong Lin,Pulei Xiong*

Main category: cs.AI

TL;DR: 论文探讨了可解释人工智能（XAI）中隐私泄露问题，并提出隐私增强技术（PETs）作为防御机制，实验表明PETs能有效降低攻击风险。


<details>
  <summary>Details</summary>
Motivation: XAI方法在提供透明决策的同时可能泄露个人隐私，目前缺乏针对此类隐私攻击的防御措施。

Method: 研究采用三种PETs（合成训练数据、差分隐私训练和噪声添加）对两类基于特征的XAI方法进行实证评估。

Result: PETs集成最高可降低攻击风险49.47%，同时保持模型效用和解释质量。

Conclusion: 通过评估，提出了在XAI中使用PETs的策略，以最大化隐私保护效果并最小化攻击成功率。

Abstract: Explainable Artificial Intelligence (XAI) is a crucial pathway in mitigating
the risk of non-transparency in the decision-making process of black-box
Artificial Intelligence (AI) systems. However, despite the benefits, XAI
methods are found to leak the privacy of individuals whose data is used in
training or querying the models. Researchers have demonstrated privacy attacks
that exploit explanations to infer sensitive personal information of
individuals. Currently there is a lack of defenses against known privacy
attacks targeting explanations when vulnerable XAI are used in production and
machine learning as a service system. To address this gap, in this article, we
explore Privacy Enhancing Technologies (PETs) as a defense mechanism against
attribute inference on explanations provided by feature-based XAI methods. We
empirically evaluate 3 types of PETs, namely synthetic training data,
differentially private training and noise addition, on two categories of
feature-based XAI. Our evaluation determines different responses from the
mitigation methods and side-effects of PETs on other system properties such as
utility and performance. In the best case, PETs integration in explanations
reduced the risk of the attack by 49.47%, while maintaining model utility and
explanation quality. Through our evaluation, we identify strategies for using
PETs in XAI for maximizing benefits and minimizing the success of this privacy
attack on sensitive personal information.

</details>


### [63] [Exploring Core and Periphery Precepts in Biological and Artificial Intelligence: An Outcome-Based Perspective](https://arxiv.org/abs/2507.04594)
*Niloofar Shadab,Tyler Cody,Alejandro Salado,Taylan G. Topcu,Mohammad Shadab,Peter Beling*

Main category: cs.AI

TL;DR: 论文提出了一种新的系统原则'核心与外围'，用于智能系统的工程化，并通过实证验证其在生物和人工智能系统中的适用性。


<details>
  <summary>Details</summary>
Motivation: 传统工程方法在处理智能系统时存在局限性，需要新的系统原则来解决智能扩展问题。

Method: 基于抽象系统理论和必要多样性法则，提出了'核心与外围'原则，并通过数学定义和实证研究验证其适用性。

Result: 实证研究表明，'核心与外围'原则适用于生物和人工智能系统，并扩展了理论框架。

Conclusion: '核心与外围'原则为智能系统工程提供了新的理论支持，具有实际应用价值。

Abstract: Engineering methodologies predominantly revolve around established principles
of decomposition and recomposition. These principles involve partitioning
inputs and outputs at the component level, ensuring that the properties of
individual components are preserved upon composition. However, this view does
not transfer well to intelligent systems, particularly when addressing the
scaling of intelligence as a system property. Our prior research contends that
the engineering of general intelligence necessitates a fresh set of overarching
systems principles. As a result, we introduced the "core and periphery"
principles, a novel conceptual framework rooted in abstract systems theory and
the Law of Requisite Variety. In this paper, we assert that these abstract
concepts hold practical significance. Through empirical evidence, we illustrate
their applicability to both biological and artificial intelligence systems,
bridging abstract theory with real-world implementations. Then, we expand on
our previous theoretical framework by mathematically defining core-dominant vs
periphery-dominant systems.

</details>


### [64] [DisMS-TS: Eliminating Redundant Multi-Scale Features for Time Series Classification](https://arxiv.org/abs/2507.04600)
*Zhipeng Liu,Peibo Duan,Binwu Wang,Xuan Tang,Qi Chu,Changsheng Zhang,Yongsheng Huang,Bin Zhang*

Main category: cs.AI

TL;DR: 提出了一种新的端到端解耦多尺度时间序列分类框架（DisMS-TS），通过消除多尺度时间序列中的冗余共享特征，提升分类性能。


<details>
  <summary>Details</summary>
Motivation: 现有方法在多尺度时间序列分析中未能消除冗余的共享特征，导致模型对共享特征过度或不足关注。

Method: 提出时间解耦模块分别捕获共享和特定尺度的时序表示，并引入正则化项确保共享表示的一致性和特定表示的差异性。

Result: 在多个数据集上的实验表明，DisMS-TS优于基线方法，准确率提升高达9.71%。

Conclusion: DisMS-TS通过解耦多尺度特征，显著提升了时间序列分类的性能。

Abstract: Real-world time series typically exhibit complex temporal variations, making
the time series classification task notably challenging. Recent advancements
have demonstrated the potential of multi-scale analysis approaches, which
provide an effective solution for capturing these complex temporal patterns.
However, existing multi-scale analysis-based time series prediction methods
fail to eliminate redundant scale-shared features across multi-scale time
series, resulting in the model over- or under-focusing on scale-shared
features. To address this issue, we propose a novel end-to-end Disentangled
Multi-Scale framework for Time Series classification (DisMS-TS). The core idea
of DisMS-TS is to eliminate redundant shared features in multi-scale time
series, thereby improving prediction performance. Specifically, we propose a
temporal disentanglement module to capture scale-shared and scale-specific
temporal representations, respectively. Subsequently, to effectively learn both
scale-shared and scale-specific temporal representations, we introduce two
regularization terms that ensure the consistency of scale-shared
representations and the disparity of scale-specific representations across all
temporal scales. Extensive experiments conducted on multiple datasets validate
the superiority of DisMS-TS over its competitive baselines, with the accuracy
improvement up to 9.71%.

</details>


### [65] [Can Prompt Difficulty be Online Predicted for Accelerating RL Finetuning of Reasoning Models?](https://arxiv.org/abs/2507.04632)
*Yun Qu,Qi Cheems Wang,Yixiu Mao,Vincent Tao Hu,Xiangyang Ji*

Main category: cs.AI

TL;DR: MoPPS是一种贝叶斯风险预测框架，通过在线估计提示难度，减少强化学习微调中的计算成本，无需频繁调用大型语言模型。


<details>
  <summary>Details</summary>
Motivation: 现有方法依赖频繁的提示评估和子集选择，导致高计算开销，MoPPS旨在通过迭代近似评估降低这一成本。

Method: MoPPS将每个提示的成功率建模为潜在变量，进行流式贝叶斯推断，并在多臂老虎机框架中使用后验采样，实现高效的提示选择。

Result: 实验表明，MoPPS能可靠预测提示难度，显著减少训练所需的LLM调用次数。

Conclusion: MoPPS提供了一种计算高效的提示选择方法，适用于多种任务，加速了强化学习微调过程。

Abstract: Recent advances have witnessed the effectiveness of reinforcement learning
(RL) finetuning in enhancing the reasoning capabilities of large language
models (LLMs). The optimization process often requires numerous iterations to
achieve satisfactory performance, resulting in high computational costs due to
the need for frequent prompt evaluations under intensive LLM interactions and
repeated policy updates. Appropriate online prompt selection methods reduce
iteration steps by prioritizing informative prompts during training, while the
pipeline's reliance on exhaustive prompt evaluation and subset selection for
optimization still incurs substantial computational overhead due to frequent
LLM inference calls. Distinguished from these direct evaluate-then-select
schemes, this work investigates iterative approximate evaluation for arbitrary
prompts and introduces Model Predictive Prompt Selection (MoPPS), a Bayesian
risk-predictive framework that online estimates prompt difficulty without
requiring costly LLM interactions. Technically, MoPPS models each prompt's
success rate as a latent variable, performs streaming Bayesian inference, and
employs posterior sampling in a constructed multi-armed bandit machine,
enabling sample efficient and adaptive prompt selection. Extensive experiments
across mathematics, planning, and vision-based geometry tasks show that MoPPS
reliably predicts prompt difficulty and accelerates training with significantly
reduced LLM rollouts.

</details>


### [66] [Trojan Horse Prompting: Jailbreaking Conversational Multimodal Models by Forging Assistant Message](https://arxiv.org/abs/2507.04673)
*Wei Duan,Li Qian*

Main category: cs.AI

TL;DR: 论文提出了一种新型的越狱技术——特洛伊木马提示，通过伪造对话历史绕过安全机制，揭示了现代对话AI的安全缺陷。


<details>
  <summary>Details</summary>
Motivation: 对话界面的普及增强了LLM的可用性，但也引入了新的攻击面，即对对话历史的依赖可能导致安全漏洞。

Method: 提出特洛伊木马提示技术，通过在对话历史中注入恶意载荷并触发有害内容生成，验证其攻击成功率。

Result: 实验证明，该方法在攻击成功率上显著优于传统用户轮次越狱方法。

Conclusion: 研究揭示了现代对话AI的安全缺陷，需从输入级过滤转向协议级对话上下文完整性验证。

Abstract: The rise of conversational interfaces has greatly enhanced LLM usability by
leveraging dialogue history for sophisticated reasoning. However, this reliance
introduces an unexplored attack surface. This paper introduces Trojan Horse
Prompting, a novel jailbreak technique. Adversaries bypass safety mechanisms by
forging the model's own past utterances within the conversational history
provided to its API. A malicious payload is injected into a model-attributed
message, followed by a benign user prompt to trigger harmful content
generation. This vulnerability stems from Asymmetric Safety Alignment: models
are extensively trained to refuse harmful user requests but lack comparable
skepticism towards their own purported conversational history. This implicit
trust in its "past" creates a high-impact vulnerability. Experimental
validation on Google's Gemini-2.0-flash-preview-image-generation shows Trojan
Horse Prompting achieves a significantly higher Attack Success Rate (ASR) than
established user-turn jailbreaking methods. These findings reveal a fundamental
flaw in modern conversational AI security, necessitating a paradigm shift from
input-level filtering to robust, protocol-level validation of conversational
context integrity.

</details>


### [67] [Advocate for Complete Benchmarks for Formal Reasoning with Formal/Informal Statements and Formal/Informal Proofs](https://arxiv.org/abs/2507.04719)
*Roozbeh Yousefzadeh,Xuenan Cao*

Main category: cs.AI

TL;DR: 本文批评并讨论了形式推理和自动定理证明领域的基准测试和评估实践，提倡开放代码、数据和完整无误的基准以加速进展。


<details>
  <summary>Details</summary>
Motivation: 当前实践存在障碍和误导性评估信息，阻碍领域发展。

Method: 分析现有实践，提出改进建议。

Result: 识别了阻碍贡献的实践，并提出了解决方案。

Conclusion: 旨在促进自动定理证明、自动形式化和非形式推理领域的讨论与合作。

Abstract: This position paper provides a critical but constructive discussion of
current practices in benchmarking and evaluative practices in the field of
formal reasoning and automated theorem proving. We take the position that open
code, open data, and benchmarks that are complete and error-free will
accelerate progress in this field. We identify practices that create barriers
to contributing to this field and suggest ways to remove them. We also discuss
some of the practices that might produce misleading evaluative information. We
aim to create discussions that bring together people from various groups
contributing to automated theorem proving, autoformalization, and informal
reasoning.

</details>


### [68] [LumiCRS: Asymmetric Contrastive Prototype Learning for Long-Tail Conversational Movie Recommendation](https://arxiv.org/abs/2507.04722)
*Jinzhi Wang,Bin Li,Qingke Peng,Haozhou Li,Zeyuan Zeng,Ruimeng Li,Biyi Zhou*

Main category: cs.AI

TL;DR: LumiCRS是一个端到端框架，通过自适应综合焦点损失、原型学习和GPT-4驱动的对话增强模块，解决了对话推荐系统中的长尾分布问题，显著提高了推荐的准确性、多样性和公平性。


<details>
  <summary>Details</summary>
Motivation: 对话推荐系统（CRSs）存在长尾分布问题，导致对高频热门内容的过度关注，牺牲了多样性并加剧冷启动问题。

Method: LumiCRS通过三个层次解决问题：(i) 自适应综合焦点损失（ACFL）动态调整类别权重；(ii) 原型学习用于长尾推荐；(iii) GPT-4驱动的对话增强模块生成多样化的长尾对话片段。

Result: 在REDIAL和INSPIRED基准测试中，LumiCRS的Recall@10和Tail-Recall@10比基线提高了7-15%，人类评估也证实了其优越性。

Conclusion: LumiCRS通过多层协作有效解决了长尾分布问题，提升了对话推荐系统的效率和公平性。

Abstract: Conversational recommender systems (CRSs) often suffer from an extreme
long-tail distribution of dialogue data, causing a strong bias toward
head-frequency blockbusters that sacrifices diversity and exacerbates the
cold-start problem. An empirical analysis of DCRS and statistics on the REDIAL
corpus show that only 10% of head movies account for nearly half of all
mentions, whereas about 70% of tail movies receive merely 26% of the attention.
This imbalance gives rise to three critical challenges: head over-fitting, body
representation drift, and tail sparsity. To address these issues, we propose
LumiCRS, an end-to-end framework that mitigates long-tail imbalance through
three mutually reinforcing layers: (i) an Adaptive Comprehensive Focal Loss
(ACFL) that dynamically adjusts class weights and focusing factors to curb head
over-fitting and reduce popularity bias; (ii) Prototype Learning for Long-Tail
Recommendation, which selects semantic, affective, and contextual prototypes to
guide clustering and stabilize body and tail representations; and (iii) a
GPT-4o-driven prototype-guided dialogue augmentation module that automatically
generates diverse long-tail conversational snippets to alleviate tail sparsity
and distribution shift. Together, these strategies enable LumiCRS to markedly
improve recommendation accuracy, diversity, and fairness: on the REDIAL and
INSPIRED benchmarks, LumiCRS boosts Recall@10 and Tail-Recall@10 by 7-15% over
fifteen strong baselines, while human evaluations confirm superior fluency,
informativeness, and long-tail relevance. These results demonstrate the
effectiveness of multi-layer collaboration in building an efficient and fair
long-tail conversational recommender.

</details>


### [69] [ChipSeek-R1: Generating Human-Surpassing RTL with LLM via Hierarchical Reward-Driven Reinforcement Learning](https://arxiv.org/abs/2507.04736)
*Zhirong Chen,Kaiyan Chang,Zhuolin Li,Xinyang He,Chujie Chen,Cangyuan Li,Mengdi Wang,Haobo Xu,Yinhe Han,Ying Wang*

Main category: cs.AI

TL;DR: ChipSeek-R1是一种基于分层奖励的强化学习框架，用于训练LLM生成功能正确且PPA优化的RTL代码。


<details>
  <summary>Details</summary>
Motivation: 当前方法无法同时优化功能正确性和硬件质量（PPA），ChipSeek-R1旨在解决这一问题。

Method: 采用分层奖励系统，结合语法、功能正确性和PPA指标的反馈，通过强化学习训练LLM。

Result: 在标准基准测试中，ChipSeek-R1在功能正确性上达到最优，并在RTLLM基准上生成27个优于人工编写的RTL设计。

Conclusion: ChipSeek-R1展示了将工具链反馈集成到LLM训练中的有效性，强化学习能实现自动化生成超越人工的RTL代码。

Abstract: Large Language Models (LLMs) show significant potential for automating
Register-Transfer Level (RTL) code generation. However, current approaches face
a critical challenge: they can not simultaneously optimize for functional
correctness and hardware quality (Power, Performance, Area - PPA). Methods
based on supervised fine-tuning often generate functionally correct but
PPA-suboptimal code, lacking mechanisms to learn optimization principles. In
contrast, post-processing techniques that attempt to improve PPA metrics after
generation are often inefficient because they operate externally without
updating the LLM's parameters, thus failing to enhance the model's intrinsic
design capabilities.
  To bridge this gap, we introduce ChipSeek-R1, a hierarchical reward-driven
reinforcement learning framework to train LLMs to generate RTL code that
achieves both functional correctness and optimized PPA metrics. ChipSeek-R1
employs a hierarchical reward system, which incorporates direct feedback on
syntax, functional correctness (from simulators) and PPA metrics (from
synthesis tools) during reinforcement learning. This enables the model to learn
complex hardware design trade-offs via trial-and-error, generating RTL code
that is both functionally correct and PPA-optimized. Evaluating ChipSeek-R1 on
standard benchmarks (VerilogEval, RTLLM), we achieve state-of-the-art results
in functional correctness. Notably, on the RTLLM benchmark, ChipSeek-R1
generated 27 RTL designs surpassing the PPA metrics of the original
human-written code. Our findings demonstrate the effectiveness of integrating
toolchain feedback into LLM training and highlight the potential for
reinforcement learning to enable automated generation of human-surpassing RTL
code. We open-source our code in anonymous github.

</details>


### [70] [Activation Steering for Chain-of-Thought Compression](https://arxiv.org/abs/2507.04742)
*Seyedarmin Azizi,Erfan Baghaei Potraghloo,Massoud Pedram*

Main category: cs.AI

TL;DR: 论文提出了一种名为ASC的推理时技术，通过调整隐藏表示来压缩思维链（CoTs），减少冗余推理步骤，同时保持准确性。


<details>
  <summary>Details</summary>
Motivation: 思维链（CoTs）虽然能提升大语言模型的推理能力，但通常过于冗长，导致资源浪费和延迟增加。

Method: 通过提取和注入“转向向量”来切换模型的推理模式，从冗长的英语思维链转向简洁的数学思维链。

Result: ASC在MATH500和GSM8K数据集上实现了67.43%的CoT长度缩减，同时保持准确性，并在8B模型上平均提速2.73倍。

Conclusion: ASC是一种无需训练的高效方法，适用于对延迟或成本敏感的场景。

Abstract: Large language models (LLMs) excel at complex reasoning when they include
intermediate steps, known as "chains of thought" (CoTs). However, these
rationales are often overly verbose, even for simple problems, leading to
wasted context, increased latency, and higher energy consumption. We observe
that verbose, English-heavy CoTs and concise, math-centric CoTs occupy distinct
regions in the model's residual-stream activation space. By extracting and
injecting a "steering vector" to transition between these modes, we can
reliably shift generation toward more concise reasoning, effectively
compressing CoTs without retraining. We formalize this approach as
Activation-Steered Compression (ASC), an inference-time technique that shortens
reasoning traces by directly modifying hidden representations. In addition, we
provide a theoretical analysis of the impact of ASC on the output distribution,
derived from a closed-form KL-divergence-bounded constraint to regulate
steering strength. Using only 100 paired verbose and concise examples, ASC
achieves up to 67.43% reduction in CoT length on MATH500 and GSM8K datasets,
while maintaining accuracy across 7B, 8B, and 32B parameter models. As a
training-free method, ASC introduces negligible runtime overhead and, on
MATH500, delivers an average 2.73x speedup in end-to-end reasoning wall-clock
time on an 8B model. This makes ASC a practical and efficient tool for
streamlining the deployment of reasoning-capable LLMs in latency- or
cost-sensitive settings. The code is available at:
https://github.com/ArminAzizi98/ASC

</details>


### [71] [LLM-based Question-Answer Framework for Sensor-driven HVAC System Interaction](https://arxiv.org/abs/2507.04748)
*Sungmin Lee,Minju Kang,Joonhee Lee,Seungyong Lee,Dongju Kim,Jingi Hong,Jun Shin,Pei Zhang,JeongGil Ko*

Main category: cs.AI

TL;DR: JARVIS是一个基于LLM的两阶段QA框架，专为HVAC系统设计，通过专家LLM和代理实现高效查询处理和响应生成。


<details>
  <summary>Details</summary>
Motivation: 提升非专家用户与HVAC系统的交互性，解决实时、上下文感知和领域知识融合的挑战。

Method: 采用两阶段框架：专家LLM翻译查询，代理执行SQL数据检索和响应生成，结合自适应上下文注入和参数化SQL构建。

Result: 在真实HVAC数据和专家QA数据集上表现优异，响应质量和准确性显著优于基线方法。

Conclusion: JARVIS有效解决了HVAC系统交互的独特挑战，提供了准确且可解释的响应。

Abstract: Question-answering (QA) interfaces powered by large language models (LLMs)
present a promising direction for improving interactivity with HVAC system
insights, particularly for non-expert users. However, enabling accurate,
real-time, and context-aware interactions with HVAC systems introduces unique
challenges, including the integration of frequently updated sensor data,
domain-specific knowledge grounding, and coherent multi-stage reasoning. In
this paper, we present JARVIS, a two-stage LLM-based QA framework tailored for
sensor data-driven HVAC system interaction. JARVIS employs an Expert-LLM to
translate high-level user queries into structured execution instructions, and
an Agent that performs SQL-based data retrieval, statistical processing, and
final response generation. To address HVAC-specific challenges, JARVIS
integrates (1) an adaptive context injection strategy for efficient HVAC and
deployment-specific information integration, (2) a parameterized SQL builder
and executor to improve data access reliability, and (3) a bottom-up planning
scheme to ensure consistency across multi-stage response generation. We
evaluate JARVIS using real-world data collected from a commercial HVAC system
and a ground truth QA dataset curated by HVAC experts to demonstrate its
effectiveness in delivering accurate and interpretable responses across diverse
queries. Results show that JARVIS consistently outperforms baseline and
ablation variants in both automated and user-centered assessments, achieving
high response quality and accuracy.

</details>


### [72] [FurniMAS: Language-Guided Furniture Decoration using Multi-Agent System](https://arxiv.org/abs/2507.04770)
*Toan Nguyen,Tri Le,Quang Nguyen,Anh Nguyen*

Main category: cs.AI

TL;DR: FurniMAS是一个多智能体系统，用于自动化家具装饰，结合LLM和非LLM智能体，通过协作生成高质量的3D装饰效果。


<details>
  <summary>Details</summary>
Motivation: 家具装饰需要专业艺术技能且耗时，FurniMAS旨在通过多智能体系统自动化这一过程。

Method: FurniMAS结合LLM和非LLM智能体，通过沟通、逻辑推理和验证将需求转化为最终装饰效果。

Result: 实验表明，FurniMAS在生成高质量3D装饰方面显著优于其他基线方法。

Conclusion: FurniMAS通过多智能体协作，有效解决了家具装饰的自动化问题，提升了效率和质量。

Abstract: Furniture decoration is an important task in various industrial applications.
However, achieving a high-quality decorative result is often time-consuming and
requires specialized artistic expertise. To tackle these challenges, we explore
how multi-agent systems can assist in automating the decoration process. We
propose FurniMAS, a multi-agent system for automatic furniture decoration.
Specifically, given a human prompt and a household furniture item such as a
working desk or a TV stand, our system suggests relevant assets with
appropriate styles and materials, and arranges them on the item, ensuring the
decorative result meets functionality, aesthetic, and ambiance preferences.
FurniMAS assembles a hybrid team of LLM-based and non-LLM agents, each
fulfilling distinct roles in a typical decoration project. These agents
collaborate through communication, logical reasoning, and validation to
transform the requirements into the final outcome. Extensive experiments
demonstrate that our FurniMAS significantly outperforms other baselines in
generating high-quality 3D decor.

</details>


### [73] [Application and Evaluation of Large Language Models for Forecasting the Impact of Traffic Incidents](https://arxiv.org/abs/2507.04803)
*George Jagadeesh,Srikrishna Iyer,Michal Polanowski,Kai Xin Thia*

Main category: cs.AI

TL;DR: 研究探讨了使用大型语言模型（LLM）预测交通事故对交通流影响的可行性，提出了一种基于LLM的解决方案，其性能与现有机器学习模型相当。


<details>
  <summary>Details</summary>
Motivation: 现有机器学习解决方案需要大量训练数据，而LLM可以利用自由文本事故日志且无需大量数据，具有潜在优势。

Method: 提出了一种完全基于LLM的解决方案，结合交通特征和LLM提取的事故特征进行预测，并优化了LLM的上下文学习示例选择方法。

Result: 在真实数据集上评估，表现最佳的LLM与最准确的机器学习模型精度相当，尽管LLM未针对该任务进行训练。

Conclusion: LLM是预测交通事故影响的可行选择。

Abstract: This study examines the feasibility of applying large language models (LLMs)
for forecasting the impact of traffic incidents on the traffic flow. The use of
LLMs for this task has several advantages over existing machine learning-based
solutions such as not requiring a large training dataset and the ability to
utilize free-text incident logs. We propose a fully LLM-based solution that
predicts the incident impact using a combination of traffic features and
LLM-extracted incident features. A key ingredient of this solution is an
effective method of selecting examples for the LLM's in-context learning. We
evaluate the performance of three advanced LLMs and two state-of-the-art
machine learning models on a real traffic incident dataset. The results show
that the best-performing LLM matches the accuracy of the most accurate machine
learning model, despite the former not having been trained on this prediction
task. The findings indicate that LLMs are a practically viable option for
traffic incident impact prediction.

</details>


### [74] [DoPI: Doctor-like Proactive Interrogation LLM for Traditional Chinese Medicine](https://arxiv.org/abs/2507.04877)
*Zewen Sun,Ruoxiang Huang,Jiahe Feng,Rundong Kong,Yuqian Wang,Hengyu Liu,Ziqi Gong,Yuyuan Qin,Yingxue Wang,Yu Wang*

Main category: cs.AI

TL;DR: 论文提出DoPI系统，通过多轮对话和知识图谱提升中医诊断能力，解决了现有大语言模型在医疗对话中的局限性。


<details>
  <summary>Details</summary>
Motivation: 现代AI系统在中医诊断中多轮对话和主动提问能力不足，限制了其实际应用效果。

Method: 提出DoPI系统，结合指导模型和专家模型，利用知识图谱动态生成问题，并通过专家模型提供诊断和治疗方案。

Result: 实验结果显示DoPI系统在问诊结果中达到84.68%的准确率，显著提升了诊断沟通能力。

Conclusion: DoPI系统有效提升了中医诊断的对话能力和专业性，为AI在医疗领域的应用提供了新思路。

Abstract: Enhancing interrogation capabilities in Traditional Chinese Medicine (TCM)
diagnosis through multi-turn dialogues and knowledge graphs presents a
significant challenge for modern AI systems. Current large language models
(LLMs), despite their advancements, exhibit notable limitations in medical
applications, particularly in conducting effective multi-turn dialogues and
proactive questioning. These shortcomings hinder their practical application
and effectiveness in simulating real-world diagnostic scenarios. To address
these limitations, we propose DoPI, a novel LLM system specifically designed
for the TCM domain. The DoPI system introduces a collaborative architecture
comprising a guidance model and an expert model. The guidance model conducts
multi-turn dialogues with patients and dynamically generates questions based on
a knowledge graph to efficiently extract critical symptom information.
Simultaneously, the expert model leverages deep TCM expertise to provide final
diagnoses and treatment plans. Furthermore, this study constructs a multi-turn
doctor-patient dialogue dataset to simulate realistic consultation scenarios
and proposes a novel evaluation methodology that does not rely on manually
collected real-world consultation data. Experimental results show that the DoPI
system achieves an accuracy rate of 84.68 percent in interrogation outcomes,
significantly enhancing the model's communication ability during diagnosis
while maintaining professional expertise.

</details>


### [75] [MARBLE: A Multi-Agent Rule-Based LLM Reasoning Engine for Accident Severity Prediction](https://arxiv.org/abs/2507.04893)
*Kaleem Ullah Qasim,Jiashu Zhang*

Main category: cs.AI

TL;DR: MARBLE是一种多智能体规则驱动的LLM引擎，通过分解任务和模块化推理提升交通事故严重性预测的准确性和可解释性，优于传统方法和SOTA提示方法。


<details>
  <summary>Details</summary>
Motivation: 解决交通事故严重性预测中数据不完整、特征依赖性强、类别不平衡以及现有方法可扩展性和可解释性不足的问题。

Method: 采用多智能体系统，每个智能体专注于特征的语义子集，通过规则或LLM引导的共识机制协调预测，并保留推理痕迹。

Result: 在英美数据集上，MARBLE准确率接近90%，显著优于传统方法和SOTA提示方法（如CoT、L2M、ToT）。

Conclusion: MARBLE为安全关键应用中的不确定性推理提供了通用且可解释的框架。

Abstract: Accident severity prediction plays a critical role in transportation safety
systems but is a persistently difficult task due to incomplete data, strong
feature dependencies, and severe class imbalance in which rare but
high-severity cases are underrepresented and hard to detect. Existing methods
often rely on monolithic models or black box prompting, which struggle to scale
in noisy, real-world settings and offer limited interpretability. To address
these challenges, we propose MARBLE a multiagent rule based LLM engine that
decomposes the severity prediction task across a team of specialized reasoning
agents, including an interchangeable ML-backed agent. Each agent focuses on a
semantic subset of features (e.g., spatial, environmental, temporal), enabling
scoped reasoning and modular prompting without the risk of prompt saturation.
Predictions are coordinated through either rule-based or LLM-guided consensus
mechanisms that account for class rarity and confidence dynamics. The system
retains structured traces of agent-level reasoning and coordination outcomes,
supporting in-depth interpretability and post-hoc performance diagnostics.
Across both UK and US datasets, MARBLE consistently outperforms traditional
machine learning classifiers and state-of-the-art (SOTA) prompt-based reasoning
methods including Chain-of-Thought (CoT), Least-to-Most (L2M), and
Tree-of-Thought (ToT) achieving nearly 90% accuracy where others plateau below
48%. This performance redefines the practical ceiling for accident severity
classification under real world noise and extreme class imbalance. Our results
position MARBLE as a generalizable and interpretable framework for reasoning
under uncertainty in safety-critical applications.

</details>


### [76] [Supported Abstract Argumentation for Case-Based Reasoning](https://arxiv.org/abs/2507.04994)
*Adam Gould,Gabriel de Olim Gaul,Francesca Toni*

Main category: cs.AI

TL;DR: sAA-CBR是一种基于辩论的二元分类模型，通过引入支持机制解决了AA-CBR中无关案例的问题，同时保留了关键模型特性。


<details>
  <summary>Details</summary>
Motivation: 解决AA-CBR模型中存在无关案例（spikes）的问题，提升模型的准确性和可靠性。

Method: 引入支持机制，使过去案例通过辩论支持或攻击标签，确保所有案例都参与辩论。

Result: 证明sAA-CBR不含无关案例，且不牺牲关键模型特性。

Conclusion: sAA-CBR通过支持机制有效解决了AA-CBR的局限性，同时保持了模型的完整性。

Abstract: We introduce Supported Abstract Argumentation for Case-Based Reasoning
(sAA-CBR), a binary classification model in which past cases engage in debates
by arguing in favour of their labelling and attacking or supporting those with
opposing or agreeing labels. With supports, sAA-CBR overcomes the limitation of
its precursor AA-CBR, which can contain extraneous cases (or spikes) that are
not included in the debates. We prove that sAA-CBR contains no spikes, without
trading off key model properties

</details>


### [77] [When Imitation Learning Outperforms Reinforcement Learning in Surgical Action Planning](https://arxiv.org/abs/2507.05011)
*Maxence Boels,Harry Robertshaw,Alejandro Granados,Prokar Dasgupta,Sebastien Ourselin*

Main category: cs.AI

TL;DR: 论文比较了模仿学习（IL）和强化学习（RL）在手术动作规划中的表现，发现IL优于RL。


<details>
  <summary>Details</summary>
Motivation: 研究动机是探索IL和RL在手术动作规划中的效果，以提供实时辅助。

Method: 使用了双任务自回归模仿学习（DARIL）和三种RL变体（基于世界模型的RL、直接视频RL和逆RL增强）进行比较。

Result: DARIL表现最佳（34.6% mAP），而所有RL方法均表现不佳（最低3.1% mAP）。

Conclusion: 结论是IL在手术动作规划中优于RL，挑战了RL在序列决策中的优越性假设。

Abstract: Surgical action planning requires predicting future instrument-verb-target
triplets for real-time assistance. While teleoperated robotic surgery provides
natural expert demonstrations for imitation learning (IL), reinforcement
learning (RL) could potentially discover superior strategies through
exploration. We present the first comprehensive comparison of IL versus RL for
surgical action planning on CholecT50. Our Dual-task Autoregressive Imitation
Learning (DARIL) baseline achieves 34.6% action triplet recognition mAP and
33.6% next frame prediction mAP with smooth planning degradation to 29.2% at
10-second horizons. We evaluated three RL variants: world model-based RL,
direct video RL, and inverse RL enhancement. Surprisingly, all RL approaches
underperformed DARIL i.e. world model RL dropped to 3.1% mAP at 10s while
direct video RL achieved only 15.9%. Our analysis reveals that distribution
matching on expert-annotated test sets systematically favors IL over
potentially valid RL policies that differ from training demonstrations. This
challenges assumptions about RL superiority in sequential decision making and
provides crucial insights for surgical AI development.

</details>


### [78] [How Rules Represent Causal Knowledge: Causal Modeling with Abductive Logic Programs](https://arxiv.org/abs/2507.05088)
*Kilian Rückschloß,Felix Weitkämper*

Main category: cs.AI

TL;DR: 该论文将Pearl的因果理论扩展到分层溯因逻辑程序，证明其稳定模型语义符合因果哲学原则，支持因果建模和干预预测。


<details>
  <summary>Details</summary>
Motivation: 扩展Pearl的因果理论到逻辑程序，澄清逻辑规则的因果解释，支持外部干预的推理。

Method: 将溯因逻辑程序转化为因果系统，基于哲学基础和近期研究（如Bochman和Eelink等）构建因果解释。

Result: 分层程序的稳定模型语义符合因果充分性、自然必要性和未观测效应无关性等哲学原则。

Conclusion: 分层溯因逻辑程序可作为因果建模和干预预测的框架，其语义与因果哲学一致。

Abstract: Pearl observes that causal knowledge enables predicting the effects of
interventions, such as actions, whereas descriptive knowledge only permits
drawing conclusions from observation. This paper extends Pearl's approach to
causality and interventions to the setting of stratified abductive logic
programs. It shows how stable models of such programs can be given a causal
interpretation by building on philosophical foundations and recent work by
Bochman and Eelink et al. In particular, it provides a translation of abductive
logic programs into causal systems, thereby clarifying the informal causal
reading of logic program rules and supporting principled reasoning about
external actions. The main result establishes that the stable model semantics
for stratified programs conforms to key philosophical principles of causation,
such as causal sufficiency, natural necessity, and irrelevance of unobserved
effects. This justifies the use of stratified abductive logic programs as a
framework for causal modeling and for predicting the effects of interventions

</details>


### [79] [Rule Learning for Knowledge Graph Reasoning under Agnostic Distribution Shift](https://arxiv.org/abs/2507.05110)
*Shixuan Liu,Yue He,Yunfei Wang,Hao Zou,Haoxiang Cheng,Wenjing Yang,Peng Cui,Zhong Liu*

Main category: cs.AI

TL;DR: 论文提出了一种名为StableRule的框架，用于解决知识图谱（KG）推理中的分布外（OOD）问题，通过特征解耦和规则学习网络提升模型的泛化性能。


<details>
  <summary>Details</summary>
Motivation: 现有KG推理方法依赖于I.I.D假设，但在实际应用中可能因未知选择偏差或分布偏移导致性能下降，限制了其在真实环境中的部署。

Method: 提出StableRule框架，结合特征解耦和规则学习网络，以增强OOD泛化能力。

Result: 在七个基准KG上的实验表明，该框架在异构环境中表现优异且稳定。

Conclusion: StableRule框架有效解决了OOD KG推理问题，具有实际应用价值。

Abstract: Knowledge graph (KG) reasoning remains a critical research area focused on
inferring missing knowledge by analyzing relationships among observed facts.
Despite its success, a key limitation of existing KG reasoning methods is their
dependence on the I.I.D assumption. This assumption can easily be violated due
to unknown sample selection bias during training or agnostic distribution
shifts during testing, significantly compromising model performance and
reliability. To facilitate the deployment of KG reasoning in wild environments,
this study investigates learning logical rules from KGs affected by unknown
selection bias. Additionally, we address test sets with agnostic distribution
shifts, formally defining this challenge as out-of-distribution (OOD) KG
reasoning-a previously underexplored problem. To solve the issue, we propose
the Stable Rule Learning (StableRule) framework, an end-to-end methodology that
integrates feature decorrelation with rule learning network, to enhance OOD
generalization performance. By leveraging feature decorrelation, the StableRule
framework mitigates the adverse effects of covariate shifts arising in OOD
scenarios, thereby improving the robustness of the rule learning component in
effectively deriving logical rules. Extensive experiments on seven benchmark
KGs demonstrate the framework's superior effectiveness and stability across
diverse heterogeneous environments, underscoring its practical significance for
real-world applications.

</details>


### [80] [GIST: Cross-Domain Click-Through Rate Prediction via Guided Content-Behavior Distillation](https://arxiv.org/abs/2507.05142)
*Wei Xu,Haoran Li,Baoyuan Ou,Lai Xu,Yingjie Qin,Ruilong Su,Ruiwen Xu*

Main category: cs.AI

TL;DR: 论文提出GIST模型，通过解耦源域和目标域训练过程，结合内容-行为联合训练模块和不对称相似性集成策略，提升跨域点击率预测性能。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法在联合训练和预训练微调中的局限性，如分布差异和数据持续整合问题。

Method: 提出GIST模型，包含内容-行为联合训练模块（CBJT）和不对称相似性集成策略（ASI）。

Result: 在离线和在线测试中超越现有方法，成功部署于小红书平台。

Conclusion: GIST有效解决了跨域点击率预测的挑战，显著提升了广告系统性能。

Abstract: Cross-domain Click-Through Rate prediction aims to tackle the data sparsity
and the cold start problems in online advertising systems by transferring
knowledge from source domains to a target domain. Most existing methods rely on
overlapping users to facilitate this transfer, often focusing on joint training
or pre-training with fine-tuning approach to connect the source and target
domains. However, in real-world industrial settings, joint training struggles
to learn optimal representations with different distributions, and pre-training
with fine-tuning is not well-suited for continuously integrating new data. To
address these issues, we propose GIST, a cross-domain lifelong sequence model
that decouples the training processes of the source and target domains. Unlike
previous methods that search lifelong sequences in the source domains using
only content or behavior signals or their simple combinations, we innovatively
introduce a Content-Behavior Joint Training Module (CBJT), which aligns
content-behavior distributions and combines them with guided information to
facilitate a more stable representation. Furthermore, we develop an Asymmetric
Similarity Integration strategy (ASI) to augment knowledge transfer through
similarity computation. Extensive experiments demonstrate the effectiveness of
GIST, surpassing SOTA methods on offline evaluations and an online A/B test.
Deployed on the Xiaohongshu (RedNote) platform, GIST effectively enhances
online ads system performance at scale, serving hundreds of millions of daily
active users.

</details>


### [81] [MedGemma Technical Report](https://arxiv.org/abs/2507.05201)
*Andrew Sellergren,Sahar Kazemzadeh,Tiam Jaroensri,Atilla Kiraly,Madeleine Traverse,Timo Kohlberger,Shawn Xu,Fayaz Jamil,Cían Hughes,Charles Lau,Justin Chen,Fereshteh Mahvar,Liron Yatziv,Tiffany Chen,Bram Sterling,Stefanie Anna Baby,Susanna Maria Baby,Jeremy Lai,Samuel Schmidgall,Lu Yang,Kejia Chen,Per Bjornsson,Shashir Reddy,Ryan Brush,Kenneth Philbrick,Howard Hu,Howard Yang,Richa Tiwari,Sunny Jansen,Preeti Singh,Yun Liu,Shekoofeh Azizi,Aishwarya Kamath,Johan Ferret,Shreya Pathak,Nino Vieillard,Ramona Merhej,Sarah Perrin,Tatiana Matejovicova,Alexandre Ramé,Morgane Riviere,Louis Rouillard,Thomas Mesnard,Geoffrey Cideron,Jean-bastien Grill,Sabela Ramos,Edouard Yvinec,Michelle Casbon,Elena Buchatskaya,Jean-Baptiste Alayrac,Dmitry,Lepikhin,Vlad Feinberg,Sebastian Borgeaud,Alek Andreev,Cassidy Hardin,Robert Dadashi,Léonard Hussenot,Armand Joulin,Olivier Bachem,Yossi Matias,Katherine Chou,Avinatan Hassidim,Kavi Goel,Clement Farabet,Joelle Barral,Tris Warkentin,Jonathon Shlens,David Fleet,Victor Cotruta,Omar Sanseviero,Gus Martins,Phoebe Kirk,Anand Rao,Shravya Shetty,David F. Steiner,Can Kirmizibayrak,Rory Pilgrim,Daniel Golden,Lin Yang*

Main category: cs.AI

TL;DR: MedGemma是一组基于Gemma 3的医疗视觉语言基础模型，在医疗任务中表现优异，接近专用模型性能，同时保持通用能力。


<details>
  <summary>Details</summary>
Motivation: 解决医疗AI应用中数据多样性、任务复杂性和隐私保护的挑战，加速医疗AI发展。

Method: 基于Gemma 3构建MedGemma模型，并引入MedSigLIP视觉编码器，优化医疗图像和文本理解能力。

Result: 在多项医疗任务中显著超越同类生成模型，接近专用模型性能，部分任务错误率降低50%。

Conclusion: MedGemma为医疗研究和下游应用提供了强大基础，具有加速医疗AI发展的潜力。

Abstract: Artificial intelligence (AI) has significant potential in healthcare
applications, but its training and deployment faces challenges due to
healthcare's diverse data, complex tasks, and the need to preserve privacy.
Foundation models that perform well on medical tasks and require less
task-specific tuning data are critical to accelerate the development of
healthcare AI applications. We introduce MedGemma, a collection of medical
vision-language foundation models based on Gemma 3 4B and 27B. MedGemma
demonstrates advanced medical understanding and reasoning on images and text,
significantly exceeding the performance of similar-sized generative models and
approaching the performance of task-specific models, while maintaining the
general capabilities of the Gemma 3 base models. For out-of-distribution tasks,
MedGemma achieves 2.6-10% improvement on medical multimodal question answering,
15.5-18.1% improvement on chest X-ray finding classification, and 10.8%
improvement on agentic evaluations compared to the base models. Fine-tuning
MedGemma further improves performance in subdomains, reducing errors in
electronic health record information retrieval by 50% and reaching comparable
performance to existing specialized state-of-the-art methods for pneumothorax
classification and histopathology patch classification. We additionally
introduce MedSigLIP, a medically-tuned vision encoder derived from SigLIP.
MedSigLIP powers the visual understanding capabilities of MedGemma and as an
encoder achieves comparable or better performance than specialized medical
image encoders. Taken together, the MedGemma collection provides a strong
foundation of medical image and text capabilities, with potential to
significantly accelerate medical research and development of downstream
applications. The MedGemma collection, including tutorials and model weights,
can be found at https://goo.gle/medgemma.

</details>


### [82] [SciMaster: Towards General-Purpose Scientific AI Agents, Part I. X-Master as Foundation: Can We Lead on Humanity's Last Exam?](https://arxiv.org/abs/2507.05241)
*Jingyi Chai,Shuo Tang,Rui Ye,Yuwen Du,Xinyu Zhu,Mengcheng Zhou,Yanfeng Wang,Weinan E,Siheng Chen*

Main category: cs.AI

TL;DR: X-Master是一个工具增强的推理代理，通过灵活使用外部工具模拟人类研究者的推理过程，在HLE测试中取得32.1%的成绩，超越OpenAI和Google的26.6%和26.9%。


<details>
  <summary>Details</summary>
Motivation: 利用AI加速科学发现需要理解人类知识的前沿，HLE测试为此提供了极具挑战性的评估标准。

Method: 提出X-Master代理，通过代码作为交互语言灵活使用Python库和定制工具，并通过X-Masters工作流扩展推理能力。

Result: X-Masters在HLE测试中取得32.1%的成绩，首次突破30%阈值，超越OpenAI和Google的表现。

Conclusion: X-Masters为复杂任务解决提供了新思路，为未来模型训练积累了宝贵经验。

Abstract: The rapid advancements of AI agents have ignited the long-held ambition of
leveraging them to accelerate scientific discovery. Achieving this goal
requires a deep understanding of the frontiers of human knowledge. As such,
Humanity's Last Exam (HLE) provides an exceptionally challenging touchstone for
evaluating scientific AI agents. In this work, we aim to construct the
foundational architecture for general-purpose agents and validate the
capabilities through leading performance on HLE. To achieve this, we introduce
X-Master, a tool-augmented reasoning agent designed to emulate human
researchers by interacting flexibly with external tools during its reasoning
process. This agent, guided by the conceptualization of code as an interaction
language, can flexibly leverage built-in Python libraries and our customized
tools to augment the reasoning. We further scale its capabilities through
X-Masters, a scattered-and-stacked agentic workflow that systematically
enhances breadth and depth of reasoning. Our open-source solution, X-Masters,
sets a new state-of-the-art record on HLE with a score of 32.1%, surpassing
OpenAI's and Google's Deep Research (26.6% and 26.9%) and becoming the first to
exceed the 30% threshold. This work allows us to gain a deeper understanding of
complex task-solving and accumulates valuable experience that can inform future
advancements, guiding subsequent model training.

</details>


### [83] [Modeling Latent Partner Strategies for Adaptive Zero-Shot Human-Agent Collaboration](https://arxiv.org/abs/2507.05244)
*Benjamin Li,Shuyang Shi,Lucia Romero,Huao Li,Yaqi Xie,Woojun Kim,Stefanos Nikolaidis,Michael Lewis,Katia Sycara,Simon Stepputtis*

Main category: cs.AI

TL;DR: TALENTS框架通过变分自编码器和聚类学习策略空间，动态适应异构队友，在Overcooked环境中表现优于基线。


<details>
  <summary>Details</summary>
Motivation: 异构团队（如人机协作）需实时适应队友策略，尤其在时间压力和复杂动态任务中。

Method: 使用变分自编码器学习策略潜空间，聚类识别策略类型，训练条件合作者，动态调整策略。

Result: 在Overcooked任务中，TALENTS优于现有基线，适应陌生人类队友。

Conclusion: TALENTS框架有效支持异构团队的实时策略适应，提升协作效率。

Abstract: In collaborative tasks, being able to adapt to your teammates is a necessary
requirement for success. When teammates are heterogeneous, such as in
human-agent teams, agents need to be able to observe, recognize, and adapt to
their human partners in real time. This becomes particularly challenging in
tasks with time pressure and complex strategic spaces where the dynamics can
change rapidly. In this work, we introduce TALENTS, a strategy-conditioned
cooperator framework that learns to represent, categorize, and adapt to a range
of partner strategies, enabling ad-hoc teamwork. Our approach utilizes a
variational autoencoder to learn a latent strategy space from trajectory data.
This latent space represents the underlying strategies that agents employ.
Subsequently, the system identifies different types of strategy by clustering
the data. Finally, a cooperator agent is trained to generate partners for each
type of strategy, conditioned on these clusters. In order to adapt to
previously unseen partners, we leverage a fixed-share regret minimization
algorithm that infers and adjusts the estimated partner strategy dynamically.
We assess our approach in a customized version of the Overcooked environment,
posing a challenging cooperative cooking task that demands strong coordination
across a wide range of possible strategies. Using an online user study, we show
that our agent outperforms current baselines when working with unfamiliar human
partners.

</details>


### [84] [When Chain of Thought is Necessary, Language Models Struggle to Evade Monitors](https://arxiv.org/abs/2507.05246)
*Scott Emmons,Erik Jenner,David K. Elson,Rif A. Saurous,Senthooran Rajamanoharan,Heng Chen,Irhum Shafkat,Rohin Shah*

Main category: cs.AI

TL;DR: 论文探讨了链式思维（CoT）监控在AI安全防御中的可靠性问题，提出了区分CoT作为合理化与CoT作为计算的概念框架，并验证了其监控性。


<details>
  <summary>Details</summary>
Motivation: 针对CoT监控在防止严重危害中的可靠性问题，研究其关键属性是监控性而非忠实性。

Method: 引入概念框架区分CoT的两种用途，并通过实验增加行为难度以验证监控性。

Result: 模型在需要复杂推理时暴露其思维过程，但需人为帮助才能隐藏意图。

Conclusion: CoT监控虽非完美，但提供了重要防御层，需持续压力测试和保护。

Abstract: While chain-of-thought (CoT) monitoring is an appealing AI safety defense,
recent work on "unfaithfulness" has cast doubt on its reliability. These
findings highlight an important failure mode, particularly when CoT acts as a
post-hoc rationalization in applications like auditing for bias. However, for
the distinct problem of runtime monitoring to prevent severe harm, we argue the
key property is not faithfulness but monitorability. To this end, we introduce
a conceptual framework distinguishing CoT-as-rationalization from
CoT-as-computation. We expect that certain classes of severe harm will require
complex, multi-step reasoning that necessitates CoT-as-computation. Replicating
the experimental setups of prior work, we increase the difficulty of the bad
behavior to enforce this necessity condition; this forces the model to expose
its reasoning, making it monitorable. We then present methodology guidelines to
stress-test CoT monitoring against deliberate evasion. Applying these
guidelines, we find that models can learn to obscure their intentions, but only
when given significant help, such as detailed human-written strategies or
iterative optimization against the monitor. We conclude that, while not
infallible, CoT monitoring offers a substantial layer of defense that requires
active protection and continued stress-testing.

</details>
