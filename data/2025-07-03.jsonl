{"id": "2507.01231", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01231", "abs": "https://arxiv.org/abs/2507.01231", "authors": ["IÃ±aki Dellibarda Varela", "Pablo Romero-Sorozabal", "Eduardo Rocon", "Manuel Cebrian"], "title": "Rethinking the Illusion of Thinking", "comment": "8 pages, 4 figures", "summary": "Earlier this year, Apple ignited controversy by publishing \"The Illusion of\nThinking,\" prompting heated debate within the AI community. Critics seized upon\nthe findings as conclusive evidence that Large Reasoning Models (LRMs) lack\ngenuine reasoning capabilities, branding them as mere stochastic parrots.\nMeanwhile, defenders-spearheaded by Lawsen et al. (2025)-fired back, condemning\nthe experimental setup as flawed and the conclusions overstated. We clarify\nthis debate by replicating and refining two of the original study's most\ncontentious benchmarks: Towers of Hanoi and River Crossing. By introducing\nincremental stepwise prompting and agentic collaborative dialogue, we show that\npreviously reported failures solving the Towers of Hanoi were not purely result\nof output constraints, but also partly a result of cognition limitations: LRMs\nstill stumble when complexity rises moderately (around 8 disks). Moreover, the\nRiver Crossing results initially heralded as catastrophic failures turn out to\nhinge upon testing unsolvable configurations. Once we limit tests strictly to\nsolvable problems-LRMs effortlessly solve large instances involving over 100\nagent pairs. Our findings ultimately defy simplistic narratives: today's LRMs\nare stochastic, RL-tuned searchers in a discrete state space we barely\nunderstand. Real progress in symbolic, long-horizon reasoning demands mapping\nthat terrain through fine-grained ablations like those introduced here."}
{"id": "2507.01282", "categories": ["cs.AI", "cs.HC"], "pdf": "https://arxiv.org/pdf/2507.01282", "abs": "https://arxiv.org/abs/2507.01282", "authors": ["Matthew JY Kang", "Wenli Yang", "Monica R Roberts", "Byeong Ho Kang", "Charles B Malpas"], "title": "Beyond Black-Box AI: Interpretable Hybrid Systems for Dementia Care", "comment": null, "summary": "The recent boom of large language models (LLMs) has re-ignited the hope that\nartificial intelligence (AI) systems could aid medical diagnosis. Yet despite\ndazzling benchmark scores, LLM assistants have yet to deliver measurable\nimprovements at the bedside. This scoping review aims to highlight the areas\nwhere AI is limited to make practical contributions in the clinical setting,\nspecifically in dementia diagnosis and care.\n  Standalone machine-learning models excel at pattern recognition but seldom\nprovide actionable, interpretable guidance, eroding clinician trust. Adjacent\nuse of LLMs by physicians did not result in better diagnostic accuracy or\nspeed. Key limitations trace to the data-driven paradigm: black-box outputs\nwhich lack transparency, vulnerability to hallucinations, and weak causal\nreasoning. Hybrid approaches that combine statistical learning with expert\nrule-based knowledge, and involve clinicians throughout the process help bring\nback interpretability. They also fit better with existing clinical workflows,\nas seen in examples like PEIRS and ATHENA-CDS.\n  Future decision-support should prioritise explanatory coherence by linking\npredictions to clinically meaningful causes. This can be done through\nneuro-symbolic or hybrid AI that combines the language ability of LLMs with\nhuman causal expertise. AI researchers have addressed this direction, with\nexplainable AI and neuro-symbolic AI being the next logical steps in further\nadvancement in AI. However, they are still based on data-driven knowledge\nintegration instead of human-in-the-loop approaches. Future research should\nmeasure success not only by accuracy but by improvements in clinician\nunderstanding, workflow fit, and patient outcomes. A better understanding of\nwhat helps improve human-computer interactions is greatly needed for AI systems\nto become part of clinical practice."}
{"id": "2507.01376", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01376", "abs": "https://arxiv.org/abs/2507.01376", "authors": ["Yinwang Ren", "Yangyang Liu", "Tang Ji", "Xun Xu"], "title": "AI Agents and Agentic AI-Navigating a Plethora of Concepts for Future Manufacturing", "comment": "Submitted to JMS(March 2025)", "summary": "AI agents are autonomous systems designed to perceive, reason, and act within\ndynamic environments. With the rapid advancements in generative AI (GenAI),\nlarge language models (LLMs) and multimodal large language models (MLLMs) have\nsignificantly improved AI agents' capabilities in semantic comprehension,\ncomplex reasoning, and autonomous decision-making. At the same time, the rise\nof Agentic AI highlights adaptability and goal-directed autonomy in dynamic and\ncomplex environments. LLMs-based AI Agents (LLM-Agents), MLLMs-based AI Agents\n(MLLM-Agents), and Agentic AI contribute to expanding AI's capabilities in\ninformation processing, environmental perception, and autonomous\ndecision-making, opening new avenues for smart manufacturing. However, the\ndefinitions, capability boundaries, and practical applications of these\nemerging AI paradigms in smart manufacturing remain unclear. To address this\ngap, this study systematically reviews the evolution of AI and AI agent\ntechnologies, examines the core concepts and technological advancements of\nLLM-Agents, MLLM-Agents, and Agentic AI, and explores their potential\napplications in and integration into manufacturing, along with the potential\nchallenges they may face."}
{"id": "2507.01410", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01410", "abs": "https://arxiv.org/abs/2507.01410", "authors": ["Abeer Dyoub", "Francesca A. Lisi"], "title": "A Fuzzy Approach to the Specification, Verification and Validation of Risk-Based Ethical Decision Making Models", "comment": null, "summary": "The ontological and epistemic complexities inherent in the moral domain make\nit challenging to establish clear standards for evaluating the performance of a\nmoral machine. In this paper, we present a formal method to describe Ethical\nDecision Making models based on ethical risk assessment. Then, we show how\nthese models that are specified as fuzzy rules can be verified and validated\nusing fuzzy Petri nets. A case study from the medical field is considered to\nillustrate the proposed approach."}
{"id": "2507.01431", "categories": ["cs.AI", "cs.CL", "cs.HC", "cs.LG"], "pdf": "https://arxiv.org/pdf/2507.01431", "abs": "https://arxiv.org/abs/2507.01431", "authors": ["Yoonseok Yang", "Minjune Kim", "Marlon Rondinelli", "Keren Shao"], "title": "Pensieve Grader: An AI-Powered, Ready-to-Use Platform for Effortless Handwritten STEM Grading", "comment": "7 pages, 5 figues, 1 table", "summary": "Grading handwritten, open-ended responses remains a major bottleneck in large\nuniversity STEM courses. We introduce Pensieve (https://www.pensieve.co), an\nAI-assisted grading platform that leverages large language models (LLMs) to\ntranscribe and evaluate student work, providing instructors with rubric-aligned\nscores, transcriptions, and confidence ratings. Unlike prior tools that focus\nnarrowly on specific tasks like transcription or rubric generation, Pensieve\nsupports the entire grading pipeline-from scanned student submissions to final\nfeedback-within a human-in-the-loop interface.\n  Pensieve has been deployed in real-world courses at over 20 institutions and\nhas graded more than 300,000 student responses. We present system details and\nempirical results across four core STEM disciplines: Computer Science,\nMathematics, Physics, and Chemistry. Our findings show that Pensieve reduces\ngrading time by an average of 65%, while maintaining a 95.4% agreement rate\nwith instructor-assigned grades for high-confidence predictions."}
{"id": "2507.01446", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01446", "abs": "https://arxiv.org/abs/2507.01446", "authors": ["Abd Elrahman Amer", "Magdi Amer"], "title": "Using multi-agent architecture to mitigate the risk of LLM hallucinations", "comment": null, "summary": "Improving customer service quality and response time are critical factors for\nmaintaining customer loyalty and increasing a company's market share. While\nadopting emerging technologies such as Large Language Models (LLMs) is becoming\na necessity to achieve these goals, the risk of hallucination remains a major\nchallenge. In this paper, we present a multi-agent system to handle customer\nrequests sent via SMS. This system integrates LLM based agents with fuzzy logic\nto mitigate hallucination risks."}
{"id": "2507.01489", "categories": ["cs.AI", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01489", "abs": "https://arxiv.org/abs/2507.01489", "authors": ["Yanfei Zhang"], "title": "Agent-as-Tool: A Study on the Hierarchical Decision Making with Reinforcement Learning", "comment": "12 pages", "summary": "Large Language Models (LLMs) have emerged as one of the most significant\ntechnological advancements in artificial intelligence in recent years. Their\nability to understand, generate, and reason with natural language has\ntransformed how we interact with AI systems. With the development of LLM-based\nagents and reinforcement-learning-based reasoning models, the study of applying\nreinforcement learning in agent frameworks has become a new research focus.\nHowever, all previous studies face the challenge of deciding the tool calling\nprocess and the reasoning process simultaneously, and the chain of reasoning\nwas solely relied on the unprocessed raw result with redundant information and\nsymbols unrelated to the task from the tool, which impose a heavy burden on the\nmodel's capability to reason. Therefore, in our research, we proposed a\nhierarchical framework Agent-as-tool that detach the tool calling process and\nthe reasoning process, which enables the model to focus on the verbally\nreasoning process while the tool calling process is handled by another agent.\nOur work had achieved comparable results with only a slight reinforcement\nfine-tuning on 180 samples, and had achieved exceptionally well performance in\nBamboogle with 63.2% of exact match and 75.2% in cover exact match, exceeding\nSearch-R1 by 4.8% in exact match and 3.2% in cover exact match."}
{"id": "2507.01597", "categories": ["cs.AI", "cs.CL"], "pdf": "https://arxiv.org/pdf/2507.01597", "abs": "https://arxiv.org/abs/2507.01597", "authors": ["Yuehang Si", "Zefan Zeng", "Jincai Huang", "Qing Cheng"], "title": "T3DM: Test-Time Training-Guided Distribution Shift Modelling for Temporal Knowledge Graph Reasoning", "comment": null, "summary": "Temporal Knowledge Graph (TKG) is an efficient method for describing the\ndynamic development of facts along a timeline. Most research on TKG reasoning\n(TKGR) focuses on modelling the repetition of global facts and designing\npatterns of local historical facts. However, they face two significant\nchallenges: inadequate modeling of the event distribution shift between\ntraining and test samples, and reliance on random entity substitution for\ngenerating negative samples, which often results in low-quality sampling. To\nthis end, we propose a novel distributional feature modeling approach for\ntraining TKGR models, Test-Time Training-guided Distribution shift Modelling\n(T3DM), to adjust the model based on distribution shift and ensure the global\nconsistency of model reasoning. In addition, we design a negative-sampling\nstrategy to generate higher-quality negative quadruples based on adversarial\ntraining. Extensive experiments show that T3DM provides better and more robust\nresults than the state-of-the-art baselines in most cases."}
{"id": "2507.01717", "categories": ["cs.AI", "cs.IR", "cs.LG", "cs.MA"], "pdf": "https://arxiv.org/pdf/2507.01717", "abs": "https://arxiv.org/abs/2507.01717", "authors": ["Gopichand Kanumolu", "Ashok Urlana", "Charaka Vinayak Kumar", "Bala Mallikarjunarao Garlapati"], "title": "Agent Ideate: A Framework for Product Idea Generation from Patents Using Agentic AI", "comment": "AgentScen Workshop, IJCAI 2025", "summary": "Patents contain rich technical knowledge that can inspire innovative product\nideas, yet accessing and interpreting this information remains a challenge.\nThis work explores the use of Large Language Models (LLMs) and autonomous\nagents to mine and generate product concepts from a given patent. In this work,\nwe design Agent Ideate, a framework for automatically generating product-based\nbusiness ideas from patents. We experimented with open-source LLMs and\nagent-based architectures across three domains: Computer Science, Natural\nLanguage Processing, and Material Chemistry. Evaluation results show that the\nagentic approach consistently outperformed standalone LLMs in terms of idea\nquality, relevance, and novelty. These findings suggest that combining LLMs\nwith agentic workflows can significantly enhance the innovation pipeline by\nunlocking the untapped potential of business idea generation from patent data."}
{"id": "2507.01749", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01749", "abs": "https://arxiv.org/abs/2507.01749", "authors": ["Arash Dehghan", "Mucahit Cevik", "Merve Bodur", "Bissan Ghaddar"], "title": "Joint Matching and Pricing for Crowd-shipping with In-store Customers", "comment": null, "summary": "This paper examines the use of in-store customers as delivery couriers in a\ncentralized crowd-shipping system, targeting the growing need for efficient\nlast-mile delivery in urban areas. We consider a brick-and-mortar retail\nsetting where shoppers are offered compensation to deliver time-sensitive\nonline orders. To manage this process, we propose a Markov Decision Process\n(MDP) model that captures key uncertainties, including the stochastic arrival\nof orders and crowd-shippers, and the probabilistic acceptance of delivery\noffers. Our solution approach integrates Neural Approximate Dynamic Programming\n(NeurADP) for adaptive order-to-shopper assignment with a Deep Double Q-Network\n(DDQN) for dynamic pricing. This joint optimization strategy enables multi-drop\nrouting and accounts for offer acceptance uncertainty, aligning more closely\nwith real-world operations. Experimental results demonstrate that the\nintegrated NeurADP + DDQN policy achieves notable improvements in delivery cost\nefficiency, with up to 6.7\\% savings over NeurADP with fixed pricing and\napproximately 18\\% over myopic baselines. We also show that allowing flexible\ndelivery delays and enabling multi-destination routing further reduces\noperational costs by 8\\% and 17\\%, respectively. These findings underscore the\nadvantages of dynamic, forward-looking policies in crowd-shipping systems and\noffer practical guidance for urban logistics operators."}
{"id": "2507.01833", "categories": ["cs.AI"], "pdf": "https://arxiv.org/pdf/2507.01833", "abs": "https://arxiv.org/abs/2507.01833", "authors": ["Yi-Dong Shen", "Thomas Eiter"], "title": "Refining Gelfond Rationality Principle Towards More Comprehensive Foundational Principles for Answer Set Semantics", "comment": "76 pages. This article is a significantly extended version of a paper\n  presented by the authors at IJCAI-2022", "summary": "Non-monotonic logic programming is the basis for a declarative problem\nsolving paradigm known as answer set programming (ASP). Departing from the\nseminal definition by Gelfond and Lifschitz in 1988 for simple normal logic\nprograms, various answer set semantics have been proposed for extensions. We\nconsider two important questions: (1) Should the minimal model property,\nconstraint monotonicity and foundedness as defined in the literature be\nmandatory conditions for an answer set semantics in general? (2) If not, what\nother properties could be considered as general principles for answer set\nsemantics? We address the two questions. First, it seems that the three\naforementioned conditions may sometimes be too strong, and we illustrate with\nexamples that enforcing them may exclude expected answer sets. Second, we\nevolve the Gelfond answer set (GAS) principles for answer set construction by\nrefining the Gelfond's rationality principle to well-supportedness, minimality\nw.r.t. negation by default and minimality w.r.t. epistemic negation. The\nprinciple of well-supportedness guarantees that every answer set is\nconstructible from if-then rules obeying a level mapping and is thus free of\ncircular justification, while the two minimality principles ensure that the\nformalism minimizes knowledge both at the level of answer sets and of world\nviews. Third, to embody the refined GAS principles, we extend the notion of\nwell-supportedness substantially to answer sets and world views, respectively.\nFourth, we define new answer set semantics in terms of the refined GAS\nprinciples. Fifth, we use the refined GAS principles as an alternative baseline\nto intuitively assess the existing answer set semantics. Finally, we analyze\nthe computational complexity."}
