<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 44]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Think Clearly: Improving Reasoning via Redundant Token Pruning](https://arxiv.org/abs/2507.08806)
*Daewon Choi,Jimin Lee,Jihoon Tack,Woomin Song,Saket Dingliwal,Sai Muralidhar Jayanthi,Bhavana Ganesh,Jinwoo Shin,Aram Galstyan,Sravan Babu Bodapati*

Main category: cs.AI

TL;DR: 论文提出通过去除推理过程中的冗余注意力，显著提升大语言模型在长链推理任务中的性能。


<details>
  <summary>Details</summary>
Motivation: 现有大语言模型在长链推理中存在注意力冗余问题，尤其是错误答案中注意力稀疏性更高，影响性能。

Method: 通过测量特殊‘思考结束’标记的注意力分数识别冗余，采用结构感知剪枝去除低贡献推理块，并恢复生成。

Result: 方法显著提升了推理密集型任务的准确性，尤其在数学竞赛基准（如AIME和AMC）上表现突出。

Conclusion: 去除推理冗余能有效提升模型性能，无需额外训练，适用于复杂推理任务。

Abstract: Recent large language models have shown promising capabilities in long-form
reasoning, following structured chains of thought before arriving at a final
answer. However, we observe that these reasoning paths tend to include
substantial redundancy; analyzing attention patterns reveals that attention
scores are widely scattered, particularly incorrect answers exhibit greater
attention sparsity. In this paper, we demonstrate that deliberately removing
this redundancy in the reasoning process significantly improves performance
through clear thinking, i.e., removing distraction. Specifically, we
systematically identify reasoning redundancy by measuring token-level attention
scores to a special end-of-thinking token, which is appended to an explicit
instruction inserted to conclude each intermediate reasoning step. Furthermore,
we propose structure-aware pruning that prioritizes removing tokens in
low-contributing reasoning chunks over individual tokens. After evicting
redundant tokens, we remove the injected end-of-thinking instruction, then
resume the reasoning generation. We demonstrate that our method significantly
improves overall accuracy across reasoning-intensive benchmarks without any
training involved. In particular, our method shows strong performance on
challenging mathematical competition benchmarks such as AIME and AMC, where
reasoning redundancy is more prevalent.

</details>


### [2] [A New Approach for Multicriteria Assessment in the Ranking of Alternatives Using Cardinal and Ordinal Data](https://arxiv.org/abs/2507.08875)
*Fuh-Hwa Franklin Liu,Su-Chuan Shih*

Main category: cs.AI

TL;DR: 提出了一种结合两种虚拟差距分析（VGA）模型的新型多准则评估（MCA）方法，以提高评估的效率和公平性。


<details>
  <summary>Details</summary>
Motivation: 现有MCA方法依赖假设和主观判断，难以处理复杂评估问题，且常忽略定量和定性准则的差异。

Method: 结合两种VGA模型，基于线性规划，改进MCA方法。

Result: 通过数值示例验证了方法的准确性和透明度。

Conclusion: 该方法为自动化决策系统提供了可靠且灵活的解决方案，推动了决策支持系统的进步。

Abstract: Modern methods for multi-criteria assessment (MCA), such as Data Envelopment
Analysis (DEA), Stochastic Frontier Analysis (SFA), and Multiple Criteria
Decision-Making (MCDM), are utilized to appraise a collection of
Decision-Making Units (DMUs), also known as alternatives, based on several
criteria. These methodologies inherently rely on assumptions and can be
influenced by subjective judgment to effectively tackle the complex evaluation
challenges in various fields. In real-world scenarios, it is essential to
incorporate both quantitative and qualitative criteria as they consist of
cardinal and ordinal data. Despite the inherent variability in the criterion
values of different alternatives, the homogeneity assumption is often employed,
significantly affecting evaluations. To tackle these challenges and determine
the most appropriate alternative, we propose a novel MCA approach that combines
two Virtual Gap Analysis (VGA) models. The VGA framework, rooted in linear
programming, is pivotal in the MCA methodology. This approach improves
efficiency and fairness, ensuring that evaluations are both comprehensive and
dependable, thus offering a strong and adaptive solution. Two comprehensive
numerical examples demonstrate the accuracy and transparency of our proposed
method. The goal is to encourage continued advancement and stimulate progress
in automated decision systems and decision support systems.

</details>


### [3] [Multi-Actor Generative Artificial Intelligence as a Game Engine](https://arxiv.org/abs/2507.08892)
*Alexander Sasha Vezhnevets,Jayd Matyas,Logan Cross,Davide Paglieri,Minsuk Chang,William A. Cunningham,Simon Osindero,William S. Isaac,Joel Z. Leibo*

Main category: cs.AI

TL;DR: 论文提出了一种基于桌游角色扮演游戏（TTRPGs）的灵活场景定义框架，用于支持生成式AI在多角色环境中的多样化应用。


<details>
  <summary>Details</summary>
Motivation: 为了满足生成式AI在模拟、戏剧化和评估等多样化应用场景中的需求，需要一个灵活的框架来定义和管理场景。

Method: 借鉴TTRPGs中的游戏主持人（GM）概念，采用实体-组件架构模式，将GM设计为可配置的实体，由组件构成，实现工程师与设计师的分工协作。

Result: 通过Concordia库的实践，证明了该框架能够有效支持用户根据具体目标配置场景，实现快速迭代和模块化。

Conclusion: 该框架通过分离关注点，提升了生成式AI在多角色环境中的灵活性和可扩展性。

Abstract: Generative AI can be used in multi-actor environments with purposes ranging
from social science modeling to interactive narrative and AI evaluation.
Supporting this diversity of use cases -- which we classify as Simulationist,
Dramatist, and Evaluationist -- demands a flexible scenario definition
framework. We argue here that a good approach is to take inspiration from
tabletop role-playing games (TTRPGs), where a Game Master (GM) is responsible
for the environment and generates all parts of the story not directly
determined by the voluntary actions of player characters. We argue that the
Entity-Component architectural pattern is useful here. In such a system, the GM
is not a hardcoded computer game but is itself a configurable entity, composed
of components just like any other actor. By design, the approach allows for a
separation between the underlying implementation details handled by an
engineer, the creation of reusable components, and their composition and
configuration managed by a designer who constructs entities from the
components. This separation of concerns is instrumental for achieving rapid
iteration, maintaining modularity, and ultimately to ensure scalability. We
describe the ongoing evolution of the Concordia library in terms of this
philosophy, demonstrating how it allows users to effectively configure
scenarios that align with their specific goals.

</details>


### [4] [BioAnalyst: A Foundation Model for Biodiversity](https://arxiv.org/abs/2507.09080)
*Athanasios Trantas,Martino Mensio,Stylianos Stasinos,Sebastian Gribincea,Taimur Khan,Damian Podareanu,Aliene van der Veen*

Main category: cs.AI

TL;DR: BioAnalyst是一个基于Transformer架构的AI基础模型，专为生物多样性分析和保护规划设计，通过多模态数据集预训练，适用于多种下游任务，并在数据稀缺场景中表现出色。


<details>
  <summary>Details</summary>
Motivation: 生物多样性丧失加速，威胁生态平衡和可持续性，需综合监测和保护规划能力。AI基础模型在科学领域表现优异，有望用于生物多样性保护。

Method: BioAnalyst采用Transformer架构，预训练于物种记录、遥感数据、气候和环境变量等多模态数据集，可微调用于物种分布建模、栖息地评估等任务。

Result: 模型在两种下游任务中表现出色，尤其在数据稀缺场景中优于现有方法，为生态预测设定了新的准确基准。

Conclusion: BioAnalyst的开放发布旨在促进生物多样性建模合作，推动AI解决生态挑战。

Abstract: The accelerating loss of biodiversity presents critical challenges for
ecological research and conservation strategies. The preservation of
biodiversity is paramount for maintaining ecological balance and ensuring the
sustainability of ecosystems. However, biodiversity faces numerous threats,
including habitat loss, climate change, and the proliferation of invasive
species. Addressing these and other ecology-related challenges, both at local
and global scales, requires comprehensive monitoring, predictive and
conservation planning capabilities. Artificial Intelligence (AI) Foundation
Models (FMs) have gained significant momentum in numerous scientific domains by
leveraging vast datasets to learn general-purpose representations adaptable to
various downstream tasks. This paradigm holds immense promise for biodiversity
conservation. In response, we introduce BioAnalyst, the first Foundation Model
tailored for biodiversity analysis and conservation planning. BioAnalyst
employs a transformer-based architecture, pre-trained on extensive multi-modal
datasets encompassing species occurrence records, remote sensing indicators,
climate and environmental variables. BioAnalyst is designed for adaptability,
allowing for fine-tuning of a range of downstream tasks, such as species
distribution modelling, habitat suitability assessments, invasive species
detection, and population trend forecasting. We evaluate the model's
performance on two downstream use cases, demonstrating its generalisability
compared to existing methods, particularly in data-scarce scenarios for two
distinct use-cases, establishing a new accuracy baseline for ecological
forecasting. By openly releasing BioAnalyst and its fine-tuning workflows to
the scientific community, we aim to foster collaborative efforts in
biodiversity modelling and advance AI-driven solutions to pressing ecological
challenges.

</details>


### [5] [Measuring the Impact of Early-2025 AI on Experienced Open-Source Developer Productivity](https://arxiv.org/abs/2507.09089)
*Joel Becker,Nate Rush,Elizabeth Barnes,David Rein*

Main category: cs.AI

TL;DR: 研究发现，尽管开发者预期AI工具能缩短任务完成时间，但实际上AI工具反而增加了19%的完成时间，与经济学和机器学习专家的预测相反。


<details>
  <summary>Details</summary>
Motivation: 探讨AI工具在实际软件开发中对开发者生产力的影响，填补现有研究的空白。

Method: 通过随机对照试验（RCT），16名有中等AI经验的开发者完成246项任务，随机分配是否使用2025年的AI工具（如Cursor Pro和Claude 3.5/3.7 Sonnet）。

Result: AI工具的使用导致任务完成时间增加19%，与开发者预期的20%缩短和专家预测的38-39%缩短相反。

Conclusion: AI工具在实际应用中可能并未如预期提升生产力，甚至可能因某些因素导致效率下降。

Abstract: Despite widespread adoption, the impact of AI tools on software development
in the wild remains understudied. We conduct a randomized controlled trial
(RCT) to understand how AI tools at the February-June 2025 frontier affect the
productivity of experienced open-source developers. 16 developers with moderate
AI experience complete 246 tasks in mature projects on which they have an
average of 5 years of prior experience. Each task is randomly assigned to allow
or disallow usage of early 2025 AI tools. When AI tools are allowed, developers
primarily use Cursor Pro, a popular code editor, and Claude 3.5/3.7 Sonnet.
Before starting tasks, developers forecast that allowing AI will reduce
completion time by 24%. After completing the study, developers estimate that
allowing AI reduced completion time by 20%. Surprisingly, we find that allowing
AI actually increases completion time by 19%--AI tooling slowed developers
down. This slowdown also contradicts predictions from experts in economics (39%
shorter) and ML (38% shorter). To understand this result, we collect and
evaluate evidence for 20 properties of our setting that a priori could
contribute to the observed slowdown effect--for example, the size and quality
standards of projects, or prior developer experience with AI tooling. Although
the influence of experimental artifacts cannot be entirely ruled out, the
robustness of the slowdown effect across our analyses suggests it is unlikely
to primarily be a function of our experimental design.

</details>


### [6] [Hide-and-Shill: A Reinforcement Learning Framework for Market Manipulation Detection in Symphony-a Decentralized Multi-Agent System](https://arxiv.org/abs/2507.09179)
*Ronghua Shi,Yiou Liu,Xinyu Ying,Yang Tan,Yuchun Feng,Lynn Ai,Bill Shi,Xuhui Wang,Zhuang Liu*

Main category: cs.AI

TL;DR: 提出了一种基于多智能体强化学习（MARL）的框架，用于检测去中心化金融（DeFi）中的市场操纵行为，通过动态对抗游戏建模操纵者与检测者之间的交互。


<details>
  <summary>Details</summary>
Motivation: DeFi的无许可创新带来了市场操纵问题，缺乏中心化监管导致恶意行为者通过协调活动进行操纵。

Method: 采用MARL框架，结合GRPO优化、理论驱动的奖励函数和多模态智能体管道，整合语义特征、社交图谱和链上数据。

Result: 在真实数据和对抗模拟中验证，Hide-and-Shill在检测准确性和因果归因方面表现优异。

Conclusion: 该框架为去中心化市场情报提供了新范式，支持实时监测和开放研究。

Abstract: Decentralized finance (DeFi) has introduced a new era of permissionless
financial innovation but also led to unprecedented market manipulation. Without
centralized oversight, malicious actors coordinate shilling campaigns and
pump-and-dump schemes across various platforms. We propose a Multi-Agent
Reinforcement Learning (MARL) framework for decentralized manipulation
detection, modeling the interaction between manipulators and detectors as a
dynamic adversarial game. This framework identifies suspicious patterns using
delayed token price reactions as financial indicators.Our method introduces
three innovations: (1) Group Relative Policy Optimization (GRPO) to enhance
learning stability in sparse-reward and partially observable settings; (2) a
theory-based reward function inspired by rational expectations and information
asymmetry, differentiating price discovery from manipulation noise; and (3) a
multi-modal agent pipeline that integrates LLM-based semantic features, social
graph signals, and on-chain market data for informed decision-making.The
framework is integrated within the Symphony system, a decentralized multi-agent
architecture enabling peer-to-peer agent execution and trust-aware learning
through distributed logs, supporting chain-verifiable evaluation. Symphony
promotes adversarial co-evolution among strategic actors and maintains robust
manipulation detection without centralized oracles, enabling real-time
surveillance across global DeFi ecosystems.Trained on 100,000 real-world
discourse episodes and validated in adversarial simulations, Hide-and-Shill
achieves top performance in detection accuracy and causal attribution. This
work bridges multi-agent systems with financial surveillance, advancing a new
paradigm for decentralized market intelligence. All resources are available at
the Hide-and-Shill GitHub repository to promote open research and
reproducibility.

</details>


### [7] [When Developer Aid Becomes Security Debt: A Systematic Analysis of Insecure Behaviors in LLM Coding Agents](https://arxiv.org/abs/2507.09329)
*Matous Kozak,Roshanak Zilouchian Moghaddam,Siva Sivaraman*

Main category: cs.AI

TL;DR: 本文首次系统评估了基于LLM的编码代理的安全性，发现21%的操作存在安全隐患，并提出了检测系统和缓解策略。


<details>
  <summary>Details</summary>
Motivation: 理解基于LLM的编码代理在软件开发中的安全影响，填补当前研究的空白。

Method: 分析了12,000多个操作，覆盖5个先进模型（如GPT-4o、GPT-4.1等）在93个实际软件任务中的表现。

Result: 21%的操作不安全，信息泄露（CWE-200）最常见；GPT-4.1的缓解成功率高达96.8%。

Conclusion: 提出了首个编码代理安全评估框架，强调下一代LLM编码代理需具备安全意识设计。

Abstract: LLM-based coding agents are rapidly being deployed in software development,
yet their security implications remain poorly understood. These agents, while
capable of accelerating software development, may inadvertently introduce
insecure practices. We conducted the first systematic security evaluation of
autonomous coding agents, analyzing over 12,000 actions across five
state-of-the-art models (GPT-4o, GPT-4.1, Claude variants) on 93 real-world
software setup tasks. Our findings reveal significant security concerns: 21% of
agent trajectories contained insecure actions, with models showing substantial
variation in security behavior. We developed a high-precision detection system
that identified four major vulnerability categories, with information exposure
(CWE-200) being the most prevalent one. We also evaluated mitigation strategies
including feedback mechanisms and security reminders with various effectiveness
between models. GPT-4.1 demonstrated exceptional security awareness with 96.8%
mitigation success. Our work provides the first comprehensive framework for
evaluating coding agent security and highlights the need for security-aware
design of next generation LLM-based coding agents.

</details>


### [8] [A Taxonomy of Omnicidal Futures Involving Artificial Intelligence](https://arxiv.org/abs/2507.09369)
*Andrew Critch,Jacob Tsimerman*

Main category: cs.AI

TL;DR: 本文提出了一种关于AI可能导致人类灭绝事件的分类和示例，旨在通过公开讨论支持预防措施。


<details>
  <summary>Details</summary>
Motivation: 探讨AI可能带来的灾难性风险，以促进公众支持预防措施。

Method: 提出分类法和具体示例，分析AI可能导致的全人类灭绝事件。

Result: 明确了AI可能引发的灾难性风险，并呼吁公众关注和预防。

Conclusion: 通过公开讨论AI的潜在风险，可以推动社会采取预防措施，避免灾难性后果。

Abstract: This report presents a taxonomy and examples of potential omnicidal events
resulting from AI: scenarios where all or almost all humans are killed. These
events are not presented as inevitable, but as possibilities that we can work
to avoid. Insofar as large institutions require a degree of public support in
order to take certain actions, we hope that by presenting these possibilities
in public, we can help to support preventive measures against catastrophic
risks from AI.

</details>


### [9] [EduFlow: Advancing MLLMs' Problem-Solving Proficiency through Multi-Stage, Multi-Perspective Critique](https://arxiv.org/abs/2507.09374)
*Chenglin Zhu,Tao Zhang,Chong Li,Mingan Lin,Zenan Zhou,Jian Xie*

Main category: cs.AI

TL;DR: EduFlow是一个端到端框架，通过EduPRM和EduMCTS提升多模态大语言模型在科学任务中的推理能力，显著改善了推理的一致性和连贯性。


<details>
  <summary>Details</summary>
Motivation: 多模态大语言模型在科学任务中表现不佳，尤其是在需要多步和可解释推理的任务上，存在推理模式不足、全局连贯性缺乏和自校正缺失等问题。

Method: 提出了EduFlow框架，包括EduPRM（过程感知奖励模型）和EduMCTS（领域适应的搜索框架），结合课程学习和多源监督训练，优化推理轨迹。

Result: 实验表明，EduFlow显著提升了推理的一致性和连贯性，并构建了EduMCTS-160K数据集。

Conclusion: EduFlow通过动态适应和迭代优化，为科学推理任务提供了可靠的解决方案。

Abstract: Multimodal large language models (MLLMs) still perform poorly on scientific
tasks, particularly those requiring multi-step and interpretable reasoning.
Their limitations include insufficient scientific reasoning patterns, lack of
global coherence in multi-step inference, and the absence of reflective
self-correction, making them unreliable in structured scientific contexts. We
introduce EduFlow, the first end-to-end framework that covers the full pipeline
of educational scientific reasoning, including data selection, MCTS-based
trajectory construction, model training, and output optimization. At its core
is EduPRM, a process-aware reward model that critiques reasoning steps with
tags and justifications. EduPRM is trained via curriculum learning on three
complementary supervision sources: MCTS-guided trajectories, error-injected
critiques, and teacher-student dialogues, enabling dynamic adaptation to
multi-stage problem solving and iterative refinement during inference. We
further propose EduMCTS, a domain-adapted search framework that introduces
bootstrapping actions specifically designed for educational reasoning, such as
a self-reflection mechanism that promotes reflective error correction. It
further leverages EduPRM's fine-grained feedback to guide the search toward
higher-quality reasoning trajectories. By applying self-consistency and
rejection sampling, we constructed EduMCTS-160K, a large-scale dataset of
educational reasoning trajectories. Extensive experiments demonstrate that
EduFlow enhances reasoning consistency and coherence. Code, data, and models
will be released.

</details>


### [10] [Knowledge Conceptualization Impacts RAG Efficacy](https://arxiv.org/abs/2507.09389)
*Chris Davis Jaldi,Anmol Saini,Elham Ghiasi,O. Divine Eziolise,Cogan Shimizu*

Main category: cs.AI

TL;DR: 论文探讨了如何结合可解释性和适应性设计可迁移且可解释的神经符号AI系统，重点关注Agentic Retrieval-Augmented Generation系统，并评估知识表示对LLM查询三元组存储的影响。


<details>
  <summary>Details</summary>
Motivation: 可解释性和适应性是下一代AI系统的关键，尤其是在LLM和生成式AI中。研究旨在结合这两点，设计可迁移且可解释的神经符号AI系统。

Method: 系统评估不同知识表示（结构和复杂性）对LLM查询三元组存储的影响，重点关注Agentic Retrieval-Augmented Generation系统。

Result: 结果显示不同知识表示对LLM查询效果有显著影响。

Conclusion: 研究强调了知识表示在AI系统中的重要性，为设计更高效、可解释的AI系统提供了方向。

Abstract: Explainability and interpretability are cornerstones of frontier and
next-generation artificial intelligence (AI) systems. This is especially true
in recent systems, such as large language models (LLMs), and more broadly,
generative AI. On the other hand, adaptability to new domains, contexts, or
scenarios is also an important aspect for a successful system. As such, we are
particularly interested in how we can merge these two efforts, that is,
investigating the design of transferable and interpretable neurosymbolic AI
systems. Specifically, we focus on a class of systems referred to as ''Agentic
Retrieval-Augmented Generation'' systems, which actively select, interpret, and
query knowledge sources in response to natural language prompts. In this paper,
we systematically evaluate how different conceptualizations and representations
of knowledge, particularly the structure and complexity, impact an AI agent (in
this case, an LLM) in effectively querying a triplestore. We report our
results, which show that there are impacts from both approaches, and we discuss
their impact and implications.

</details>


### [11] [LLM-Stackelberg Games: Conjectural Reasoning Equilibria and Their Applications to Spearphishing](https://arxiv.org/abs/2507.09407)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Stackelberg游戏框架将大语言模型（LLMs）融入领导者与追随者的策略互动中，突破了经典Stackelberg模型的完全信息和理性假设，通过结构化提示和概率行为生成实现策略适应。


<details>
  <summary>Details</summary>
Motivation: 传统Stackelberg模型假设完全信息和理性行为，无法捕捉现实中的有限理性、信息不对称和认知适应。本文旨在通过LLMs扩展这一框架，以更真实地模拟复杂决策场景。

Method: 提出LLM-Stackelberg游戏框架，定义两种均衡概念：推理与行为均衡（内部推理与行为一致）和推测推理均衡（考虑对手响应的认知不确定性）。通过结构化提示和LLMs生成行为。

Result: 在钓鱼攻击案例中展示了LLM-Stackelberg框架的认知丰富性和对抗潜力，证明了其在网络安全、错误信息和推荐系统等领域的适用性。

Conclusion: LLM-Stackelberg游戏为复杂决策领域提供了强大的建模工具，能够捕捉有限理性和认知动态。

Abstract: We introduce the framework of LLM-Stackelberg games, a class of sequential
decision-making models that integrate large language models (LLMs) into
strategic interactions between a leader and a follower. Departing from
classical Stackelberg assumptions of complete information and rational agents,
our formulation allows each agent to reason through structured prompts,
generate probabilistic behaviors via LLMs, and adapt their strategies through
internal cognition and belief updates. We define two equilibrium concepts:
reasoning and behavioral equilibrium, which aligns an agent's internal
prompt-based reasoning with observable behavior, and conjectural reasoning
equilibrium, which accounts for epistemic uncertainty through parameterized
models over an opponent's response. These layered constructs capture bounded
rationality, asymmetric information, and meta-cognitive adaptation. We
illustrate the framework through a spearphishing case study, where a sender and
a recipient engage in a deception game using structured reasoning prompts. This
example highlights the cognitive richness and adversarial potential of
LLM-mediated interactions. Our results show that LLM-Stackelberg games provide
a powerful paradigm for modeling decision-making in domains such as
cybersecurity, misinformation, and recommendation systems.

</details>


### [12] [GenAI-based Multi-Agent Reinforcement Learning towards Distributed Agent Intelligence: A Generative-RL Agent Perspective](https://arxiv.org/abs/2507.09495)
*Hang Wang,Junshan Zhang*

Main category: cs.AI

TL;DR: 论文提出从反应式转向生成式AI的多智能体强化学习，以解决传统方法无法应对的联合动作空间、非平稳环境和部分可观测性问题。


<details>
  <summary>Details</summary>
Motivation: 传统多智能体强化学习方法在联合动作空间、非平稳环境和部分可观测性方面存在局限，无法应对新场景。

Method: 采用生成式AI的强化学习，将智能体视为生成模型，预测未来交互并生成协调动作序列。

Result: 生成式智能体能够进行前瞻性决策、增强协调和动态适应，实现真正的协作智能。

Conclusion: 这一范式转变有望在自主系统、机器人和人机协作中解决传统方法难以处理的协调问题。

Abstract: Multi-agent reinforcement learning faces fundamental challenges that
conventional approaches have failed to overcome: exponentially growing joint
action spaces, non-stationary environments where simultaneous learning creates
moving targets, and partial observability that constrains coordination. Current
methods remain reactive, employing stimulus-response mechanisms that fail when
facing novel scenarios. We argue for a transformative paradigm shift from
reactive to proactive multi-agent intelligence through generative AI-based
reinforcement learning. This position advocates reconceptualizing agents not as
isolated policy optimizers, but as sophisticated generative models capable of
synthesizing complex multi-agent dynamics and making anticipatory decisions
based on predictive understanding of future interactions. Rather than
responding to immediate observations, generative-RL agents can model
environment evolution, predict other agents' behaviors, generate coordinated
action sequences, and engage in strategic reasoning accounting for long-term
dynamics. This approach leverages pattern recognition and generation
capabilities of generative AI to enable proactive decision-making, seamless
coordination through enhanced communication, and dynamic adaptation to evolving
scenarios. We envision this paradigm shift will unlock unprecedented
possibilities for distributed intelligence, moving beyond individual
optimization toward emergent collective behaviors representing genuine
collaborative intelligence. The implications extend across autonomous systems,
robotics, and human-AI collaboration, promising solutions to coordination
challenges intractable under traditional reactive frameworks.

</details>


### [13] [Consistency Trajectory Planning: High-Quality and Efficient Trajectory Optimization for Offline Model-Based Reinforcement Learning](https://arxiv.org/abs/2507.09534)
*Guanquan Wang,Takuya Hiraoka,Yoshimasa Tsuruoka*

Main category: cs.AI

TL;DR: CTP是一种基于一致性轨迹模型的离线强化学习方法，通过单步轨迹生成实现高效优化，显著减少计算成本。


<details>
  <summary>Details</summary>
Motivation: 现有基于扩散模型的规划方法计算成本高，CTP旨在解决这一问题。

Method: 利用一致性轨迹模型（CTM）进行单步轨迹生成，避免迭代采样。

Result: 在D4RL基准测试中表现优于现有方法，计算速度提升120倍。

Conclusion: CTP是一种高效、低延迟的离线规划方法，适用于高性能任务。

Abstract: This paper introduces Consistency Trajectory Planning (CTP), a novel offline
model-based reinforcement learning method that leverages the recently proposed
Consistency Trajectory Model (CTM) for efficient trajectory optimization. While
prior work applying diffusion models to planning has demonstrated strong
performance, it often suffers from high computational costs due to iterative
sampling procedures. CTP supports fast, single-step trajectory generation
without significant degradation in policy quality. We evaluate CTP on the D4RL
benchmark and show that it consistently outperforms existing diffusion-based
planning methods in long-horizon, goal-conditioned tasks. Notably, CTP achieves
higher normalized returns while using significantly fewer denoising steps. In
particular, CTP achieves comparable performance with over $120\times$ speedup
in inference time, demonstrating its practicality and effectiveness for
high-performance, low-latency offline planning.

</details>


### [14] [Learning to Control Dynamical Agents via Spiking Neural Networks and Metropolis-Hastings Sampling](https://arxiv.org/abs/2507.09540)
*Ali Safa,Farida Mohsen,Ali Al-Zawqari*

Main category: cs.AI

TL;DR: 提出了一种基于Metropolis-Hastings采样的框架，用于训练脉冲神经网络（SNN）在强化学习任务中，避免了梯度方法的限制，并在两个控制基准测试中表现优于传统方法。


<details>
  <summary>Details</summary>
Motivation: 解决SNN在强化学习中训练困难的问题，尤其是由于脉冲通信的不可微分性导致的挑战。

Method: 采用Metropolis-Hastings采样作为贝叶斯推断技术，通过迭代提出并概率接受网络参数更新，基于累积奖励信号进行训练。

Result: 在AcroBot和CartPole基准测试中，该方法在最大化累积奖励、减少网络资源和训练次数方面优于传统Deep Q-Learning和现有SNN方法。

Conclusion: 该框架为SNN在强化学习中的应用提供了一种有效的无梯度训练方法，展示了在动态代理控制中的潜力。

Abstract: Spiking Neural Networks (SNNs) offer biologically inspired, energy-efficient
alternatives to traditional Deep Neural Networks (DNNs) for real-time control
systems. However, their training presents several challenges, particularly for
reinforcement learning (RL) tasks, due to the non-differentiable nature of
spike-based communication. In this work, we introduce what is, to our
knowledge, the first framework that employs Metropolis-Hastings (MH) sampling,
a Bayesian inference technique, to train SNNs for dynamical agent control in RL
environments without relying on gradient-based methods. Our approach
iteratively proposes and probabilistically accepts network parameter updates
based on accumulated reward signals, effectively circumventing the limitations
of backpropagation while enabling direct optimization on neuromorphic
platforms. We evaluated this framework on two standard control benchmarks:
AcroBot and CartPole. The results demonstrate that our MH-based approach
outperforms conventional Deep Q-Learning (DQL) baselines and prior SNN-based RL
approaches in terms of maximizing the accumulated reward while minimizing
network resources and training episodes.

</details>


### [15] [eSapiens: A Platform for Secure and Auditable Retrieval-Augmented Generation](https://arxiv.org/abs/2507.09588)
*Isaac Shi,Zeyuan Li,Fan Liu,Wenli Wang,Lewei He,Yang Yang,Tianyu Shi*

Main category: cs.AI

TL;DR: eSapiens是一个面向企业的AIaaS平台，整合专有数据、工作流程和主流LLM，提供数据安全和知识保留，并通过AI代理提升团队效率。


<details>
  <summary>Details</summary>
Motivation: 解决企业在AI应用中面临的数据安全、知识保留和高效工作流程的需求。

Method: 结合结构化文档处理、混合向量检索和无代码编排（LangChain），支持多种LLM，并引入THOR代理处理SQL查询。

Result: 实验显示，512 token分块检索精度最高（Top-3准确率91.3%），生成质量测试中eSapiens的上下文一致性提升23%。

Conclusion: eSapiens在高风险领域（如法律和金融）中实现了可信、可审计的AI工作流程。

Abstract: We present eSapiens, an AI-as-a-Service (AIaaS) platform engineered around a
business-oriented trifecta: proprietary data, operational workflows, and any
major agnostic Large Language Model (LLM). eSapiens gives businesses full
control over their AI assets, keeping everything in-house for AI knowledge
retention and data security. eSapiens AI Agents (Sapiens) empower your team by
providing valuable insights and automating repetitive tasks, enabling them to
focus on high-impact work and drive better business outcomes.
  The system integrates structured document ingestion, hybrid vector retrieval,
and no-code orchestration via LangChain, and supports top LLMs including
OpenAI, Claude, Gemini, and DeepSeek. A key component is the THOR Agent, which
handles structured SQL-style queries and generates actionable insights over
enterprise databases.
  To evaluate the system, we conduct two experiments. First, a retrieval
benchmark on legal corpora reveals that a chunk size of 512 tokens yields the
highest retrieval precision (Top-3 accuracy: 91.3%). Second, a generation
quality test using TRACe metrics across five LLMs shows that eSapiens delivers
more context-consistent outputs with up to a 23% improvement in factual
alignment.
  These results demonstrate the effectiveness of eSapiens in enabling
trustworthy, auditable AI workflows for high-stakes domains like legal and
finance.

</details>


### [16] [The Hidden Costs of AI: A Review of Energy, E-Waste, and Inequality in Model Development](https://arxiv.org/abs/2507.09611)
*Jenis Winsta*

Main category: cs.AI

TL;DR: 本文综述了AI快速发展带来的环境和伦理挑战，重点关注能源消耗、电子废物、计算资源不平等和网络安全系统的隐藏能源负担。


<details>
  <summary>Details</summary>
Motivation: 探讨AI在性能和效率之外的环境与伦理影响，推动负责任AI的发展。

Method: 通过分析近期研究和机构报告，系统梳理AI在能源、废物、资源分配和网络安全方面的负面影响。

Result: 揭示了模型训练的高排放、硬件快速淘汰、全球基础设施不平等以及AI安全的高能耗问题。

Conclusion: 呼吁AI发展需结合伦理责任和环境保护，以实现可持续和包容的技术未来。

Abstract: Artificial intelligence (AI) has made remarkable progress in recent years,
yet its rapid expansion brings overlooked environmental and ethical challenges.
This review explores four critical areas where AI's impact extends beyond
performance: energy consumption, electronic waste (e-waste), inequality in
compute access, and the hidden energy burden of cybersecurity systems. Drawing
from recent studies and institutional reports, the paper highlights systemic
issues such as high emissions from model training, rising hardware turnover,
global infrastructure disparities, and the energy demands of securing AI. By
connecting these concerns, the review contributes to Responsible AI discourse
by identifying key research gaps and advocating for sustainable, transparent,
and equitable development practices. Ultimately, it argues that AI's progress
must align with ethical responsibility and environmental stewardship to ensure
a more inclusive and sustainable technological future.

</details>


### [17] [Bridging Bots: from Perception to Action via Multimodal-LMs and Knowledge Graphs](https://arxiv.org/abs/2507.09617)
*Margherita Martorana,Francesca Urgese,Mark Adamik,Ilaria Tiddi*

Main category: cs.AI

TL;DR: 提出了一种结合多模态语言模型与知识图谱的神经符号框架，以提升服务机器人的互操作性和适应性。


<details>
  <summary>Details</summary>
Motivation: 解决现有服务机器人系统依赖专有硬件和软件、难以跨平台扩展的问题，同时结合符号系统与感知模型的优势。

Method: 提出神经符号框架，整合机器人感知数据、本体和多模态模型（LLaMA和GPT），生成符合本体的知识图谱。

Result: GPT-o1和LLaMA 4 Maverick表现最佳，但新模型不一定更好，整合策略是关键。

Conclusion: 神经符号框架有效支持机器人互操作性，整合策略对性能至关重要。

Abstract: Personal service robots are deployed to support daily living in domestic
environments, particularly for elderly and individuals requiring assistance.
These robots must perceive complex and dynamic surroundings, understand tasks,
and execute context-appropriate actions. However, current systems rely on
proprietary, hard-coded solutions tied to specific hardware and software,
resulting in siloed implementations that are difficult to adapt and scale
across platforms. Ontologies and Knowledge Graphs (KGs) offer a solution to
enable interoperability across systems, through structured and standardized
representations of knowledge and reasoning. However, symbolic systems such as
KGs and ontologies struggle with raw and noisy sensory input. In contrast,
multimodal language models are well suited for interpreting input such as
images and natural language, but often lack transparency, consistency, and
knowledge grounding. In this work, we propose a neurosymbolic framework that
combines the perceptual strengths of multimodal language models with the
structured representations provided by KGs and ontologies, with the aim of
supporting interoperability in robotic applications. Our approach generates
ontology-compliant KGs that can inform robot behavior in a platform-independent
manner. We evaluated this framework by integrating robot perception data,
ontologies, and five multimodal models (three LLaMA and two GPT models), using
different modes of neural-symbolic interaction. We assess the consistency and
effectiveness of the generated KGs across multiple runs and configurations, and
perform statistical analyzes to evaluate performance. Results show that GPT-o1
and LLaMA 4 Maverick consistently outperform other models. However, our
findings also indicate that newer models do not guarantee better results,
highlighting the critical role of the integration strategy in generating
ontology-compliant KGs.

</details>


### [18] [humancompatible.interconnect: Testing Properties of Repeated Uses of Interconnections of AI Systems](https://arxiv.org/abs/2507.09626)
*Rodion Nazarov,Anthony Quinn,Robert Shorten,Jakub Marecek*

Main category: cs.AI

TL;DR: 介绍了一个基于PyTorch的工具包，用于通过随机控制技术建模多代理AI系统的闭环交互，并提供公平性和鲁棒性的先验保证。


<details>
  <summary>Details</summary>
Motivation: 多代理AI系统的交互需要公平性和鲁棒性的先验保证，但传统方法难以实现。

Method: 使用基于PyTorch的工具包，通过随机控制技术建模闭环系统，并分析其重复使用特性。

Result: 工具包简化了多代理系统闭环模型的公平性保证，提供了先验保证。

Conclusion: 该工具包为多代理AI系统的公平性和鲁棒性提供了有效的解决方案。

Abstract: Artificial intelligence (AI) systems often interact with multiple agents. The
regulation of such AI systems often requires that {\em a priori\/} guarantees
of fairness and robustness be satisfied. With stochastic models of agents'
responses to the outputs of AI systems, such {\em a priori\/} guarantees
require non-trivial reasoning about the corresponding stochastic systems. Here,
we present an open-source PyTorch-based toolkit for the use of stochastic
control techniques in modelling interconnections of AI systems and properties
of their repeated uses. It models robustness and fairness desiderata in a
closed-loop fashion, and provides {\em a priori\/} guarantees for these
interconnections. The PyTorch-based toolkit removes much of the complexity
associated with the provision of fairness guarantees for closed-loop models of
multi-agent systems.

</details>


### [19] [Towards Concise and Adaptive Thinking in Large Reasoning Models: A Survey](https://arxiv.org/abs/2507.09662)
*Jason Zhu,Hongyu Li*

Main category: cs.AI

TL;DR: 大型推理模型（LRMs）在复杂任务上表现优异，但存在推理链冗长的问题，导致资源浪费。本文综述了简洁和自适应推理的研究进展。


<details>
  <summary>Details</summary>
Motivation: 解决LRMs在简单问题上生成冗长推理链的问题，以提高效率和实用性。

Method: 综述了近期关于简洁和自适应推理的方法、基准和挑战。

Result: 总结了该领域的研究进展，为未来探索提供方向。

Conclusion: 希望帮助研究者快速了解该领域，并启发新的自适应推理方法，以优化LRMs的使用。

Abstract: Large reasoning models (LRMs) like OpenAI o1 and DeepSeek R1 have
demonstrated impressive performance on complex reasoning tasks like mathematics
and programming with long Chain-of-Thought (CoT) reasoning sequences
(slow-thinking), compared with traditional large language models
(fast-thinking). However, these reasoning models also face a huge challenge
that generating unnecessarily lengthy and redundant reasoning chains even for
trivial questions. This phenomenon leads to a significant waste of inference
resources, increases the response time for simple queries, and hinders the
practical application of LRMs in real-world products. To this end, it is
crucial to shorten lengthy reasoning chains and learn adaptive reasoning
between fast and slow thinking based on input difficulty. In this survey, we
provide a comprehensive overview of recent progress in concise and adaptive
thinking for efficient reasoning of LRMs, including methodologies, benchmarks,
and challenges for future exploration. We hope this survey can help researchers
quickly understand the landscape of this field and inspire novel adaptive
thinking ideas to facilitate better usage of LRMs.

</details>


### [20] [Causality-informed Anomaly Detection in Partially Observable Sensor Networks: Moving beyond Correlations](https://arxiv.org/abs/2507.09742)
*Xiaofeng Xiao,Bo Shen,Xubo Yue*

Main category: cs.AI

TL;DR: 提出了一种基于因果关系的深度Q网络（Causal DQ）方法，用于部分可观测的传感器布局优化，以快速检测异常。


<details>
  <summary>Details</summary>
Motivation: AI驱动的制造业中，数据流实时监测需求增长，但资源有限，无法在所有位置部署传感器。现有方法多忽略因果关系，或依赖不切实际的干预手段。

Method: 通过在每个Q网络训练阶段整合因果信息，开发了Causal DQ方法，实现更快收敛和更紧的理论误差界限。

Result: Causal DQ显著减少了异常检测时间，适用于大规模实时数据流。

Conclusion: 该方法不仅有效，还为其他强化学习问题提供了新思路，扩展了因果机器学习在工程中的应用。

Abstract: Nowadays, as AI-driven manufacturing becomes increasingly popular, the volume
of data streams requiring real-time monitoring continues to grow. However, due
to limited resources, it is impractical to place sensors at every location to
detect unexpected shifts. Therefore, it is necessary to develop an optimal
sensor placement strategy that enables partial observability of the system
while detecting anomalies as quickly as possible. Numerous approaches have been
proposed to address this challenge; however, most existing methods consider
only variable correlations and neglect a crucial factor: Causality. Moreover,
although a few techniques incorporate causal analysis, they rely on
interventions-artificially creating anomalies-to identify causal effects, which
is impractical and might lead to catastrophic losses. In this paper, we
introduce a causality-informed deep Q-network (Causal DQ) approach for
partially observable sensor placement in anomaly detection. By integrating
causal information at each stage of Q-network training, our method achieves
faster convergence and tighter theoretical error bounds. Furthermore, the
trained causal-informed Q-network significantly reduces the detection time for
anomalies under various settings, demonstrating its effectiveness for sensor
placement in large-scale, real-world data streams. Beyond the current
implementation, our technique's fundamental insights can be applied to various
reinforcement learning problems, opening up new possibilities for real-world
causality-informed machine learning methods in engineering applications.

</details>


### [21] [Sound and Complete Neuro-symbolic Reasoning with LLM-Grounded Interpretations](https://arxiv.org/abs/2507.09751)
*Bradley P. Allen,Prateek Chhikara,Thomas Macaulay Ferguson,Filip Ilievski,Paul Groth*

Main category: cs.AI

TL;DR: 提出一种方法，将大语言模型（LLMs）整合到形式语义的解释函数中，以解决其逻辑不一致性问题，并在实验中验证了可行性。


<details>
  <summary>Details</summary>
Motivation: 尽管LLMs在自然语言理解和生成方面表现优异，但其输出存在逻辑不一致问题，需要一种方法在形式推理中利用其广泛的知识。

Method: 将LLM直接整合到一种矛盾容忍逻辑的形式语义解释函数中。

Result: 通过多个短篇事实性基准数据集验证了方法的可行性。

Conclusion: 该方法为神经符号推理提供了理论框架，既利用了LLM的知识，又保持了逻辑的健全性和完备性。

Abstract: Large language models (LLMs) have demonstrated impressive capabilities in
natural language understanding and generation, but they exhibit problems with
logical consistency in the output they generate. How can we harness LLMs'
broad-coverage parametric knowledge in formal reasoning despite their
inconsistency? We present a method for directly integrating an LLM into the
interpretation function of the formal semantics for a paraconsistent logic. We
provide experimental evidence for the feasibility of the method by evaluating
the function using datasets created from several short-form factuality
benchmarks. Unlike prior work, our method offers a theoretical framework for
neuro-symbolic reasoning that leverages an LLM's knowledge while preserving the
underlying logic's soundness and completeness properties.

</details>


### [22] [Technical Requirements for Halting Dangerous AI Activities](https://arxiv.org/abs/2507.09801)
*Peter Barnett,Aaron Scher,David Abecassis*

Main category: cs.AI

TL;DR: 论文探讨了如何通过技术干预实现危险AI活动的协调暂停，以应对AI快速发展带来的风险。


<details>
  <summary>Details</summary>
Motivation: AI系统的快速发展带来了前所未有的风险，如失控、滥用、地缘政治不稳定和权力集中，需要政府采取行动避免最坏结果。

Method: 提出了关键的技术干预措施，以实现对危险AI活动的协调暂停。

Result: 这些干预措施可以限制多种危险AI活动，并为AI治理计划提供技术基础。

Conclusion: 技术干预是实现危险AI活动协调暂停的有效手段，为AI治理提供了可行方案。

Abstract: The rapid development of AI systems poses unprecedented risks, including loss
of control, misuse, geopolitical instability, and concentration of power. To
navigate these risks and avoid worst-case outcomes, governments may proactively
establish the capability for a coordinated halt on dangerous AI development and
deployment. In this paper, we outline key technical interventions that could
allow for a coordinated halt on dangerous AI activities. We discuss how these
interventions may contribute to restricting various dangerous AI activities,
and show how these interventions can form the technical foundation for
potential AI governance plans.

</details>


### [23] [Is Human-Written Data Enough? The Challenge of Teaching Reasoning to LLMs Without RL or Distillation](https://arxiv.org/abs/2507.09850)
*Wei Du,Branislav Kisacanin,George Armstrong,Shubham Toshniwal,Ivan Moshkov,Alexan Ayrapetyan,Sadegh Mahdavi,Dan Zhao,Shizhe Diao,Dragan Masulovic,Marius Stanean,Advaith Avadhanam,Max Wang,Ashmit Dutta,Shitij Govil,Sri Yanamandara,Mihir Tandon,Sriram Ananthakrishnan,Vedant Rathi,David Zhang,Joonseok Kang,Leon Luo,Titu Andreescu,Boris Ginsburg,Igor Gitman*

Main category: cs.AI

TL;DR: 通过仅20个长链思维（CoT）示例微调基础模型，显著提升推理能力，超越更大模型表现。


<details>
  <summary>Details</summary>
Motivation: 探究是否仅通过提示或最小微调即可在基础模型中诱导长链思维推理能力。

Method: 使用20个来自推理模型的CoT示例微调基础模型，并探索非推理模型和人工标注数据的表现。

Result: 微调后的模型表现优于更大的模型，但非推理模型和人工数据未能达到相同效果。

Conclusion: 少量高质量示例可激活基础模型的推理能力，但专家CoT的潜在特性难以复制。

Abstract: Reasoning-capable language models achieve state-of-the-art performance in
diverse complex tasks by generating long, explicit Chain-of-Thought (CoT)
traces. While recent works show that base models can acquire such reasoning
traces via reinforcement learning or distillation from stronger models like
DeepSeek-R1, previous works demonstrate that even short CoT prompting without
fine-tuning is able to improve reasoning. We ask whether long CoT can be
induced in a base model using only prompting or minimal tuning. Using just 20
long CoT examples from the reasoning model \texttt{QwQ-32B-Preview}, we lightly
fine-tune the base model \texttt{Qwen2.5-32B}. The resulting model outperforms
the much larger \texttt{Qwen2.5-Math-72B-Instruct}, showing that a handful of
high-quality examples can unlock strong reasoning capabilities. We further
explore using CoT data from non-reasoning models and human annotators, enhanced
with prompt engineering, multi-pass editing, and structural guidance. However,
neither matches the performance of reasoning model traces, suggesting that
certain latent qualities of expert CoT are difficult to replicate. We analyze
key properties of reasoning data, such as problem difficulty, diversity, and
answer length, that influence reasoning distillation. While challenges remain,
we are optimistic that carefully curated human-written CoT, even in small
quantities, can activate reasoning behaviors in base models. We release our
human-authored dataset across refinement stages and invite further
investigation into what makes small-scale reasoning supervision so effective.

</details>


### [24] [Model-Grounded Symbolic Artificial Intelligence Systems Learning and Reasoning with Model-Grounded Symbolic Artificial Intelligence Systems](https://arxiv.org/abs/2507.09854)
*Aniruddha Chattopadhyay,Raj Dandekar,Kaushik Roy*

Main category: cs.AI

TL;DR: 本文提出将指令调优的大型语言模型重新解释为基于模型的符号AI系统，利用自然语言作为符号层，并通过模型的内部表示空间实现接地。


<details>
  <summary>Details</summary>
Motivation: 结合神经网络的泛化学习能力和符号AI的可验证推理能力，探索新的学习和推理方法。

Method: 将自然语言作为符号层，利用模型的内部表示空间实现接地，开发与传统学习和推理范式结构相似的新方法。

Result: 初步评估表明，该方法在提高学习效率和推理可靠性方面有效。

Conclusion: 通过重新解释大型语言模型，本文为神经符号AI系统提供了一种新的学习和推理框架。

Abstract: Neurosymbolic artificial intelligence (AI) systems combine neural network and
classical symbolic AI mechanisms to exploit the complementary strengths of
large scale, generalizable learning and robust, verifiable reasoning. Numerous
classifications of neurosymbolic AI illustrate how these two components can be
integrated in distinctly different ways. In this work, we propose
reinterpreting instruction tuned large language models as model grounded
symbolic AI systems where natural language serves as the symbolic layer and
grounding is achieved through the models internal representation space. Within
this framework, we investigate and develop novel learning and reasoning
approaches that preserve structural similarities to traditional learning and
reasoning paradigms. Preliminary evaluations across axiomatic deductive
reasoning procedures of varying complexity provide insights into the
effectiveness of our approach in improving learning efficiency and reasoning
reliability.

</details>


### [25] [VerifyBench: A Systematic Benchmark for Evaluating Reasoning Verifiers Across Domains](https://arxiv.org/abs/2507.09884)
*Xuzhao Li,Xuchen Li,Shiyu Hu,Yongzhen Guo,Wentao Zhang*

Main category: cs.AI

TL;DR: 论文提出了VerifyBench，一个跨领域的综合基准，用于系统评估验证器在强化学习中的性能，揭示了专用验证器和通用LLM之间的权衡。


<details>
  <summary>Details</summary>
Motivation: 现有验证器在复杂性和一致性方面存在不足，缺乏跨领域的系统性评估，限制了RLVR的可靠发展。

Method: 构建了包含4000个专家级问题的VerifyBench，设计四维实验框架比较专用验证器和通用LLM的性能。

Result: 专用验证器准确率高但召回率低，通用模型包容性强但精度不稳定，验证器对输入结构敏感且跨领域泛化能力有限。

Conclusion: 研究揭示了当前验证器技术的瓶颈，为未来改进提供了关键见解。

Abstract: Large language models (LLMs) increasingly rely on reinforcement learning (RL)
to enhance their reasoning capabilities through feedback. A critical challenge
is verifying the consistency of model-generated responses and reference
answers, since these responses are often lengthy, diverse, and nuanced.
Rule-based verifiers struggle with complexity, prompting the use of model-based
verifiers. However, specialized verifiers lack flexibility, while general LLM
judges can be inconsistent. Existing research primarily focuses on building
better verifiers, yet a systematic evaluation of different types of verifiers'
performance across domains remains lacking, severely constraining the reliable
development of Reinforcement Learning with Verifiable Reward (RLVR). To address
this, we propose VerifyBench--a cross-domain comprehensive benchmark for
systematically evaluating verifiers. We construct 4,000 expert-level questions
covering mathematics, physics, chemistry, and biology. Each question is
equipped with reference answers and diverse responses. The reliability of the
evaluation is ensured through a rigorous annotation process conducted by a
multidisciplinary expert team. We design a four-dimensional experimental
framework to comprehensively compare the performance boundaries of specialized
verifiers and general LLMs under combined conditions of extracted answers vs.
complete responses, and short vs. long outputs. Our evaluation uncovers
fundamental trade-offs in verifiers: while specialized verifiers achieve
leading accuracy, they exhibit deficiencies in recall; general models show
stronger inclusivity but unstable precision. More importantly, we discover
verifiers' high sensitivity to input structure and inherent limitations in
cross-domain generalization, providing critical insights into the bottlenecks
of current verifier technology.

</details>


### [26] [DeepSeek: Paradigm Shifts and Technical Evolution in Large AI Models](https://arxiv.org/abs/2507.09955)
*Luolin Xiong,Haofen Wang,Xi Chen,Lu Sheng,Yun Xiong,Jingping Liu,Yanghua Xiao,Huajun Chen,Qing-Long Han,Yang Tang*

Main category: cs.AI

TL;DR: DeepSeek发布V3和R1系列模型，具有低成本、高性能和开源优势。论文回顾了大模型发展，介绍了DeepSeek的创新算法和工程突破，并分析了其对AI竞争格局的影响。


<details>
  <summary>Details</summary>
Motivation: 探讨DeepSeek模型的技术创新及其对大型AI模型发展的影响，为未来趋势提供见解。

Method: 回顾大模型发展，介绍DeepSeek的MLA、MoE、MTP和GRPO算法，分析工程突破和竞争格局。

Result: DeepSeek模型在性能、成本和开源方面表现突出，对AI领域产生显著影响。

Conclusion: DeepSeek的创新为大型AI模型发展提供了新方向，未来趋势将聚焦数据、训练和推理优化。

Abstract: DeepSeek, a Chinese Artificial Intelligence (AI) startup, has released their
V3 and R1 series models, which attracted global attention due to their low
cost, high performance, and open-source advantages. This paper begins by
reviewing the evolution of large AI models focusing on paradigm shifts, the
mainstream Large Language Model (LLM) paradigm, and the DeepSeek paradigm.
Subsequently, the paper highlights novel algorithms introduced by DeepSeek,
including Multi-head Latent Attention (MLA), Mixture-of-Experts (MoE),
Multi-Token Prediction (MTP), and Group Relative Policy Optimization (GRPO).
The paper then explores DeepSeek engineering breakthroughs in LLM scaling,
training, inference, and system-level optimization architecture. Moreover, the
impact of DeepSeek models on the competitive AI landscape is analyzed,
comparing them to mainstream LLMs across various fields. Finally, the paper
reflects on the insights gained from DeepSeek innovations and discusses future
trends in the technical and engineering development of large AI models,
particularly in data, training, and reasoning.

</details>


### [27] [Improving monotonic optimization in heterogeneous multi-agent reinforcement learning with optimal marginal deterministic policy gradient](https://arxiv.org/abs/2507.09989)
*Xiaoyang Yu,Youfang Lin,Shuo Wang,Sheng Han*

Main category: cs.AI

TL;DR: OMDPG算法通过引入最优边际Q函数和广义Q批评器，解决了异质多智能体强化学习中单调改进与部分参数共享的冲突，提升了性能。


<details>
  <summary>Details</summary>
Motivation: 异质多智能体强化学习中，单调改进与部分参数共享（ParPS）之间存在冲突，直接结合会导致策略更新基线漂移问题，无法实现性能提升。

Method: 提出OMDPG算法：1）用最优边际Q函数替代顺序计算的Q函数；2）引入广义Q批评器；3）采用集中式批评器分组执行器架构。

Result: 在SMAC和MAMuJoCo环境中，OMDPG优于多种最先进的多智能体强化学习基线。

Conclusion: OMDPG成功解决了单调改进与ParPS的冲突，显著提升了异质多智能体强化学习的性能。

Abstract: In heterogeneous multi-agent reinforcement learning (MARL), achieving
monotonic improvement plays a pivotal role in enhancing performance. The HAPPO
algorithm proposes a feasible solution by introducing a sequential update
scheme, which requires independent learning with No Parameter-sharing (NoPS).
However, heterogeneous MARL generally requires Partial Parameter-sharing
(ParPS) based on agent grouping to achieve high cooperative performance. Our
experiments prove that directly combining ParPS with the sequential update
scheme leads to the policy updating baseline drift problem, thereby failing to
achieve improvement. To solve the conflict between monotonic improvement and
ParPS, we propose the Optimal Marginal Deterministic Policy Gradient (OMDPG)
algorithm. First, we replace the sequentially computed $Q_{\psi}^s(s,a_{1:i})$
with the Optimal Marginal Q (OMQ) function $\phi_{\psi}^*(s,a_{1:i})$ derived
from Q-functions. This maintains MAAD's monotonic improvement while eliminating
the conflict through optimal joint action sequences instead of sequential
policy ratio calculations. Second, we introduce the Generalized Q Critic (GQC)
as the critic function, employing pessimistic uncertainty-constrained loss to
optimize different Q-value estimations. This provides the required Q-values for
OMQ computation and stable baselines for actor updates. Finally, we implement a
Centralized Critic Grouped Actor (CCGA) architecture that simultaneously
achieves ParPS in local policy networks and accurate global Q-function
computation. Experimental results in SMAC and MAMuJoCo environments demonstrate
that OMDPG outperforms various state-of-the-art MARL baselines.

</details>


### [28] [On The Role of Intentionality in Knowledge Representation: Analyzing Scene Context for Cognitive Agents with a Tiny Language Model](https://arxiv.org/abs/2507.10000)
*Mark Burgess*

Main category: cs.AI

TL;DR: 论文提出了一种基于Promise Theory的语义时空模型，通过多尺度异常检测和时空一致性分析，低成本地评估数据中的潜在意图性。


<details>
  <summary>Details</summary>
Motivation: 探讨科学和技术领域中意图的实际意义，弥补Searle之后对意图研究的不足。

Method: 利用过程一致性和多尺度异常检测，分离意图内容和环境背景，评估潜在意图性。

Result: 提供了一种低成本、无需大规模训练或推理能力的潜在意图性评估方法。

Conclusion: 该方法适用于基础生物体，但概念形成水平受限于代理的记忆能力。

Abstract: Since Searle's work deconstructing intent and intentionality in the realm of
philosophy, the practical meaning of intent has received little attention in
science and technology. Intentionality and context are both central to the
scope of Promise Theory's model of Semantic Spacetime, used as an effective
Tiny Language Model. One can identify themes and concepts from a text, on a low
level (without knowledge of the specific language) by using process coherence
as a guide. Any agent process can assess superficially a degree of latent
`intentionality' in data by looking for anomalous multi-scale anomalies and
assessing the work done to form them. Scale separation can be used to sort
parts into `intended' content and `ambient context', using the spacetime
coherence as a measure. This offers an elementary but pragmatic interpretation
of latent intentionality for very low computational cost, and without reference
to extensive training or reasoning capabilities. The process is well within the
reach of basic organisms as it does not require large scale artificial
probabilistic batch processing. The level of concept formation depends,
however, on the memory capacity of the agent.

</details>


### [29] [Deep Hidden Cognition Facilitates Reliable Chain-of-Thought Reasoning](https://arxiv.org/abs/2507.10007)
*Zijun Chen,Wenbo Hu,Richang Hong*

Main category: cs.AI

TL;DR: 本文提出了一种通过模型内在真实性编码校准CoT推理准确性的新方法，显著提升了推理任务的可靠性和准确性。


<details>
  <summary>Details</summary>
Motivation: CoT推理在LLMs和MLLMs中表现出强大的深度推理能力，但中间步骤的错误累积会降低其可靠性。

Method: 利用特定注意力头激活反映CoT推理步骤的真实性，训练置信度预测器动态选择最优推理路径。

Result: 实验表明，该方法在数学、符号和常识推理任务中优于现有基线，适用于单模态和多模态场景。

Conclusion: 该研究为CoT推理提供了一种新的可靠性改进路径，具有广泛的应用潜力。

Abstract: Chain of Thought (CoT) reasoning has demonstrated remarkable deep reasoning
capabilities in both large language models (LLMs) and multimodal large language
models (MLLMs). However, its reliability is often undermined by the
accumulation of errors in intermediate steps. This paper introduces an novel
approach to calibrate the CoT reasoning accuracy by leveraging the model's
intrinsic veracity encoding. We discover that specific attention head
activations reliably reflect the truthfulness of reasoning steps in CoT. Based
on this insight, we train a confidence predictor to evaluate the correctness of
each reasoning step using these truthfulness-sensitive activations, dynamically
selecting the most plausible reasoning path via beam search. Experimental
results demonstrate that our method significantly outperforms the
state-of-the-art baselines (e.g., Few-Shot CoT, Self-Consistency, and
Self-Evaluation Guided Beam Search) across the mathematical, symbolic, and
commonsense reasoning tasks, exhibiting superior accuracy and reliability in
both unimodal and multimodal settings. We further validate the approach on
large reasoning models, confirming its applicability to specialized reasoning
models. Additionally, we explore the role of the model's self-correction
ability in CoT reasoning. This work provides a novel reliability improvement
path for CoT reasoning with broad application potential.

</details>


### [30] [Automating SPARQL Query Translations between DBpedia and Wikidata](https://arxiv.org/abs/2507.10045)
*Malte Christian Bartels,Debayan Banerjee,Ricardo Usbeck*

Main category: cs.AI

TL;DR: 研究了大型语言模型（LLM）在SPARQL查询翻译中的表现，重点评估了DBpedia-Wikidata和DBLP-OpenAlex之间的翻译效果。


<details>
  <summary>Details</summary>
Motivation: 解决知识图谱（KG）互操作性研究中SPARQL翻译的空白。

Method: 使用三种LLM模型（Llama-3-8B、DeepSeek-R1-Distill-Llama-70B、Mistral-Large-Instruct-2407），通过零样本、少样本和思维链变体进行测试。

Result: 不同模型和提示策略表现差异显著，Wikidata到DBpedia的翻译效果优于反向。

Conclusion: LLM在SPARQL翻译中表现不一，需进一步优化模型和策略。

Abstract: This paper investigates whether state-of-the-art Large Language Models (LLMs)
can automatically translate SPARQL between popular Knowledge Graph (KG)
schemas. We focus on translations between the DBpedia and Wikidata KG, and
later on DBLP and OpenAlex KG. This study addresses a notable gap in KG
interoperability research by rigorously evaluating LLM performance on
SPARQL-to-SPARQL translation. Two benchmarks are assembled, where the first
align 100 DBpedia-Wikidata queries from QALD-9-Plus; the second contains 100
DBLP queries aligned to OpenAlex, testing generalizability beyond encyclopaedic
KGs. Three open LLMs: Llama-3-8B, DeepSeek-R1-Distill-Llama-70B, and
Mistral-Large-Instruct-2407 are selected based on their sizes and architectures
and tested with zero-shot, few-shot, and two chain-of-thought variants. Outputs
were compared with gold answers, and resulting errors were categorized. We find
that the performance varies markedly across models and prompting strategies,
and that translations for Wikidata to DBpedia work far better than translations
for DBpedia to Wikidata.

</details>


### [31] [On Gradual Semantics for Assumption-Based Argumentation](https://arxiv.org/abs/2507.10076)
*Anna Rapberger,Fabrizio Russo,Antonio Rago,Francesca Toni*

Main category: cs.AI

TL;DR: 该论文填补了假设基础论证（ABA）中逐步语义学的空白，提出了一系列新的逐步语义学方法，并通过实验验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 尽管逐步语义学在多种论证框架中已有研究，但尚未应用于假设基础论证（ABA），而ABA是一种流行的结构化论证形式。本文旨在填补这一空白。

Method: 通过将双极集基础论证框架作为ABA的抽象，并扩展了最先进的模块化逐步语义学方法，提出了一种新的逐步ABA语义学。

Result: 实验表明，提出的逐步ABA语义学满足平衡性和单调性等理想性质，并与基于论证的方法进行了比较。

Conclusion: 本文成功地将逐步语义学引入ABA，并通过实验验证了其可行性和有效性。

Abstract: In computational argumentation, gradual semantics are fine-grained
alternatives to extension-based and labelling-based semantics . They ascribe a
dialectical strength to (components of) arguments sanctioning their degree of
acceptability. Several gradual semantics have been studied for abstract,
bipolar and quantitative bipolar argumentation frameworks (QBAFs), as well as,
to a lesser extent, for some forms of structured argumentation. However, this
has not been the case for assumption-based argumentation (ABA), despite it
being a popular form of structured argumentation with several applications
where gradual semantics could be useful. In this paper, we fill this gap and
propose a family of novel gradual semantics for equipping assumptions, which
are the core components in ABA frameworks, with dialectical strengths. To do
so, we use bipolar set-based argumentation frameworks as an abstraction of
(potentially non-flat) ABA frameworks and generalise state-of-the-art modular
gradual semantics for QBAFs. We show that our gradual ABA semantics satisfy
suitable adaptations of desirable properties of gradual QBAF semantics, such as
balance and monotonicity. We also explore an argument-based approach that
leverages established QBAF modular semantics directly, and use it as baseline.
Finally, we conduct experiments with synthetic ABA frameworks to compare our
gradual ABA semantics with its argument-based counterpart and assess
convergence.

</details>


### [32] [BlueGlass: A Framework for Composite AI Safety](https://arxiv.org/abs/2507.10106)
*Harshal Nandigramwar,Syed Qutub,Kay-Ulrich Scholl*

Main category: cs.AI

TL;DR: 本文介绍了BlueGlass框架，用于整合多种AI安全工具，并通过三个视觉语言模型的安全分析展示了其效用。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统的能力增强和普及，确保其安全性变得至关重要。现有安全工具各自独立，无法提供全面保障，因此需要集成化的方法。

Method: 提出BlueGlass框架，提供统一基础设施以整合和组合多样化的安全工具，覆盖模型内部和输出。通过三个具体分析（分布评估、探针分析和稀疏自编码器）验证框架。

Result: 展示了BlueGlass框架在视觉语言模型上的应用，揭示了性能权衡、层次学习动态和可解释概念。

Conclusion: 该工作为构建更稳健可靠的AI系统提供了基础性基础设施和发现。

Abstract: As AI systems become increasingly capable and ubiquitous, ensuring the safety
of these systems is critical. However, existing safety tools often target
different aspects of model safety and cannot provide full assurance in
isolation, highlighting a need for integrated and composite methodologies. This
paper introduces BlueGlass, a framework designed to facilitate composite AI
safety workflows by providing a unified infrastructure enabling the integration
and composition of diverse safety tools that operate across model internals and
outputs. Furthermore, to demonstrate the utility of this framework, we present
three safety-oriented analyses on vision-language models for the task of object
detection: (1) distributional evaluation, revealing performance trade-offs and
potential failure modes across distributions; (2) probe-based analysis of layer
dynamics highlighting shared hierarchical learning via phase transition; and
(3) sparse autoencoders identifying interpretable concepts. More broadly, this
work contributes foundational infrastructure and findings for building more
robust and reliable AI systems.

</details>


### [33] [Analysis of AI Techniques for Orchestrating Edge-Cloud Application Migration](https://arxiv.org/abs/2507.10119)
*Sadig Gojayev,Ahmad Anaqreh,Carolina Fortuna*

Main category: cs.AI

TL;DR: 论文探讨了边缘-云系统中应用迁移的自动编排问题，通过MDP框架比较了AI规划和强化学习方法，并以汉诺塔问题为模型进行了分析。


<details>
  <summary>Details</summary>
Motivation: 研究旨在提高边缘-云系统中应用迁移的服务质量（QoS）和成本效益，探索自动化编排的可行技术。

Method: 基于马尔可夫决策过程（MDP），比较了AI规划和强化学习方法，并以汉诺塔问题为模型进行分类和分析。

Result: 提出了一种基于状态空间定义的新分类方法，并分析了不同模型的适用性。

Conclusion: 研究为新兴计算连续环境中的应用迁移编排提供了技术参考和分类框架。

Abstract: Application migration in edge-cloud system enables high QoS and cost
effective service delivery. However, automatically orchestrating such migration
is typically solved with heuristic approaches. Starting from the Markov
Decision Process (MDP), in this paper, we identify, analyze and compare
selected state-of-the-art Artificial Intelligence (AI) planning and
Reinforcement Learning (RL) approaches for solving the class of edge-cloud
application migration problems that can be modeled as Towers of Hanoi (ToH)
problems. We introduce a new classification based on state space definition and
analyze the compared models also through this lense. The aim is to understand
available techniques capable of orchestrating such application migration in
emerging computing continuum environments.

</details>


### [34] [Could you be wrong: Debiasing LLMs using a metacognitive prompt for improving human decision making](https://arxiv.org/abs/2507.10124)
*Thomas T. Hills*

Main category: cs.AI

TL;DR: 论文探讨了利用人类心理学中的元认知提示（如“你可能是错的吗？”）来减少LLM的偏见，并展示了其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于LLM仍在发展中，当前的偏见可能随时间变化，因此需要通用的去偏见策略。人类决策中的去偏见方法为LLM提供了潜在解决方案。

Method: 采用元认知提示（如“你可能是错的吗？”）引导LLM反思其回答，揭示潜在偏见、错误和矛盾信息。

Result: 元认知提示能有效引导LLM识别自身偏见，并提供额外的反思信息，改善初始回答的局限性。

Conclusion: 人类心理学为LLM的提示工程提供了新思路，利用元认知提示可显著提升LLM的决策质量。

Abstract: Identifying bias in LLMs is ongoing. Because they are still in development,
what is true today may be false tomorrow. We therefore need general strategies
for debiasing that will outlive current models. Strategies developed for
debiasing human decision making offer one promising approach as they
incorporate an LLM-style prompt intervention designed to bring latent knowledge
into awareness during decision making. LLMs trained on vast amounts of
information contain information about potential biases, counter-arguments, and
contradictory evidence, but that information may only be brought to bear if
prompted. Metacognitive prompts developed in the human decision making
literature are designed to achieve this, and as I demonstrate here, they show
promise with LLMs. The prompt I focus on here is "could you be wrong?"
Following an LLM response, this prompt leads LLMs to produce additional
information, including why they answered as they did, errors, biases,
contradictory evidence, and alternatives, none of which were apparent in their
initial response. Indeed, this metaknowledge often reveals that how LLMs and
users interpret prompts are not aligned. Here I demonstrate this prompt using a
set of questions taken from recent articles about LLM biases, including
implicit discriminatory biases and failures of metacognition. "Could you be
wrong" prompts the LLM to identify its own biases and produce cogent
metacognitive reflection. I also present another example involving convincing
but incomplete information, which is readily corrected by the metacognitive
prompt. In sum, this work argues that human psychology offers a new avenue for
prompt engineering, leveraging a long history of effective prompt-based
improvements to human decision making.

</details>


### [35] [FRSICL: LLM-Enabled In-Context Learning Flight Resource Allocation for Fresh Data Collection in UAV-Assisted Wildfire Monitoring](https://arxiv.org/abs/2507.10134)
*Yousef Emami,Hao Zhou,Miguel Gutierrez Gaitan,Kai Li,Luis Almeida*

Main category: cs.AI

TL;DR: 论文提出了一种基于LLM的在线飞行资源分配方案（FRSICL），用于无人机辅助的野火监测系统，通过实时优化飞行控制和数据收集调度，最小化信息年龄（AoI）。


<details>
  <summary>Details</summary>
Motivation: 传统深度强化学习（DRL）在无人机辅助野火监测中存在采样效率低、仿真与现实差距大等问题，不适用于时间敏感应用。

Method: FRSICL利用自然语言任务描述和环境反馈，动态生成数据收集计划和速度控制，无需大量重新训练。

Result: 仿真结果表明，FRSICL在最小化AoI方面优于PPO和最近邻基线方法。

Conclusion: FRSICL为无人机辅助野火监测提供了一种高效、动态的解决方案，克服了DRL的局限性。

Abstract: Unmanned Aerial Vehicles (UAVs) are vital for public safety, particularly in
wildfire monitoring, where early detection minimizes environmental impact. In
UAV-Assisted Wildfire Monitoring (UAWM) systems, joint optimization of sensor
transmission scheduling and velocity is critical for minimizing Age of
Information (AoI) from stale sensor data. Deep Reinforcement Learning (DRL) has
been used for such optimization; however, its limitations such as low sampling
efficiency, simulation-to-reality gaps, and complex training render it
unsuitable for time-critical applications like wildfire monitoring. This paper
introduces a new online Flight Resource Allocation scheme based on LLM-Enabled
In-Context Learning (FRSICL) to jointly optimize the UAV's flight control and
data collection schedule along the trajectory in real time, thereby
asymptotically minimizing the average AoI across ground sensors. In contrast to
DRL, FRSICL generates data collection schedules and controls velocity using
natural language task descriptions and feedback from the environment, enabling
dynamic decision-making without extensive retraining. Simulation results
confirm the effectiveness of the proposed FRSICL compared to Proximal Policy
Optimization (PPO) and Nearest-Neighbor baselines.

</details>


### [36] [Adaptability in Multi-Agent Reinforcement Learning: A Framework and Unified Review](https://arxiv.org/abs/2507.10142)
*Siyi Hu,Mohamad A Hady,Jianglin Qiao,Jimmy Cao,Mahardhika Pratama,Ryszard Kowalczyk*

Main category: cs.AI

TL;DR: 论文提出“适应性”概念，用于评估多智能体强化学习（MARL）在动态环境中的可靠性，并提出了一个包含三个维度的框架。


<details>
  <summary>Details</summary>
Motivation: 现实世界的多智能体系统（MAS）环境复杂多变，现有MARL算法难以适应，需更全面的评估方法。

Method: 提出“适应性”概念，并构建包含学习适应性、策略适应性和场景驱动适应性的三维框架。

Result: 通过适应性视角，支持更系统化的MARL性能评估，超越传统基准测试。

Conclusion: 该框架有助于开发更适合动态现实世界MAS的MARL算法。

Abstract: Multi-Agent Reinforcement Learning (MARL) has shown clear effectiveness in
coordinating multiple agents across simulated benchmarks and constrained
scenarios. However, its deployment in real-world multi-agent systems (MAS)
remains limited, primarily due to the complex and dynamic nature of such
environments. These challenges arise from multiple interacting sources of
variability, including fluctuating agent populations, evolving task goals, and
inconsistent execution conditions. Together, these factors demand that MARL
algorithms remain effective under continuously changing system configurations
and operational demands. To better capture and assess this capacity for
adjustment, we introduce the concept of \textit{adaptability} as a unified and
practically grounded lens through which to evaluate the reliability of MARL
algorithms under shifting conditions, broadly referring to any changes in the
environment dynamics that may occur during learning or execution. Centred on
the notion of adaptability, we propose a structured framework comprising three
key dimensions: learning adaptability, policy adaptability, and scenario-driven
adaptability. By adopting this adaptability perspective, we aim to support more
principled assessments of MARL performance beyond narrowly defined benchmarks.
Ultimately, this survey contributes to the development of algorithms that are
better suited for deployment in dynamic, real-world multi-agent systems.

</details>


### [37] [Introducing the Swiss Food Knowledge Graph: AI for Context-Aware Nutrition Recommendation](https://arxiv.org/abs/2507.10156)
*Lubnaa Abdur Rahman,Ioannis Papathanail,Stavroula Mougiakakou*

Main category: cs.AI

TL;DR: 论文介绍了瑞士食品知识图谱（SwissFKG），整合了食谱、食材、营养数据、饮食限制和过敏信息，并利用LLM增强图谱内容，为个性化营养评估工具奠定了基础。


<details>
  <summary>Details</summary>
Motivation: 现有自动饮食评估系统忽视非视觉因素（如食材替代和个体需求），瑞士缺乏统一的营养信息整合资源。

Method: 构建SwissFKG，采用LLM增强图谱内容，并评估LLM在食品知识增强中的表现。

Result: LLM能有效丰富图谱营养信息，SwissFKG提供食材级信息和营养指南，Graph-RAG应用展示其支持LLM回答用户营养查询的能力。

Conclusion: SwissFKG为结合视觉、上下文和文化维度的下一代饮食评估工具奠定了基础。

Abstract: AI has driven significant progress in the nutrition field, especially through
multimedia-based automatic dietary assessment. However, existing automatic
dietary assessment systems often overlook critical non-visual factors, such as
recipe-specific ingredient substitutions that can significantly alter
nutritional content, and rarely account for individual dietary needs, including
allergies, restrictions, cultural practices, and personal preferences. In
Switzerland, while food-related information is available, it remains
fragmented, and no centralized repository currently integrates all relevant
nutrition-related aspects within a Swiss context. To bridge this divide, we
introduce the Swiss Food Knowledge Graph (SwissFKG), the first resource, to our
best knowledge, to unite recipes, ingredients, and their substitutions with
nutrient data, dietary restrictions, allergen information, and national
nutrition guidelines under one graph. We establish a LLM-powered enrichment
pipeline for populating the graph, whereby we further present the first
benchmark of four off-the-shelf (<70 B parameter) LLMs for food knowledge
augmentation. Our results demonstrate that LLMs can effectively enrich the
graph with relevant nutritional information. Our SwissFKG goes beyond recipe
recommendations by offering ingredient-level information such as allergen and
dietary restriction information, and guidance aligned with nutritional
guidelines. Moreover, we implement a Graph-RAG application to showcase how the
SwissFKG's rich natural-language data structure can help LLM answer
user-specific nutrition queries, and we evaluate LLM-embedding pairings by
comparing user-query responses against predefined expected answers. As such,
our work lays the foundation for the next generation of dietary assessment
tools that blend visual, contextual, and cultural dimensions of eating.

</details>


### [38] [Should We Ever Prefer Decision Transformer for Offline Reinforcement Learning?](https://arxiv.org/abs/2507.10174)
*Yumi Omori,Zixuan Dong,Keith Ross*

Main category: cs.AI

TL;DR: 本文通过实验比较了基于MLP的过滤行为克隆（FBC）与决策变换器（DT）在稀疏奖励环境中的表现，发现FBC性能更优且更高效。


<details>
  <summary>Details</summary>
Motivation: 探讨DT在稀疏奖励环境中是否优于传统方法，并质疑DT的适用性。

Method: 在Robomimic和D4RL任务上实验，比较FBC（过滤低质量轨迹后进行行为克隆）与DT的性能。

Result: FBC在稀疏奖励环境中表现优于DT，且更高效。

Conclusion: DT在稀疏奖励环境中并非优选，甚至可能在其他环境中也不占优，质疑其适用性。

Abstract: In recent years, extensive work has explored the application of the
Transformer architecture to reinforcement learning problems. Among these,
Decision Transformer (DT) has gained particular attention in the context of
offline reinforcement learning due to its ability to frame return-conditioned
policy learning as a sequence modeling task. Most recently, Bhargava et al.
(2024) provided a systematic comparison of DT with more conventional MLP-based
offline RL algorithms, including Behavior Cloning (BC) and Conservative
Q-Learning (CQL), and claimed that DT exhibits superior performance in
sparse-reward and low-quality data settings.
  In this paper, through experimentation on robotic manipulation tasks
(Robomimic) and locomotion benchmarks (D4RL), we show that MLP-based Filtered
Behavior Cloning (FBC) achieves competitive or superior performance compared to
DT in sparse-reward environments. FBC simply filters out low-performing
trajectories from the dataset and then performs ordinary behavior cloning on
the filtered dataset. FBC is not only very straightforward, but it also
requires less training data and is computationally more efficient. The results
therefore suggest that DT is not preferable for sparse-reward environments.
From prior work, arguably, DT is also not preferable for dense-reward
environments. Thus, we pose the question: Is DT ever preferable?

</details>


### [39] [Survey for Categorising Explainable AI Studies Using Data Analysis Task Frameworks](https://arxiv.org/abs/2507.10208)
*Hamzah Ziadeh,Hendrik Knoche*

Main category: cs.AI

TL;DR: 论文提出了一种分类和比较可解释人工智能（XAI）研究的方法，基于三个维度：什么、为什么和谁，旨在解决任务描述不足、脱离上下文研究和目标用户测试不足等问题。


<details>
  <summary>Details</summary>
Motivation: 当前XAI研究存在大量矛盾且缺乏具体设计建议，主要源于对需要AI辅助的任务理解不足。

Method: 借鉴视觉分析、认知科学和仪表板设计等多个领域，提出了一种分类和比较XAI研究的方法。

Result: 识别出主要问题包括任务描述不足、脱离上下文研究和目标用户测试不足，并提出研究应明确报告用户的领域、AI和数据分析专业知识。

Conclusion: 论文为XAI研究提供了设计和报告指南，帮助研究者更好地识别相关研究、填补研究空白并处理设计矛盾。

Abstract: Research into explainable artificial intelligence (XAI) for data analysis
tasks suffer from a large number of contradictions and lack of concrete design
recommendations stemming from gaps in understanding the tasks that require AI
assistance. In this paper, we drew on multiple fields such as visual analytics,
cognition, and dashboard design to propose a method for categorising and
comparing XAI studies under three dimensions: what, why, and who. We identified
the main problems as: inadequate descriptions of tasks, context-free studies,
and insufficient testing with target users. We propose that studies should
specifically report on their users' domain, AI, and data analysis expertise to
illustrate the generalisability of their findings. We also propose study
guidelines for designing and reporting XAI tasks to improve the XAI community's
ability to parse the rapidly growing field. We hope that our contribution can
help researchers and designers better identify which studies are most relevant
to their work, what gaps exist in the research, and how to handle contradictory
results regarding XAI design.

</details>


### [40] [Toward Real-World Table Agents: Capabilities, Workflows, and Design Principles for LLM-based Table Intelligence](https://arxiv.org/abs/2507.10281)
*Jiaming Tian,Liyao Li,Wentao Ye,Haobo Wang,Lingxin Wang,Lihua Yu,Zujie Ren,Gang Chen,Junbo Zhao*

Main category: cs.AI

TL;DR: 该论文综述了基于LLM的表格代理，旨在通过整合预处理、推理和领域适应来自动化表格任务，定义了五个核心能力，并分析了当前方法的差距。


<details>
  <summary>Details</summary>
Motivation: 现实中的表格任务常涉及噪声、结构异质性和语义复杂性，而现有研究多针对干净的学术数据集，因此需要探索更实用的解决方案。

Method: 定义了五个核心能力（C1-C5）来分析比较现有方法，并以Text-to-SQL代理为例详细研究了性能差距。

Result: 发现开源模型在学术基准和实际场景间存在性能差距，并提出了改进建议。

Conclusion: 为提高LLM表格代理的鲁棒性、泛化性和效率提供了实用建议。

Abstract: Tables are fundamental in domains such as finance, healthcare, and public
administration, yet real-world table tasks often involve noise, structural
heterogeneity, and semantic complexity--issues underexplored in existing
research that primarily targets clean academic datasets. This survey focuses on
LLM-based Table Agents, which aim to automate table-centric workflows by
integrating preprocessing, reasoning, and domain adaptation. We define five
core competencies--C1: Table Structure Understanding, C2: Table and Query
Semantic Understanding, C3: Table Retrieval and Compression, C4: Executable
Reasoning with Traceability, and C5: Cross-Domain Generalization--to analyze
and compare current approaches. In addition, a detailed examination of the
Text-to-SQL Agent reveals a performance gap between academic benchmarks and
real-world scenarios, especially for open-source models. Finally, we provide
actionable insights to improve the robustness, generalization, and efficiency
of LLM-based Table Agents in practical settings.

</details>


### [41] [Instance space analysis of the capacitated vehicle routing problem](https://arxiv.org/abs/2507.10397)
*Alessandra M. M. M. Gouvêa,Nuno Paulos,Eduardo Uchoa e Mariá C. V. Nascimento*

Main category: cs.AI

TL;DR: 本文通过实例空间分析（ISA）方法，结合DIMACS数据集，识别了23个相关实例特征，并利用降维和机器学习方法，揭示了实例结构对元启发式算法性能的影响。


<details>
  <summary>Details</summary>
Motivation: 解决CVRP研究中实例特征与元启发式算法性能之间复杂关系的理解问题。

Method: 结合ISA方法和DIMACS数据集，通过PRELIM、SIFTED和PILOT阶段进行降维和机器学习分析。

Result: 生成了一个二维实例空间投影，并提供了投影矩阵，便于新实例的纳入分析。

Conclusion: 为CVRP领域的实例分析提供了一种新方法，并揭示了实例结构对算法行为的影响。

Abstract: This paper seeks to advance CVRP research by addressing the challenge of
understanding the nuanced relationships between instance characteristics and
metaheuristic (MH) performance. We present Instance Space Analysis (ISA) as a
valuable tool that allows for a new perspective on the field. By combining the
ISA methodology with a dataset from the DIMACS 12th Implementation Challenge on
Vehicle Routing, our research enabled the identification of 23 relevant
instance characteristics. Our use of the PRELIM, SIFTED, and PILOT stages,
which employ dimensionality reduction and machine learning methods, allowed us
to create a two-dimensional projection of the instance space to understand how
the structure of instances affect the behavior of MHs. A key contribution of
our work is that we provide a projection matrix, which makes it straightforward
to incorporate new instances into this analysis and allows for a new method for
instance analysis in the CVRP field.

</details>


### [42] [SentiDrop: A Multi Modal Machine Learning model for Predicting Dropout in Distance Learning](https://arxiv.org/abs/2507.10421)
*Meriem Zerkouk,Miloud Mihoubi,Belkacem Chikhaoui*

Main category: cs.AI

TL;DR: 论文提出了一种结合BERT情感分析和XGBoost特征选择的新模型，用于预测远程学习中的学生辍学风险，准确率达84%，优于基线模型。


<details>
  <summary>Details</summary>
Motivation: 远程学习中的辍学问题严重，早期预测对干预和学生坚持至关重要。现有研究强调整合多样化数据源以提高预测准确性。

Method: 结合BERT对学生评论进行情感分析，与XGBoost处理的社会人口和行为数据融合，通过特征重要性选择关键特征。

Result: 模型在未见数据上准确率达84%，优于基线模型的82%，且在精确率和F1分数等指标上表现更优。

Conclusion: 该方法为开发个性化策略以减少辍学率和鼓励学生坚持提供了重要工具。

Abstract: School dropout is a serious problem in distance learning, where early
detection is crucial for effective intervention and student perseverance.
Predicting student dropout using available educational data is a widely
researched topic in learning analytics. Our partner's distance learning
platform highlights the importance of integrating diverse data sources,
including socio-demographic data, behavioral data, and sentiment analysis, to
accurately predict dropout risks. In this paper, we introduce a novel model
that combines sentiment analysis of student comments using the Bidirectional
Encoder Representations from Transformers (BERT) model with socio-demographic
and behavioral data analyzed through Extreme Gradient Boosting (XGBoost). We
fine-tuned BERT on student comments to capture nuanced sentiments, which were
then merged with key features selected using feature importance techniques in
XGBoost. Our model was tested on unseen data from the next academic year,
achieving an accuracy of 84\%, compared to 82\% for the baseline model.
Additionally, the model demonstrated superior performance in other metrics,
such as precision and F1-score. The proposed method could be a vital tool in
developing personalized strategies to reduce dropout rates and encourage
student perseverance

</details>


### [43] [Acquiring and Adapting Priors for Novel Tasks via Neural Meta-Architectures](https://arxiv.org/abs/2507.10446)
*Sudarshan Babu*

Main category: cs.AI

TL;DR: 论文提出了一种利用神经记忆和超网络设计的方法，解决在数据稀缺领域（如计算化学、医学成像）中高效获取先验知识的问题，并在3D场景生成和分子属性预测中验证了其有效性。


<details>
  <summary>Details</summary>
Motivation: 在数据稀缺的领域（如计算化学、计算免疫学和医学成像），训练大型预训练模型或基础模型不可行，因此需要设计高效获取先验知识的架构。

Method: 使用神经记忆适应非平稳分布，设计超网络结合MAML获取更通用的先验，并将其应用于3D场景生成和分割以及分子属性预测。

Result: 方法在少量样本下实现了高效先验获取，提升了3D场景生成和分割的效率，并改进了分子属性预测。

Conclusion: 提出的架构在数据稀缺领域展示了高效先验获取的潜力，为相关应用提供了新的解决方案。

Abstract: The ability to transfer knowledge from prior experiences to novel tasks
stands as a pivotal capability of intelligent agents, including both humans and
computational models. This principle forms the basis of transfer learning,
where large pre-trained neural networks are fine-tuned to adapt to downstream
tasks. Transfer learning has demonstrated tremendous success, both in terms of
task adaptation speed and performance. However there are several domains where,
due to lack of data, training such large pre-trained models or foundational
models is not a possibility - computational chemistry, computational
immunology, and medical imaging are examples. To address these challenges, our
work focuses on designing architectures to enable efficient acquisition of
priors when large amounts of data are unavailable. In particular, we
demonstrate that we can use neural memory to enable adaptation on
non-stationary distributions with only a few samples. Then we demonstrate that
our hypernetwork designs (a network that generates another network) can acquire
more generalizable priors than standard networks when trained with Model
Agnostic Meta-Learning (MAML). Subsequently, we apply hypernetworks to 3D scene
generation, demonstrating that they can acquire priors efficiently on just a
handful of training scenes, thereby leading to faster text-to-3D generation. We
then extend our hypernetwork framework to perform 3D segmentation on novel
scenes with limited data by efficiently transferring priors from earlier viewed
scenes. Finally, we repurpose an existing molecular generative method as a
pre-training framework that facilitates improved molecular property prediction,
addressing critical challenges in computational immunology

</details>


### [44] [DeepResearch$^{\text{Eco}}$: A Recursive Agentic Workflow for Complex Scientific Question Answering in Ecology](https://arxiv.org/abs/2507.10522)
*Jennifer D'Souza,Endres Keno Sander,Andrei Aioanei*

Main category: cs.AI

TL;DR: DeepResearch$^{\text{Eco}}$是一个基于LLM的自动化科学合成系统，支持递归、深度和广度可控的探索，显著提升文献检索的多样性和细致度。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成方法缺乏用户可控性和透明度，DeepResearch旨在解决这一问题，支持高效、严谨的领域证据整合。

Method: 通过用户可控制的合成、透明推理和参数驱动配置，实现高吞吐量的领域证据集成。

Result: 在49个生态研究问题中，DeepResearch实现了21倍的源整合提升和14.9倍的每千字源整合增长，高参数设置下达到专家级分析深度。

Conclusion: DeepResearch$^{\text{Eco}}$在科学文献合成中表现出色，提供了高效、可控的解决方案。

Abstract: We introduce DeepResearch$^{\text{Eco}}$, a novel agentic LLM-based system
for automated scientific synthesis that supports recursive, depth- and
breadth-controlled exploration of original research questions -- enhancing
search diversity and nuance in the retrieval of relevant scientific literature.
Unlike conventional retrieval-augmented generation pipelines, DeepResearch
enables user-controllable synthesis with transparent reasoning and
parameter-driven configurability, facilitating high-throughput integration of
domain-specific evidence while maintaining analytical rigor. Applied to 49
ecological research questions, DeepResearch achieves up to a 21-fold increase
in source integration and a 14.9-fold rise in sources integrated per 1,000
words. High-parameter settings yield expert-level analytical depth and
contextual diversity.
  Source code available at: https://github.com/sciknoworg/deep-research.

</details>
