<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 22]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Human Creativity and AI](https://arxiv.org/abs/2507.08001)
*Shengyi Xie*

Main category: cs.AI

TL;DR: 本文探讨了科技进步对创造力哲学的重新诠释，聚焦于AI是否具备创造力的问题，结合心理学、认知神经科学和哲学视角进行分析。


<details>
  <summary>Details</summary>
Motivation: 研究动机在于探讨AI技术发展背景下创造力的本质及其哲学意义，尤其是AI是否能够展现创造力。

Method: 方法包括回顾创造力哲学的历史视角，分析心理学进展对创造力研究的影响，以及探讨自然主义和认知神经科学对创造力的定义与回应。

Result: 研究结果揭示了创造力定义的多样性，并展示了AI在创造力研究中的潜在角色。

Conclusion: 结论指出，尽管AI在模拟创造力方面取得进展，但其是否真正具备创造力仍需进一步探讨。

Abstract: With the advancement of science and technology, the philosophy of creativity
has undergone significant reinterpretation. This paper investigates
contemporary research in the fields of psychology, cognitive neuroscience, and
the philosophy of creativity, particularly in the context of the development of
artificial intelligence (AI) techniques. It aims to address the central
question: Can AI exhibit creativity? The paper reviews the historical
perspectives on the philosophy of creativity and explores the influence of
psychological advancements on the study of creativity. Furthermore, it analyzes
various definitions of creativity and examines the responses of naturalism and
cognitive neuroscience to the concept of creativity.

</details>


### [2] [TableReasoner: Advancing Table Reasoning Framework with Large Language Models](https://arxiv.org/abs/2507.08046)
*Sishi Xiong,Dakai Wang,Yu Zhao,Jie Zhang,Changzai Pan,Haowei He,Xiangyu Li,Wenhan Chang,Zhongjiang He,Shuangyong Song,Yongxiang Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于大语言模型和编程的表推理框架TableReasoner，用于解决表问答任务中的挑战，如表格大、列语义不完整和实体歧义。


<details>
  <summary>Details</summary>
Motivation: 解决真实世界表格数据在表问答任务中的挑战，如表格规模大、语义不完整和实体歧义。

Method: 提出TableReasoner框架，结合结构和语义表示建模表格，设计多步模式链接计划以聚焦查询相关信息，并集成迭代思考架构。

Result: 系统在SemEval-2025 Task 8的两个子任务中均获得第一名。

Conclusion: TableReasoner通过结合结构和语义表示以及迭代推理，有效解决了表问答任务中的挑战。

Abstract: The paper presents our system developed for table question answering (TQA).
TQA tasks face challenges due to the characteristics of real-world tabular
data, such as large size, incomplete column semantics, and entity ambiguity. To
address these issues, we propose a large language model (LLM)-powered and
programming-based table reasoning framework, named TableReasoner. It models a
table using the schema that combines structural and semantic representations,
enabling holistic understanding and efficient processing of large tables. We
design a multi-step schema linking plan to derive a focused table schema that
retains only query-relevant information, eliminating ambiguity and alleviating
hallucinations. This focused table schema provides precise and sufficient table
details for query refinement and programming. Furthermore, we integrate the
reasoning workflow into an iterative thinking architecture, allowing
incremental cycles of thinking, reasoning and reflection. Our system achieves
first place in both subtasks of SemEval-2025 Task 8.

</details>


### [3] [A Dynamic Stackelberg Game Framework for Agentic AI Defense Against LLM Jailbreaking](https://arxiv.org/abs/2507.08207)
*Zhengye Han,Quanyan Zhu*

Main category: cs.AI

TL;DR: 本文提出了一种动态Stackelberg博弈框架，用于建模大型语言模型（LLM）越狱攻击中的攻防交互，并引入了一种名为“紫色代理”的AI解决方案。


<details>
  <summary>Details</summary>
Motivation: 随着LLM在关键应用中的部署增加，越狱攻击（绕过安全机制）成为重要问题，需要一种系统化的防御方法。

Method: 采用Stackelberg博弈框架，将提示-响应动态建模为序贯扩展形式博弈，并开发了结合对抗探索和防御策略的“紫色代理”。

Result: 紫色代理通过RRT模拟攻击轨迹并主动干预，有效减少有害输出，为分析对抗动态提供了理论基础。

Conclusion: 该框架为缓解LLM越狱风险提供了原则性方法，并展示了主动防御策略的潜力。

Abstract: As large language models (LLMs) are increasingly deployed in critical
applications, the challenge of jailbreaking, where adversaries manipulate the
models to bypass safety mechanisms, has become a significant concern. This
paper presents a dynamic Stackelberg game framework to model the interactions
between attackers and defenders in the context of LLM jailbreaking. The
framework treats the prompt-response dynamics as a sequential extensive-form
game, where the defender, as the leader, commits to a strategy while
anticipating the attacker's optimal responses. We propose a novel agentic AI
solution, the "Purple Agent," which integrates adversarial exploration and
defensive strategies using Rapidly-exploring Random Trees (RRT). The Purple
Agent actively simulates potential attack trajectories and intervenes
proactively to prevent harmful outputs. This approach offers a principled
method for analyzing adversarial dynamics and provides a foundation for
mitigating the risk of jailbreaking.

</details>


### [4] [Reasoning and Behavioral Equilibria in LLM-Nash Games: From Mindsets to Actions](https://arxiv.org/abs/2507.08208)
*Quanyan Zhu*

Main category: cs.AI

TL;DR: LLM-Nash框架通过游戏理论模型研究LLM驱动的决策，将推理提示作为策略空间，探索有限理性下的均衡。


<details>
  <summary>Details</summary>
Motivation: 传统博弈论假设完全理性，而LLM-Nash框架旨在捕捉有限理性，通过显式建模推理过程研究认知约束和策略互动。

Method: 框架将提示选择作为策略，LLM推理生成行为输出，定义提示空间上的均衡。

Result: 推理均衡与传统Nash均衡存在差异，为LLM系统提供了新的战略互动基础。

Conclusion: LLM-Nash框架为有限理性下的战略互动提供了新视角，适用于LLM驱动的决策系统。

Abstract: We introduce the LLM-Nash framework, a game-theoretic model where agents
select reasoning prompts to guide decision-making via Large Language Models
(LLMs). Unlike classical games that assume utility-maximizing agents with full
rationality, this framework captures bounded rationality by modeling the
reasoning process explicitly. Equilibrium is defined over the prompt space,
with actions emerging as the behavioral output of LLM inference. This approach
enables the study of cognitive constraints, mindset expressiveness, and
epistemic learning. Through illustrative examples, we show how reasoning
equilibria can diverge from classical Nash outcomes, offering a new foundation
for strategic interaction in LLM-enabled systems.

</details>


### [5] [From Curiosity to Competence: How World Models Interact with the Dynamics of Exploration](https://arxiv.org/abs/2507.08210)
*Fryderyk Mantiuk,Hanqi Zhou,Charley M. Wu*

Main category: cs.AI

TL;DR: 论文探讨了智能体如何在探索（好奇心）与控制（能力）之间取得平衡，通过比较两种模型（Tabular和Dreamer）揭示了探索与内部表征学习的互动关系。


<details>
  <summary>Details</summary>
Motivation: 研究智能体如何在好奇心（寻求知识）和能力（控制环境）之间取得平衡，以理解探索行为的驱动力。

Method: 比较了两种基于模型的智能体：使用手工状态抽象的Tabular和学习内部世界模型的Dreamer。

Result: Tabular智能体显示好奇心和能力引导探索的不同模式，而Dreamer智能体揭示了探索与表征学习的双向互动。

Conclusion: 研究形式化了适应性探索为追求未知与可控之间的平衡，为认知理论和高效强化学习提供了见解。

Abstract: What drives an agent to explore the world while also maintaining control over
the environment? From a child at play to scientists in the lab, intelligent
agents must balance curiosity (the drive to seek knowledge) with competence
(the drive to master and control the environment). Bridging cognitive theories
of intrinsic motivation with reinforcement learning, we ask how evolving
internal representations mediate the trade-off between curiosity (novelty or
information gain) and competence (empowerment). We compare two model-based
agents using handcrafted state abstractions (Tabular) or learning an internal
world model (Dreamer). The Tabular agent shows curiosity and competence guide
exploration in distinct patterns, while prioritizing both improves exploration.
The Dreamer agent reveals a two-way interaction between exploration and
representation learning, mirroring the developmental co-evolution of curiosity
and competence. Our findings formalize adaptive exploration as a balance
between pursuing the unknown and the controllable, offering insights for
cognitive theories and efficient reinforcement learning.

</details>


### [6] [Grounding Methods for Neural-Symbolic AI](https://arxiv.org/abs/2507.08216)
*Rodrigo Castellano Ontiveros,Francesco Giannini,Marco Gori,Giuseppe Marra,Michelangelo Diligenti*

Main category: cs.AI

TL;DR: 本文提出了一种参数化的逻辑接地方法家族，以平衡神经符号方法的表达能力和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决传统神经符号方法中逻辑接地过程导致的组合爆炸或信息丢失问题。

Method: 提出基于多跳符号推理的参数化接地方法家族，泛化经典反向链。

Result: 实验表明接地标准的选择对神经符号方法的性能至关重要。

Conclusion: 参数化接地方法能够灵活控制表达能力和可扩展性的权衡。

Abstract: A large class of Neural-Symbolic (NeSy) methods employs a machine learner to
process the input entities, while relying on a reasoner based on First-Order
Logic to represent and process more complex relationships among the entities. A
fundamental role for these methods is played by the process of logic grounding,
which determines the relevant substitutions for the logic rules using a
(sub)set of entities. Some NeSy methods use an exhaustive derivation of all
possible substitutions, preserving the full expressive power of the logic
knowledge. This leads to a combinatorial explosion in the number of ground
formulas to consider and, therefore, strongly limits their scalability. Other
methods rely on heuristic-based selective derivations, which are generally more
computationally efficient, but lack a justification and provide no guarantees
of preserving the information provided to and returned by the reasoner. Taking
inspiration from multi-hop symbolic reasoning, this paper proposes a
parametrized family of grounding methods generalizing classic Backward
Chaining. Different selections within this family allow us to obtain commonly
employed grounding methods as special cases, and to control the trade-off
between expressiveness and scalability of the reasoner. The experimental
results show that the selection of the grounding criterion is often as
important as the NeSy method itself.

</details>


### [7] [Quantum Federated Learning for Multimodal Data: A Modality-Agnostic Approach](https://arxiv.org/abs/2507.08217)
*Atit Pokharel,Ratun Rahman,Thomas Morris,Dinh C. Nguyen*

Main category: cs.AI

TL;DR: 论文提出了一种多模态量子联邦学习（QFL）方法，利用量子纠缠实现中间融合，并引入缺失模态无关（MMA）机制以提升模型稳定性。


<details>
  <summary>Details</summary>
Motivation: 现有QFL框架主要针对单模态系统，难以应对现实任务中的多模态需求，因此需要填补这一空白。

Method: 采用多模态QFL框架，结合量子纠缠进行中间融合，并设计MMA机制隔离未训练的量子电路。

Result: 在IID和非IID数据分布下，准确率分别提升6.84%和7.25%。

Conclusion: 多模态QFL结合MMA机制显著提升了模型性能，解决了模态缺失问题。

Abstract: Quantum federated learning (QFL) has been recently introduced to enable a
distributed privacy-preserving quantum machine learning (QML) model training
across quantum processors (clients). Despite recent research efforts, existing
QFL frameworks predominantly focus on unimodal systems, limiting their
applicability to real-world tasks that often naturally involve multiple
modalities. To fill this significant gap, we present for the first time a novel
multimodal approach specifically tailored for the QFL setting with the
intermediate fusion using quantum entanglement. Furthermore, to address a major
bottleneck in multimodal QFL, where the absence of certain modalities during
training can degrade model performance, we introduce a Missing Modality
Agnostic (MMA) mechanism that isolates untrained quantum circuits, ensuring
stable training without corrupted states. Simulation results demonstrate that
the proposed multimodal QFL method with MMA yields an improvement in accuracy
of 6.84% in independent and identically distributed (IID) and 7.25% in non-IID
data distributions compared to the state-of-the-art methods.

</details>


### [8] [Giving AI Agents Access to Cryptocurrency and Smart Contracts Creates New Vectors of AI Harm](https://arxiv.org/abs/2507.08249)
*Bill Marino,Ari Juels*

Main category: cs.AI

TL;DR: 本文探讨了赋予AI代理加密货币和智能合约访问权限可能带来的新型危害，并呼吁更多技术研究以预防和减轻这些危害。


<details>
  <summary>Details</summary>
Motivation: 随着对AI代理使用加密货币和智能合约的兴趣增加，本文旨在揭示由此可能引发的潜在危害。

Method: 分析了加密货币和智能合约的独特属性，并详细描述了这些属性如何导致新型危害。

Result: 识别了新型危害的潜在路径，强调了技术研究的必要性。

Conclusion: 呼吁加强技术研究以确保AI代理安全使用加密货币和智能合约。

Abstract: There is growing interest in giving AI agents access to cryptocurrencies as
well as to the smart contracts that transact them. But doing so, this position
paper argues, could lead to formidable new vectors of AI harm. To support this
argument, we first examine the unique properties of cryptocurrencies and smart
contracts that could lead to these new vectors of harm. Next, we describe each
of these new vectors of harm in detail. Finally, we conclude with a call for
more technical research aimed at preventing and mitigating these harms and,
thereby making it safer to endow AI agents with cryptocurrencies and smart
contracts.

</details>


### [9] [Abductive Computational Systems: Creative Abduction and Future Directions](https://arxiv.org/abs/2507.08264)
*Abhinav Sood,Kazjon Grace,Stephen Wan,Cecile Paris*

Main category: cs.AI

TL;DR: 本文综述了溯因推理在不同领域的讨论，分析了计算系统中的应用，指出理论和计算实现均未能有效支持创造性假设的生成，并提出了未来研究方向。


<details>
  <summary>Details</summary>
Motivation: 探讨溯因推理在认识论、科学和设计中的理解差异，以及计算系统中如何应用溯因推理。

Method: 通过文献综述和分析计算系统的实现方式，分解溯因推理的组成部分。

Result: 理论和计算实现均未能充分支持创造性假设的生成，计算系统多采用三段论形式。

Conclusion: 提出未来研究应关注如何在计算系统中实现创造性溯因推理的具体方向。

Abstract: Abductive reasoning, reasoning for inferring explanations for observations,
is often mentioned in scientific, design-related and artistic contexts, but its
understanding varies across these domains. This paper reviews how abductive
reasoning is discussed in epistemology, science and design, and then analyses
how various computational systems use abductive reasoning. Our analysis shows
that neither theoretical accounts nor computational implementations of
abductive reasoning adequately address generating creative hypotheses.
Theoretical frameworks do not provide a straightforward model for generating
creative abductive hypotheses, computational systems largely implement
syllogistic forms of abductive reasoning. We break down abductive computational
systems into components and conclude by identifying specific directions for
future research that could advance the state of creative abductive reasoning in
computational systems.

</details>


### [10] [Agent Safety Alignment via Reinforcement Learning](https://arxiv.org/abs/2507.08270)
*Zeyang Sha,Hanling Tian,Zhuoer Xu,Shiwen Cui,Changhua Meng,Weiqiang Wang*

Main category: cs.AI

TL;DR: 论文提出了一种统一的安全对齐框架，用于处理使用工具的LLM代理的安全风险，通过结构化推理和沙盒强化学习优化安全性和实用性。


<details>
  <summary>Details</summary>
Motivation: 随着能够使用工具的自主LLM代理的出现，传统对话滥用之外的新安全风险（如用户或工具发起的威胁）需要解决。

Method: 提出三模态分类法（良性、恶意、敏感）和策略驱动决策模型，使用定制沙盒环境模拟工具执行并进行细粒度奖励塑造。

Result: 在多个基准测试中，安全对齐的代理显著提高了对安全威胁的抵抗能力，同时保持了良性任务的实用性。

Conclusion: 研究表明安全性和有效性可以共同优化，为自主LLM代理的可信部署奠定了基础。

Abstract: The emergence of autonomous Large Language Model (LLM) agents capable of tool
usage has introduced new safety risks that go beyond traditional conversational
misuse. These agents, empowered to execute external functions, are vulnerable
to both user-initiated threats (e.g., adversarial prompts) and tool-initiated
threats (e.g., malicious outputs from compromised tools). In this paper, we
propose the first unified safety-alignment framework for tool-using agents,
enabling models to handle both channels of threat via structured reasoning and
sandboxed reinforcement learning. We introduce a tri-modal taxonomy, including
benign, malicious, and sensitive for both user prompts and tool responses, and
define a policy-driven decision model. Our framework employs a custom-designed
sandbox environment that simulates real-world tool execution and allows
fine-grained reward shaping. Through extensive evaluations on public and
self-built benchmarks, including Agent SafetyBench, InjecAgent, and BFCL, we
demonstrate that our safety-aligned agents significantly improve resistance to
security threats while preserving strong utility on benign tasks. Our results
show that safety and effectiveness can be jointly optimized, laying the
groundwork for trustworthy deployment of autonomous LLM agents.

</details>


### [11] [M2-Reasoning: Empowering MLLMs with Unified General and Spatial Reasoning](https://arxiv.org/abs/2507.08306)
*Inclusion AI,:,Fudong Wang,Jiajia Liu,Jingdong Chen,Jun Zhou,Kaixiang Ji,Lixiang Ru,Qingpei Guo,Ruobing Zheng,Tianqi Li,Yi Yuan,Yifan Mao,Yuting Xiao,Ziping Ma*

Main category: cs.AI

TL;DR: M2-Reasoning-7B模型通过创新的数据管道和动态多任务训练策略，解决了MLLMs在动态空间交互上的不足，并在8个基准测试中达到SOTA。


<details>
  <summary>Details</summary>
Motivation: 现有MLLMs在动态空间交互能力上存在不足，限制了其在实际应用中的表现。

Method: 1. 构建高质量数据管道（294.2K样本）；2. 动态多任务训练策略与任务特定奖励。

Result: M2-Reasoning-7B在8个基准测试中表现最优，尤其在通用和空间推理领域。

Conclusion: M2-Reasoning-7B通过数据与训练方法的创新，显著提升了MLLMs的动态空间推理能力。

Abstract: Recent advancements in Multimodal Large Language Models (MLLMs), particularly
through Reinforcement Learning with Verifiable Rewards (RLVR), have
significantly enhanced their reasoning abilities. However, a critical gap
persists: these models struggle with dynamic spatial interactions, a capability
essential for real-world applications. To bridge this gap, we introduce
M2-Reasoning-7B, a model designed to excel in both general and spatial
reasoning. Our approach integrates two key innovations: (1) a novel data
pipeline that generates 294.2K high-quality data samples (168K for cold-start
fine-tuning and 126.2K for RLVR), which feature logically coherent reasoning
trajectories and have undergone comprehensive assessment; and (2) a dynamic
multi-task training strategy with step-wise optimization to mitigate conflicts
between data, and task-specific rewards for delivering tailored incentive
signals. This combination of curated data and advanced training allows
M2-Reasoning-7B to set a new state-of-the-art (SOTA) across 8 benchmarks,
showcasing superior performance in both general and spatial reasoning domains.

</details>


### [12] [Multi-Agent LLMs as Ethics Advocates in AI-Based Systems](https://arxiv.org/abs/2507.08392)
*Asma Yamani,Malak Baslyman,Moataz Ahmed*

Main category: cs.AI

TL;DR: 论文提出了一种在多智能体LLM环境中引入伦理倡导者代理的框架，用于自动生成伦理需求草稿，并通过案例研究验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 由于时间和资源限制，手动获取伦理需求具有挑战性且优先级较低，因此需要一种自动化方法来促进伦理需求的生成。

Method: 在多智能体LLM环境中引入伦理倡导者代理，根据系统描述提供伦理问题的批评和建议。

Result: 框架在案例研究中捕获了大部分研究人员在30分钟访谈中识别的伦理需求，并引入了额外相关需求，但也存在可靠性问题。

Conclusion: 该框架有助于在需求工程中更广泛地采用伦理考量，但仍需人类反馈以确保可靠性。

Abstract: Incorporating ethics into the requirement elicitation process is essential
for creating ethically aligned systems. Although eliciting manual ethics
requirements is effective, it requires diverse input from multiple
stakeholders, which can be challenging due to time and resource constraints.
Moreover, it is often given a low priority in the requirements elicitation
process. This study proposes a framework for generating ethics requirements
drafts by introducing an ethics advocate agent in a multi-agent LLM setting.
This agent critiques and provides input on ethical issues based on the system
description. The proposed framework is evaluated through two case studies from
different contexts, demonstrating that it captures the majority of ethics
requirements identified by researchers during 30-minute interviews and
introduces several additional relevant requirements. However, it also
highlights reliability issues in generating ethics requirements, emphasizing
the need for human feedback in this sensitive domain. We believe this work can
facilitate the broader adoption of ethics in the requirements engineering
process, ultimately leading to more ethically aligned products.

</details>


### [13] [Why this and not that? A Logic-based Framework for Contrastive Explanations](https://arxiv.org/abs/2507.08454)
*Tobias Geibinger,Reijo Jaakkola,Antti Kuusisto,Xinghan Liu,Miikka Vilander*

Main category: cs.AI

TL;DR: 论文定义了几种与对比解释相关的规范问题，探讨了它们在命题逻辑中的基本性质，并分析了计算复杂性。


<details>
  <summary>Details</summary>
Motivation: 研究对比解释（回答“为什么P而非Q？”）的规范问题，以填补现有文献中的空白。

Method: 在命题逻辑中定义问题，分析其性质，并使用答案集编程实现CNF公式的解决方案。

Result: 框架捕捉了现有对比解释的最小基数版本，并提供了计算复杂性的详细分析。

Conclusion: 论文通过理论和实践验证了对比解释问题的可行性和实用性。

Abstract: We define several canonical problems related to contrastive explanations,
each answering a question of the form ''Why P but not Q?''. The problems
compute causes for both P and Q, explicitly comparing their differences. We
investigate the basic properties of our definitions in the setting of
propositional logic. We show, inter alia, that our framework captures a
cardinality-minimal version of existing contrastive explanations in the
literature. Furthermore, we provide an extensive analysis of the computational
complexities of the problems. We also implement the problems for CNF-formulas
using answer set programming and present several examples demonstrating how
they work in practice.

</details>


### [14] [From Language to Logic: A Bi-Level Framework for Structured Reasoning](https://arxiv.org/abs/2507.08501)
*Keying Yang,Hao Wang,Kai Yang*

Main category: cs.AI

TL;DR: 论文提出了一种双层框架，通过任务抽象和逻辑生成两阶段过程，将自然语言映射到逻辑表示，显著提升了推理任务的准确性和可解释性。


<details>
  <summary>Details</summary>
Motivation: 解决自然语言输入与形式逻辑表示之间的差距，实现更准确、可解释的系统化推理。

Method: 采用双层框架：高层任务抽象（LLM解析自然语言为结构化表示）和低层逻辑生成（LLM生成符号化工作流或可执行程序）。通过端到端双层优化联合优化两阶段。

Result: 在多个推理基准测试中，准确率显著优于基线方法，最高提升40%。

Conclusion: 双层设计提升了透明度和错误可追溯性，为LLM的可信系统化推理提供了有前景的方向。

Abstract: Structured reasoning over natural language inputs remains a core challenge in
artificial intelligence, as it requires bridging the gap between unstructured
linguistic expressions and formal logical representations. In this paper, we
propose a novel \textbf{bi-level framework} that maps language to logic through
a two-stage process: high-level task abstraction and low-level logic
generation. At the upper level, a large language model (LLM) parses natural
language queries into intermediate structured representations specifying the
problem type, objectives, decision variables, and symbolic constraints. At the
lower level, the LLM uses these representations to generate symbolic workflows
or executable reasoning programs for accurate and interpretable decision
making. The framework supports modular reasoning, enforces explicit
constraints, and generalizes across domains such as mathematical problem
solving, question answering, and logical inference. We further optimize the
framework with an end-to-end {bi-level} optimization approach that jointly
refines both the high-level abstraction and low-level logic generation stages.
Experiments on multiple realistic reasoning benchmarks demonstrate that our
approach significantly outperforms existing baselines in accuracy, with
accuracy gains reaching as high as 40\%. Moreover, the bi-level design enhances
transparency and error traceability, offering a promising step toward
trustworthy and systematic reasoning with LLMs.

</details>


### [15] [A Multi-granularity Concept Sparse Activation and Hierarchical Knowledge Graph Fusion Framework for Rare Disease Diagnosis](https://arxiv.org/abs/2507.08529)
*Mingda Zhang,Na Zhao,Jianglong Qin,Guoyu Ye,Ruixiang Tang*

Main category: cs.AI

TL;DR: 提出了一种结合多粒度稀疏激活和分层知识图谱的框架，用于提升罕见病诊断的准确性和信息质量。


<details>
  <summary>Details</summary>
Motivation: 罕见病诊断因知识表示深度不足、概念理解有限和临床推理受限而进展缓慢。

Method: 采用多粒度稀疏激活医学概念，结合分层知识图谱（分类、临床特征、实例），并辅以四种匹配算法、多样性控制和五级回退策略。

Result: 在BioASQ罕见病QA数据集上，BLEU提升0.09，ROUGE提升0.05，准确率提升0.12，峰值准确率达0.89。专家评估显示信息质量、推理和专业表达均有改进。

Conclusion: 该方法缩短了罕见病患者的“诊断旅程”，接近临床阈值（0.90）。

Abstract: Despite advances from medical large language models in healthcare,
rare-disease diagnosis remains hampered by insufficient
knowledge-representation depth, limited concept understanding, and constrained
clinical reasoning. We propose a framework that couples multi-granularity
sparse activation of medical concepts with a hierarchical knowledge graph. Four
complementary matching algorithms, diversity control, and a five-level fallback
strategy enable precise concept activation, while a three-layer knowledge graph
(taxonomy, clinical features, instances) provides structured, up-to-date
context. Experiments on the BioASQ rare-disease QA set show BLEU gains of 0.09,
ROUGE gains of 0.05, and accuracy gains of 0.12, with peak accuracy of 0.89
approaching the 0.90 clinical threshold. Expert evaluation confirms
improvements in information quality, reasoning, and professional expression,
suggesting our approach shortens the "diagnostic odyssey" for rare-disease
patients.

</details>


### [16] [Large Multi-modal Model Cartographic Map Comprehension for Textual Locality Georeferencing](https://arxiv.org/abs/2507.08575)
*Kalana Wijegunarathna,Kristin Stock,Christopher B. Jones*

Main category: cs.AI

TL;DR: 提出了一种利用多模态大模型（LMM）自动地理参考生物样本记录的新方法，通过视觉上下文理解空间关系，实验结果显示优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 解决自然历史收藏中大量未地理参考的生物样本记录问题，传统方法效率低下且未充分利用地图信息。

Method: 采用基于网格的多模态大模型（LMM），在零样本设置下实现地理参考，结合视觉和文本信息。

Result: 实验显示平均距离误差约1公里，优于单模态语言模型和现有工具。

Conclusion: 多模态大模型在理解精细地图方面表现优异，提出了将其整合到地理参考工作流的实用框架。

Abstract: Millions of biological sample records collected in the last few centuries
archived in natural history collections are un-georeferenced. Georeferencing
complex locality descriptions associated with these collection samples is a
highly labour-intensive task collection agencies struggle with. None of the
existing automated methods exploit maps that are an essential tool for
georeferencing complex relations. We present preliminary experiments and
results of a novel method that exploits multi-modal capabilities of recent
Large Multi-Modal Models (LMM). This method enables the model to visually
contextualize spatial relations it reads in the locality description. We use a
grid-based approach to adapt these auto-regressive models for this task in a
zero-shot setting. Our experiments conducted on a small manually annotated
dataset show impressive results for our approach ($\sim$1 km Average distance
error) compared to uni-modal georeferencing with Large Language Models and
existing georeferencing tools. The paper also discusses the findings of the
experiments in light of an LMM's ability to comprehend fine-grained maps.
Motivated by these results, a practical framework is proposed to integrate this
method into a georeferencing workflow.

</details>


### [17] [Unlocking Speech Instruction Data Potential with Query Rewriting](https://arxiv.org/abs/2507.08603)
*Yonghua Hei,Yibo Yan,Shuliang Liu,Huiyu Zhou,Linfeng Zhang,Xuming Hu*

Main category: cs.AI

TL;DR: 论文提出了一种通过多LLM知识融合的查询重写框架，用于构建高质量语音指令数据集，解决了TTS模型在分布外文本转换中的问题。


<details>
  <summary>Details</summary>
Motivation: 当前语音指令数据集缺乏且训练任务偏差大，LLM生成结果与人类响应存在差距，人工标注成本高，因此需要一种高效替代方案。

Method: 采用多LLM知识融合的查询重写框架，通过多智能体标注和验证合成语音，无需人工标注。

Result: 实验表明，该方法通过零样本重写将数据可用性从72%提升至93%，在复杂知识和上下文相关任务中表现优异。

Conclusion: 该方法为构建高质量语音指令数据集提供了高效且鲁棒的解决方案，显著提升了TTS模型的适应性。

Abstract: End-to-end Large Speech Language Models~(\textbf{LSLMs}) demonstrate strong
potential in response latency and speech comprehension capabilities, showcasing
general intelligence across speech understanding tasks. However, the ability to
follow speech instructions has not been fully realized due to the lack of
datasets and heavily biased training tasks. Leveraging the rich ASR datasets,
previous approaches have used Large Language Models~(\textbf{LLMs}) to continue
the linguistic information of speech to construct speech instruction datasets.
Yet, due to the gap between LLM-generated results and real human responses, the
continuation methods further amplify these shortcomings. Given the high costs
of collecting and annotating speech instruction datasets by humans, using
speech synthesis to construct large-scale speech instruction datasets has
become a balanced and robust alternative. Although modern
Text-To-Speech~(\textbf{TTS}) models have achieved near-human-level synthesis
quality, it is challenging to appropriately convert out-of-distribution text
instruction to speech due to the limitations of the training data distribution
in TTS models. To address this issue, we propose a query rewriting framework
with multi-LLM knowledge fusion, employing multiple agents to annotate and
validate the synthesized speech, making it possible to construct high-quality
speech instruction datasets without relying on human annotation. Experiments
show that this method can transform text instructions into distributions more
suitable for TTS models for speech synthesis through zero-shot rewriting,
increasing data usability from 72\% to 93\%. It also demonstrates unique
advantages in rewriting tasks that require complex knowledge and
context-related abilities.

</details>


### [18] [Agentic Large Language Models for Conceptual Systems Engineering and Design](https://arxiv.org/abs/2507.08619)
*Soheyl Massoudi,Mark Fuge*

Main category: cs.AI

TL;DR: 论文探讨了多智能体系统（MAS）在早期工程设计中的表现，发现其在设计细节上优于双智能体系统（2AS），但需求覆盖率和代码兼容性仍有不足。


<details>
  <summary>Details</summary>
Motivation: 解决现有大语言模型（LLM）工作流在任务连续性和生成可执行模型方面的不足，探索结构化多智能体系统在工程设计中的潜力。

Method: 通过设计状态图（DSG）表示需求、物理实现和物理模型，比较九角色MAS与双角色2AS在太阳能水过滤系统设计中的表现。

Result: MAS生成更细粒度的DSG，但需求覆盖率低（<20%）；2AS在特定条件下代码兼容性达100%，但平均低于50%。推理蒸馏模型显著提升工作流完成率。

Conclusion: 结构化多智能体系统能增强设计细节，但需求覆盖和代码兼容性仍需改进；推理蒸馏模型对任务完成率有积极影响。

Abstract: Early-stage engineering design involves complex, iterative reasoning, yet
existing large language model (LLM) workflows struggle to maintain task
continuity and generate executable models. We evaluate whether a structured
multi-agent system (MAS) can more effectively manage requirements extraction,
functional decomposition, and simulator code generation than a simpler
two-agent system (2AS). The target application is a solar-powered water
filtration system as described in a cahier des charges. We introduce the
Design-State Graph (DSG), a JSON-serializable representation that bundles
requirements, physical embodiments, and Python-based physics models into graph
nodes. A nine-role MAS iteratively builds and refines the DSG, while the 2AS
collapses the process to a Generator-Reflector loop. Both systems run a total
of 60 experiments (2 LLMs - Llama 3.3 70B vs reasoning-distilled DeepSeek R1
70B x 2 agent configurations x 3 temperatures x 5 seeds). We report a JSON
validity, requirement coverage, embodiment presence, code compatibility,
workflow completion, runtime, and graph size. Across all runs, both MAS and 2AS
maintained perfect JSON integrity and embodiment tagging. Requirement coverage
remained minimal (less than 20\%). Code compatibility peaked at 100\% under
specific 2AS settings but averaged below 50\% for MAS. Only the
reasoning-distilled model reliably flagged workflow completion. Powered by
DeepSeek R1 70B, the MAS generated more granular DSGs (average 5-6 nodes)
whereas 2AS mode-collapsed. Structured multi-agent orchestration enhanced
design detail. Reasoning-distilled LLM improved completion rates, yet low
requirements and fidelity gaps in coding persisted.

</details>


### [19] [Leanabell-Prover-V2: Verifier-integrated Reasoning for Formal Theorem Proving via Reinforcement Learning](https://arxiv.org/abs/2507.08649)
*Xingguang Ji,Yahui Liu,Qi Wang,Jingyuan Zhang,Yang Yue,Rui Shi,Chenxi Sun,Fuzheng Zhang,Guorui Zhou,Kun Gai*

Main category: cs.AI

TL;DR: Leanabell-Prover-V2是一个7B参数的大型语言模型，用于生成Lean 4中的形式化定理证明，通过验证器集成的长链思维（CoT）和改进的强化学习（RL）提升性能。


<details>
  <summary>Details</summary>
Motivation: 在Leanabell-Prover-V1的基础上，进一步优化模型性能，通过验证器反馈使模型能够自我感知推理过程的正确性并纠正错误。

Method: 采用验证器反馈的强化学习，结合反馈令牌掩码和多轮验证器交互，优化推理轨迹。

Result: 在MiniF2F测试集上，性能分别提升了3.2%（Kimina-Prover-Preview-Distill-7B）和2.0%（DeepSeek-Prover-V2-7B）。

Conclusion: Leanabell-Prover-V2通过验证器反馈和多轮交互显著提升了定理证明的性能，代码和数据已开源。

Abstract: We introduce our Leanabell-Prover-V2, a 7B large language models (LLMs) that
can produce formal theorem proofs in Lean 4, with verifier-integrated Long
Chain-of-Thoughts (CoT). Following our previous work Leanabell-Prover-V1, we
continual to choose to posttrain existing strong prover models for further
performance improvement. In our V2 version, we mainly upgrade the Reinforcement
Learning (RL) with feedback provided by the Lean 4 verifier. Crucially,
verifier feedback, such as indicating success or detailing specific errors,
allows the LLM to become ``self-aware'' of the correctness of its own reasoning
process and learn to reflexively correct errors. Leanabell-Prover-V2 directly
optimizes LLM reasoning trajectories with multi-turn verifier interactions,
together with feedback token masking for stable RL training and a simple reward
strategy. Experiments show that Leanabell-Prover-V2 improves performance by
3.2% (pass@128) with Kimina-Prover-Preview-Distill-7B and 2.0% (pass@128) with
DeepSeek-Prover-V2-7B on the MiniF2F test set. The source codes, curated data
and models are available at:
https://github.com/Leanabell-LM/Leanabell-Prover-V2.

</details>


### [20] [Introspection of Thought Helps AI Agents](https://arxiv.org/abs/2507.08664)
*Haoran Sun,Shaoning Zeng*

Main category: cs.AI

TL;DR: 论文提出了一种名为INoT的新AI代理推理框架，通过设计新的LLM-Read代码提示，减少推理成本并提高性能。


<details>
  <summary>Details</summary>
Motivation: 现有AI代理依赖LLMs和MLLMs，但受限于其自然语言理解能力和高推理成本。

Method: 设计了INoT框架，通过LLM-Read代码提示实现程序化对话推理，减少外部迭代成本。

Result: 在六个基准测试中，INoT平均性能提升7.95%，推理成本降低58.3%。

Conclusion: INoT在性能和成本效率上优于基线方法，并展示了在图像任务中的通用性。

Abstract: AI Agents rely on Large Language Models (LLMs) and Multimodal-LLMs (MLLMs) to
perform interpretation and inference in text and image tasks without
post-training, where LLMs and MLLMs play the most critical role and determine
the initial ability and limitations of AI Agents. Usually, AI Agents utilize
sophisticated prompt engineering and external reasoning framework to obtain a
promising interaction with LLMs, e.g., Chain-of-Thought, Iteration of Thought
and Image-of-Thought. However, they are still constrained by the inherent
limitations of LLM in understanding natural language, and the iterative
reasoning process will generate a large amount of inference cost. To this end,
we propose a novel AI Agent Reasoning Framework with Introspection of Thought
(INoT) by designing a new LLM-Read code in prompt. It enables LLM to execute
programmatic dialogue reasoning processes following the code in prompt.
Therefore, self-denial and reflection occur within LLM instead of outside LLM,
which can reduce token cost effectively. Through our experiments on six
benchmarks for three different tasks, the effectiveness of INoT is verified,
with an average improvement of 7.95\% in performance, exceeding the baselines.
Furthermore, the token cost of INoT is lower on average than the best
performing method at baseline by 58.3\%. In addition, we demonstrate the
versatility of INoT in image interpretation and inference through verification
experiments.

</details>


### [21] [elsciRL: Integrating Language Solutions into Reinforcement Learning Problem Settings](https://arxiv.org/abs/2507.08705)
*Philip Osborne,Danilo S. Carvalho,André Freitas*

Main category: cs.AI

TL;DR: elsciRL是一个开源的Python库，旨在将语言解决方案应用于强化学习问题，通过结合LLMs扩展了现有框架，并提供GUI支持用户输入文本生成指令，实验表明其能提升强化学习代理的性能。


<details>
  <summary>Details</summary>
Motivation: 加速语言解决方案在基于奖励的环境中的评估，为科学发现提供新机会。

Method: 结合LLMs扩展Language Adapter with Self-Completing Instruction框架，提供GUI支持用户输入文本生成指令。

Result: 实验结果表明生成的指令能提升强化学习代理的性能。

Conclusion: elsciRL为语言解决方案在强化学习中的应用提供了便捷工具，具有广泛的应用潜力。

Abstract: We present elsciRL, an open-source Python library to facilitate the
application of language solutions on reinforcement learning problems. We
demonstrate the potential of our software by extending the Language Adapter
with Self-Completing Instruction framework defined in (Osborne, 2024) with the
use of LLMs. Our approach can be re-applied to new applications with minimal
setup requirements. We provide a novel GUI that allows a user to provide text
input for an LLM to generate instructions which it can then self-complete.
Empirical results indicate that these instructions \textit{can} improve a
reinforcement learning agent's performance. Therefore, we present this work to
accelerate the evaluation of language solutions on reward based environments to
enable new opportunities for scientific discovery.

</details>


### [22] [System-of-systems Modeling and Optimization: An Integrated Framework for Intermodal Mobility](https://arxiv.org/abs/2507.08715)
*Paul Saves,Jasper Bussemaker,Rémi Lafage,Thierry Lefebvre,Nathalie Bartoli,Youssef Diouane,Joseph Morlier*

Main category: cs.AI

TL;DR: 论文探讨了在系统架构开发中，使用基于代理的优化算法（如贝叶斯优化）来解决传统物理模拟方法带来的高计算成本和潜在失败问题。


<details>
  <summary>Details</summary>
Motivation: 在系统架构开发中，传统的物理模拟方法虽然高效，但可能导致计算复杂度高和优化算法失败，因此需要探索更高效的优化方法。

Method: 提出使用基于代理的优化算法，特别是贝叶斯优化和Gaussian process模型，以减少计算成本和优化失败。

Result: 研究表明，基于代理的优化算法能有效降低计算复杂度并提高优化效率。

Conclusion: 基于代理的优化算法是解决系统架构开发中高计算成本和优化失败问题的有效方法。

Abstract: For developing innovative systems architectures, modeling and optimization
techniques have been central to frame the architecting process and define the
optimization and modeling problems. In this context, for system-of-systems the
use of efficient dedicated approaches (often physics-based simulations) is
highly recommended to reduce the computational complexity of the targeted
applications. However, exploring novel architectures using such dedicated
approaches might pose challenges for optimization algorithms, including
increased evaluation costs and potential failures. To address these challenges,
surrogate-based optimization algorithms, such as Bayesian optimization
utilizing Gaussian process models have emerged.

</details>
