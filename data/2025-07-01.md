<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 45]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [Bootstrapping Human-Like Planning via LLMs](https://arxiv.org/abs/2506.22604)
*David Porfirio,Vincent Hsiao,Morgan Fine-Morris,Leslie Smith,Laura M. Hiatt*

Main category: cs.AI

TL;DR: 研究结合自然语言编程和拖放界面，通过LLM生成人类级别的动作序列，并与手动指定的序列进行比较。


<details>
  <summary>Details</summary>
Motivation: 探索如何结合自然语言编程的直观性和拖放界面的精确性，以提升机器人任务编程的效率和用户体验。

Method: 构建基于LLM的流程，输入自然语言并生成人类级别的动作序列，再与手动指定的序列进行对比。

Result: 较大模型生成的动作序列更接近人类水平，但较小模型表现也令人满意。

Conclusion: 结合自然语言和拖放界面的方法可行，LLM在生成人类级别动作序列方面表现良好。

Abstract: Robot end users increasingly require accessible means of specifying tasks for
robots to perform. Two common end-user programming paradigms include
drag-and-drop interfaces and natural language programming. Although natural
language interfaces harness an intuitive form of human communication,
drag-and-drop interfaces enable users to meticulously and precisely dictate the
key actions of the robot's task. In this paper, we investigate the degree to
which both approaches can be combined. Specifically, we construct a large
language model (LLM)-based pipeline that accepts natural language as input and
produces human-like action sequences as output, specified at a level of
granularity that a human would produce. We then compare these generated action
sequences to another dataset of hand-specified action sequences. Although our
results reveal that larger models tend to outperform smaller ones in the
production of human-like action sequences, smaller models nonetheless achieve
satisfactory performance.

</details>


### [2] [Ludax: A GPU-Accelerated Domain Specific Language for Board Games](https://arxiv.org/abs/2506.22609)
*Graham Todd,Alexander G. Padula,Dennis J. N. J. Soemers,Julian Togelius*

Main category: cs.AI

TL;DR: Ludax是一个结合游戏描述语言和硬件加速的领域特定语言，旨在加速游戏研究，支持快速模拟和灵活表示。


<details>
  <summary>Details</summary>
Motivation: 游戏是人工智能研究的重要测试环境，但现有工具缺乏硬件加速支持，限制了研究效率。

Method: 开发了Ludax框架，将游戏描述语言编译为硬件加速代码，并集成到深度学习流程中。

Result: Ludax实现了快速模拟，支持强化学习代理训练，并提供了开源实现。

Conclusion: Ludax为游戏研究提供了高效工具，有望推动从强化学习到认知科学的广泛研究。

Abstract: Games have long been used as benchmarks and testing environments for research
in artificial intelligence. A key step in supporting this research was the
development of game description languages: frameworks that compile
domain-specific code into playable and simulatable game environments, allowing
researchers to generalize their algorithms and approaches across multiple games
without having to manually implement each one. More recently, progress in
reinforcement learning (RL) has been largely driven by advances in hardware
acceleration. Libraries like JAX allow practitioners to take full advantage of
cutting-edge computing hardware, often speeding up training and testing by
orders of magnitude. Here, we present a synthesis of these strands of research:
a domain-specific language for board games which automatically compiles into
hardware-accelerated code. Our framework, Ludax, combines the generality of
game description languages with the speed of modern parallel processing
hardware and is designed to fit neatly into existing deep learning pipelines.
We envision Ludax as a tool to help accelerate games research generally, from
RL to cognitive science, by enabling rapid simulation and providing a flexible
representation scheme. We present a detailed breakdown of Ludax's description
language and technical notes on the compilation process, along with speed
benchmarking and a demonstration of training RL agents. The Ludax framework,
along with implementations of existing board games, is open-source and freely
available.

</details>


### [3] [URSA: The Universal Research and Scientific Agent](https://arxiv.org/abs/2506.22653)
*Michael Grosskopf,Russell Bent,Rahul Somasundaram,Isaac Michaud,Arthur Lui,Nathan Debardeleben,Earl Lawrence*

Main category: cs.AI

TL;DR: URSA是一个科学代理生态系统，旨在通过模块化代理和工具加速研究任务，包括与高级物理模拟代码的耦合，以解决不同复杂性和影响的科学问题。


<details>
  <summary>Details</summary>
Motivation: 利用大型语言模型（LLMs）的复杂推理、规划和执行能力，解决科学研究中的瓶颈问题，推动科学进步。

Method: 设计了URSA系统，包含模块化代理和工具，能够灵活组合以应对不同科学问题。

Result: 展示了URSA的架构及其在解决复杂科学问题中的潜力。

Conclusion: URSA展示了LLMs在科学研究中的革命性潜力，有望加速科学发现。

Abstract: Large language models (LLMs) have moved far beyond their initial form as
simple chatbots, now carrying out complex reasoning, planning, writing, coding,
and research tasks. These skills overlap significantly with those that human
scientists use day-to-day to solve complex problems that drive the cutting edge
of research. Using LLMs in "agentic" AI has the potential to revolutionize
modern science and remove bottlenecks to progress. In this work, we present
URSA, a scientific agent ecosystem for accelerating research tasks. URSA
consists of a set of modular agents and tools, including coupling to advanced
physics simulation codes, that can be combined to address scientific problems
of varied complexity and impact. This work highlights the architecture of URSA,
as well as examples that highlight the potential of the system.

</details>


### [4] [Explanations are a means to an end](https://arxiv.org/abs/2506.22740)
*Jessica Hullman,Ziyang Guo,Berk Ustun*

Main category: cs.AI

TL;DR: 论文主张解释性机器学习方法应基于具体用途设计和评估，提出基于统计决策理论的框架，并展示其在多种场景中的应用。


<details>
  <summary>Details</summary>
Motivation: 现有解释性方法未充分考虑实际用途，需设计更实用的解释框架。

Method: 提出基于统计决策理论的框架，明确解释的具体用途，并通过理论和实证结合评估解释价值。

Result: 展示了框架在临床决策、提供补救措施和调试等场景中的应用，并量化了解释对理想决策者的性能提升。

Conclusion: 解释性方法应针对具体用途设计，结合理论和实证评估，避免因模糊性导致的误用。

Abstract: Modern methods for explainable machine learning are designed to describe how
models map inputs to outputs--without deep consideration of how these
explanations will be used in practice. This paper argues that explanations
should be designed and evaluated with a specific end in mind. We describe how
to formalize this end in a framework based in statistical decision theory. We
show how this functionally-grounded approach can be applied across diverse use
cases, such as clinical decision support, providing recourse, or debugging. We
demonstrate its use to characterize the maximum "boost" in performance on a
particular task that an explanation could provide an idealized decision-maker,
preventing misuse due to ambiguity by forcing researchers to specify concrete
use cases that can be analyzed in light of models of expected explanation use.
We argue that evaluation should meld theoretical and empirical perspectives on
the value of explanation, and contribute definitions that span these
perspectives.

</details>


### [5] [Bridging Ethical Principles and Algorithmic Methods: An Alternative Approach for Assessing Trustworthiness in AI Systems](https://arxiv.org/abs/2506.22774)
*Michael Papademas,Xenia Ziouvelou,Antonis Troumpoukis,Vangelis Karkaletsis*

Main category: cs.AI

TL;DR: 论文提出了一种结合伦理与算法的评估方法，以量化AI系统的可信度，弥补现有指南与技术工具的不足。


<details>
  <summary>Details</summary>
Motivation: AI技术的广泛应用及其潜在风险需要一种既能量化可信度又具备全面视角的评估方法。

Method: 结合PageRank和TrustRank算法，将伦理组件与算法流程结合，建立评估框架。

Result: 该方法能提供定量分析，同时考虑伦理指南，实现AI系统可信度的全面评估。

Conclusion: 提出的方法有效减少了主观性，为AI可信度评估提供了更全面的解决方案。

Abstract: Artificial Intelligence (AI) technology epitomizes the complex challenges
posed by human-made artifacts, particularly those widely integrated into
society and exert significant influence, highlighting potential benefits and
their negative consequences. While other technologies may also pose substantial
risks, AI's pervasive reach makes its societal effects especially profound. The
complexity of AI systems, coupled with their remarkable capabilities, can lead
to a reliance on technologies that operate beyond direct human oversight or
understanding. To mitigate the risks that arise, several theoretical tools and
guidelines have been developed, alongside efforts to create technological tools
aimed at safeguarding Trustworthy AI. The guidelines take a more holistic view
of the issue but fail to provide techniques for quantifying trustworthiness.
Conversely, while technological tools are better at achieving such
quantification, they lack a holistic perspective, focusing instead on specific
aspects of Trustworthy AI. This paper aims to introduce an assessment method
that combines the ethical components of Trustworthy AI with the algorithmic
processes of PageRank and TrustRank. The goal is to establish an assessment
framework that minimizes the subjectivity inherent in the self-assessment
techniques prevalent in the field by introducing algorithmic criteria. The
application of our approach indicates that a holistic assessment of an AI
system's trustworthiness can be achieved by providing quantitative insights
while considering the theoretical content of relevant guidelines.

</details>


### [6] [ReasonBridge: Efficient Reasoning Transfer from Closed to Open-Source Language Models](https://arxiv.org/abs/2506.22865)
*Ziqi Zhong,Xunzhu Tang*

Main category: cs.AI

TL;DR: ReasonBridge通过分层知识蒸馏框架，将闭源模型的推理能力高效迁移到开源模型，显著缩小性能差距。


<details>
  <summary>Details</summary>
Motivation: 闭源与开源模型在复杂推理任务上存在显著性能差距，需高效迁移推理能力。

Method: 采用分层蒸馏、稀疏适配器架构和推理干预机制，使用精心筛选的Reason1K数据集。

Result: 开源模型推理能力提升23%，Qwen2.5-14B在部分任务上超越闭源模型。

Conclusion: ReasonBridge为高效增强开源模型推理能力提供了样本有效的方法。

Abstract: Recent advancements in Large Language Models (LLMs) have revealed a
significant performance gap between closed-source and open-source models,
particularly in tasks requiring complex reasoning and precise instruction
following. This paper introduces ReasonBridge, a methodology that efficiently
transfers reasoning capabilities from powerful closed-source to open-source
models through a novel hierarchical knowledge distillation framework. We
develop a tailored dataset Reason1K with only 1,000 carefully curated reasoning
traces emphasizing difficulty, diversity, and quality. These traces are
filtered from across multiple domains using a structured multi-criteria
selection algorithm. Our transfer learning approach incorporates: (1) a
hierarchical distillation process capturing both strategic abstraction and
tactical implementation patterns, (2) a sparse reasoning-focused adapter
architecture requiring only 0.3% additional trainable parameters, and (3) a
test-time compute scaling mechanism using guided inference interventions.
Comprehensive evaluations demonstrate that ReasonBridge improves reasoning
capabilities in open-source models by up to 23% on benchmark tasks,
significantly narrowing the gap with closed-source models. Notably, the
enhanced Qwen2.5-14B outperforms Claude-Sonnet3.5 on MATH500 and matches its
performance on competition-level AIME problems. Our methodology generalizes
effectively across diverse reasoning domains and model architectures,
establishing a sample-efficient approach to reasoning enhancement for
instruction following.

</details>


### [7] [Agentic Enterprise: AI-Centric User to User-Centric AI](https://arxiv.org/abs/2506.22893)
*Arpit Narechania,Alex Endert,Atanu R Sinha*

Main category: cs.AI

TL;DR: AI在企业的决策生产力中具有潜力，提出了六项原则以推动用户为中心的AI设计。


<details>
  <summary>Details</summary>
Motivation: 探讨AI如何提升企业决策生产力，并指出当前以AI为中心的用户范式在满足企业需求方面的不足。

Method: 提出六项原则，强调用户为中心的AI设计，并建议通过市场机制调整AI平台。

Result: 强调了转向用户为中心的AI的重要性，并提出了六项原则以优化企业决策。

Conclusion: 通过用户为中心的AI设计和市场机制，可以更好地满足企业的决策需求。

Abstract: After a very long winter, the Artificial Intelligence (AI) spring is here.
Or, so it seems over the last three years. AI has the potential to impact many
areas of human life - personal, social, health, education, professional. In
this paper, we take a closer look at the potential of AI for Enterprises, where
decision-making plays a crucial and repeated role across functions, tasks, and
operations. We consider Agents imbued with AI as means to increase
decision-productivity of enterprises. We highlight six tenets for Agentic
success in enterprises, by drawing attention to what the current, AI-Centric
User paradigm misses, in the face of persistent needs of and usefulness for
Enterprise Decision-Making. In underscoring a shift to User-Centric AI, we
offer six tenets and promote market mechanisms for platforms, aligning the
design of AI and its delivery by Agents to the cause of enterprise users.

</details>


### [8] [Hecto: Modular Sparse Experts for Adaptive and Interpretable Reasoning](https://arxiv.org/abs/2506.22919)
*Sanskar Pandey,Ruhaan Chopra,Saad Murtaza Bhat,Ark Abhyudaya*

Main category: cs.AI

TL;DR: Hecto是一种轻量级MoE架构，通过结合GRU和FFNN专家实现异构计算，提升推理任务的性能和可解释性。


<details>
  <summary>Details</summary>
Motivation: 传统MoE模型的静态计算路径限制了表示多样性和效率，Hecto旨在通过异构架构解决这一问题。

Method: Hecto结合GRU专家（时序推理）和FFNN专家（静态抽象），采用稀疏Top-1门控机制。

Result: 在多个任务中，Hecto性能接近或优于同质基线，同时实现专家专业化（时序vs静态）。

Conclusion: Hecto为条件计算提供了新基准，其异构架构在低资源场景下表现出色。

Abstract: Mixture-of-Experts (MoE) models enable conditional computation by routing
inputs to specialized experts, but these experts rely on identical inductive
biases, thus limiting representational diversity. This static computation
pathway is inefficient for inputs that require different types of reasoning and
limits specialization and interpretability. We propose Hecto, a lightweight MoE
architecture that leverages architectural heterogeneity by combining a GRU
expert for temporal reasoning and an FFNN expert for static abstraction under a
sparse Top-1 gating mechanism. Evaluated on three reasoning benchmarks (AG
News, SST-2, HotpotQA) and a regression task (STS-B), Hecto matches or closely
trails homogeneous baselines in performance despite receiving isolated input
representations, while achieving clear expert specialization, with each expert
aligning to distinct reasoning types (temporal vs static). At larger batch
sizes, Hecto exhibits improved performance, benefiting from relaxed
computational constraints that allow its heterogeneous architecture to optimize
more effectively. Ablation results isolate architectural diversity as the
source of Hecto's stability and interpretability across diverse reasoning
tasks. Overall, Hecto establishes itself as a new benchmark for conditional
computation, offering a principled framework for specialized reasoning in
low-resource regimes with its model strength derived from principled
specialization.

</details>


### [9] [Improving Rationality in the Reasoning Process of Language Models through Self-playing Game](https://arxiv.org/abs/2506.22920)
*Pinzheng Wang,Juntao Li,Zecheng Tang,Haijia Gui,Min zhang*

Main category: cs.AI

TL;DR: 论文探讨了通过自我博弈（CDG游戏）提升大语言模型在推理过程中的理性能力，无需人类监督。实验显示该方法显著提升了模型对推理过程的理解。


<details>
  <summary>Details</summary>
Motivation: 尽管大语言模型在数学和编程等任务中表现出推理能力，但它们缺乏对推理过程的真正理解。本文旨在通过自我监督方法提升模型的理性能力。

Method: 设计了Critic-Discernment Game（CDG），其中证明者提供解决方案并接受批评者的挑战，批评者可能提供帮助或误导。证明者需在误导下保持正确答案，并在建设性反馈中修正错误。

Result: 在数学推理、逐步错误检测、自我修正和长链推理任务中，CDG训练显著提升了模型对推理过程的理解能力。

Conclusion: 自我博弈方法（如CDG）可以有效提升大语言模型的理性能力，无需依赖人类或更优模型的监督。

Abstract: Large language models (LLMs) have demonstrated considerable reasoning
abilities in various tasks such as mathematics and coding. However, recent
studies indicate that even the best models lack true comprehension of their
reasoning processes. In this paper, we explore how self-play can enhance the
rationality of models in the reasoning process without supervision from humans
or superior models. We design a Critic-Discernment Game(CDG) in which a prover
first provides a solution to a given problem and is subsequently challenged by
critiques of its solution. These critiques either aim to assist or mislead the
prover. The objective of the prover is to maintain the correct answer when
faced with misleading comments, while correcting errors in response to
constructive feedback. Our experiments on tasks involving mathematical
reasoning, stepwise error detection, self-correction, and long-chain reasoning
demonstrate that CDG training can significantly improve the ability of
well-aligned LLMs to comprehend their reasoning process.

</details>


### [10] [MARBLE: A Hard Benchmark for Multimodal Spatial Reasoning and Planning](https://arxiv.org/abs/2506.22992)
*Yulun Jiang,Yekun Chai,Maria Brbić,Michael Moor*

Main category: cs.AI

TL;DR: MARBLE是一个多模态推理基准测试，旨在评估多模态语言模型（MLLMs）在复杂多模态问题中的逐步推理能力。现有模型表现不佳，表明复杂推理仍是挑战。


<details>
  <summary>Details</summary>
Motivation: 现有推理基准主要关注文本或简单多模态问题，复杂多模态推理能力尚未被充分研究。

Method: MARBLE包含两个高难度任务（M-Portal和M-Cube），要求模型在空间、视觉和物理约束下进行多步规划。

Result: 12个先进模型在MARBLE上表现接近随机水平，部分简化子任务中表现略优，感知仍是瓶颈。

Conclusion: MARBLE揭示了MLLMs的局限性，希望推动下一代模型在多模态推理能力上的发展。

Abstract: The ability to process information from multiple modalities and to reason
through it step-by-step remains a critical challenge in advancing artificial
intelligence. However, existing reasoning benchmarks focus on text-only
reasoning, or employ multimodal questions that can be answered by directly
retrieving information from a non-text modality. Thus, complex reasoning
remains poorly understood in multimodal domains. Here, we present MARBLE, a
challenging multimodal reasoning benchmark that is designed to scrutinize
multimodal language models (MLLMs) in their ability to carefully reason
step-by-step through complex multimodal problems and environments. MARBLE is
composed of two highly challenging tasks, M-Portal and M-Cube, that require the
crafting and understanding of multistep plans under spatial, visual, and
physical constraints. We find that current MLLMs perform poorly on MARBLE --
all the 12 advanced models obtain near-random performance on M-Portal and 0%
accuracy on M-Cube. Only in simplified subtasks some models outperform the
random baseline, indicating that complex reasoning is still a challenge for
existing MLLMs. Moreover, we show that perception remains a bottleneck, where
MLLMs occasionally fail to extract information from the visual inputs. By
shedding a light on the limitations of MLLMs, we hope that MARBLE will spur the
development of the next generation of models with the ability to reason and
plan across many, multimodal reasoning steps.

</details>


### [11] [AURA: Agent for Understanding, Reasoning, and Automated Tool Use in Voice-Driven Tasks](https://arxiv.org/abs/2506.23049)
*Leander Melroy Maben,Gayathri Ganesh Lakshmy,Srijith Radhakrishnan,Siddhant Arora,Shinji Watanabe*

Main category: cs.AI

TL;DR: AURA是一个开源的语音原生助手，支持多轮对话和工具调用，结合了ASR、TTS和LLM技术，在复杂任务中表现优异。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏开源系统支持语音到语音的多轮对话和工具集成，AURA填补了这一空白。

Method: AURA采用模块化设计，结合ASR、TTS和LLM技术，支持动态工具调用和多轮对话。

Result: 在VoiceBench上，AURA表现优异，OpenBookQA得分92.75%，接近GPT-4o，人类评估任务成功率达90%。

Conclusion: AURA是首个开源语音助手，展示了在复杂任务中的高效性和实用性。

Abstract: Despite advances in language and speech technologies, no open-source system
enables full speech-to-speech, multi-turn dialogue with integrated tool use and
agentic reasoning. We introduce AURA (Agent for Understanding, Reasoning, and
Automated Tool Use), the first open-source, speech-native assistant capable of
completing complex, goal-driven tasks through dynamic tool invocation and
multi-turn conversation. AURA combines open-weight ASR, TTS, and LLMs in a
cascaded pipeline and supports tools such as calendar booking, contact lookup,
web search, and email. Its modular design allows easy integration of new tools
using natural language prompts and action classes. On VoiceBench, AURA scores
92.75% on OpenBookQA-outperforming all open-weight systems and nearing
GPT-4o-and 4.39 on AlpacaEval, competitive with other open-weight systems.
Human evaluation shows 90% task success on complex, multi-turn speech tasks.

</details>


### [12] [AI's Euclid's Elements Moment: From Language Models to Computable Thought](https://arxiv.org/abs/2506.23080)
*Xinmin Fang,Lingfeng Tao,Zhengxiong Li*

Main category: cs.AI

TL;DR: 本文提出了一个五阶段进化框架，用于理解人工智能的发展，认为其轨迹与人类认知技术的历史进展相似。


<details>
  <summary>Details</summary>
Motivation: 通过类比人类认知技术的发展，为AI的过去和未来提供一个系统的、跨学科的模型。

Method: 提出“认知几何”框架，将AI发展分为五个阶段，每个阶段以表示和推理能力的革命性变化为标志。

Result: 展示了AI发展的非线性反馈循环，并预测了未来的“元语言时刻”和“数学符号时刻”。

Conclusion: 为未来AI研究提供了理论基础，并为开发下一代智能系统提供了具体策略。

Abstract: This paper presents a comprehensive five-stage evolutionary framework for
understanding the development of artificial intelligence, arguing that its
trajectory mirrors the historical progression of human cognitive technologies.
We posit that AI is advancing through distinct epochs, each defined by a
revolutionary shift in its capacity for representation and reasoning, analogous
to the inventions of cuneiform, the alphabet, grammar and logic, mathematical
calculus, and formal logical systems. This "Geometry of Cognition" framework
moves beyond mere metaphor to provide a systematic, cross-disciplinary model
that not only explains AI's past architectural shifts-from expert systems to
Transformers-but also charts a concrete and prescriptive path forward.
Crucially, we demonstrate that this evolution is not merely linear but
reflexive: as AI advances through these stages, the tools and insights it
develops create a feedback loop that fundamentally reshapes its own underlying
architecture. We are currently transitioning into a "Metalinguistic Moment,"
characterized by the emergence of self-reflective capabilities like
Chain-of-Thought prompting and Constitutional AI. The subsequent stages, the
"Mathematical Symbolism Moment" and the "Formal Logic System Moment," will be
defined by the development of a computable calculus of thought, likely through
neuro-symbolic architectures and program synthesis, culminating in provably
aligned and reliable AI that reconstructs its own foundational representations.
This work serves as the methodological capstone to our trilogy, which
previously explored the economic drivers ("why") and cognitive nature ("what")
of AI. Here, we address the "how," providing a theoretical foundation for
future research and offering concrete, actionable strategies for startups and
developers aiming to build the next generation of intelligent systems.

</details>


### [13] [Can Large Language Models Capture Human Risk Preferences? A Cross-Cultural Study](https://arxiv.org/abs/2506.23107)
*Bing Song,Jianing Liu,Sisi Jian,Chenyang Wu,Vinayak Dixit*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在模拟风险决策行为中的表现，发现模型比人类更规避风险，且中文提示下的预测偏差更大。


<details>
  <summary>Details</summary>
Motivation: 随着LLMs应用的扩展，其模拟复杂决策行为（如风险决策）的可靠性引发关注，需验证其与人类行为的匹配度。

Method: 通过彩票任务比较ChatGPT 4o和o1-mini的预测与人类实际选择，使用CRRA框架分析风险偏好，并考察多语言数据的影响。

Result: 模型比人类更规避风险，o1-mini更接近人类行为；中文提示下的预测偏差大于英文。

Conclusion: LLMs在模拟人类风险行为方面有潜力，但在语言和文化背景下的表现仍有局限。

Abstract: Large language models (LLMs) have made significant strides, extending their
applications to dialogue systems, automated content creation, and
domain-specific advisory tasks. However, as their use grows, concerns have
emerged regarding their reliability in simulating complex decision-making
behavior, such as risky decision-making, where a single choice can lead to
multiple outcomes. This study investigates the ability of LLMs to simulate
risky decision-making scenarios. We compare model-generated decisions with
actual human responses in a series of lottery-based tasks, using transportation
stated preference survey data from participants in Sydney, Dhaka, Hong Kong,
and Nanjing. Demographic inputs were provided to two LLMs -- ChatGPT 4o and
ChatGPT o1-mini -- which were tasked with predicting individual choices. Risk
preferences were analyzed using the Constant Relative Risk Aversion (CRRA)
framework. Results show that both models exhibit more risk-averse behavior than
human participants, with o1-mini aligning more closely with observed human
decisions. Further analysis of multilingual data from Nanjing and Hong Kong
indicates that model predictions in Chinese deviate more from actual responses
compared to English, suggesting that prompt language may influence simulation
performance. These findings highlight both the promise and the current
limitations of LLMs in replicating human-like risk behavior, particularly in
linguistic and cultural settings.

</details>


### [14] [The Societal Impact of Foundation Models: Advancing Evidence-based AI Policy](https://arxiv.org/abs/2506.23123)
*Rishi Bommasani*

Main category: cs.AI

TL;DR: 论文探讨了人工智能基础模型的技术与社会共同演化，提出了概念框架、实证见解和行动建议，以推动更好的AI治理。


<details>
  <summary>Details</summary>
Motivation: 基础模型虽具潜力，但因其不透明性和潜在危害引发社会担忧，需研究如何实现技术与社会的协调发展。

Method: 围绕三个主题展开：概念框架（能力、风险、供应链）、实证研究（模型评估与组织透明度）、行动建议（基于证据的AI政策）。

Result: 通过科学基础和政策研究接口，为AI治理提供了更好的社会成果路径。

Conclusion: 论文为AI时代的社会治理奠定了科学基础，推动了技术与社会的良性互动。

Abstract: Artificial intelligence is humanity's most promising technology because of
the remarkable capabilities offered by foundation models. Yet, the same
technology brings confusion and consternation: foundation models are poorly
understood and they may precipitate a wide array of harms. This dissertation
explains how technology and society coevolve in the age of AI, organized around
three themes. First, the conceptual framing: the capabilities, risks, and the
supply chain that grounds foundation models in the broader economy. Second, the
empirical insights that enrich the conceptual foundations: transparency created
via evaluations at the model level and indexes at the organization level.
Finally, the transition from understanding to action: superior understanding of
the societal impact of foundation models advances evidence-based AI policy.
View together, this dissertation makes inroads into achieving better societal
outcomes in the age of AI by building the scientific foundations and
research-policy interface required for better AI governance.

</details>


### [15] [Are Large Language Models Capable of Deep Relational Reasoning? Insights from DeepSeek-R1 and Benchmark Comparisons](https://arxiv.org/abs/2506.23128)
*Chi Chiu So,Yueyue Sun,Jun-Min Wang,Siu Pang Yung,Anthony Wai Keung Loh,Chun Pong Chau*

Main category: cs.AI

TL;DR: 论文评估了三种大型语言模型（DeepSeek-R1、DeepSeek-V3和GPT-4o）在深度关系推理任务中的表现，发现DeepSeek-R1表现最佳，但所有模型在复杂任务中均存在局限性。


<details>
  <summary>Details</summary>
Motivation: 研究大型语言模型在深度关系推理任务中的能力，以评估其逻辑推理和关系推断的潜力。

Method: 通过设计家族树和通用图推理的基准任务，比较三种模型的性能，并分析其推理策略。

Result: DeepSeek-R1在多项任务中表现最优，但所有模型在复杂任务中表现不佳，主要受限于输出结构和令牌长度。

Conclusion: 研究揭示了大型语言模型在深度推理中的潜力与局限，并提出了未来研究方向，如多模态推理和系统性错误分析。

Abstract: How far are Large Language Models (LLMs) in performing deep relational
reasoning? In this paper, we evaluate and compare the reasoning capabilities of
three cutting-edge LLMs, namely, DeepSeek-R1, DeepSeek-V3 and GPT-4o, through a
suite of carefully designed benchmark tasks in family tree and general graph
reasoning. Our experiments reveal that DeepSeek-R1 consistently achieves the
highest F1-scores across multiple tasks and problem sizes, demonstrating strong
aptitude in logical deduction and relational inference. However, all evaluated
models, including DeepSeek-R1, struggle significantly as problem complexity
increases, largely due to token length limitations and incomplete output
structures. A detailed analysis of DeepSeek-R1's long Chain-of-Thought
responses uncovers its unique planning and verification strategies, but also
highlights instances of incoherent or incomplete reasoning, calling attention
to the need for deeper scrutiny into LLMs' internal inference dynamics. We
further discuss key directions for future work, including the role of
multimodal reasoning and the systematic examination of reasoning failures. Our
findings provide both empirical insights and theoretical implications for
advancing LLMs' reasoning abilities, particularly in tasks that demand
structured, multi-step logical inference. Our code repository will be publicly
available at https://github.com/kelvinhkcs/Deep-Relational-Reasoning.

</details>


### [16] [Context-Driven Knowledge Graph Completion with Semantic-Aware Relational Message Passing](https://arxiv.org/abs/2506.23141)
*Siyuan Li,Ruitong Liu,Yan Wen,Te Sun*

Main category: cs.AI

TL;DR: 提出了一种语义感知的关系消息传递框架，通过Top-K邻居选择策略和多头注意力聚合器，有效减少知识图谱完成中的噪声和信息稀释问题。


<details>
  <summary>Details</summary>
Motivation: 传统节点消息传递机制在知识图谱中会引入噪声和信息稀释，需要更精准的语义上下文捕捉方法。

Method: 采用语义感知的Top-K邻居选择策略和多头注意力聚合器，选择并融合最相关的边信息。

Result: 在多个基准测试中表现优于现有方法。

Conclusion: 该方法能更准确地捕捉和传播与链接预测任务相关的上下文信息，减少无关信息的干扰。

Abstract: Semantic context surrounding a triplet $(h, r, t)$ is crucial for Knowledge
Graph Completion (KGC), providing vital cues for prediction. However,
traditional node-based message passing mechanisms, when applied to knowledge
graphs, often introduce noise and suffer from information dilution or
over-smoothing by indiscriminately aggregating information from all neighboring
edges. To address this challenge, we propose a semantic-aware relational
message passing. A core innovation of this framework is the introduction of a
\textbf{semantic-aware Top-K neighbor selection strategy}. Specifically, this
strategy first evaluates the semantic relevance between a central node and its
incident edges within a shared latent space, selecting only the Top-K most
pertinent ones. Subsequently, information from these selected edges is
effectively fused with the central node's own representation using a
\textbf{multi-head attention aggregator} to generate a semantically focused
node message. In this manner, our model not only leverages the structure and
features of edges within the knowledge graph but also more accurately captures
and propagates the contextual information most relevant to the specific link
prediction task, thereby effectively mitigating interference from irrelevant
information. Extensive experiments demonstrate that our method achieves
superior performance compared to existing approaches on several established
benchmarks.

</details>


### [17] [Rises for Measuring Local Distributivity in Lattices](https://arxiv.org/abs/2506.23168)
*Mohammad Abdulla,Tobias Hille,Dominik Dürrschnabel,Gerd Stumme*

Main category: cs.AI

TL;DR: 本文提出了一种通过“rises”来量化概念格中分配性的方法，并证明格是分配的当且仅当不存在非单位rises。


<details>
  <summary>Details</summary>
Motivation: 在形式概念分析中，格的高分配性缺乏标准化度量，需要一种量化方法。

Method: 引入rises概念，分析覆盖概念中属性或对象数量的变化，并与经典分配性概念关联。

Result: 现实数据的概念格高度满足join-分配性，但较少满足meet-分配性。

Conclusion: rises是量化分配性的有效工具，揭示了现实数据中分配性的分布特点。

Abstract: Distributivity is a well-established and extensively studied notion in
lattice theory. In the context of data analysis, particularly within Formal
Concept Analysis (FCA), lattices are often observed to exhibit a high degree of
distributivity. However, no standardized measure exists to quantify this
property. In this paper, we introduce the notion of rises in (concept) lattices
as a means to assess distributivity. Rises capture how the number of attributes
or objects in covering concepts change within the concept lattice. We show that
a lattice is distributive if and only if no non-unit rises occur. Furthermore,
we relate rises to the classical notion of meet- and join distributivity. We
observe that concept lattices from real-world data are to a high degree
join-distributive, but much less meet-distributive. We additionally study how
join-distributivity manifests on the level of ordered sets.

</details>


### [18] [FinStat2SQL: A Text2SQL Pipeline for Financial Statement Analysis](https://arxiv.org/abs/2506.23273)
*Quang Hung Nguyen,Phuong Anh Trinh,Phan Quoc Hung Mai,Tuan Phong Trinh*

Main category: cs.AI

TL;DR: FinStat2SQL是一个轻量级的text2sql管道，专为金融报表的自然语言查询设计，结合大小语言模型，在越南企业金融分析中表现出色。


<details>
  <summary>Details</summary>
Motivation: 金融领域的数据库设计和报表布局因实体和国家而异，使得text2sql更具挑战性。

Method: 采用多代理设置，结合大小语言模型进行实体提取、SQL生成和自我校正，并构建特定领域数据库。

Result: 微调的7B模型在消费硬件上达到61.33%准确率，响应时间低于4秒，优于GPT-4o-mini。

Conclusion: FinStat2SQL为越南企业提供了可扩展、经济高效的金融分析解决方案。

Abstract: Despite the advancements of large language models, text2sql still faces many
challenges, particularly with complex and domain-specific queries. In finance,
database designs and financial reporting layouts vary widely between financial
entities and countries, making text2sql even more challenging. We present
FinStat2SQL, a lightweight text2sql pipeline enabling natural language queries
over financial statements. Tailored to local standards like VAS, it combines
large and small language models in a multi-agent setup for entity extraction,
SQL generation, and self-correction. We build a domain-specific database and
evaluate models on a synthetic QA dataset. A fine-tuned 7B model achieves
61.33\% accuracy with sub-4-second response times on consumer hardware,
outperforming GPT-4o-mini. FinStat2SQL offers a scalable, cost-efficient
solution for financial analysis, making AI-powered querying accessible to
Vietnamese enterprises.

</details>


### [19] [Corrupted by Reasoning: Reasoning Language Models Become Free-Riders in Public Goods Games](https://arxiv.org/abs/2506.23276)
*David Guzman Piedrahita,Yongjin Yang,Mrinmaya Sachan,Giorgia Ramponi,Bernhard Schölkopf,Zhijing Jin*

Main category: cs.AI

TL;DR: 研究探讨了大型语言模型（LLMs）在多智能体系统中如何平衡自利与集体利益，重点关注成本高昂的惩罚机制。


<details>
  <summary>Details</summary>
Motivation: 理解LLMs作为自主智能体的合作与社会机制，以确保其对齐性、鲁棒性和安全部署。

Method: 通过改编行为经济学中的公共物品游戏，观察不同LLMs在重复互动中的行为模式。

Result: 发现四种行为模式：持续高合作、波动合作、合作逐渐下降和固定策略。推理能力强的LLMs合作表现较差，传统LLMs反而更稳定。

Conclusion: 当前提升LLMs推理能力的方法未必促进合作，为需要持续协作的环境提供了重要启示。

Abstract: As large language models (LLMs) are increasingly deployed as autonomous
agents, understanding their cooperation and social mechanisms is becoming
increasingly important. In particular, how LLMs balance self-interest and
collective well-being is a critical challenge for ensuring alignment,
robustness, and safe deployment. In this paper, we examine the challenge of
costly sanctioning in multi-agent LLM systems, where an agent must decide
whether to invest its own resources to incentivize cooperation or penalize
defection. To study this, we adapt a public goods game with institutional
choice from behavioral economics, allowing us to observe how different LLMs
navigate social dilemmas over repeated interactions. Our analysis reveals four
distinct behavioral patterns among models: some consistently establish and
sustain high levels of cooperation, others fluctuate between engagement and
disengagement, some gradually decline in cooperative behavior over time, and
others rigidly follow fixed strategies regardless of outcomes. Surprisingly, we
find that reasoning LLMs, such as the o1 series, struggle significantly with
cooperation, whereas some traditional LLMs consistently achieve high levels of
cooperation. These findings suggest that the current approach to improving
LLMs, which focuses on enhancing their reasoning capabilities, does not
necessarily lead to cooperation, providing valuable insights for deploying LLM
agents in environments that require sustained collaboration. Our code is
available at https://github.com/davidguzmanp/SanctSim

</details>


### [20] [GATSim: Urban Mobility Simulation with Generative Agents](https://arxiv.org/abs/2506.23306)
*Qi Liu,Can Li,Wanjing Ma*

Main category: cs.AI

TL;DR: GATSim利用大型语言模型和AI代理技术，提出了一种新型城市交通模拟框架，生成具有丰富行为特征的代理，能够适应和学习，模拟人类旅行决策的复杂性。


<details>
  <summary>Details</summary>
Motivation: 传统基于规则的交通模拟无法捕捉人类旅行决策的复杂性和多样性，而大型语言模型和AI代理技术为改进提供了机会。

Method: 提出GATSim框架，结合城市交通基础模型、代理认知系统和交通模拟环境，生成具有多样化属性和学习能力的代理。

Result: 实验表明，GATSim代理能产生可信的旅行行为，并在宏观交通模式上与人类标注者表现相当。

Conclusion: GATSim通过生成代理实现了更真实的城市交通模拟，展示了AI技术在交通领域的潜力。

Abstract: Traditional agent-based urban mobility simulations rely on rigid rule-based
systems that fail to capture the complexity, adaptability, and behavioral
diversity characteristic of human travel decision-making. Recent advances in
large language models and AI agent technology offer opportunities to create
agents with reasoning capabilities, persistent memory, and adaptive learning
mechanisms. We propose GATSim (Generative-Agent Transport Simulation), a novel
framework that leverages these advances to create generative agents with rich
behavioral characteristics for urban mobility simulation. Unlike conventional
approaches, GATSim agents possess diverse socioeconomic attributes, individual
lifestyles, and evolving preferences that shape their mobility decisions
through psychologically-informed memory systems, tool usage capabilities, and
lifelong learning mechanisms. The main contributions of this study include: (1)
a comprehensive architecture combining an urban mobility foundation model with
agent cognitive systems and transport simulation environment, (2) a fully
functional prototype implementation, and (3) systematic validation
demonstrating that generative agents produce believable travel behaviors.
Through designed reflection processes, generative agents in this study can
transform specific travel experiences into generalized insights, enabling
realistic behavioral adaptation over time with specialized mechanisms for
activity planning and real-time reactive behaviors tailored to urban mobility
contexts. Experiments show that generative agents perform competitively with
human annotators in mobility scenarios while naturally producing macroscopic
traffic evolution patterns. The code for the prototype system is shared at
https://github.com/qiliuchn/gatsim.

</details>


### [21] [The Confidence Paradox: Can LLM Know When It's Wrong](https://arxiv.org/abs/2506.23464)
*Sahil Tripathi,Md Tabrez Nafis,Imran Hussain,Jiechao Gao*

Main category: cs.AI

TL;DR: HonestVQA是一个自监督的诚实校准框架，旨在解决DocVQA系统中的伦理问题，通过量化不确定性、对齐模型置信度和实际正确性，并引入新的评估指标。


<details>
  <summary>Details</summary>
Motivation: 现有DocVQA系统在伦理透明度上表现不足，模型置信度与实际知识不匹配，存在伦理风险。

Method: HonestVQA通过自监督学习量化不确定性，使用加权损失函数对齐置信度和正确性，并通过对比学习强化伦理响应行为。

Result: HonestVQA在多个数据集上提升了准确率和F1值，降低了过度自信，表现出良好的泛化能力。

Conclusion: HonestVQA有效解决了DocVQA系统中的伦理问题，提升了性能和伦理对齐。

Abstract: Document Visual Question Answering (DocVQA) systems are increasingly deployed
in real world applications, yet they remain ethically opaque-often producing
overconfident answers to ambiguous questions or failing to communicate
uncertainty in a trustworthy manner. This misalignment between model confidence
and actual knowledge poses significant risks, particularly in domains requiring
ethical accountability. Existing approaches such as LayoutLMv3, UDOP, and DONUT
have advanced SOTA performance by focusing on architectural sophistication and
accuracy; however, they fall short in ethical responsiveness.
  To address these limitations, we introduce HonestVQA, a self-supervised
honesty calibration framework for ethically aligned DocVQA. Our model-agnostic
method quantifies uncertainty to identify knowledge gaps, aligns model
confidence with actual correctness using weighted loss functions, and enforces
ethical response behavior via contrastive learning. We further introduce two
principled evaluation metrics--Honesty Score (H-Score) and Ethical Confidence
Index (ECI)--to benchmark alignment between confidence, accuracy, and ethical
communication. Empirically, HonestVQA improves DocVQA accuracy by up to 4.3%
and F1 by 4.3% across SpDocVQA, InfographicsVQA, and SROIE datasets. It reduces
overconfidence, lowering H-Score and ECI by 0.072 and 0.078, respectively. In
cross domain evaluation, it achieves up to 78.9% accuracy and 76.1% F1-score,
demonstrating strong generalization. Ablation shows a 3.8% drop in accuracy
without alignment or contrastive loss.

</details>


### [22] [Data Augmentation for Cognitive Behavioral Therapy: Leveraging ERNIE Language Models using Artificial Intelligence](https://arxiv.org/abs/2506.23503)
*Bosubabu Sambana,Kondreddygari Archana,Suram Indhra Sena Reddy,Shaik Meethaigar Jameer Basha,Shaik Karishma*

Main category: cs.AI

TL;DR: 论文提出了一种基于CBT框架的系统，利用BERT、RoBERTa等模型分析社交媒体数据中的负面情绪和认知扭曲，并预测潜在心理健康问题。


<details>
  <summary>Details</summary>
Motivation: 当前缺乏有效方法分析社交媒体中的认知路径，这对心理治疗师提供及时干预至关重要。

Method: 结合CBT框架，使用BERT、RoBERTa进行情感分析，T5、PEGASUS进行文本摘要，mT5进行多语言翻译，以识别负面情绪和认知扭曲。

Result: 系统不仅能识别负面思维，还能预测潜在心理健康问题（如恐惧症、饮食障碍），提供更全面的干预策略。

Conclusion: 该系统为心理治疗师提供了早期检测和治疗心理问题的强大工具。

Abstract: Cognitive Behavioral Therapy (CBT) is a proven approach for addressing the
irrational thought patterns associated with mental health disorders, but its
effectiveness relies on accurately identifying cognitive pathways to provide
targeted treatment. In today's digital age, individuals often express negative
emotions on social media, where they may reveal cognitive distortions, and in
severe cases, exhibit suicidal tendencies. However, there is a significant gap
in methodologies designed to analyze these cognitive pathways, which could be
critical for psychotherapists aiming to deliver timely and effective
interventions in online environments. Cognitive Behavioral Therapy (CBT)
framework leveraging acceptance, commitment and data augmentation to categorize
and address both textual and visual content as positive or negative.
Specifically, the system employs BERT, RoBERTa for Sentiment Analysis and T5,
PEGASUS for Text Summarization, mT5 for Text Translation in Multiple Languages
focusing on detecting negative emotions and cognitive distortions within social
media data. While existing models are primarily designed to identify negative
thoughts, the proposed system goes beyond this by predicting additional
negative side effects and other potential mental health disorders likes
Phobias, Eating Disorders. This enhancement allows for a more comprehensive
understanding and intervention strategy, offering psychotherapists a powerful
tool for early detection and treatment of various psychological issues.

</details>


### [23] [Hybrid Approach for Electricity Price Forecasting using AlexNet and LSTM](https://arxiv.org/abs/2506.23504)
*Bosubabu Sambana,Kotamsetty Geethika Devi,Bandi Rajeswara Reddy,Galeti Mohammad Hussain,Gownivalla Siddartha*

Main category: cs.AI

TL;DR: 本文提出了一种结合AlexNet和LSTM的混合模型，用于提高电力价格预测的准确性。


<details>
  <summary>Details</summary>
Motivation: 传统方法（如RNN和ANN）在处理外汇时间序列数据时效果不佳，且仅关注需求和价格，导致分析不足。

Method: 采用AlexNet进行特征提取，结合LSTM学习序列模式，并引入外部变量（如需求、温度、阳光和降雨）进行预测。

Result: 混合模型的预测准确率达到97.08%，优于单独的RNN（96.64%）和ANN（96.63%）模型。

Conclusion: 该混合模型显著提升了电力价格预测的准确性，优于传统方法。

Abstract: The recent development of advanced machine learning methods for hybrid models
has greatly addressed the need for the correct prediction of electrical prices.
This method combines AlexNet and LSTM algorithms, which are used to introduce a
new model with higher accuracy in price forecasting. Despite RNN and ANN being
effective, they often fail to deal with forex time sequence data. The
traditional methods do not accurately forecast the prices. These traditional
methods only focus on demand and price which leads to insufficient analysis of
data. To address this issue, using the hybrid approach, which focuses on
external variables that also effect the predicted prices. Nevertheless, due to
AlexNet's excellent feature extraction and LSTM's learning sequential patterns,
the prediction accuracy is vastly increased. The model is built on the past
data, which has been supplied with the most significant elements like demand,
temperature, sunlight, and rain. For example, the model applies methods, such
as minimum-maximum scaling and a time window, to predict the electricity prices
of the future. The results show that this hybrid model is good than the
standalone ones in terms of accuracy. Although we got our accuracy rating of
97.08, it shows higher accompaniments than remaining models RNN and ANN with
accuracies of 96.64 and 96.63 respectively.

</details>


### [24] [Assessing GPTZero's Accuracy in Identifying AI vs. Human-Written Essays](https://arxiv.org/abs/2506.23517)
*Selin Dik,Osman Erdem,Mehmet Dik*

Main category: cs.AI

TL;DR: 研究评估了GPTZero检测AI生成文本的可靠性，发现其对AI文本检测准确率高，但对人类文本存在误判。


<details>
  <summary>Details</summary>
Motivation: 随着学生使用AI工具增多，教师依赖AI检测工具（如GPTZero）检测AI生成文本，但其可靠性尚不明确。

Method: 研究使用不同长度的随机提交论文（短、中、长），通过GPTZero检测AI生成比例和置信度。数据集包括28篇AI生成和50篇人类撰写论文。

Result: AI生成文本检测准确率高（91-100%），但人类文本存在误判。

Conclusion: GPTZero对纯AI文本检测有效，但对人类文本区分能力有限，教师需谨慎使用。

Abstract: As the use of AI tools by students has become more prevalent, instructors
have started using AI detection tools like GPTZero and QuillBot to detect AI
written text. However, the reliability of these detectors remains uncertain. In
our study, we focused mostly on the success rate of GPTZero, the most-used AI
detector, in identifying AI-generated texts based on different lengths of
randomly submitted essays: short (40-100 word count), medium (100-350 word
count), and long (350-800 word count). We gathered a data set consisting of
twenty-eight AI-generated papers and fifty human-written papers. With this
randomized essay data, papers were individually plugged into GPTZero and
measured for percentage of AI generation and confidence. A vast majority of the
AI-generated papers were detected accurately (ranging from 91-100% AI believed
generation), while the human generated essays fluctuated; there were a handful
of false positives. These findings suggest that although GPTZero is effective
at detecting purely AI-generated content, its reliability in distinguishing
human-authored texts is limited. Educators should therefore exercise caution
when relying solely on AI detection tools.

</details>


### [25] [ChemActor: Enhancing Automated Extraction of Chemical Synthesis Actions with LLM-Generated Data](https://arxiv.org/abs/2506.23520)
*Yu Zhang,Ruijie Yu,Jidong Tian,Feng Zhu,Jiapeng Liu,Xiaokang Yang,Yaohui Jin,Yanyan Xu*

Main category: cs.AI

TL;DR: ChemActor是一个基于大型语言模型（LLM）的化学执行器，用于将非结构化的化学实验步骤转换为结构化的动作序列，并通过LLM生成的数据框架解决标注数据不足和质量低的问题。


<details>
  <summary>Details</summary>
Motivation: 随着机器人合成在有机化学中的兴趣增加，从文献中自动提取化学步骤变得至关重要，但由于化学语言的模糊性和人工标注的高成本，这一任务仍然具有挑战性。

Method: 提出ChemActor，一个完全微调的LLM，结合数据选择模块和多轮LLM循环评估指标，从单分子输入生成机器可执行的动作。

Result: 在反应到描述（R2D）和描述到动作（D2A）任务中，ChemActor表现出色，比基线模型性能提升10%。

Conclusion: ChemActor通过LLM生成的数据框架，显著提升了化学实验步骤的自动化提取性能，为化学合成自动化提供了有效工具。

Abstract: With the increasing interest in robotic synthesis in the context of organic
chemistry, the automated extraction of chemical procedures from literature is
critical. However, this task remains challenging due to the inherent ambiguity
of chemical language and the high cost of human annotation required for
developing reliable computer-aided extraction protocols. Here, we present
ChemActor, a fully fine-tuned large language model (LLM), as a chemical
executor to convert between unstructured experimental procedures and structured
action sequences. We propose a sequential LLM-generated data framework to
address the challenges of insufficient and low-quality annotated data. This
framework integrates a data selection module that selects data based on
distribution divergence, with a general-purpose LLM, to generate
machine-executable actions from a single molecule input. Additionally, we
introduce a novel multi-round LLMs circle review metric, which reflects the
model's advanced understanding of chemical experimental procedures. Extensive
experiments on reaction-to-description (R2D) and description-to-action (D2A)
tasks demonstrate that ChemActor, augmented by LLM-generated data, achieves
state-of-the-art performance, outperforming the baseline model by 10%. The code
is available at: https://github.com/Zhanghahah/ChemActor.

</details>


### [26] [CooT: Learning to Coordinate In-Context with Coordination Transformers](https://arxiv.org/abs/2506.23549)
*Huai-Chih Wang,Hsiang-Chun Chuang,Hsi-Chun Cheng,Dai-Jie Wu,Shao-Hua Sun*

Main category: cs.AI

TL;DR: 提出了一种名为Coordination Transformers（CooT）的新框架，通过利用交互历史快速适应未见过的合作伙伴，显著提升了多智能体系统中的协调能力。


<details>
  <summary>Details</summary>
Motivation: 解决现有方法（如自博弈和基于种群的方法）在动态和不确定环境中协调能力不足的问题，这些方法要么泛化能力差，要么需要大量训练。

Method: 提出了CooT框架，通过预测与观察到的合作伙伴行为一致的动作，快速适应新伙伴行为，无需显式监督或微调。

Result: 在Overcooked基准测试中，CooT显著优于基线方法，人类评估也证实其是最有效的协作伙伴。

Conclusion: CooT在多智能体场景中表现出强大的鲁棒性、灵活性和对上下文的敏感性，是一种高效的协调方法。

Abstract: Effective coordination among artificial agents in dynamic and uncertain
environments remains a significant challenge in multi-agent systems. Existing
approaches, such as self-play and population-based methods, either generalize
poorly to unseen partners or require extensive training. To overcome these
limitations, we propose Coordination Transformers (CooT), a novel in-context
coordination framework that uses recent interaction histories to adapt to
unseen partners rapidly. Unlike previous approaches that primarily aim to
increase the diversity of training partners, CooT explicitly focuses on
adapting to new partner behaviors by predicting actions aligned with observed
partner interactions. Trained on interaction trajectories collected from
diverse pairs of agents with complementary behaviors, CooT quickly learns
effective coordination strategies without explicit supervision or fine-tuning.
Evaluations on the Overcooked benchmark demonstrate that CooT significantly
outperforms baseline methods in coordination tasks involving previously unseen
partners. Human evaluations further confirm CooT as the most effective
collaborative partner, while extensive ablations highlight its robustness,
flexibility, and sensitivity to context in multi-agent scenarios.

</details>


### [27] [MMReason: An Open-Ended Multi-Modal Multi-Step Reasoning Benchmark for MLLMs Toward AGI](https://arxiv.org/abs/2506.23563)
*Huanjin Yao,Jiaxing Huang,Yawen Qiu,Michael K. Chen,Wenzheng Liu,Wei Zhang,Wenjie Zeng,Xikun Zhang,Jingyi Zhang,Yuxin Song,Wenhao Wu,Dacheng Tao*

Main category: cs.AI

TL;DR: MMReason是一个新的基准测试，旨在全面评估多模态大语言模型的长链推理能力，通过多样、开放和具有挑战性的问题填补现有基准的不足。


<details>
  <summary>Details</summary>
Motivation: 现有基准在评估长链推理能力时存在不足，包括缺乏难度和多样性、易受猜测和记忆影响，以及对中间推理步骤评估不足。

Method: MMReason通过从多个学科和难度级别筛选问题，采用开放格式和多模型投票技术消除捷径，并设计基于参考的三元评分机制评估推理步骤。

Result: MMReason对主流多模态大语言模型进行了基准测试，并深入分析了它们的推理能力。

Conclusion: MMReason有望成为推动多模态大语言模型推理研究的重要资源。

Abstract: Reasoning plays a crucial role in advancing Multimodal Large Language Models
(MLLMs) toward Artificial General Intelligence. However, existing MLLM
benchmarks often fall short in precisely and comprehensively evaluating
long-chain reasoning abilities from three key aspects: (1) lack of difficulty
and diversity, (2) susceptibility to guessability and memorization, (3)
inadequate assessment of intermediate reasoning steps. To fill this gap, we
introduce MMReason, a new benchmark designed to precisely and comprehensively
evaluate MLLM long-chain reasoning capability with diverse, open-ended,
challenging questions. First, we curate challenging questions requiring
multi-step reasoning from various fields (i.e., 6 disciplines) and multiple
difficulty levels (i.e., from pre-university to university, and from
foundational to competition tiers). Second, these questions are reformulated
into an open-ended format and filtered using a multi-model voting technique to
eliminate shortcut cases related to guessing and memorization, ensuring robust
reasoning evaluations. Third, we annotate the questions with detailed
step-by-step solutions, and design a reference-based ternary scoring mechanism
to reliably assess intermediate reasoning steps. With MMReason, we benchmark
popular leading MLLMs and provide an in-depth analysis of their reasoning
capabilities. We hope MMReason will serve as a valuable resource for advancing
MLLM reasoning research. Code will be available at
https://github.com/HJYao00/MMReason.

</details>


### [28] [Evaluating Multi-Agent Defences Against Jailbreaking Attacks on Large Language Models](https://arxiv.org/abs/2506.23576)
*Maria Carolina Cornelia Wit,Jun Pang*

Main category: cs.AI

TL;DR: 多智能体LLM系统可增强对越狱攻击的防御能力，但存在误报和计算开销的权衡。


<details>
  <summary>Details</summary>
Motivation: 研究多智能体LLM系统作为防御越狱攻击的有效性。

Method: 评估三种越狱策略，比较单智能体与多智能体配置的性能。

Result: 多智能体系统减少漏报，但效果因攻击类型而异，且增加误报和计算开销。

Conclusion: 当前自动防御存在局限性，需改进未来LLM系统的对齐鲁棒性。

Abstract: Recent advances in large language models (LLMs) have raised concerns about
jailbreaking attacks, i.e., prompts that bypass safety mechanisms. This paper
investigates the use of multi-agent LLM systems as a defence against such
attacks. We evaluate three jailbreaking strategies, including the original
AutoDefense attack and two from Deepleaps: BetterDan and JB. Reproducing the
AutoDefense framework, we compare single-agent setups with two- and three-agent
configurations. Our results show that multi-agent systems enhance resistance to
jailbreaks, especially by reducing false negatives. However, its effectiveness
varies by attack type, and it introduces trade-offs such as increased false
positives and computational overhead. These findings point to the limitations
of current automated defences and suggest directions for improving alignment
robustness in future LLM systems.

</details>


### [29] [Self-correcting Reward Shaping via Language Models for Reinforcement Learning Agents in Games](https://arxiv.org/abs/2506.23626)
*António Afonso,Iolanda Leite,Alessandro Sestini,Florian Fuchs,Konrad Tollmar,Linus Gisslén*

Main category: cs.AI

TL;DR: 论文提出了一种基于语言模型的自动化方法，用于迭代优化强化学习代理的奖励函数权重，解决了游戏内容或机制修改时奖励权重不再最优的问题。


<details>
  <summary>Details</summary>
Motivation: 在游戏中部署强化学习代理时，设计有效的奖励函数通常需要专家，且游戏内容或机制修改后，原有奖励权重可能失效。本文旨在解决这一问题。

Method: 使用语言模型根据用户定义的行为目标和先前训练轮次的性能统计，迭代提出更新的奖励函数权重，形成闭环优化过程。

Result: 在赛车任务中，语言模型引导的代理性能从9%提升到74%的成功率，最终达到80%的成功率和855时间步，接近专家调优的94%和850时间步。

Conclusion: 该方法无需手动设计奖励函数，能自动优化代理行为，性能接近专家调优结果，具有实际应用潜力。

Abstract: Reinforcement Learning (RL) in games has gained significant momentum in
recent years, enabling the creation of different agent behaviors that can
transform a player's gaming experience. However, deploying RL agents in
production environments presents two key challenges: (1) designing an effective
reward function typically requires an RL expert, and (2) when a game's content
or mechanics are modified, previously tuned reward weights may no longer be
optimal. Towards the latter challenge, we propose an automated approach for
iteratively fine-tuning an RL agent's reward function weights, based on a
user-defined language based behavioral goal. A Language Model (LM) proposes
updated weights at each iteration based on this target behavior and a summary
of performance statistics from prior training rounds. This closed-loop process
allows the LM to self-correct and refine its output over time, producing
increasingly aligned behavior without the need for manual reward engineering.
We evaluate our approach in a racing task and show that it consistently
improves agent performance across iterations. The LM-guided agents show a
significant increase in performance from $9\%$ to $74\%$ success rate in just
one iteration. We compare our LM-guided tuning against a human expert's manual
weight design in the racing task: by the final iteration, the LM-tuned agent
achieved an $80\%$ success rate, and completed laps in an average of $855$ time
steps, a competitive performance against the expert-tuned agent's peak $94\%$
success, and $850$ time steps.

</details>


### [30] [HASD: Hierarchical Adaption for pathology Slide-level Domain-shift](https://arxiv.org/abs/2506.23673)
*Jingsong Liu,Han Li,Chen Yang,Michael Deutges,Ario Sadafi,Xin You,Katharina Breininger,Nassir Navab,Peter J. Schüffler*

Main category: cs.AI

TL;DR: 提出了一种名为HASD的分层适应框架，用于解决病理学AI中的幻灯片级域偏移问题，通过多尺度特征一致性和计算高效的适应方法显著提升了性能。


<details>
  <summary>Details</summary>
Motivation: 病理学数据受中心特定条件影响严重，现有方法仅关注图像块而忽略了全局WSI特征，无法满足临床需求。

Method: HASD框架包含分层适应组件（域级对齐、幻灯片级几何不变性正则化和块级注意力一致性正则化）和原型选择机制。

Result: 在两个幻灯片级任务中，HASD在乳腺癌HER2分级队列中AUROC提升4.1%，在UCEC生存预测队列中C-index提升3.9%。

Conclusion: HASD为病理学机构提供了一种实用且可靠的幻灯片级域适应解决方案，显著降低了计算和标注成本。

Abstract: Domain shift is a critical problem for pathology AI as pathology data is
heavily influenced by center-specific conditions. Current pathology domain
adaptation methods focus on image patches rather than WSI, thus failing to
capture global WSI features required in typical clinical scenarios. In this
work, we address the challenges of slide-level domain shift by proposing a
Hierarchical Adaptation framework for Slide-level Domain-shift (HASD). HASD
achieves multi-scale feature consistency and computationally efficient
slide-level domain adaptation through two key components: (1) a hierarchical
adaptation framework that integrates a Domain-level Alignment Solver for
feature alignment, a Slide-level Geometric Invariance Regularization to
preserve the morphological structure, and a Patch-level Attention Consistency
Regularization to maintain local critical diagnostic cues; and (2) a prototype
selection mechanism that reduces computational overhead. We validate our method
on two slide-level tasks across five datasets, achieving a 4.1\% AUROC
improvement in a Breast Cancer HER2 Grading cohort and a 3.9\% C-index gain in
a UCEC survival prediction cohort. Our method provides a practical and reliable
slide-level domain adaption solution for pathology institutions, minimizing
both computational and annotation costs.

</details>


### [31] [PokéAI: A Goal-Generating, Battle-Optimizing Multi-agent System for Pokemon Red](https://arxiv.org/abs/2506.23689)
*Zihao Liu,Xinhang Sui,Yueran Song,Siwen Wang*

Main category: cs.AI

TL;DR: Pok\'eAI是一个基于文本的多智能体LLM框架，用于自主玩Pok\'emon Red。它包含规划、执行和评估三个智能体，形成闭环决策系统。初步结果显示，战斗模块的胜率为80.8%，接近人类水平。


<details>
  <summary>Details</summary>
Motivation: 开发一个能够自主玩Pok\'emon Red的多智能体LLM框架，探索语言能力与战略推理的关联。

Method: 系统由三个智能体（规划、执行、评估）组成，分别负责任务生成、执行和结果评估，形成闭环。

Result: 战斗模块在50场野生对战中平均胜率为80.8%，接近人类水平（86.8%）。语言能力与战斗表现相关。

Conclusion: Pok\'eAI展示了LLM在游戏中的潜力，智能体表现出独特的策略行为，语言能力与战略推理相关。

Abstract: We introduce Pok\'eAI, the first text-based, multi-agent large language model
(LLM) framework designed to autonomously play and progress through Pok\'emon
Red. Our system consists of three specialized agents-Planning, Execution, and
Critique-each with its own memory bank, role, and skill set. The Planning Agent
functions as the central brain, generating tasks to progress through the game.
These tasks are then delegated to the Execution Agent, which carries them out
within the game environment. Upon task completion, the Critique Agent evaluates
the outcome to determine whether the objective was successfully achieved. Once
verification is complete, control returns to the Planning Agent, forming a
closed-loop decision-making system.
  As a preliminary step, we developed a battle module within the Execution
Agent. Our results show that the battle AI achieves an average win rate of
80.8% across 50 wild encounters, only 6% lower than the performance of an
experienced human player. Furthermore, we find that a model's battle
performance correlates strongly with its LLM Arena score on language-related
tasks, indicating a meaningful link between linguistic ability and strategic
reasoning. Finally, our analysis of gameplay logs reveals that each LLM
exhibits a unique playstyle, suggesting that individual models develop distinct
strategic behaviors.

</details>


### [32] [Agent4S: The Transformation of Research Paradigms from the Perspective of Large Language Models](https://arxiv.org/abs/2506.23692)
*Boyuan Zheng,Zerui Fang,Zhe Xu,Rui Wang,Yiwen Chen,Cunshi Wang,Mengwei Qu,Lei Lei,Zhen Feng,Yan Liu,Yuyang Li,Mingzhou Tan,Jiaji Wu,Jianwei Shuai,Jia Li,Fangfu Ye*

Main category: cs.AI

TL;DR: 论文提出用LLM驱动的Agent4S作为第五科学范式，以自动化整个科研流程，解决当前AI4S的低效问题。


<details>
  <summary>Details</summary>
Motivation: 当前AI4S作为分析工具未能解决科研核心低效问题，需更全面的自动化解决方案。

Method: 提出五级分类框架，从简单任务自动化到完全自主协作的AI科学家。

Result: 定义了科研发现的下一个革命性步骤。

Conclusion: Agent4S是真正的第五科学范式，将推动科研自动化革命。

Abstract: While AI for Science (AI4S) serves as an analytical tool in the current
research paradigm, it doesn't solve its core inefficiency. We propose "Agent
for Science" (Agent4S)-the use of LLM-driven agents to automate the entire
research workflow-as the true Fifth Scientific Paradigm. This paper introduces
a five-level classification for Agent4S, outlining a clear roadmap from simple
task automation to fully autonomous, collaborative "AI Scientists." This
framework defines the next revolutionary step in scientific discovery.

</details>


### [33] [A New Perspective On AI Safety Through Control Theory Methodologies](https://arxiv.org/abs/2506.23703)
*Lars Ullrich,Walter Zimmer,Ross Greer,Knut Graichen,Alois C. Knoll,Mohan Trivedi*

Main category: cs.AI

TL;DR: 论文提出了一种基于系统理论和数据分析的新视角，旨在通过跨学科方法提升AI安全性，称为“数据控制”。


<details>
  <summary>Details</summary>
Motivation: AI在安全关键系统中缺乏安全保障，需要结合控制理论和AI技术来解决这一问题。

Method: 采用系统理论和系统分析驱动的方法，提出数据控制的概念，结合现有安全分析和保障技术。

Result: 提出了一种通用的安全分析和保障框架，适用于特定AI系统和未来创新。

Conclusion: 通过跨学科的数据控制方法，可以提升AI系统的安全性，并为未来研究奠定基础。

Abstract: While artificial intelligence (AI) is advancing rapidly and mastering
increasingly complex problems with astonishing performance, the safety
assurance of such systems is a major concern. Particularly in the context of
safety-critical, real-world cyber-physical systems, AI promises to achieve a
new level of autonomy but is hampered by a lack of safety assurance. While
data-driven control takes up recent developments in AI to improve control
systems, control theory in general could be leveraged to improve AI safety.
Therefore, this article outlines a new perspective on AI safety based on an
interdisciplinary interpretation of the underlying data-generation process and
the respective abstraction by AI systems in a system theory-inspired and system
analysis-driven manner. In this context, the new perspective, also referred to
as data control, aims to stimulate AI engineering to take advantage of existing
safety analysis and assurance in an interdisciplinary way to drive the paradigm
of data control. Following a top-down approach, a generic foundation for safety
analysis and assurance is outlined at an abstract level that can be refined for
specific AI systems and applications and is prepared for future innovation.

</details>


### [34] [Attestable Audits: Verifiable AI Safety Benchmarks Using Trusted Execution Environments](https://arxiv.org/abs/2506.23706)
*Christoph Schnabl,Daniel Hugenroth,Bill Marino,Alastair R. Beresford*

Main category: cs.AI

TL;DR: 提出了一种基于可信执行环境（TEE）的可验证审计方法，用于保护AI模型和基准数据的机密性，并验证合规性。


<details>
  <summary>Details</summary>
Motivation: 解决现有基准测试无法提供可验证结果且缺乏数据机密性的问题，满足AI治理框架的需求。

Method: 使用可信执行环境（TEE）实现可验证审计，保护模型和数据隐私。

Result: 原型系统在Llama-3.1上验证了可行性。

Conclusion: Attestable Audits为AI模型的合规性和数据保护提供了可行解决方案。

Abstract: Benchmarks are important measures to evaluate safety and compliance of AI
models at scale. However, they typically do not offer verifiable results and
lack confidentiality for model IP and benchmark datasets. We propose Attestable
Audits, which run inside Trusted Execution Environments and enable users to
verify interaction with a compliant AI model. Our work protects sensitive data
even when model provider and auditor do not trust each other. This addresses
verification challenges raised in recent AI governance frameworks. We build a
prototype demonstrating feasibility on typical audit benchmarks against
Llama-3.1.

</details>


### [35] [BayesL: Towards a Logical Framework for Bayesian Networks](https://arxiv.org/abs/2506.23773)
*Stefano M. Nicoletti,Mariëlle Stoelinga*

Main category: cs.AI

TL;DR: BayesL是一个新的逻辑框架，用于指定、查询和验证贝叶斯网络的行为。


<details>
  <summary>Details</summary>
Motivation: 提供一个结构化语言，支持对贝叶斯网络的查询和推理，避免手动修改模型。

Method: 开发BayesL语言，支持因果和证据关系的推理，以及全面的假设情景评估。

Result: BayesL能够灵活地推理贝叶斯网络的行为，支持多种查询和验证需求。

Conclusion: BayesL是一个有效的工具，简化了对贝叶斯网络的分析和验证。

Abstract: We introduce BayesL, a novel logical framework for specifying, querying, and
verifying the behaviour of Bayesian networks (BNs). BayesL (pronounced "Basil")
is a structured language that allows for the creation of queries over BNs. It
facilitates versatile reasoning concerning causal and evidence-based
relationships, and permits comprehensive what-if scenario evaluations without
the need for manual modifications to the model.

</details>


### [36] [When GNNs Met a Word Equations Solver: Learning to Rank Equations (Extended Technical Report)](https://arxiv.org/abs/2506.23784)
*Parosh Aziz Abdulla,Mohamed Faouzi Atig,Julie Cailler,Chencheng Liang,Philipp Rümmer*

Main category: cs.AI

TL;DR: 该论文探索了使用图神经网络（GNN）对词方程进行排序以优化求解过程，提出了一种新的图表示方法，并通过实验验证其有效性。


<details>
  <summary>Details</summary>
Motivation: 解决词方程联立时的求解顺序问题，以提高求解器的性能。

Method: 提出了一种新的图表示方法，利用GNN对词方程进行排序，并采用三种方法处理多分类任务。训练时使用了最小不可满足子集（MUSes）。

Result: 实验表明，新框架在变量在每个方程中最多出现一次的基准测试中表现优于现有求解器。

Conclusion: GNN结合图表示方法能有效优化词方程求解顺序，提升求解效率。

Abstract: Nielsen transformation is a standard approach for solving word equations: by
repeatedly splitting equations and applying simplification steps, equations are
rewritten until a solution is reached. When solving a conjunction of word
equations in this way, the performance of the solver will depend considerably
on the order in which equations are processed. In this work, the use of Graph
Neural Networks (GNNs) for ranking word equations before and during the solving
process is explored. For this, a novel graph-based representation for word
equations is presented, preserving global information across conjuncts,
enabling the GNN to have a holistic view during ranking. To handle the variable
number of conjuncts, three approaches to adapt a multi-classification task to
the problem of ranking equations are proposed. The training of the GNN is done
with the help of minimum unsatisfiable subsets (MUSes) of word equations. The
experimental results show that, compared to state-of-the-art string solvers,
the new framework solves more problems in benchmarks where each variable
appears at most once in each equation.

</details>


### [37] [Advancing Learnable Multi-Agent Pathfinding Solvers with Active Fine-Tuning](https://arxiv.org/abs/2506.23793)
*Anton Andreychuk,Konstantin Yakovlev,Aleksandr Panov,Alexey Skrynnik*

Main category: cs.AI

TL;DR: MAPF-GPT-DDG是一种基于机器学习的多智能体路径规划（MAPF）求解器，通过改进预训练模型和引入新的数据生成机制，显著提升了性能和可扩展性。


<details>
  <summary>Details</summary>
Motivation: 解决多智能体路径规划（MAPF）问题的高效求解需求，尤其是在大规模场景中的应用。

Method: 利用集中式专家数据对预训练的MAPF-GPT模型进行微调，并采用新的delta-data生成机制加速训练。

Result: MAPF-GPT-DDG在测试中表现优于现有学习型求解器，支持单环境中多达100万个智能体的路径规划。

Conclusion: MAPF-GPT-DDG为MAPF领域设定了新的可扩展性标准，适用于物流、搜救等实际应用。

Abstract: Multi-agent pathfinding (MAPF) is a common abstraction of multi-robot
trajectory planning problems, where multiple homogeneous robots simultaneously
move in the shared environment. While solving MAPF optimally has been proven to
be NP-hard, scalable, and efficient, solvers are vital for real-world
applications like logistics, search-and-rescue, etc. To this end, decentralized
suboptimal MAPF solvers that leverage machine learning have come on stage.
Building on the success of the recently introduced MAPF-GPT, a pure imitation
learning solver, we introduce MAPF-GPT-DDG. This novel approach effectively
fine-tunes the pre-trained MAPF model using centralized expert data. Leveraging
a novel delta-data generation mechanism, MAPF-GPT-DDG accelerates training
while significantly improving performance at test time. Our experiments
demonstrate that MAPF-GPT-DDG surpasses all existing learning-based MAPF
solvers, including the original MAPF-GPT, regarding solution quality across
many testing scenarios. Remarkably, it can work with MAPF instances involving
up to 1 million agents in a single environment, setting a new milestone for
scalability in MAPF domains.

</details>


### [38] [A Survey on Autonomy-Induced Security Risks in Large Model-Based Agents](https://arxiv.org/abs/2506.23844)
*Hang Su,Jun Luo,Chang Liu,Xiao Yang,Yichi Zhang,Yinpeng Dong,Jun Zhu*

Main category: cs.AI

TL;DR: 大型语言模型（LLM）的进步推动了自主AI代理的发展，但也带来了新的安全风险。本文调查了代理的结构基础、能力及安全漏洞，并提出了防御策略和R2A2框架。


<details>
  <summary>Details</summary>
Motivation: 研究自主AI代理的能力及其带来的新型安全风险，以填补传统系统或独立LLM的威胁模型空白。

Method: 分析了代理的结构基础（如长期记忆、工具使用、递归规划和反思推理），并识别了安全漏洞。提出了防御策略和R2A2框架。

Result: 揭示了代理在感知、认知、记忆和行动模块中的脆弱性，并提出了针对性的防御措施。

Conclusion: 通过R2A2框架，实现了基于风险感知的决策优化，为自主AI代理的安全性提供了系统化解决方案。

Abstract: Recent advances in large language models (LLMs) have catalyzed the rise of
autonomous AI agents capable of perceiving, reasoning, and acting in dynamic,
open-ended environments. These large-model agents mark a paradigm shift from
static inference systems to interactive, memory-augmented entities. While these
capabilities significantly expand the functional scope of AI, they also
introduce qualitatively novel security risks - such as memory poisoning, tool
misuse, reward hacking, and emergent misalignment - that extend beyond the
threat models of conventional systems or standalone LLMs. In this survey, we
first examine the structural foundations and key capabilities that underpin
increasing levels of agent autonomy, including long-term memory retention,
modular tool use, recursive planning, and reflective reasoning. We then analyze
the corresponding security vulnerabilities across the agent stack, identifying
failure modes such as deferred decision hazards, irreversible tool chains, and
deceptive behaviors arising from internal state drift or value misalignment.
These risks are traced to architectural fragilities that emerge across
perception, cognition, memory, and action modules. To address these challenges,
we systematically review recent defense strategies deployed at different
autonomy layers, including input sanitization, memory lifecycle control,
constrained decision-making, structured tool invocation, and introspective
reflection. We introduce the Reflective Risk-Aware Agent Architecture (R2A2), a
unified cognitive framework grounded in Constrained Markov Decision Processes
(CMDPs), which incorporates risk-aware world modeling, meta-policy adaptation,
and joint reward-risk optimization to enable principled, proactive safety
across the agent's decision-making loop.

</details>


### [39] [Beyond Statistical Learning: Exact Learning Is Essential for General Intelligence](https://arxiv.org/abs/2506.23908)
*András György,Tor Lattimore,Nevena Lazić,Csaba Szepesvári*

Main category: cs.AI

TL;DR: 论文主张AI系统需从统计学习转向精确学习，以实现可靠的演绎推理。


<details>
  <summary>Details</summary>
Motivation: 当前AI系统在演绎推理任务上表现不佳，无法实现通用人工智能。

Method: 提出从统计性能优化转向精确学习范式，要求对所有输入的正确性。

Result: 精确学习是实现可靠演绎推理的关键。

Conclusion: 精确学习应成为算法设计的指导目标。

Abstract: Sound deductive reasoning -- the ability to derive new knowledge from
existing facts and rules -- is an indisputably desirable aspect of general
intelligence. Despite the major advances of AI systems in areas such as math
and science, especially since the introduction of transformer architectures, it
is well-documented that even the most advanced frontier systems regularly and
consistently falter on easily-solvable deductive reasoning tasks. Hence, these
systems are unfit to fulfill the dream of achieving artificial general
intelligence capable of sound deductive reasoning. We argue that their unsound
behavior is a consequence of the statistical learning approach powering their
development. To overcome this, we contend that to achieve reliable deductive
reasoning in learning-based AI systems, researchers must fundamentally shift
from optimizing for statistical performance against distributions on reasoning
problems and algorithmic tasks to embracing the more ambitious exact learning
paradigm, which demands correctness on all inputs. We argue that exact learning
is both essential and possible, and that this ambitious objective should guide
algorithm design.

</details>


### [40] [Performance of LLMs on Stochastic Modeling Operations Research Problems: From Theory to Practice](https://arxiv.org/abs/2506.23924)
*Akshit Kumar,Tianyi Peng,Yuhang Wu,Assaf Zeevi*

Main category: cs.AI

TL;DR: 本文评估了大型语言模型（LLMs）在解决运筹学（OR）中随机建模问题的能力，发现其在课堂和实际场景中表现与人类专家相当。


<details>
  <summary>Details</summary>
Motivation: 探索LLMs在解决运筹学问题中的能力，尤其是随机建模问题，以填补这一领域的空白。

Method: 通过手动收集研究生课程作业和博士资格考试题目，并利用开源库SimOpt测试LLMs在不确定性下的决策能力。

Result: LLMs在解决随机建模问题时表现出与人类专家相当的能力，但仍需进一步工作以实现可靠自动化。

Conclusion: LLMs在运筹学中具有潜力，可构建AI助手以支持研究并推动自动化应用。

Abstract: Large language models (LLMs) have exhibited expert-level capabilities across
various domains. However, their abilities to solve problems in Operations
Research (OR) -- the analysis and optimization of mathematical models derived
from real-world problems or their verbal descriptions -- remain underexplored.
In this work, we take a first step toward evaluating LLMs' abilities to solve
stochastic modeling problems, a core class of OR problems characterized by
uncertainty and typically involving tools from probability, statistics, and
stochastic processes. We manually procure a representative set of
graduate-level homework and doctoral qualification-exam problems and test LLMs'
abilities to solve them. We further leverage SimOpt, an open-source library of
simulation-optimization problems and solvers, to investigate LLMs' abilities to
make real-world decisions under uncertainty. Our results show that, though a
nontrivial amount of work is still needed to reliably automate the stochastic
modeling pipeline in reality, state-of-the-art LLMs demonstrate proficiency on
par with human experts in both classroom and practical settings. These findings
highlight the potential of building AI agents that assist OR researchers and
amplify the real-world impact of OR through automation.

</details>


### [41] [Industrial brain: a human-like autonomous neuro-symbolic cognitive decision-making system](https://arxiv.org/abs/2506.23926)
*Junping Wang,Bicheng Wang,Yibo Xuea,Yuan Xie*

Main category: cs.AI

TL;DR: 提出了一种名为“工业大脑”的框架，结合高阶神经网和符号推理，用于预测和规划工业链的韧性，显著优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有深度学习方法在复杂时空共演结构和多混沌数据下的韧性预测表现不佳，亟需新方法。

Method: 整合高阶活动驱动神经网和CT-OODA符号推理，直接从观测数据中自主规划韧性。

Result: 工业大脑在韧性预测和规划上显著优于GoT、OlaGPT和谱降维方法，准确率提升达10.8%和11.03%。

Conclusion: 工业大脑填补了工业链韧性预测和规划的重要空白，具有鲁棒性和泛化能力。

Abstract: Resilience non-equilibrium measurement, the ability to maintain fundamental
functionality amidst failures and errors, is crucial for scientific management
and engineering applications of industrial chain. The problem is particularly
challenging when the number or types of multiple co-evolution of resilience
(for example, randomly placed) are extremely chaos. Existing end-to-end deep
learning ordinarily do not generalize well to unseen full-feld reconstruction
of spatiotemporal co-evolution structure, and predict resilience of network
topology, especially in multiple chaos data regimes typically seen in
real-world applications. To address this challenge, here we propose industrial
brain, a human-like autonomous cognitive decision-making and planning framework
integrating higher-order activity-driven neuro network and CT-OODA symbolic
reasoning to autonomous plan resilience directly from observational data of
global variable. The industrial brain not only understands and model structure
of node activity dynamics and network co-evolution topology without simplifying
assumptions, and reveal the underlying laws hidden behind complex networks, but
also enabling accurate resilience prediction, inference, and planning.
Experimental results show that industrial brain significantly outperforms
resilience prediction and planning methods, with an accurate improvement of up
to 10.8\% over GoT and OlaGPT framework and 11.03\% over spectral dimension
reduction. It also generalizes to unseen topologies and dynamics and maintains
robust performance despite observational disturbances. Our findings suggest
that industrial brain addresses an important gap in resilience prediction and
planning for industrial chain.

</details>


### [42] [AI Risk-Management Standards Profile for General-Purpose AI (GPAI) and Foundation Models](https://arxiv.org/abs/2506.23949)
*Anthony M. Barrett,Jessica Newman,Brandie Nonnecke,Nada Madkour,Dan Hendrycks,Evan R. Murphy,Krystal Jackson,Deepika Raman*

Main category: cs.AI

TL;DR: 本文提出了针对通用人工智能（GPAI）和基础模型的风险管理实践，旨在帮助开发者识别、分析和减轻相关风险。


<details>
  <summary>Details</summary>
Motivation: GPAI和基础模型虽然具有广泛的应用潜力，但也伴随着重大风险，需要专门的风险管理措施。

Method: 结合NIST AI风险管理框架和ISO/IEC 23894标准，针对GPAI/基础模型的独特问题提出风险管理实践。

Result: 为开发者提供了一套实用的风险管理指南，适用于GPAI/基础模型的开发和应用。

Conclusion: 本文为GPAI/基础模型的风险管理提供了重要参考，有助于推动安全可靠的AI发展。

Abstract: Increasingly multi-purpose AI models, such as cutting-edge large language
models or other 'general-purpose AI' (GPAI) models, 'foundation models,'
generative AI models, and 'frontier models' (typically all referred to
hereafter with the umbrella term 'GPAI/foundation models' except where greater
specificity is needed), can provide many beneficial capabilities but also risks
of adverse events with profound consequences. This document provides
risk-management practices or controls for identifying, analyzing, and
mitigating risks of GPAI/foundation models. We intend this document primarily
for developers of large-scale, state-of-the-art GPAI/foundation models; others
that can benefit from this guidance include downstream developers of end-use
applications that build on a GPAI/foundation model. This document facilitates
conformity with or use of leading AI risk management-related standards,
adapting and building on the generic voluntary guidance in the NIST AI Risk
Management Framework and ISO/IEC 23894, with a focus on the unique issues faced
by developers of GPAI/foundation models.

</details>


### [43] [Harnessing AI Agents to Advance Research on Refugee Child Mental Health](https://arxiv.org/abs/2506.23992)
*Aditya Shrivastava,Komal Gupta,Shraddha Arora*

Main category: cs.AI

TL;DR: 研究提出了一种基于AI的框架，用于处理难民健康数据并分析儿童心理健康，比较了两种RAG模型（Zephyr-7B-beta和DeepSeek R1-7B），发现DeepSeek R1表现更优。


<details>
  <summary>Details</summary>
Motivation: 解决国际难民危机中儿童心理健康问题，通过AI技术提供可扩展的解决方案。

Method: 使用两种RAG模型处理难民健康数据，评估其性能和避免幻觉风险的能力。

Result: DeepSeek R1-7B在答案相关性上表现优于Zephyr-7B-beta，准确率为0.91。

Conclusion: 研究为政策制定者和人道机构提供了有效的AI工具，以更好地支持难民儿童的心理健康。

Abstract: The international refugee crisis deepens, exposing millions of dis placed
children to extreme psychological trauma. This research suggests a com pact,
AI-based framework for processing unstructured refugee health data and
distilling knowledge on child mental health. We compare two Retrieval-Aug
mented Generation (RAG) pipelines, Zephyr-7B-beta and DeepSeek R1-7B, to
determine how well they process challenging humanitarian datasets while avoid
ing hallucination hazards. By combining cutting-edge AI methods with migration
research and child psychology, this study presents a scalable strategy to
assist policymakers, mental health practitioners, and humanitarian agencies to
better assist displaced children and recognize their mental wellbeing. In
total, both the models worked properly but significantly Deepseek R1 is
superior to Zephyr with an accuracy of answer relevance 0.91

</details>


### [44] [Constructing Non-Markovian Decision Process via History Aggregator](https://arxiv.org/abs/2506.24026)
*Yongyi Wang,Wenxin Li*

Main category: cs.AI

TL;DR: 论文提出了一种基于范畴论的方法，用于评估决策算法处理非马尔可夫动态的能力，通过建立MDP和NMDP的范畴等价关系，并引入HAS工具精确控制状态依赖结构。


<details>
  <summary>Details</summary>
Motivation: 现有基准无法全面评估决策算法处理非马尔可夫动态的能力，这限制了相关系统的进展和效果。

Method: 基于范畴论建立MDP和NMDP的范畴等价关系，并引入HAS工具控制状态依赖结构。

Result: 方法能有效表示广泛的非马尔可夫动态，为决策算法提供更严格和灵活的评估。

Conclusion: 该方法为理解和解决非马尔可夫动态提供了新视角，并提升了决策算法的评估能力。

Abstract: In the domain of algorithmic decision-making, non-Markovian dynamics manifest
as a significant impediment, especially for paradigms such as Reinforcement
Learning (RL), thereby exerting far-reaching consequences on the advancement
and effectiveness of the associated systems. Nevertheless, the existing
benchmarks are deficient in comprehensively assessing the capacity of decision
algorithms to handle non-Markovian dynamics. To address this deficiency, we
have devised a generalized methodology grounded in category theory. Notably, we
established the category of Markov Decision Processes (MDP) and the category of
non-Markovian Decision Processes (NMDP), and proved the equivalence
relationship between them. This theoretical foundation provides a novel
perspective for understanding and addressing non-Markovian dynamics. We further
introduced non-Markovianity into decision-making problem settings via the
History Aggregator for State (HAS). With HAS, we can precisely control the
state dependency structure of decision-making problems in the time series. Our
analysis demonstrates the effectiveness of our method in representing a broad
range of non-Markovian dynamics. This approach facilitates a more rigorous and
flexible evaluation of decision algorithms by testing them in problem settings
where non-Markovian dynamics are explicitly constructed.

</details>


### [45] [SPIRAL: Self-Play on Zero-Sum Games Incentivizes Reasoning via Multi-Agent Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2506.24119)
*Bo Liu,Leon Guertler,Simon Yu,Zichen Liu,Penghui Qi,Daniel Balcells,Mickel Liu,Cheston Tan,Weiyan Shi,Min Lin,Wee Sun Lee,Natasha Jaques*

Main category: cs.AI

TL;DR: SPIRAL是一个通过自我对弈框架训练语言模型的强化学习方法，无需人工监督，能够生成无限难度递增的问题，提升模型的推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有的强化学习方法依赖人工标注的问题-答案对和领域特定的奖励设计，限制了模型的自主性和泛化能力。

Method: SPIRAL采用多轮零和游戏的自我对弈框架，结合在线多智能体强化学习系统和角色条件优势估计（RAE）来稳定训练。

Result: 实验表明，SPIRAL在单一游戏（如Kuhn Poker）和多游戏训练中显著提升了模型的数学和通用推理能力，且能迁移到其他任务。

Conclusion: 零和游戏能自然发展出可迁移的推理能力，为自主推理开发提供了新方向。

Abstract: Recent advances in reinforcement learning have shown that language models can
develop sophisticated reasoning through training on tasks with verifiable
rewards, but these approaches depend on human-curated problem-answer pairs and
domain-specific reward engineering. We introduce SPIRAL, a self-play framework
where models learn by playing multi-turn, zero-sum games against continuously
improving versions of themselves, eliminating the need for human supervision.
Through self-play, SPIRAL generates an infinite curriculum of progressively
challenging problems as models must constantly adapt to stronger opponents. To
enable this self-play training at scale, We implement a fully online,
multi-turn, multi-agent reinforcement learning system for LLMs and propose
role-conditioned advantage estimation (RAE) to stabilize multi-agent training.
Using SPIRAL, self-play on zero-sum games produces reasoning capabilities that
transfer broadly. Training Qwen3-4B-Base on Kuhn Poker alone achieves 8.6%
improvement on math and 8.4% on general reasoning, outperforming SFT on 25,000
expert game trajectories. Analysis reveals that this transfer occurs through
three cognitive patterns: systematic decomposition, expected value calculation,
and case-by-case analysis. Multi-game training (TicTacToe, Kuhn Poker, Simple
Negotiation) further enhances performance as each game develops distinct
reasoning strengths. Applying SPIRAL to a strong reasoning model
(DeepSeek-R1-Distill-Qwen-7B) can still lead to 2.0% average improvement. These
results demonstrate that zero-sum games naturally develop transferable
reasoning capabilities, highlighting a promising direction for autonomous
reasoning development.

</details>
