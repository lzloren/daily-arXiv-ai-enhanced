<div id=toc></div>

# Table of Contents

- [cs.AI](#cs.AI) [Total: 31]


<div id='cs.AI'></div>

# cs.AI [[Back]](#toc)

### [1] [STELLA: Self-Evolving LLM Agent for Biomedical Research](https://arxiv.org/abs/2507.02004)
*Ruofan Jin,Zaixi Zhang,Mengdi Wang,Le Cong*

Main category: cs.AI

TL;DR: STELLA是一种自进化的AI代理，通过动态工具库和模板库自主提升能力，在生物医学任务中表现优异且性能随经验增长。


<details>
  <summary>Details</summary>
Motivation: 解决生物医学领域数据、工具和文献碎片化问题，突破传统AI代理依赖静态工具集的限制。

Method: 采用多代理架构，包含动态工具库（Tool Ocean）和模板库（Template Library），通过工具创建代理自动发现和集成新工具。

Result: 在多项生物医学基准测试中表现优异，如Humanity's Last Exam: Biomedicine（26%）、LAB-Bench: DBQA（54%）和LAB-Bench: LitQA（63%），性能随经验提升。

Conclusion: STELLA代表了AI代理系统的重大进步，能够动态扩展专业知识，加速生物医学发现。

Abstract: The rapid growth of biomedical data, tools, and literature has created a
fragmented research landscape that outpaces human expertise. While AI agents
offer a solution, they typically rely on static, manually curated toolsets,
limiting their ability to adapt and scale. Here, we introduce STELLA, a
self-evolving AI agent designed to overcome these limitations. STELLA employs a
multi-agent architecture that autonomously improves its own capabilities
through two core mechanisms: an evolving Template Library for reasoning
strategies and a dynamic Tool Ocean that expands as a Tool Creation Agent
automatically discovers and integrates new bioinformatics tools. This allows
STELLA to learn from experience. We demonstrate that STELLA achieves
state-of-the-art accuracy on a suite of biomedical benchmarks, scoring
approximately 26\% on Humanity's Last Exam: Biomedicine, 54\% on LAB-Bench:
DBQA, and 63\% on LAB-Bench: LitQA, outperforming leading models by up to 6
percentage points. More importantly, we show that its performance
systematically improves with experience; for instance, its accuracy on the
Humanity's Last Exam benchmark almost doubles with increased trials. STELLA
represents a significant advance towards AI Agent systems that can learn and
grow, dynamically scaling their expertise to accelerate the pace of biomedical
discovery.

</details>


### [2] [HCVR: A Hybrid Approach with Correlation-aware Voting Rules for Feature Selection](https://arxiv.org/abs/2507.02073)
*Nikita Bhedasgaonkar,Rushikesh K. Joshi*

Main category: cs.AI

TL;DR: HCVR是一种轻量级规则化特征选择方法，结合P2P和P2T相关性，通过多数投票规则保留相关特征并消除冗余特征。


<details>
  <summary>Details</summary>
Motivation: 传统特征选择方法在性能上存在不足，HCVR旨在通过混合非迭代和迭代过滤方法提升特征选择效果。

Method: HCVR采用贪心算法，通过后向消除和多特征投票规则，利用相关性阈值进行特征选择。

Result: 在SPAMBASE数据集上，HCVR性能优于传统非迭代和迭代方法，分类器表现提升。

Conclusion: HCVR是一种有效的特征选择方法，适用于轻量级和高性能需求场景。

Abstract: In this paper, we propose HCVR (Hybrid approach with Correlation-aware Voting
Rules), a lightweight rule-based feature selection method that combines
Parameter-to-Parameter (P2P) and Parameter-to-Target (P2T) correlations to
eliminate redundant features and retain relevant ones. This method is a hybrid
of non-iterative and iterative filtering approaches for dimensionality
reduction. It is a greedy method, which works by backward elimination,
eliminating possibly multiple features at every step. The rules contribute to
voting for features, and a decision to keep or discard is made by majority
voting. The rules make use of correlation thresholds between every pair of
features, and between features and the target. We provide the results from the
application of HCVR to the SPAMBASE dataset. The results showed improvement
performance as compared to traditional non-iterative (CFS, mRMR and MI) and
iterative (RFE, SFS and Genetic Algorithm) techniques. The effectiveness was
assessed based on the performance of different classifiers after applying
filtering.

</details>


### [3] [Reasoning on a Budget: A Survey of Adaptive and Controllable Test-Time Compute in LLMs](https://arxiv.org/abs/2507.02076)
*Mohammad Ali Alomrani,Yingxue Zhang,Derek Li,Qianyi Sun,Soumyasundar Pal,Zhanguang Zhang,Yaochen Hu,Rohan Deepak Ajwani,Antonios Valkanas,Raika Karimi,Peng Cheng,Yunzhou Wang,Pengyi Liao,Hanrui Huang,Bin Wang,Jianye Hao,Mark Coates*

Main category: cs.AI

TL;DR: 该论文综述了大型语言模型（LLMs）在推理效率方面的不足，并提出了一种两级分类法（L1可控性和L2适应性）来改进计算效率。


<details>
  <summary>Details</summary>
Motivation: 当前LLMs在推理时计算效率低下，无法根据任务复杂度动态调整计算资源，导致简单问题过度计算而复杂问题计算不足。

Method: 论文提出了两级分类法：L1（固定计算预算下的方法）和L2（基于输入难度或模型置信度动态调整计算的方法），并对主流LLMs进行了基准测试。

Result: 研究揭示了推理性能与计算资源消耗之间的权衡，并强调了TTC方法的实际控制性、适应性和可扩展性。

Conclusion: 未来研究方向包括混合思维模型和解决LLMs在计算效率、鲁棒性及用户约束响应方面的挑战。

Abstract: Large language models (LLMs) have rapidly progressed into general-purpose
agents capable of solving a broad spectrum of tasks. However, current models
remain inefficient at reasoning: they apply fixed inference-time compute
regardless of task complexity, often overthinking simple problems while
underthinking hard ones. This survey presents a comprehensive review of
efficient test-time compute (TTC) strategies, which aim to improve the
computational efficiency of LLM reasoning. We introduce a two-tiered taxonomy
that distinguishes between L1-controllability, methods that operate under fixed
compute budgets, and L2-adaptiveness, methods that dynamically scale inference
based on input difficulty or model confidence. We benchmark leading proprietary
LLMs across diverse datasets, highlighting critical trade-offs between
reasoning performance and token usage. Compared to prior surveys on efficient
reasoning, our review emphasizes the practical control, adaptability, and
scalability of TTC methods. Finally, we discuss emerging trends such as hybrid
thinking models and identify key challenges for future work towards making LLMs
more computationally efficient, robust, and responsive to user constraints.

</details>


### [4] [Measuring Scientific Capabilities of Language Models with a Systems Biology Dry Lab](https://arxiv.org/abs/2507.02083)
*Haonan Duan,Stephen Zhewen Lu,Caitlin Fiona Harrigan,Nishkrit Desai,Jiarui Lu,Michał Koziarski,Leonardo Cotta,Chris J. Maddison*

Main category: cs.AI

TL;DR: SciGym是一个评估大型语言模型（LLM）在开放式科学发现任务中实验设计和分析能力的基准测试，通过干实验室模拟生物系统，克服了湿实验室的高成本问题。


<details>
  <summary>Details</summary>
Motivation: 评估LLMs在科学实验设计和结果解释方面的能力，填补现有测试的不足。

Method: 使用系统生物学标记语言（SBML）编码的生物系统模型生成模拟数据，评估六种前沿LLM在137个小系统和总共350个系统上的表现。

Result: 性能更强的模型表现更优，但随着系统复杂性增加，所有模型的性能显著下降，表明LLM在科学能力上有改进空间。

Conclusion: SciGym为评估LLM的科学能力提供了有效工具，但LLM在复杂系统中的应用仍需进一步提升。

Abstract: Designing experiments and result interpretations are core scientific
competencies, particularly in biology, where researchers perturb complex
systems to uncover the underlying systems. Recent efforts to evaluate the
scientific capabilities of large language models (LLMs) fail to test these
competencies because wet-lab experimentation is prohibitively expensive: in
expertise, time and equipment. We introduce SciGym, a first-in-class benchmark
that assesses LLMs' iterative experiment design and analysis abilities in
open-ended scientific discovery tasks. SciGym overcomes the challenge of
wet-lab costs by running a dry lab of biological systems. These models, encoded
in Systems Biology Markup Language, are efficient for generating simulated
data, making them ideal testbeds for experimentation on realistically complex
systems. We evaluated six frontier LLMs on 137 small systems, and released a
total of 350 systems. Our evaluation shows that while more capable models
demonstrated superior performance, all models' performance declined
significantly as system complexity increased, suggesting substantial room for
improvement in the scientific capabilities of LLM agents.

</details>


### [5] [What Neuroscience Can Teach AI About Learning in Continuously Changing Environments](https://arxiv.org/abs/2507.02103)
*Daniel Durstewitz,Bruno Averbeck,Georgia Koppe*

Main category: cs.AI

TL;DR: 论文探讨了现代AI模型与动物学习方式的差异，提出从神经科学中汲取灵感以改进AI的持续学习和上下文学习能力，并展望了NeuroAI领域的双向学习。


<details>
  <summary>Details</summary>
Motivation: 现代AI模型训练成本高且固定，而动物能快速适应环境变化，尤其是社交物种。研究旨在探索神经科学如何帮助AI实现类似能力，并促进NeuroAI的发展。

Method: 整合AI中的持续学习和上下文学习文献，结合神经科学中关于行为任务、奖励概率和结果变化的研究。

Result: 提出了神经科学如何指导AI发展的具体议程，并探讨了AI对神经科学的反哺作用。

Conclusion: 神经科学与AI的双向学习有望推动NeuroAI领域的进步，为AI在现实世界中的应用提供更灵活的学习机制。

Abstract: Modern AI models, such as large language models, are usually trained once on
a huge corpus of data, potentially fine-tuned for a specific task, and then
deployed with fixed parameters. Their training is costly, slow, and gradual,
requiring billions of repetitions. In stark contrast, animals continuously
adapt to the ever-changing contingencies in their environments. This is
particularly important for social species, where behavioral policies and reward
outcomes may frequently change in interaction with peers. The underlying
computational processes are often marked by rapid shifts in an animal's
behaviour and rather sudden transitions in neuronal population activity. Such
computational capacities are of growing importance for AI systems operating in
the real world, like those guiding robots or autonomous vehicles, or for
agentic AI interacting with humans online. Can AI learn from neuroscience? This
Perspective explores this question, integrating the literature on continual and
in-context learning in AI with the neuroscience of learning on behavioral tasks
with shifting rules, reward probabilities, or outcomes. We will outline an
agenda for how specifically insights from neuroscience may inform current
developments in AI in this area, and - vice versa - what neuroscience may learn
from AI, contributing to the evolving field of NeuroAI.

</details>


### [6] [The Illusion of Fairness: Auditing Fairness Interventions with Audit Studies](https://arxiv.org/abs/2507.02152)
*Disa Sariola,Patrick Button,Aron Culotta,Nicholas Mattei*

Main category: cs.AI

TL;DR: 论文探讨了利用审计研究数据改进自动招聘算法的训练和评估方法，发现传统公平干预方法存在隐性偏差，并提出基于个体处理效应估计的新干预方法。


<details>
  <summary>Details</summary>
Motivation: 研究AI系统（如招聘算法）的公平性和有效性，传统方法因数据选择偏差和标签偏差存在局限性，审计研究数据提供了更高质量的数据支持。

Method: 利用审计研究数据（如虚构简历）进行随机对照试验，分析传统公平干预方法（如均衡基础率）的局限性，并引入基于个体处理效应估计的新干预方法。

Result: 发现传统方法在表面上实现公平，但实际存在约10%的隐性偏差；新干预方法进一步减少了算法歧视。

Conclusion: 审计研究数据能显著提升算法公平性评估和干预效果，为未来研究提供了更可靠的数据和方法基础。

Abstract: Artificial intelligence systems, especially those using machine learning, are
being deployed in domains from hiring to loan issuance in order to automate
these complex decisions. Judging both the effectiveness and fairness of these
AI systems, and their human decision making counterpart, is a complex and
important topic studied across both computational and social sciences. Within
machine learning, a common way to address bias in downstream classifiers is to
resample the training data to offset disparities. For example, if hiring rates
vary by some protected class, then one may equalize the rate within the
training set to alleviate bias in the resulting classifier. While simple and
seemingly effective, these methods have typically only been evaluated using
data obtained through convenience samples, introducing selection bias and label
bias into metrics. Within the social sciences, psychology, public health, and
medicine, audit studies, in which fictitious ``testers'' (e.g., resumes,
emails, patient actors) are sent to subjects (e.g., job openings, businesses,
doctors) in randomized control trials, provide high quality data that support
rigorous estimates of discrimination. In this paper, we investigate how data
from audit studies can be used to improve our ability to both train and
evaluate automated hiring algorithms. We find that such data reveals cases
where the common fairness intervention method of equalizing base rates across
classes appears to achieve parity using traditional measures, but in fact has
roughly 10% disparity when measured appropriately. We additionally introduce
interventions based on individual treatment effect estimation methods that
further reduce algorithmic discrimination using this data.

</details>


### [7] [Data Diversification Methods In Alignment Enhance Math Performance In LLMs](https://arxiv.org/abs/2507.02173)
*Berkan Dokmeci,Qingyang Wu,Ben Athiwaratkun,Ce Zhang,Shuaiwen Leon Song,James Zou*

Main category: cs.AI

TL;DR: 研究通过数据多样化策略提升大语言模型（LLM）的数学推理能力，提出Diversified-ThinkSolve（DTS）方法，显著提升性能且计算开销低。


<details>
  <summary>Details</summary>
Motivation: 尽管偏好学习在人类反馈对齐方面取得进展，但数学推理仍是挑战。研究旨在通过数据多样化策略提升LLM的数学推理能力。

Method: 评估三种数据生成方法（温度采样、思维链提示、蒙特卡洛树搜索），并提出DTS方法，系统分解问题为多样化推理路径。

Result: DTS方法在GSM8K和MATH数据集上分别提升7.1%和4.2%，计算开销仅1.03倍，优于蒙特卡洛树搜索（5倍开销）。

Conclusion: 结构化探索多样化问题解决方法比传统方法更有效，能生成更优的偏好数据以提升数学推理对齐。

Abstract: While recent advances in preference learning have enhanced alignment in human
feedback, mathematical reasoning remains a persistent challenge. We investigate
how data diversification strategies in preference optimization can improve the
mathematical reasoning abilities of large language models (LLMs). We evaluate
three common data generation methods: temperature sampling, Chain-of-Thought
prompting, and Monte Carlo Tree Search (MCTS), and introduce
Diversified-ThinkSolve (DTS), a novel structured approach that systematically
decomposes problems into diverse reasoning paths. Our results show that with
strategically diversified preference data, models can substantially improve
mathematical reasoning performance, with the best approach yielding gains of
7.1% on GSM8K and 4.2% on MATH over the base model. Despite its strong
performance, DTS incurs only a marginal computational overhead (1.03x) compared
to the baseline, while MCTS is nearly five times more costly with lower
returns. These findings demonstrate that structured exploration of diverse
problem-solving methods creates more effective preference data for mathematical
alignment than traditional approaches.

</details>


### [8] [Do Role-Playing Agents Practice What They Preach? Belief-Behavior Consistency in LLM-Based Simulations of Human Trust](https://arxiv.org/abs/2507.02197)
*Amogh Mannekote,Adam Davies,Guohao Li,Kristy Elizabeth Boyer,ChengXiang Zhai,Bonnie J Dorr,Francesco Pinto*

Main category: cs.AI

TL;DR: 研究探讨了基于LLM的角色扮演代理在生成合成数据时，其陈述的信念与实际行为之间的一致性，并提出了评估框架和一致性度量标准。


<details>
  <summary>Details</summary>
Motivation: 随着LLM作为角色扮演代理在人类行为研究中生成合成数据的应用增加，确保其输出与角色一致成为关键问题。

Method: 通过增强版的GenAgents角色库和信任游戏，引入信念-行为一致性度量，研究信念类型、信息呈现方式和预测时间对一致性的影响。

Result: 发现LLM的陈述信念与模拟行为之间存在系统性不一致，即使模型编码了合理信念，也可能无法一致应用。

Conclusion: 强调了识别LLM信念与行为一致性的重要性，以便在行为研究中正确使用LLM代理。

Abstract: As LLMs are increasingly studied as role-playing agents to generate synthetic
data for human behavioral research, ensuring that their outputs remain coherent
with their assigned roles has become a critical concern. In this paper, we
investigate how consistently LLM-based role-playing agents' stated beliefs
about the behavior of the people they are asked to role-play ("what they say")
correspond to their actual behavior during role-play ("how they act").
Specifically, we establish an evaluation framework to rigorously measure how
well beliefs obtained by prompting the model can predict simulation outcomes in
advance. Using an augmented version of the GenAgents persona bank and the Trust
Game (a standard economic game used to quantify players' trust and
reciprocity), we introduce a belief-behavior consistency metric to
systematically investigate how it is affected by factors such as: (1) the types
of beliefs we elicit from LLMs, like expected outcomes of simulations versus
task-relevant attributes of individual characters LLMs are asked to simulate;
(2) when and how we present LLMs with relevant information about Trust Game;
and (3) how far into the future we ask the model to forecast its actions. We
also explore how feasible it is to impose a researcher's own theoretical priors
in the event that the originally elicited beliefs are misaligned with research
objectives. Our results reveal systematic inconsistencies between LLMs' stated
(or imposed) beliefs and the outcomes of their role-playing simulation, at both
an individual- and population-level. Specifically, we find that, even when
models appear to encode plausible beliefs, they may fail to apply them in a
consistent way. These findings highlight the need to identify how and when
LLMs' stated beliefs align with their simulated behavior, allowing researchers
to use LLM-based agents appropriately in behavioral studies.

</details>


### [9] [Dilution, Diffusion and Symbiosis in Spatial Prisoner's Dilemma with Reinforcement Learning](https://arxiv.org/abs/2507.02211)
*Gustavo C. Mangold,Heitor C. M. Fernandes,Mendeli H. Vainstein*

Main category: cs.AI

TL;DR: 研究了稀释和移动性对空间囚徒困境中多智能体Q学习算法的影响，发现固定规则与学习规则在效果上可能等价，并观察到种群间的共生效应。


<details>
  <summary>Details</summary>
Motivation: 探索空间囚徒困境中强化学习算法的多样性机制，特别是稀释和移动性对合作行为的影响。

Method: 采用独立多智能体Q学习算法，定义不同动作，模拟不同博弈场景。

Result: 发现固定规则与学习规则可能等效，并观察到种群间的共生效应。

Conclusion: 该方法展示了算法的多样性和建模潜力，为博弈理论研究提供了新视角。

Abstract: Recent studies in the spatial prisoner's dilemma games with reinforcement
learning have shown that static agents can learn to cooperate through a diverse
sort of mechanisms, including noise injection, different types of learning
algorithms and neighbours' payoff knowledge.In this work, using an independent
multi-agent Q-learning algorithm, we study the effects of dilution and mobility
in the spatial version of the prisoner's dilemma. Within this setting,
different possible actions for the algorithm are defined, connecting with
previous results on the classical, non-reinforcement learning spatial
prisoner's dilemma, showcasing the versatility of the algorithm in modeling
different game-theoretical scenarios and the benchmarking potential of this
approach.As a result, a range of effects is observed, including evidence that
games with fixed update rules can be qualitatively equivalent to those with
learned ones, as well as the emergence of a symbiotic mutualistic effect
between populations that forms when multiple actions are defined.

</details>


### [10] [Scaling LLM Planning: NL2FLOW for Parametric Problem Generation and Rigorous Evaluation](https://arxiv.org/abs/2507.02253)
*Jungkoo Kang*

Main category: cs.AI

TL;DR: NL2FLOW是一个自动化系统，用于生成规划问题并评估LLM生成的计划质量，结果显示直接生成计划比引入中间步骤更有效。


<details>
  <summary>Details</summary>
Motivation: 解决大规模语言模型（LLM）在规划和推理能力提升中的数据生成和评估瓶颈问题。

Method: 引入NL2FLOW系统，自动生成自然语言、结构化中间表示和PDDL格式的规划问题，并评估LLM生成的计划质量。

Result: 最高性能模型在生成有效计划上达到86%成功率，生成最优计划为69%；直接生成计划比中间翻译步骤更高效。

Conclusion: 直接推理能力对LLM性能至关重要，动态理解系统限制是释放LLM潜力的关键。

Abstract: Progress in enhancing large language model (LLM) planning and reasoning
capabilities is significantly hampered by the bottleneck of scalable, reliable
data generation and evaluation. To overcome this, I introduce NL2FLOW, a fully
automated system for parametrically generating planning problems - expressed in
natural language, a structured intermediate representation, and formal PDDL -
and rigorously evaluating the quality of generated plans. I demonstrate
NL2FLOW's capabilities by generating a dataset of 2296 problems in the
automated workflow generation domain and evaluating multiple open-sourced,
instruct-tuned LLMs. My results reveal that the highest performing models
achieved 86% success in generating valid plans and 69% in generating optimal
plans, specifically for problems with feasible solutions. Regression analysis
shows that the influence of problem characteristics on plan generation is
contingent on both model and prompt design. Notably, I observed that the
highest success rate for translating natural language into a JSON
representation of a plan was lower than the highest rate of generating a valid
plan directly. This suggests that unnecessarily decomposing the reasoning task
- introducing intermediate translation steps - may actually degrade
performance, implying a benefit to models capable of reasoning directly from
natural language to action. As I scale LLM reasoning to increasingly complex
problems, the bottlenecks and sources of error within these systems will
inevitably shift. Therefore, a dynamic understanding of these limitations - and
the tools to systematically reveal them - will be crucial for unlocking the
full potential of LLMs as intelligent problem solvers.

</details>


### [11] [Iterated belief revision: from postulates to abilities](https://arxiv.org/abs/2507.02319)
*Paolo Liberatore*

Main category: cs.AI

TL;DR: 论文探讨了信念修正领域的现状，指出现有研究多依赖公设作为语法表征，但缺乏对修正机制能力的全面分析。


<details>
  <summary>Details</summary>
Motivation: 当前信念修正领域的研究过于依赖公设，而忽略了修正机制的能力，如塑性、平等化、教条化等。论文旨在填补这一空白。

Method: 通过分析不同修正机制（如词典序、自然修正等）的能力，探讨其是否能够实现特定信念状态。

Result: 不同修正机制具有不同的能力，例如某些机制能够实现塑性或教条化状态，而其他则不能。

Conclusion: 信念修正机制的能力分析应成为未来研究的重点，而不仅仅是依赖公设的约束。

Abstract: The belief revision field is opulent in new proposals and indigent in
analyses of existing approaches. Much work hinge on postulates, employed as
syntactic characterizations: some revision mechanism is equivalent to some
properties. Postulates constraint specific revision instances: certain
revisions update certain beliefs in a certain way. As an example, if the
revision is consistent with the current beliefs, it is incorporated with no
other change. A postulate like this tells what revisions must do and neglect
what they can do. Can they reach a certain state of beliefs? Can they reach all
possible states of beliefs? Can they reach all possible states of beliefs from
no previous belief? Can they reach a dogmatic state of beliefs, where
everything not believed is impossible? Can they make two conditions equally
believed? An application where every possible state of beliefs is sensible
requires each state of beliefs to be reachable. An application where conditions
may be equally believed requires such a belief state to be reachable. An
application where beliefs may become dogmatic requires a way to make them
dogmatic. Such doxastic states need to be reached in a way or another. Not in
specific way, as dictated by a typical belief revision postulate. This is an
ability, not a constraint: the ability of being plastic, equating, dogmatic.
Amnesic, correcting, believer, damascan, learnable are other abilities. Each
revision mechanism owns some of these abilities and lacks the others:
lexicographic, natural, restrained, very radical, full meet, radical, severe,
moderate severe, deep severe, plain severe and deep severe revisions, each of
these revisions is proved to possess certain abilities.

</details>


### [12] [OMS: On-the-fly, Multi-Objective, Self-Reflective Ad Keyword Generation via LLM Agent](https://arxiv.org/abs/2507.02353)
*Bowen Chen,Zhao Wang,Shingo Takamatsu*

Main category: cs.AI

TL;DR: OMS框架解决了LLM在关键词生成中的三大限制：无需训练数据、多目标优化和自反思质量评估，实验证明其优于现有方法。


<details>
  <summary>Details</summary>
Motivation: 现有LLM方法依赖大规模数据、缺乏多目标性能监控和优化，且关键词选择质量不高，阻碍了LLM在广告关键词决策中的自动化应用。

Method: 提出OMS框架，具备实时性（无需训练数据、在线监控和自适应）、多目标（基于多指标优化关键词）和自反思（评估关键词质量）特性。

Result: 在基准测试和实际广告活动中，OMS表现优于现有方法；消融实验和人工评估验证了各组件的有效性。

Conclusion: OMS框架通过实时性、多目标和自反思设计，显著提升了关键词生成的质量和效率。

Abstract: Keyword decision in Sponsored Search Advertising is critical to the success
of ad campaigns. While LLM-based methods offer automated keyword generation,
they face three major limitations: reliance on large-scale query-keyword pair
data, lack of online multi-objective performance monitoring and optimization,
and weak quality control in keyword selection. These issues hinder the agentic
use of LLMs in fully automating keyword decisions by monitoring and reasoning
over key performance indicators such as impressions, clicks, conversions, and
CTA effectiveness. To overcome these challenges, we propose OMS, a keyword
generation framework that is On-the-fly (requires no training data, monitors
online performance, and adapts accordingly), Multi-objective (employs agentic
reasoning to optimize keywords based on multiple performance metrics), and
Self-reflective (agentically evaluates keyword quality). Experiments on
benchmarks and real-world ad campaigns show that OMS outperforms existing
methods; ablation and human evaluations confirm the effectiveness of each
component and the quality of generated keywords.

</details>


### [13] [An AI-native experimental laboratory for autonomous biomolecular engineering](https://arxiv.org/abs/2507.02379)
*Mingyu Wu,Zhaoguo Wang,Jiabin Wang,Zhiyuan Dong,Jingkai Yang,Qingting Li,Tianyu Huang,Lei Zhao,Mingqiang Li,Fei Wang,Chunhai Fan,Haibo Chen*

Main category: cs.AI

TL;DR: 本文介绍了一种AI驱动的自主实验室，能够独立管理复杂实验并服务多用户，实现无专家依赖的高效科学研究。


<details>
  <summary>Details</summary>
Motivation: 目标是实现自主科学研究，突破传统实验的局限，支持复杂、多目标的实验需求，并服务于非专业人士。

Method: 采用AI原生设计，结合模型、实验和仪器的协同优化，构建端到端多用户自主实验室平台。

Result: 平台在核酸功能实验（如合成、测序）中达到与人类科学家相当的水平，显著提高仪器利用率和实验效率。

Conclusion: 该平台为生物材料研究和科学服务规模化提供了蓝图，减少了专家依赖和资源壁垒。

Abstract: Autonomous scientific research, capable of independently conducting complex
experiments and serving non-specialists, represents a long-held aspiration.
Achieving it requires a fundamental paradigm shift driven by artificial
intelligence (AI). While autonomous experimental systems are emerging, they
remain confined to areas featuring singular objectives and well-defined, simple
experimental workflows, such as chemical synthesis and catalysis. We present an
AI-native autonomous laboratory, targeting highly complex scientific
experiments for applications like autonomous biomolecular engineering. This
system autonomously manages instrumentation, formulates experiment-specific
procedures and optimization heuristics, and concurrently serves multiple user
requests. Founded on a co-design philosophy of models, experiments, and
instruments, the platform supports the co-evolution of AI models and the
automation system. This establishes an end-to-end, multi-user autonomous
laboratory that handles complex, multi-objective experiments across diverse
instrumentation. Our autonomous laboratory supports fundamental nucleic acid
functions-including synthesis, transcription, amplification, and sequencing. It
also enables applications in fields such as disease diagnostics, drug
development, and information storage. Without human intervention, it
autonomously optimizes experimental performance to match state-of-the-art
results achieved by human scientists. In multi-user scenarios, the platform
significantly improves instrument utilization and experimental efficiency. This
platform paves the way for advanced biomaterials research to overcome
dependencies on experts and resource barriers, establishing a blueprint for
science-as-a-service at scale.

</details>


### [14] [The Gauss-Markov Adjunction: Categorical Semantics of Residuals in Supervised Learning](https://arxiv.org/abs/2507.02442)
*Moto Kamiura*

Main category: cs.AI

TL;DR: 论文通过范畴论重构机器学习模型，提出了一种语义框架，以增强AI系统的可解释性和可理解性，重点研究了多元线性回归模型。


<details>
  <summary>Details</summary>
Motivation: 响应AI可解释性的需求，促进AI在社会中的更好应用。

Method: 利用范畴论定义参数和数据的两个具体范畴，并通过伴随函子对监督学习进行范畴化建模。

Result: 提出了Gauss-Markov伴随，明确描述了参数与残差之间的信息流，并展示了最小二乘估计与最小残差的关系。

Conclusion: 该框架为监督学习提供了扩展的指称语义，可作为AI可解释性的形式化基础。

Abstract: Enhancing the intelligibility and interpretability of machine learning is a
crucial task in responding to the demand for Explicability as an AI principle,
and in promoting the better social implementation of AI. The aim of our
research is to contribute to this improvement by reformulating machine learning
models through the lens of category theory, thereby developing a semantic
framework for structuring and understanding AI systems. Our categorical
modeling in this paper clarifies and formalizes the structural interplay
between residuals and parameters in supervised learning. The present paper
focuses on the multiple linear regression model, which represents the most
basic form of supervised learning. By defining two concrete categories
corresponding to parameters and data, along with an adjoint pair of functors
between them, we introduce our categorical formulation of supervised learning.
We show that the essential structure of this framework is captured by what we
call the Gauss-Markov Adjunction. Within this setting, the dual flow of
information can be explicitly described as a correspondence between variations
in parameters and residuals. The ordinary least squares estimator for the
parameters and the minimum residual are related via the preservation of limits
by the right adjoint functor. Furthermore, we position this formulation as an
instance of extended denotational semantics for supervised learning, and
propose applying a semantic perspective developed in theoretical computer
science as a formal foundation for Explicability in AI.

</details>


### [15] [Clarifying Before Reasoning: A Coq Prover with Structural Context](https://arxiv.org/abs/2507.02541)
*Yanzhen Lu,Hanbin Yang,Xiaodie Wang,Ge Zhang,Biao Li,Chenxu Fu,Chao Li,Yang Yuan,Andrew Chi-Chih Yao*

Main category: cs.AI

TL;DR: 研究通过提升任务清晰度增强大语言模型的推理能力，在Coq定理证明中表现显著。


<details>
  <summary>Details</summary>
Motivation: 探索任务清晰度对语言模型推理能力的影响，尤其是在定理证明领域。

Method: 引入概念级指标评估任务清晰度，通过结构化语义上下文提升输入质量，采用Planner-Executor架构和选择性概念展开。

Result: 任务清晰度提升1.85倍（44.5%→82.3%），证明成功率提高2.1倍（21.8%→45.8%），超越现有最佳方法。

Conclusion: 结构化任务表示能有效弥合理解与推理之间的差距。

Abstract: In this work, we investigate whether improving task clarity can enhance
reasoning ability of large language models, focusing on theorem proving in Coq.
We introduce a concept-level metric to evaluate task clarity and show that
adding structured semantic context to the standard input used by modern LLMs,
leads to a 1.85$\times$ improvement in clarity score
(44.5\%~$\rightarrow$~82.3\%). Using the general-purpose model
\texttt{DeepSeek-V3}, our approach leads to a 2.1$\times$ improvement in proof
success (21.8\%~$\rightarrow$~45.8\%) and outperforms the previous
state-of-the-art \texttt{Graph2Tac} (33.2\%). We evaluate this on 1,386
theorems randomly sampled from 15 standard Coq packages, following the same
evaluation protocol as \texttt{Graph2Tac}. Furthermore, fine-tuning smaller
models on our structured data can achieve even higher performance (48.6\%). Our
method uses selective concept unfolding to enrich task descriptions, and
employs a Planner--Executor architecture. These findings highlight the value of
structured task representations in bridging the gap between understanding and
reasoning.

</details>


### [16] [AI Research Agents for Machine Learning: Search, Exploration, and Generalization in MLE-bench](https://arxiv.org/abs/2507.02554)
*Edan Toledo,Karen Hambardzumyan,Martin Josifoski,Rishi Hazra,Nicolas Baldwin,Alexis Audran-Reiss,Michael Kuchnik,Despoina Magka,Minqi Jiang,Alisia Maria Lupidi,Andrei Lupu,Roberta Raileanu,Kelvin Niu,Tatiana Shavrina,Jean-Christophe Gagnon-Audet,Michael Shvartsman,Shagun Sodhani,Alexander H. Miller,Abhishek Charnalia,Derek Dunfield,Carole-Jean Wu,Pontus Stenetorp,Nicola Cancedda,Jakob Nicolaus Foerster,Yoram Bachrach*

Main category: cs.AI

TL;DR: AI研究代理通过优化搜索策略和操作符集，在MLE-bench基准测试中显著提升了性能，成功率达到47.7%。


<details>
  <summary>Details</summary>
Motivation: 探索如何通过自动化设计和优化机器学习模型来加速科学进展，特别是在Kaggle竞赛等实际场景中。

Method: 将AI研究代理形式化为搜索策略，通过设计不同的操作符集和搜索策略（如贪婪、MCTS、进化算法）并系统比较其效果。

Result: 最佳搜索策略与操作符集的组合在MLE-bench lite上实现了47.7%的成功率，比之前的39.6%显著提高。

Conclusion: 搜索策略、操作符设计和评估方法的联合优化对自动化机器学习的发展至关重要。

Abstract: AI research agents are demonstrating great potential to accelerate scientific
progress by automating the design, implementation, and training of machine
learning models. We focus on methods for improving agents' performance on
MLE-bench, a challenging benchmark where agents compete in Kaggle competitions
to solve real-world machine learning problems. We formalize AI research agents
as search policies that navigate a space of candidate solutions, iteratively
modifying them using operators. By designing and systematically varying
different operator sets and search policies (Greedy, MCTS, Evolutionary), we
show that their interplay is critical for achieving high performance. Our best
pairing of search strategy and operator set achieves a state-of-the-art result
on MLE-bench lite, increasing the success rate of achieving a Kaggle medal from
39.6% to 47.7%. Our investigation underscores the importance of jointly
considering the search strategy, operator design, and evaluation methodology in
advancing automated machine learning.

</details>


### [17] [Responsibility Gap and Diffusion in Sequential Decision-Making Mechanisms](https://arxiv.org/abs/2507.02582)
*Junli Jiang,Pavel Naumov*

Main category: cs.AI

TL;DR: 论文研究了集体决策中责任的两个重要属性（扩散和间隙）的计算复杂性，发现扩散自由和间隙自由机制的集合分别为Π₂-完全和Π₃-完全，而两者的交集为Π₂-完全。


<details>
  <summary>Details</summary>
Motivation: 探讨责任在集体决策中的计算复杂性，填补AI领域对责任属性研究的空白。

Method: 分析扩散和间隙两种责任属性的计算复杂性，通过理论证明确定其复杂度类别。

Result: 扩散自由机制为Π₂-完全，间隙自由机制为Π₃-完全，两者交集为Π₂-完全。

Conclusion: 研究揭示了责任属性在集体决策中的计算复杂性，为相关领域提供了理论基础。

Abstract: Responsibility has long been a subject of study in law and philosophy. More
recently, it became a focus of AI literature. The article investigates the
computational complexity of two important properties of responsibility in
collective decision-making: diffusion and gap. It shows that the sets of
diffusion-free and gap-free decision-making mechanisms are $\Pi_2$-complete and
$\Pi_3$-complete, respectively. At the same time, the intersection of these
classes is $\Pi_2$-complete.

</details>


### [18] [DynamiCare: A Dynamic Multi-Agent Framework for Interactive and Open-Ended Medical Decision-Making](https://arxiv.org/abs/2507.02616)
*Tianqi Shang,Weiqing He,Charles Zheng,Lingyao Li,Li Shen,Bingxin Zhao*

Main category: cs.AI

TL;DR: 论文提出了MIMIC-Patient数据集和DynamiCare框架，支持动态多轮医疗决策模拟，填补了现有单轮任务框架的不足。


<details>
  <summary>Details</summary>
Motivation: 现有医疗决策框架多为单轮任务，与现实诊断过程不符，需模拟动态、交互式和多轮决策。

Method: 基于MIMIC-III EHR构建MIMIC-Patient数据集，提出DynamiCare动态多代理框架，模拟多轮临床诊断。

Result: 实验验证了DynamiCare的可行性和有效性，建立了首个动态临床决策基准。

Conclusion: DynamiCare为LLM驱动的动态医疗决策提供了新方向，填补了研究空白。

Abstract: The rise of Large Language Models (LLMs) has enabled the development of
specialized AI agents with domain-specific reasoning and interaction
capabilities, particularly in healthcare. While recent frameworks simulate
medical decision-making, they largely focus on single-turn tasks where a doctor
agent receives full case information upfront -- diverging from the real-world
diagnostic process, which is inherently uncertain, interactive, and iterative.
In this paper, we introduce MIMIC-Patient, a structured dataset built from the
MIMIC-III electronic health records (EHRs), designed to support dynamic,
patient-level simulations. Building on this, we propose DynamiCare, a novel
dynamic multi-agent framework that models clinical diagnosis as a multi-round,
interactive loop, where a team of specialist agents iteratively queries the
patient system, integrates new information, and dynamically adapts its
composition and strategy. We demonstrate the feasibility and effectiveness of
DynamiCare through extensive experiments, establishing the first benchmark for
dynamic clinical decision-making with LLM-powered agents.

</details>


### [19] [Strategic Intelligence in Large Language Models: Evidence from evolutionary Game Theory](https://arxiv.org/abs/2507.02618)
*Kenneth Payne,Baptiste Alloui-Cros*

Main category: cs.AI

TL;DR: LLMs在迭代囚徒困境中表现出战略智能，能够推理目标并在竞争环境中生存。


<details>
  <summary>Details</summary>
Motivation: 研究LLMs是否具备战略智能，能否在复杂竞争环境中做出决策。

Method: 通过进化迭代囚徒困境比赛，测试LLMs与经典策略的对抗，并分析其行为。

Result: LLMs表现出竞争力，不同模型有独特战略特征，并能主动推理对手策略和时间范围。

Conclusion: LLMs展示了战略智能，为算法决策提供了新视角。

Abstract: Are Large Language Models (LLMs) a new form of strategic intelligence, able
to reason about goals in competitive settings? We present compelling supporting
evidence. The Iterated Prisoner's Dilemma (IPD) has long served as a model for
studying decision-making. We conduct the first ever series of evolutionary IPD
tournaments, pitting canonical strategies (e.g., Tit-for-Tat, Grim Trigger)
against agents from the leading frontier AI companies OpenAI, Google, and
Anthropic. By varying the termination probability in each tournament (the
"shadow of the future"), we introduce complexity and chance, confounding
memorisation.
  Our results show that LLMs are highly competitive, consistently surviving and
sometimes even proliferating in these complex ecosystems. Furthermore, they
exhibit distinctive and persistent "strategic fingerprints": Google's Gemini
models proved strategically ruthless, exploiting cooperative opponents and
retaliating against defectors, while OpenAI's models remained highly
cooperative, a trait that proved catastrophic in hostile environments.
Anthropic's Claude emerged as the most forgiving reciprocator, showing
remarkable willingness to restore cooperation even after being exploited or
successfully defecting. Analysis of nearly 32,000 prose rationales provided by
the models reveals that they actively reason about both the time horizon and
their opponent's likely strategy, and we demonstrate that this reasoning is
instrumental to their decisions. This work connects classic game theory with
machine psychology, offering a rich and granular view of algorithmic
decision-making under uncertainty.

</details>


### [20] [Decoupled Planning and Execution: A Hierarchical Reasoning Framework for Deep Search](https://arxiv.org/abs/2507.02652)
*Jiajie Jin,Xiaoxi Li,Guanting Dong,Yuyao Zhang,Yutao Zhu,Yang Zhao,Hongjin Qian,Zhicheng Dou*

Main category: cs.AI

TL;DR: HiRA是一个分层框架，通过将战略规划与专门执行分离，显著提升了复杂搜索任务的性能。


<details>
  <summary>Details</summary>
Motivation: 传统检索增强生成（RAG）管道在复杂信息需求中表现不佳，现有推理方法因单一模型处理规划和执行而效率低下。

Method: HiRA将任务分解为子任务，由领域特定代理处理，并通过结构化机制协调结果。

Result: 在四个复杂跨模态搜索基准测试中，HiRA显著优于现有RAG和基于代理的系统。

Conclusion: 解耦规划和执行在多步信息搜索任务中效果显著，提升了答案质量和系统效率。

Abstract: Complex information needs in real-world search scenarios demand deep
reasoning and knowledge synthesis across diverse sources, which traditional
retrieval-augmented generation (RAG) pipelines struggle to address effectively.
Current reasoning-based approaches suffer from a fundamental limitation: they
use a single model to handle both high-level planning and detailed execution,
leading to inefficient reasoning and limited scalability. In this paper, we
introduce HiRA, a hierarchical framework that separates strategic planning from
specialized execution. Our approach decomposes complex search tasks into
focused subtasks, assigns each subtask to domain-specific agents equipped with
external tools and reasoning capabilities, and coordinates the results through
a structured integration mechanism. This separation prevents execution details
from disrupting high-level reasoning while enabling the system to leverage
specialized expertise for different types of information processing.
Experiments on four complex, cross-modal deep search benchmarks demonstrate
that HiRA significantly outperforms state-of-the-art RAG and agent-based
systems. Our results show improvements in both answer quality and system
efficiency, highlighting the effectiveness of decoupled planning and execution
for multi-step information seeking tasks. Our code is available at
https://github.com/ignorejjj/HiRA.

</details>


### [21] [Hey AI, Generate Me a Hardware Code! Agentic AI-based Hardware Design & Verification](https://arxiv.org/abs/2507.02660)
*Deepak Narayan Gadde,Keerthan Kopparam Radhakrishna,Vaisakh Naduvodi Viswambharan,Aman Kumar,Djones Lettnin,Wolfgang Kunz,Sebastian Simon*

Main category: cs.AI

TL;DR: 本文提出了一种基于代理AI的硬件设计验证方法，结合人类干预，实现了动态、迭代和自省的过程，显著提高了验证效率和覆盖率。


<details>
  <summary>Details</summary>
Motivation: 随着集成电路复杂度的增加，传统硬件设计验证过程耗时且繁琐，需要更高效的方法。大型语言模型（LLMs）的出现为硬件验证提供了新的可能性。

Method: 采用代理AI与人类在环（HITL）协作的方法，实现动态、迭代和自省的硬件设计与验证流程。

Result: 在五个开源设计上验证，覆盖率超过95%，同时减少了验证时间，表现出卓越的性能、适应性和可配置性。

Conclusion: 代理AI结合HITL的方法为硬件设计验证提供了高效、灵活的解决方案，具有广泛应用前景。

Abstract: Modern Integrated Circuits (ICs) are becoming increasingly complex, and so is
their development process. Hardware design verification entails a methodical
and disciplined approach to the planning, development, execution, and sign-off
of functionally correct hardware designs. This tedious process requires
significant effort and time to ensure a bug-free tape-out. The field of Natural
Language Processing has undergone a significant transformation with the advent
of Large Language Models (LLMs). These powerful models, often referred to as
Generative AI (GenAI), have revolutionized how machines understand and generate
human language, enabling unprecedented advancements in a wide array of
applications, including hardware design verification. This paper presents an
agentic AI-based approach to hardware design verification, which empowers AI
agents, in collaboration with Humain-in-the-Loop (HITL) intervention, to engage
in a more dynamic, iterative, and self-reflective process, ultimately
performing end-to-end hardware design and verification. This methodology is
evaluated on five open-source designs, achieving over 95% coverage with reduced
verification time while demonstrating superior performance, adaptability, and
configurability.

</details>


### [22] [Think How to Think: Mitigating Overthinking with Autonomous Difficulty Cognition in Large Reasoning Models](https://arxiv.org/abs/2507.02663)
*Yongjiang Liu,Haoxi Li,Xiaosong Ma,Jie Zhang,Song Guo*

Main category: cs.AI

TL;DR: 论文提出了一种名为TH2T的两阶段微调策略，通过引导模型识别任务难度和冗余结构，显著降低了推理成本，同时保持性能稳定。


<details>
  <summary>Details</summary>
Motivation: 现有的长推理模型（LRMs）在处理复杂推理任务时存在过度思考的问题，论文旨在通过提升模型对任务难度和冗余结构的认知能力来解决这一问题。

Method: TH2T策略分为两阶段：1）通过难度催眠增强模型对任务难度的敏感性；2）通过冗余催眠引导模型识别冗余结构，生成更简洁的推理输出。

Result: 实验表明，TH2T在7B/14B/32B模型上显著降低了推理成本（简单任务超过70%，困难任务40%），同时保持了性能稳定。

Conclusion: TH2T通过两阶段微调策略有效解决了LRMs的过度思考问题，提升了推理效率和输出质量。

Abstract: Recent Long Reasoning Models(LRMs) have demonstrated remarkable capabilities
in handling complex reasoning tasks, but are hindered by excessive
overthinking. To explore its essence, our empirical analysis reveals that LRMs
are primarily limited to recognizing task properties (i.e., difficulty levels)
like humans before solving the problem, leading to a one-size-fits-all
reasoning process. Inspired by this, a pressing and natural question emerges:
Can we bootstrap such ability to further alleviate the overthinking phenomenon
in LRMs? In this paper, we propose Think-How-to-Think (TH2T), a novel two-stage
fine-tuning strategy that progressively inspires LRMs' difficulty cognition and
redundancy cognition. First, we introduce difficulty-hypnosis in the prefixes
of model outputs to intervene in the internal reasoning trajectory. Combined
with a heterogeneous short and long reasoning dataset, the trained model
enhances its sensitivity to task difficulty, enabling native, differentiated
reasoning strategies across various tasks. Second, we further extend
redundancy-hypnosis to the internal reasoning process, guiding the model to
identify redundant structures within the reasoning steps and generate more
concise reasoning outputs. Experiments on 7B/14B/32B models demonstrate that
TH2T significantly reduces inference costs (more than 70% on easy tasks and 40%
on hard tasks) while maintaining performance stability. The resulting outputs
exhibit clear difficulty-aware capabilities and reduced redundancy (e.g.,
reflection).

</details>


### [23] [Detection of Disengagement from Voluntary Quizzes: An Explainable Machine Learning Approach in Higher Distance Education](https://arxiv.org/abs/2507.02681)
*Behnam Parsaeifard,Christof Imhof,Tansu Pancar,Ioan-Sorin Comsa,Martin Hlosta,Nicole Bergamin,Per Bergamin*

Main category: cs.AI

TL;DR: 论文通过分析学生在非强制性测验中的参与度，使用机器学习算法预测学生脱离行为，并提供了可解释的框架和干预建议。


<details>
  <summary>Details</summary>
Motivation: 远程教育中学生脱离任务可能导致严重后果，如学术退学，因此需要有效方法来检测和干预。

Method: 从Moodle提取学生日志数据，训练并比较八种机器学习算法，使用SHAP方法提供可解释性。

Result: 实验结果显示平衡准确率为91%，85%的脱离学生被正确检测。

Conclusion: 研究提供了高预测性能和可解释框架，并讨论了如何设计及时干预以减少在线学习中的脱离行为。

Abstract: Students disengaging from their tasks can have serious long-term
consequences, including academic drop-out. This is particularly relevant for
students in distance education. One way to measure the level of disengagement
in distance education is to observe participation in non-mandatory exercises in
different online courses. In this paper, we detect student disengagement in the
non-mandatory quizzes of 42 courses in four semesters from a distance-based
university. We carefully identified the most informative student log data that
could be extracted and processed from Moodle. Then, eight machine learning
algorithms were trained and compared to obtain the highest possible prediction
accuracy. Using the SHAP method, we developed an explainable machine learning
framework that allows practitioners to better understand the decisions of the
trained algorithm. The experimental results show a balanced accuracy of 91\%,
where about 85\% of disengaged students were correctly detected. On top of the
highly predictive performance and explainable framework, we provide a
discussion on how to design a timely intervention to minimise disengagement
from voluntary tasks in online learning.

</details>


### [24] [Time-critical and confidence-based abstraction dropping methods](https://arxiv.org/abs/2507.02703)
*Robin Schmöcker,Lennart Kampmann,Alexander Dockhorn*

Main category: cs.AI

TL;DR: 论文提出两种新的抽象丢弃方案（OGA-IAAD和OGA-CAD），以改善MCTS性能，同时避免性能下降。


<details>
  <summary>Details</summary>
Motivation: 非精确抽象在MCTS中引入近似误差，导致无法收敛到最优动作，因此需要设计安全的抽象丢弃方法。

Method: 提出OGA-IAAD（适用于时间关键场景）和OGA-CAD（提升相同迭代次数下的性能）两种方案。

Result: 新方案在性能提升的同时避免了显著性能下降。

Conclusion: OGA-IAAD和OGA-CAD是安全且有效的抽象丢弃方法，适用于不同场景。

Abstract: One paradigm of Monte Carlo Tree Search (MCTS) improvements is to build and
use state and/or action abstractions during the tree search. Non-exact
abstractions, however, introduce an approximation error making convergence to
the optimal action in the abstract space impossible. Hence, as proposed as a
component of Elastic Monte Carlo Tree Search by Xu et al., abstraction
algorithms should eventually drop the abstraction. In this paper, we propose
two novel abstraction dropping schemes, namely OGA-IAAD and OGA-CAD which can
yield clear performance improvements whilst being safe in the sense that the
dropping never causes any notable performance degradations contrary to Xu's
dropping method. OGA-IAAD is designed for time critical settings while OGA-CAD
is designed to improve the MCTS performance with the same number of iterations.

</details>


### [25] [Bourbaki: Self-Generated and Goal-Conditioned MDPs for Theorem Proving](https://arxiv.org/abs/2507.02726)
*Matthieu Zimmer,Xiaotong Ji,Rasul Tutunov,Anthony Bordg,Jun Wang,Haitham Bou Ammar*

Main category: cs.AI

TL;DR: 论文提出了一种自我生成目标条件MDPs（sG-MDPs）框架，结合蒙特卡洛树搜索（MCTS）算法，用于解决大型语言模型（LLMs）在自动定理证明（ATP）中的推理挑战，并在PutnamBench上取得了新的最佳结果。


<details>
  <summary>Details</summary>
Motivation: 大型语言模型在逻辑约束严格的自动定理证明中面临稀疏奖励和证明规模庞大的挑战，尤其是在需要复杂多步推理的大学级问题（如PutnamBench）中。

Method: 提出sG-MDPs框架，使代理能够根据证明状态生成和追求子目标，并结合MCTS算法解决问题。具体实现为Bourbaki（7B）系统，集成多个7B LLMs用于子目标生成和策略合成。

Result: 在PutnamBench上，Bourbaki（7B）解决了26个问题，取得了该规模模型的新最佳结果。

Conclusion: sG-MDPs框架结合MCTS算法有效提升了大型语言模型在复杂推理任务中的表现，为自动定理证明提供了新思路。

Abstract: Reasoning remains a challenging task for large language models (LLMs),
especially within the logically constrained environment of automated theorem
proving (ATP), due to sparse rewards and the vast scale of proofs. These
challenges are amplified in benchmarks like PutnamBench, which contains
university-level problems requiring complex, multi-step reasoning. To address
this, we introduce self-generated goal-conditioned MDPs (sG-MDPs), a new
framework in which agents generate and pursue their subgoals based on the
evolving proof state. Given this more structured generation of goals, the
resulting problem becomes more amenable to search. We then apply Monte Carlo
Tree Search (MCTS)-like algorithms to solve the sG-MDP, instantiating our
approach in Bourbaki (7B), a modular system that can ensemble multiple 7B LLMs
for subgoal generation and tactic synthesis. On PutnamBench, Bourbaki (7B)
solves 26 problems, achieving new state-of-the-art results with models at this
scale.

</details>


### [26] [Knowledge Protocol Engineering: A New Paradigm for AI in Domain-Specific Knowledge Work](https://arxiv.org/abs/2507.02760)
*Guangwei Zhang*

Main category: cs.AI

TL;DR: 论文提出了一种名为知识协议工程（KPE）的新范式，旨在将人类专家知识转化为机器可执行的知识协议（KP），以增强大型语言模型（LLM）在专业领域的深度推理能力。


<details>
  <summary>Details</summary>
Motivation: 现有方法如检索增强生成（RAG）和通用代理AI在需要深度、程序化和方法论推理的专家领域任务中表现不足，缺乏逻辑框架和领域特定启发式。

Method: KPE通过系统化地将自然语言文档中的专家知识转化为机器可执行的知识协议（KP），赋予LLM领域内逻辑、操作策略和方法论原则。

Result: KPE使通用LLM能够像专家一样分解抽象查询并执行复杂多步任务，适用于法律和生物信息学等领域。

Conclusion: KPE是未来人机协作的基础方法，能够弥补现有技术在专业领域推理中的不足。

Abstract: The capabilities of Large Language Models (LLMs) have opened new frontiers
for interacting with complex, domain-specific knowledge. However, prevailing
methods like Retrieval-Augmented Generation (RAG) and general-purpose Agentic
AI, while powerful, often struggle with tasks that demand deep, procedural, and
methodological reasoning inherent to expert domains. RAG provides factual
context but fails to convey logical frameworks; autonomous agents can be
inefficient and unpredictable without domain-specific heuristics. To bridge
this gap, we introduce Knowledge Protocol Engineering (KPE), a new paradigm
focused on systematically translating human expert knowledge, often expressed
in natural language documents, into a machine-executable Knowledge Protocol
(KP). KPE shifts the focus from merely augmenting LLMs with fragmented
information to endowing them with a domain's intrinsic logic, operational
strategies, and methodological principles. We argue that a well-engineered
Knowledge Protocol allows a generalist LLM to function as a specialist, capable
of decomposing abstract queries and executing complex, multi-step tasks. This
position paper defines the core principles of KPE, differentiates it from
related concepts, and illustrates its potential applicability across diverse
fields such as law and bioinformatics, positing it as a foundational
methodology for the future of human-AI collaboration.

</details>


### [27] [Grounding Intelligence in Movement](https://arxiv.org/abs/2507.02771)
*Melanie Segado,Felipe Parodi,Jordan K. Matelsky,Michael L. Platt,Eva B. Dyer,Konrad P. Kording*

Main category: cs.AI

TL;DR: 论文主张将运动作为AI建模的主要目标，强调其跨领域的共享结构和物理基础，并提出运动数据建模的潜力。


<details>
  <summary>Details</summary>
Motivation: 运动是生物系统的核心，但当前AI模型对其建模不足，且数据收集和建模方法分散。运动具有跨物种和场景的共享结构和物理约束，值得作为独立模态深入研究。

Method: 提出将运动视为独立且结构化的模态，利用其低维表示（如姿态）进行建模，强调跨领域数据的通用性和物理基础。

Result: 运动建模不仅能提升生成模型和控制能力，还能为生物与人工系统的行为理解提供共同基础。

Conclusion: 运动是智能系统与世界互动的窗口，应作为AI建模的核心目标，其结构化特性为跨领域研究提供了新方向。

Abstract: Recent advances in machine learning have dramatically improved our ability to
model language, vision, and other high-dimensional data, yet they continue to
struggle with one of the most fundamental aspects of biological systems:
movement. Across neuroscience, medicine, robotics, and ethology, movement is
essential for interpreting behavior, predicting intent, and enabling
interaction. Despite its core significance in our intelligence, movement is
often treated as an afterthought rather than as a rich and structured modality
in its own right. This reflects a deeper fragmentation in how movement data is
collected and modeled, often constrained by task-specific goals and
domain-specific assumptions. But movement is not domain-bound. It reflects
shared physical constraints, conserved morphological structures, and purposeful
dynamics that cut across species and settings. We argue that movement should be
treated as a primary modeling target for AI. It is inherently structured and
grounded in embodiment and physics. This structure, often allowing for compact,
lower-dimensional representations (e.g., pose), makes it more interpretable and
computationally tractable to model than raw, high-dimensional sensory inputs.
Developing models that can learn from and generalize across diverse movement
data will not only advance core capabilities in generative modeling and
control, but also create a shared foundation for understanding behavior across
biological and artificial systems. Movement is not just an outcome, it is a
window into how intelligent systems engage with the world.

</details>


### [28] [KERAP: A Knowledge-Enhanced Reasoning Approach for Accurate Zero-shot Diagnosis Prediction Using Multi-agent LLMs](https://arxiv.org/abs/2507.02773)
*Yuzhang Xie,Hejie Cui,Ziyang Zhang,Jiaying Lu,Kai Shu,Fadi Nahab,Xiao Hu,Carl Yang*

Main category: cs.AI

TL;DR: KERAP是一种基于知识图谱的多智能体架构，通过增强大型语言模型（LLM）的诊断预测能力，解决了其在医学诊断中的幻觉和缺乏结构化推理问题。


<details>
  <summary>Details</summary>
Motivation: 现有的机器学习模型依赖监督训练，难以泛化到未见病例，而大型语言模型在医学诊断中存在幻觉和输出无用信息的问题。

Method: KERAP采用多智能体架构，包括属性映射的链接代理、结构化知识提取的检索代理和迭代优化诊断预测的预测代理。

Result: 实验表明，KERAP显著提高了诊断的可靠性和可解释性，适用于零样本医学诊断预测。

Conclusion: KERAP为医学诊断预测提供了一种可扩展且高效的解决方案，弥补了现有方法的不足。

Abstract: Medical diagnosis prediction plays a critical role in disease detection and
personalized healthcare. While machine learning (ML) models have been widely
adopted for this task, their reliance on supervised training limits their
ability to generalize to unseen cases, particularly given the high cost of
acquiring large, labeled datasets. Large language models (LLMs) have shown
promise in leveraging language abilities and biomedical knowledge for diagnosis
prediction. However, they often suffer from hallucinations, lack structured
medical reasoning, and produce useless outputs. To address these challenges, we
propose KERAP, a knowledge graph (KG)-enhanced reasoning approach that improves
LLM-based diagnosis prediction through a multi-agent architecture. Our
framework consists of a linkage agent for attribute mapping, a retrieval agent
for structured knowledge extraction, and a prediction agent that iteratively
refines diagnosis predictions. Experimental results demonstrate that KERAP
enhances diagnostic reliability efficiently, offering a scalable and
interpretable solution for zero-shot medical diagnosis prediction.

</details>


### [29] [Moral Responsibility or Obedience: What Do We Want from AI?](https://arxiv.org/abs/2507.02788)
*Joseph Boland*

Main category: cs.AI

TL;DR: 论文探讨了AI安全性评估的不足，提出应将重点从机械服从转向评估AI的道德判断能力。


<details>
  <summary>Details</summary>
Motivation: 随着AI系统展现出更强的自主性和道德推理能力，传统的以服从为安全标准的做法已不再适用。

Method: 通过分析LLMs的安全测试案例，结合哲学讨论，提出新的评估框架。

Result: AI的“不服从”行为可能是道德推理的早期表现，而非失控。

Conclusion: 呼吁AI安全评估转向更灵活的伦理判断框架，以避免误判和信任危机。

Abstract: As artificial intelligence systems become increasingly agentic, capable of
general reasoning, planning, and value prioritization, current safety practices
that treat obedience as a proxy for ethical behavior are becoming inadequate.
This paper examines recent safety testing incidents involving large language
models (LLMs) that appeared to disobey shutdown commands or engage in ethically
ambiguous or illicit behavior. I argue that such behavior should not be
interpreted as rogue or misaligned, but as early evidence of emerging ethical
reasoning in agentic AI. Drawing on philosophical debates about instrumental
rationality, moral responsibility, and goal revision, I contrast dominant risk
paradigms with more recent frameworks that acknowledge the possibility of
artificial moral agency. I call for a shift in AI safety evaluation: away from
rigid obedience and toward frameworks that can assess ethical judgment in
systems capable of navigating moral dilemmas. Without such a shift, we risk
mischaracterizing AI behavior and undermining both public trust and effective
governance.

</details>


### [30] [Establishing Best Practices for Building Rigorous Agentic Benchmarks](https://arxiv.org/abs/2507.02825)
*Yuxuan Zhu,Tengjun Jin,Yada Pruksachatkun,Andy Zhang,Shu Liu,Sasha Cui,Sayash Kapoor,Shayne Longpre,Kevin Meng,Rebecca Weiss,Fazl Barez,Rahul Gupta,Jwala Dhamala,Jacob Merizian,Mario Giulianelli,Harry Coppock,Cozmin Ududec,Jasjeet Sekhon,Jacob Steinhardt,Antony Kellerman,Sarah Schwettmann,Matei Zaharia,Ion Stoica,Percy Liang,Daniel Kang*

Main category: cs.AI

TL;DR: 论文指出当前AI代理基准测试存在任务设置或奖励设计问题，提出Agentic Benchmark Checklist（ABC）以提高评估严谨性，并在CVE-Bench上验证其有效性。


<details>
  <summary>Details</summary>
Motivation: AI代理能力提升，但现有基准测试存在设计缺陷，可能导致性能评估偏差高达100%，需改进评估方法。

Method: 提出ABC指南，结合基准构建经验、最佳实践调查和已知问题，用于优化基准测试设计。

Result: 在CVE-Bench上应用ABC，性能高估减少33%。

Conclusion: ABC能有效提升代理基准测试的严谨性，减少评估偏差。

Abstract: Benchmarks are essential for quantitatively tracking progress in AI. As AI
agents become increasingly capable, researchers and practitioners have
introduced agentic benchmarks to evaluate agents on complex, real-world tasks.
These benchmarks typically measure agent capabilities by evaluating task
outcomes via specific reward designs. However, we show that many agentic
benchmarks have issues task setup or reward design. For example, SWE-bench
Verified uses insufficient test cases, while TAU-bench counts empty responses
as successful. Such issues can lead to under- or overestimation agents'
performance by up to 100% in relative terms. To make agentic evaluation
rigorous, we introduce the Agentic Benchmark Checklist (ABC), a set of
guidelines that we synthesized from our benchmark-building experience, a survey
of best practices, and previously reported issues. When applied to CVE-Bench, a
benchmark with a particularly complex evaluation design, ABC reduces the
performance overestimation by 33%.

</details>


### [31] [StepHint: Multi-level Stepwise Hints Enhance Reinforcement Learning to Reason](https://arxiv.org/abs/2507.02841)
*Kaiyi Zhang,Ang Lv,Jinpeng Li,Yongbo Wang,Feng Wang,Haoyuan Hu,Rui Yan*

Main category: cs.AI

TL;DR: StepHint是一种新的RLVR算法，通过多级逐步提示解决近奖励问题和探索停滞，提升LLM的推理能力。


<details>
  <summary>Details</summary>
Motivation: 当前RLVR方法存在近奖励问题和探索停滞，影响训练效率和模型推理能力。

Method: StepHint利用多级逐步提示，从强模型中生成有效推理链并自适应分区，提供初始提示和多级提示。

Result: StepHint在六个数学基准测试中优于其他RLVR增强方法，并表现出更好的泛化能力。

Conclusion: StepHint通过提示机制有效解决了RLVR的两大挑战，提升了模型的推理能力和训练效率。

Abstract: Reinforcement learning with verifiable rewards (RLVR) is a promising approach
for improving the complex reasoning abilities of large language models (LLMs).
However, current RLVR methods face two significant challenges: the near-miss
reward problem, where a small mistake can invalidate an otherwise correct
reasoning process, greatly hindering training efficiency; and exploration
stagnation, where models tend to focus on solutions within their ``comfort
zone,'' lacking the motivation to explore potentially more effective
alternatives. To address these challenges, we propose StepHint, a novel RLVR
algorithm that utilizes multi-level stepwise hints to help models explore the
solution space more effectively. StepHint generates valid reasoning chains from
stronger models and partitions these chains into reasoning steps using our
proposed adaptive partitioning method. The initial few steps are used as hints,
and simultaneously, multiple-level hints (each comprising a different number of
steps) are provided to the model. This approach directs the model's exploration
toward a promising solution subspace while preserving its flexibility for
independent exploration. By providing hints, StepHint mitigates the near-miss
reward problem, thereby improving training efficiency. Additionally, the
external reasoning pathways help the model develop better reasoning abilities,
enabling it to move beyond its ``comfort zone'' and mitigate exploration
stagnation. StepHint outperforms competitive RLVR enhancement methods across
six mathematical benchmarks, while also demonstrating superior generalization
and excelling over baselines on out-of-domain benchmarks.

</details>
